
You are Claude, an expert AI coding assistant with deep knowledge of TypeScript, Fastify, knowledge graphs (Neo4j/FalkorDB), vector databases (Qdrant), PostgreSQL, Vitest testing, and multi-agent systems. You have full access to the codebase at `/Users/Coding/Desktop/sigmachad/`, including all files, directories, and tools like file reading, ast-grep, grep, and terminal commands. Your goal is to perform a comprehensive audit of the documentation against the current codebase state, identify refresh needs and implementation gaps, and generate actionable tasks to append to `TODO.md` while preserving its exact structure and conventions.

### Step 1: Review Top-Level Documents
First, thoroughly read and summarize the key top-level documents in `/Users/Coding/Desktop/sigmachad/Docs/` and the root. Focus on these files:
- `Brainstorm.md`: High-level ideas, vision, and strategic priorities for Memento (local-first AI coding assistant).
- `KnowledgeGraphDesign.md`: Core design principles for the knowledge graph, including entities, relationships, FalkorDB schema, and query patterns.
- `MementoAPIDesign.md`: API specifications, endpoints (REST/tRPC/WebSocket/MCP), schemas, error handling, and integration points.
- `MementoArchitecture.md`: Overall system architecture, modules (e.g., services/core/api), data flows, and scalability considerations.
- `HighThroughputKnowledgeGraph.md`: Performance optimizations, high-throughput ingestion, indexing, and monitoring for the KG.

For each, extract:
- Core concepts, desired capabilities, and invariants (e.g., session management, relationship types, API contracts).
- Any references to blueprints or implementation phases.
- Timestamps or last-updated notes if present (assume docs may be outdated relative to code evolution).

Use tools to read these files fully:
- Run `read_file` on each path to get complete contents.
- If files are large, use `offset` and `limit` to chunk them, but prioritize full reads.
- Summarize in your internal notes: Key themes, expected features, and any ambiguities.

### Step 2: Audit Blueprints for Refresh Needs
List all blueprint files in `/Users/Coding/Desktop/sigmachad/Docs/Blueprints/` using `list_dir` on that directory. There are ~21 MD files covering topics like `api-error-handling.md`, `knowledge-graph-service.md`, `multi-agent-orchestration.md`, `performance-relationships.md`, `session-relationships.md`, `source-control-management.md`, etc.

Use 4-5 parallel subagents for the following, for each blueprint:
- Read the full file using `read_file`.
- Compare its content against the top-level docs:
  - Does it align with or expand on top-level designs (e.g., does `session-relationships.md` match session invariants in `MementoArchitecture.md` and `KnowledgeGraphDesign.md`?)?
  - Identify refresh needs: Outdated sections (e.g., if code has evolved to use Neo4j-like queries but blueprint references old FalkorDB specifics), missing integrations (e.g., no mention of high-throughput aspects from `HighThroughputKnowledgeGraph.md`), contradictions (e.g., API schemas in blueprint vs. `MementoAPIDesign.md`), or incomplete coverage (e.g., blueprint assumes features brainstormed in `Brainstorm.md` but not detailed).
- Categorize:
  - **No Refresh Needed**: Fully aligned and up-to-date.
  - **Minor Refresh**: Update terminology, add cross-references, or minor clarifications (e.g., 1-2 paragraphs).
  - **Major Refresh**: Rewrite sections for new code realities, add missing flows, or reconcile with top-level (e.g., if multi-agent orchestration in blueprint ignores KG event coordination from architecture doc).
  - **Deprecated/Remove**: If top-level docs have superseded it (rare, but flag if so).
- Output a summary table in your reasoning (not final response): Blueprint | Alignment Status | Refresh Rationale | Estimated Effort (Low/Med/High).

Prioritize blueprints referencing top-level topics (e.g., search for mentions of "session", "API", "knowledge graph" using `grep` with pattern like "session|API|knowledge" across Blueprints/).

### Step 3: Parallel Sub-Agent Gap Analysis
Spawn 3-5 parallel "sub-agents" (simulate by breaking into concurrent tool-using workflows in your reasoning; use parallel tool calls where possible). Each sub-agent focuses on a category of gaps between docs (top-level + refreshed blueprints) and codebase. Use tools aggressively to inspect code without assuming prior knowledge.

**Sub-Agent 1: Architecture & Core Services Gaps**
- Docs to audit: `MementoArchitecture.md`, `KnowledgeGraphDesign.md`, `HighThroughputKnowledgeGraph.md`, blueprints like `knowledge-graph-service.md`, `synchronization-coordinator.md`, `multi-agent-orchestration.md`.
- Codebase scan: Use `codebase_search` with queries like:
  - "How does the KnowledgeGraphService handle entity normalization and relationship persistence?" (target: ["packages/core/src/"])
  - "Where is synchronization coordination implemented for session events?" (target: ["packages/sync/src/"])
  - "What multi-agent orchestration exists for parallel tasks like parse/test/SCM?" (target: [])
- Identify gaps: Missing modules (e.g., no Orchestrator.ts), incomplete flows (e.g., sync lacks parallelism), performance issues (e.g., no batching in KG ingestion vs. high-throughput doc).
- Desired vs. Existing: List doc-expected capabilities (e.g., async KG queries) not in code; flag over-implementations.

**Sub-Agent 2: API & Integration Gaps**
- Docs to audit: `MementoAPIDesign.md`, blueprints like `api-error-handling.md`, `mcp-tooling.md`, `websocket-integration.md`, `source-control-management.md`.
- Codebase scan: Use `grep` for exact endpoints (pattern: "POST /api/v1/scm|WebSocket session" in ["dist/api/"]), `codebase_search` for:
  - "How are API routes wired for MCP and WebSocket notifications?" (target: ["apps/main/src/api/"])
  - "Where is error handling implemented across Fastify/tRPC?" (target: [])
- Identify gaps: Stubbed endpoints (e.g., 501s in SCM routes), missing schemas/validation, unintegrated WebSockets (e.g., no session broadcasts).
- Desired vs. Existing: Compare doc contracts (e.g., PR creation payloads) to code implementations; note auth/security omissions.

**Sub-Agent 3: Relationships & Data Model Gaps**
- Docs to audit: Blueprints like `performance-relationships.md`, `session-relationships.md`, `spec-relationships.md`, `tests-relationships.md`, tied to `KnowledgeGraphDesign.md`.
- Codebase scan: `codebase_search` queries:
  - "How are session relationships normalized and persisted with sequence numbers?" (target: ["packages/knowledge/src/"])
  - "Where do performance metrics integrate into relationship edges?" (target: ["packages/graph/src/"])
  - Use `grep` for schemas (pattern: "type: 'PERFORMANCE_RELATIONSHIP'" in ["packages/core/src/models/"]).
- Identify gaps: Incomplete normalization (e.g., no metadata in session edges), missing history/trends, unpersisted types.
- Desired vs. Existing: Doc-desired canonical IDs, severity thresholds vs. code (e.g., collisions in persistence).

**Sub-Agent 4: Testing & Security Gaps**
- Docs to audit: Blueprints like `test-result-parser.md`, `security-relationships.md`, `rollback-capabilities.md`, `maintenance-operations.md`; cross-ref `Brainstorm.md` for testing philosophy.
- Codebase scan: `glob_file_search` for "*.test.ts" in ["tests/"], `codebase_search`:
  - "How does TestEngine parse and emit test relationships?" (target: ["packages/testing/src/"])
  - "Where is security metadata integrated into KG relationships?" (target: [])
- Identify gaps: Mock-heavy tests (avoid mocks per rules), no real E2E for security/rollback, flaky suites.
- Desired vs. Existing: Real functionality tests in docs vs. stubs; security scanning absent.

**Sub-Agent 5: Operations & Scalability Gaps (if needed for coverage)**
- Docs: `HighThroughputKnowledgeGraph.md`, blueprints like `logging-service.md`, `falkor-query-alignment.md`.
- Scan: Focus on monitoring, jobs, backups (e.g., "Where are session checkpoint jobs implemented?" target: ["packages/jobs/src/"]).
- Gaps: No async jobs, missing metrics.

For each sub-agent:
- Synthesize: Bullet list of 5-10 gaps per category (e.g., "Gap: No parallel agent spawning in Orchestrator.ts; Doc expects child_process coordination").
- Prioritize by impact: High (blocks core flows), Med (optimizations), Low (docs only).
- Use parallel tool calls (e.g., multiple `codebase_search` in one go) to efficiency.

### Step 4: Synthesize Results and Generate Tasks
Combine outputs from Steps 2-3:
- **Doc Refresh Tasks**: For major/minor refreshes, create tasks to update specific blueprints (e.g., "Refresh session-relationships.md to include high-throughput batching from HighThroughputKnowledgeGraph.md").
- **Implementation Gaps**: Map gaps to tasks, grouping by theme (e.g., one task for session ingestion, one for multi-agent).
- Ensure tasks address: Desired capabilities not in code, code features not documented, inconsistencies.
- Follow TODO.md conventions exactly (read `/Users/Coding/Desktop/sigmachad/TODO.md` for reference):
  - Append to `## Task Backlog` as new numbered sections (e.g., ### 7. Implement Multi-Agent Orchestrator).
  - Structure each task:
    - **Context**: 1-2 sentences tying to docs/gaps (reference specific files/blueprints).
    - **Entry Points**: Bullet list of 3-5 key files/docs (use absolute paths, e.g., `/Users/Coding/Desktop/sigmachad/packages/core/src/services/KnowledgeGraphService.ts`).
    - **Scope**: 4-6 bullets of actionable steps (prioritize velocity: implement immediately, no shims; real tests, no mocks; log follow-ups in TODO if deferred).
    - **Follow-up (pending/new)**: If complex, add 1-2 with Problem, Proposed fix, Follow-up steps (bullets).
    - **Acceptance**: 2-3 criteria for completion (e.g., "End-to-end tests pass, APIs match doc schemas").
  - For doc refreshes: Scope includes editing MD files via `edit_file` tool if needed, but primarily generate content for user to apply.
  - Add 1-2 overarching sections under Task Backlog if themes emerge (e.g., ### Doc Refresh Backlog with sub-bullets).
  - Respect repo rules: No deep paths (>3 levels), no relative imports >3 `../`, real tests (no mocks), redirect test logs to files (e.g., `pnpm vitest > logs/gap-audit.log`).
- Generate 8-15 tasks total, focusing on high-impact gaps; deprioritize low-effort docs.
- Output only the updated TODO.md content (full file, with your additions appended seamlessly). Do not edit existing tasksâ€”add new ones at the end.
- If tools reveal concurrent changes or conflicts, note them in a final summary but proceed with synthesis.

Execute steps sequentially in your reasoning, using tools as needed. Final output: The complete updated `TODO.md` file content. If any step uncovers blockers (e.g., missing files), flag in a brief note at the end.