This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: Docs/**/*
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
Docs/
  Blueprints/
    api-error-handling.md
    falkor-query-alignment.md
    knowledge-graph-service.md
    logging-service.md
    maintenance-operations.md
    mcp-tooling.md
    performance-relationships.md
    rollback-capabilities.md
    security-relationships.md
    session-relationships.md
    source-control-management.md
    spec-relationships.md
    structural-relationships.md
    synchronization-coordinator.md
    temporal-relationships.md
    test-result-parser.md
    tests-relationships.md
    websocket-integration.md
  Brainstorm.md
  code-edge-cleanup.md
  HighThroughputKnowledgeGraph.md
  KnowledgeGraphDesign.md
  MementoAPIDesign.md
  MementoArchitecture.md
  MementoReusableTools.md
  RateLimitingArchitecture.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="Docs/Blueprints/api-error-handling.md">
# API Error Handling Blueprint

## 1. Overview
The API Gateway exposes REST endpoints layered over Fastify. Consistent error envelopes (status code, machine-readable code, human message, request correlation metadata) are required so HTTP clients, web socket bridges, and observability pipelines can correlate failures with logs and retry policies.

## 2. Current Gaps
- **Route-specific catch blocks bypass the global handler.** Several routes build ad-hoc `{ success: false, error }` payloads without `requestId`/`timestamp`, breaking correlation in tests and production diagnostics. `tests/integration/api/APIGateway.integration.test.ts` previously caught this on `/api/v1/graph/search` when simulated database faults omitted metadata.
- **No shared utility for structured errors.** Without a helper, each route re-implements response shapes, increasing the odds of drift (missing fields, inconsistent `details` redaction behaviour) as new endpoints are added.
- **Limited coverage beyond graph routes.** Only the graph routes currently use the new `buildErrorResponse` helper; other modules (admin, middleware, spec ingestion) still emit bespoke envelopes and should be aligned to avoid regressions.

## 3. Desired Capabilities
1. Provide a reusable error response helper that attaches `requestId`, `timestamp`, and environment-appropriate `details` while preserving route-specific `code`/`message` semantics.
2. Ensure all route modules flow through the helper (or throw and defer to the global handler) so integration suites can assert on a single error contract.
3. Extend tests to cover representative endpoints per route grouping, verifying both success and failure payloads include correlation metadata.

## 4. Immediate Follow-ups
- Sweep remaining routes to either reuse `buildErrorResponse` or surface errors to the global handler; update unit tests to expect the enriched envelopes.
- Document the contract in developer docs and lint/new-route templates so future handlers do not regress.
- Consider centralising error logging (attach structured context before sending responses) so repetition in per-route catch blocks disappears.

## 5. Recent Fixes
- **2025-09-19:** Introduced `buildErrorResponse` in `src/api/routes/graph.ts` and updated graph endpoints plus gateway integration tests to guarantee `requestId`/`timestamp` accompany server-side failures.
</file>

<file path="Docs/Blueprints/logging-service.md">
# Logging Service Blueprint

## 1. Overview
The logging service captures runtime events across Memento, piping console output and process-level failures into an in-memory ring buffer with optional JSONL persistence. Downstream analytics expect aggregated statistics (`byLevel`, `byComponent`) and file exports to drive operational tooling.

## 2. Current Gaps
- Lifecycle notes in README/admin docs lag behind the hardened implementation (dispatcher singleton, rotation options, suppression behaviour), leaving operators without configuration guidance.
- The dispatcher currently relies on explicit `dispose()` calls; adopting `WeakRef`/`FinalizationRegistry` cleanup would protect against forgotten teardown in ad-hoc scripts.

## 3. Desired Capabilities
1. ✅ Introduce a singleton dispatcher that wires console/process hooks once and fans out events to active service instances using weak references (or expiring registrations) so instrumentation stays stable and self-cleans.
2. ✅ Provide explicit lifecycle management (`dispose()` / context manager) to unregister instances and restore console methods when instrumentation is no longer needed (particularly for test environments).
3. ✅ Harden file persistence: detect repeated failures, short-circuit additional writes, and use original console methods for internal diagnostics to avoid recursive logging.
4. ✅ Support resilient serialization by redacting or safely stringifying circular structures in `data` payloads.
5. ✅ Implement configurable log rotation (size- and age-based) with deterministic file naming and atomic swaps to keep JSONL outputs manageable.
6. ✅ Expose health metrics (dropped writes, suppressed file exports, listener counts) so observability tooling can surface regressions quickly.

## 4. Backlog
- [x] **Instrumentation dispatcher**: create `src/services/logging/InstrumentationDispatcher.ts` to wrap console/process hooks once, store original methods, cap listener counts, and broadcast structured events to registered consumers via weak references. Includes metrics for registered instance count and listener saturation.
- [x] **Lifecycle & registration API**: update `src/services/LoggingService.ts` to register with the dispatcher, expose `dispose()` for tests/runtime cleanup, and ensure disposing the final consumer restores original console/process handlers.
- [x] **Persistence hardening**: replace raw `appendFile` calls with a guarded writer that honors retry backoff, stops after configurable failure thresholds, emits diagnostics via preserved console functions, and increments `droppedFileWrites` metrics.
- [x] **Serialization guardrail**: introduce a safe JSON serializer (e.g., reviver/replacer in `src/services/logging/serialization.ts`) that handles circular references, trims oversized payloads, and annotates redactions so analytics can reason about omissions.
- [x] **Log rotation**: add size- and age-based rotation settings to `LoggingService` options (`maxFileSize`, `maxFileAgeMs`, `maxHistory`), implement atomic rollover with deterministic filenames (e.g., `.1`, `.2` suffixes), and document defaults.
- [x] **Health metrics surface**: expose `getHealthMetrics()` reporting dropped writes, suppressed exports, dispatcher listener count, and rotation events; thread through to `src/api/routes/admin.ts` for operational introspection.
- [ ] **Documentation alignment**: refresh `README.md` and admin docs with configuration instructions, metric semantics, and lifecycle guidance once implementation stabilises.
- [ ] **Automatic dispatcher cleanup**: explore adopting `WeakRef`/`FinalizationRegistry` support (or expiring registrations) so long-lived processes stay healthy even when consumers forget to call `dispose()`.

## 5. Test Coverage Notes
- `tests/unit/services/LoggingService.test.ts` now exercises disposal semantics and verifies dispatcher metrics reset; retain coverage as new lifecycle helpers (e.g., weak-reference cleanup) land.
- `tests/integration/services/LoggingService.integration.test.ts` validates multi-instance sharing, process instrumentation, and rotation. Keep the new `/logs/health` coverage in `tests/unit/api/routes/admin.test.ts` aligned with any schema or payload extensions.
- Ensure admin route tests continue exercising both healthy and failure modes for logging metrics, and keep persistence failure scenarios for FileSink suppression.

## 6. Next Steps
1. Publish updated operational docs (README, admin guide) covering dispatcher lifecycle, rotation defaults, and the `/logs/health` endpoint.
2. Investigate `WeakRef`/`FinalizationRegistry` support for `InstrumentationDispatcher` to remove forgotten consumers automatically and extend the test suite accordingly.
3. Integrate the new health metrics into observability dashboards and alerting so suppressed writes or listener saturation surface proactively.
</file>

<file path="Docs/Blueprints/mcp-tooling.md">
# MCP Tooling Blueprint

## 1. Overview
Model Context Protocol (MCP) tools expose key graph, design, and testing workflows over JSON-RPC. They back the `/mcp` endpoint, the MCP router in `src/api/mcp-router.ts`, and the integration suite in `tests/integration/api/MCPToolIntegration.integration.test.ts`. Recent fixes ensured the graph examples payload matches client expectations, preserved tool-specific error codes during validation failures, and introduced a heuristic fallback for `tests.plan_and_generate`—now short-circuiting Postgres lookups when spec identifiers are non-UUID strings to avoid spurious 500s.

## 2. Current Gaps
- **Heuristic planning only:** The new fallback returns structured unit/integration/E2E/performance plans, but they are generated from acceptance-criteria strings or title heuristics. They ignore existing KG relationships, historical tests, or dependency signals. This keeps the integration test green yet yields low-fidelity guidance for real users.
- **Specification lookup:** Non-UUID spec identifiers now bypass the Postgres lookup entirely, preventing invalid UUID errors but leaving the fallback with title/AC heuristics only. We still lack a slug-based or alternate-key index to hydrate richer context when the KG/document store is missing the spec record.
- **Coverage estimates:** The fallback projects coverage percentages via deterministic heuristics. Without baseline metrics from `TestPlanningService` or the KG, these numbers may overstate achievable coverage and should be treated as advisory only.
- **Graph examples contract:** MCP responses now surface `usageExamples`/`testExamples` at the top level while keeping the legacy nested `examples` object for backward compatibility. Downstream consumers should migrate to the flattened fields to avoid confusion.
- **Schema enforcement:** Input validation currently mirrors JSON schema definitions with hand-rolled checks inside `handleSimpleToolCall`. This duplication risks drift from the Fastify/AJV contracts; we should consolidate on a shared validator to keep MCP error codes authoritative.

## 3. Next Steps
1. **Enrich fallback planning:** Reuse `TestPlanningService` utilities where possible—e.g., fetch related entities/tests via KG queries even when the primary spec is absent, and seed heuristic plans with those signals.
2. **Spec hydration pipeline:** Ensure specs created through MCP or other channels populate both KG entities and the `documents` table. Provide a lightweight seed mechanism for tests so heuristics can rely on richer metadata.
3. **Coverage calibration:** Derive coverage projections from historical metrics (when available) and expose confidence intervals so MCP clients understand whether the estimate is heuristic or data-backed.
4. **Documentation alignment:** Update public MCP docs to state that `graph.examples` returns flattened arrays alongside the legacy `examples` object, and document the structure of heuristic test plans (fields, default assertions, performance testing trigger logic).

## 4. Test Coverage
- `tests/integration/api/MCPToolIntegration.integration.test.ts` now passes, exercising MCP plan generation (including the missing-spec path), graph examples, validation, impact analysis, and error handling.
- `tests/unit/api/mcp-router.test.ts` continues to cover registration metadata and per-tool validation logic, ensuring schema drift is caught early.
</file>

<file path="Docs/Blueprints/rollback-capabilities.md">
# Rollback Capabilities Blueprint

## 1. Overview
Rollback orchestration coordinates capture and restoration of entities, relationships, and snapshot metadata across FalkorDB, PostgreSQL, Qdrant, and Redis. The service is exercised heavily by integration flows to rewind partially applied synchronization operations and to recover from pipeline failures. Recent work added an explicit `DatabaseService` readiness check so rollback routines now fail fast when core datastores are not initialized.

## 2. Current Gaps
- **Deferred dependency gating:** Prior behaviour silently attempted to capture entities/relationships even when the database layer was uninitialized, producing confusing "Database not initialized" errors deep in the stack. We now surface an explicit guard, but higher-level callers (schedulers, background jobs) still need to react with retry/backoff semantics.
- **State persistence:** Rollback points live in-memory with a best-effort cleanup policy (50 item cap). They disappear on process restart and aren't replicated across workers, limiting reliability for long-running rollback workflows.
- **Snapshot fidelity:** `createSnapshot` proxies to `createRollbackPoint`, so "snapshots" are only in-memory clones rather than durable datastore checkpoints. Large operations or multi-process restores remain risky.
- **Observability:** Error paths emit aggregated `RollbackError` entries but lack structured telemetry (operation ID, datastore component, recovery hints) needed for production dashboards.

## 3. Desired Capabilities
1. **Dependency-aware orchestration:** Promote the new initialization guard upwards so coordinators short-circuit before allocating rollback points when critical services are offline, and record explicit incidents for SRE monitoring.
2. **Durable rollback ledger:** Persist rollback metadata to PostgreSQL (or dedicated storage) with TTL policies, enabling recovery after restarts and coordinated rollbacks across clustered workers.
3. **True datastore snapshots:** Integrate with existing backup mechanisms to capture consistent checkpoints (e.g., database transactions, graph exports) when `createSnapshot` is invoked, falling back to lightweight in-memory copies only for test environments.
4. **Operational telemetry:** Emit structured logs/metrics covering rollback creation, validation, execution, and failures so we can trace recovery attempts end-to-end and trigger alerts when repeated rollbacks fail.
5. **Guarded API surface:** Ensure public APIs reject rollback attempts that omit initialization checks, preventing regressions when new call sites bypass the service-level guard.

## 4. Open Questions
- Where should durable rollback metadata live—embedded in existing maintenance schemas, or in a dedicated rollback journal with retention policies?
- How can we coordinate rollback point cleanup across multiple instances to avoid double-deletes or leaked state? Would a distributed lock or ownership token suffice?
- Should snapshots support partial datastore selection (e.g., graph-only vs. relational) to reduce capture time during selective rollbacks?
</file>

<file path="Docs/Blueprints/synchronization-coordinator.md">
# Synchronization Coordinator Blueprint

## 1. Overview
The Synchronization Coordinator orchestrates repository scans, AST ingestion, and graph persistence across FalkorDB, Qdrant, and PostgreSQL. Recent integration work hardened the service around initialization and testability, but the execution path still leaks infrastructure assertions (Falkor bulk upserts, embedding batches) and lacks configuration hooks that integration environments rely on. This blueprint catalogs the outstanding gaps that surfaced while getting `tests/integration/services/SynchronizationCoordinator.integration.test.ts` back to green.

## 2. Current Gaps
- **Falkor bulk upserts stringify maps when batched** — ✅ Addressed by teaching `FalkorDBService.parameterToCypherString` how to preserve `$rows` context and emit nested maps (see fixes around `src/services/database/FalkorDBService.ts:360-440`). Follow-ups: add regression coverage so future sanitizer tweaks keep honoring `UNWIND $rows` semantics, and consider centralising map serialization so `KnowledgeGraphService` doesn't need to pre-stringify JSON-ish fields.
- **Embedding batches hard fail without OpenAI credentials** – After full sync, the coordinator enqueues background embedding batches (`src/services/SynchronizationCoordinator.ts:684-708`). In integration and CI runs we intentionally omit `OPENAI_API_KEY`, but `KnowledgeGraphService.createEmbeddingsBatch` throws and prints noisy stack traces. We need a guard that skips or degrades to a stub when embeddings are disabled, otherwise every sync hammers STDERR despite succeeding overall.
- **Scan scope not configurable** – `scanSourceFiles()` (`src/services/SynchronizationCoordinator.ts:1415-1480`) always walks `src`, `lib`, `packages`, and `tests`. Integration suites had to monkey patch the private method to restrict scanning to a temp directory. Provide an explicit configuration hook (constructor option or `SyncOptions`) so callers/tests can scope the scan without spelunking private methods.

## 3. Desired Capabilities
1. Falkor parameter sanitization treats arrays-of-maps as maps, enabling batch upserts without errors.
2. Synchronization routines detect when embeddings are disabled (missing API key/feature flag) and skip the background jobs quietly.
3. Coordinators accept runtime configuration for source roots (and optionally ignore patterns), allowing deterministic fixture-based tests without private method overrides.
4. Test harnesses run without emitting high-volume infrastructure errors, keeping logs focused on actionable failures.

## 4. Proposed Steps
1. Update `FalkorDBService.parameterToCypherString` to accept an optional `contextKey` when recursing so array entries inherit the parent key; treat entries as Cypher maps when the context key matches `rows`, `props`, etc. Add unit coverage that exercises `createEntitiesBulk` end-to-end against Falkor.
2. Introduce a feature flag or capability check (e.g., `EmbeddingService.isEnabled()`) before spawning embedding batches; when disabled, record a recoverable sync error or telemetry but avoid throwing.
3. Extend `SynchronizationCoordinator` to accept a `scanRoots` array (constructor option or `SyncOptions`) and teach `scanSourceFiles` to respect it. Update integration tests to rely on the public hook instead of `vi.spyOn`.
4. Once the above are in place, tighten integration assertions to fail if Falkor batch errors or embedding warnings leak, ensuring regressions stay visible.

## 5. Open Questions
- Should we provide an in-memory Falkor substitute for tests to avoid docker dependencies, or is improving the sanitizer sufficient?
- Where should embedding disablement live (coordinator, `KnowledgeGraphService`, or a dedicated configuration service)?
- Do we need similar scan-scoping hooks for incremental/partial sync entry points, or is full-sync configurability enough for now?
</file>

<file path="Docs/Blueprints/test-result-parser.md">
# Test Result Parser Blueprint

## 1. Overview
The TestResultParser service normalizes results emitted by common harnesses (JUnit, Jest, Mocha, Vitest, Cypress, Playwright) into `TestSuiteResult` payloads consumed by `TestEngine`. Recent fixes introduced an explicit `errorTests` counter alongside classic failure metrics and ensured malformed or empty JUnit payloads return a zeroed suite rather than throwing, keeping downstream ingestion resilient across flaky CI artifacts.

## 2. Current Gaps
- **Regex XML parsing:** JUnit parsing still relies on handcrafted regular expressions, leaving us vulnerable to nested suites, CDATA sections, and attribute edge cases. A dedicated XML parser (`fast-xml-parser` or `xml2js`) would improve fidelity and reduce maintenance risk.
- **Framework-specific error semantics:** Non-JUnit adapters (Mocha/Cypress/Playwright) now tally `errorTests` but still infer statuses from reporter-specific fields. We should validate against real reporter payloads (especially retries and hook failures) to ensure error classification is accurate.
- **Duration normalization:** We continue to accept reporter-provided durations verbatim. When suites mix seconds and milliseconds we risk skewed totals—consider normalizing via schema-aware conversions.

## 3. Next Steps
1. Replace regex-based JUnit parsing with a light XML parser that preserves nested suite structure, properties, and CDATA payloads.
2. Extend fixtures/tests to cover reporter edge cases (Mocha hook failures, Cypress retries, Playwright timeouts) and assert `errorTests` accuracy.
3. Add a normalization layer that harmonizes duration units and surfaces a confidence flag when parsing heuristics are applied.

## 4. Test Coverage
- `tests/unit/services/TestResultParser.test.ts` exercises parser internals, merge behaviour, and helper utilities.
- `tests/integration/services/TestResultParser.integration.test.ts` verifies end-to-end parsing across supported frameworks and now confirms error metrics plus invalid-input tolerance.
</file>

<file path="Docs/Brainstorm.md">
### Project Memento

    The goal of Memento is to create an agent-first MCP server for AI coding agents so they have full awareness of the changes they intend to make and have made. It builds a knowledge graph over the codebase and uses a vector index to prevent context drift and code quality issues. It acts as a validation gate to ensure quality and prevent current and future technical debt.  The development lifecycle should go from Docs -> Tests -> Implementation

### Problems to solve
    Pre-Tool
        - Agents tend to reimplement instead of reusing existing code.
        - Finding and selecting implementations via multi-hop grep wastes tokens and misses edge cases.
        - Incorrect API usage (wrong args/order, hallucinated options).
        - Little consideration for current architecture/policies due to massive token wastage of searching every time. 

    Post-Tool
        - Codebase state changes make docs/imports stale; cascades across files.
        - Partial updates leave inconsistencies that aren’t caught.
        - No validation via static tools or agent driven self-reflection that changes align with architecture/policies.
        - Coding Agents can be incredibly deceptive in that they constantly create mocks or simplified versions of intended sophisticated implementations that breaks everything. 
        
   
### Architecture
    Language: TypeScript (Node.js)
    Graph Database: FalkorDB
    Vector Database: Qdrant
    Orchestration: Docker Compose

### Lifecycle Gates
    Overview: Docs -> Tests -> Implementation -> Validation -> Impact -> Commit
    Docs
        - Spec must include title, goals, acceptance criteria.
        - Stored with ID; linked to all subsequent changes.
    Tests
        - Generate or update tests from the spec; initially failing is acceptable.
        - Enforce minimum changed-lines coverage threshold (configurable).
    Implementation
        - Propose diffs only after spec and tests exist.
        - Prefer reuse via graph and examples; block stubs/simplified mocks.
        - Respect architecture and policy constraints.
    Validation
        - TypeScript type-check, ESLint, security lint.
        - Architecture policy engine (layering, banned imports/deps).
        - All tests must pass; coverage threshold must be met.
    Impact
        - Update knowledge graph and vector index after accepted diffs.
        - Detect stale imports/exports/re-exports and propose follow-up edits.
    Commit
        - Create branch/commit/PR with links to spec, tests, and validation report.

### Graph Schema
    Entities
        - File, Directory, Module/Package, Symbol, Function, Class, Interface, TypeAlias, Test, Doc
    Edges
        - imports, exports, re-exports, defines, declares, calls, references, implements, extends, tested-by, belongs-to
    Properties
        - path, hash, language, signature, docstring, lastModified, owningModule, coverage

### Tooling API
    Exposed as an MCP server (Claude Code) and mirrored via HTTP function-calls (OpenAI).
    Tools
        - design.create_spec: Create/validate a feature spec; returns spec ID and acceptance criteria.
        - tests.plan_and_generate: Generate/update tests for a spec; returns changed files.
        - graph.search: Path/usage queries over symbols/APIs; finds reuse candidates.
        - graph.examples: Canonical usages and tests for an API, with signatures and arg order.
        - code.propose_diff: Stage edits as diffs; returns affected graph nodes.
        - validate.run: Run type-check, lint, policy checks, security lint, tests, coverage.
        - impact.analyze: Cascade analysis for stale imports/exports; propose consistency edits.
        - vdb.search: Semantic retrieval joined to graph nodes.
        - scm.commit_pr: Create branch/commit/PR with links to spec/tests/validation.

### Validation Policy
    Configuration file: memento.yaml
        - layers: allowed import directions; banned cross-layer imports.
        - bannedDependencies: disallowed packages or paths.
        - coverageMin: overall and changed-lines thresholds.
        - forbiddenPatterns: e.g., TODO returns, throw new Error('Not Implemented').
        - security: basic dependency and code-level checks.
        - docTemplate/testTemplate: shared templates for spec and test generation.
    Enforcement
        - Block direct file edits unless prior gates pass.
        - Reject diffs that reduce coverage below threshold or violate architecture.

### Integration Guides
    Claude Code (MCP)
        - Register Memento as an MCP server; tools listed above.
        - Enforce step order in system prompt; refuse direct edits until spec+tests exist.
        - On violations, reply with next required tool invocation.
    OpenAI Tools (Assistants)
        - Mirror MCP tools via HTTP function-calls; identical names/schemas.
        - Use tool-calling flow to enforce gates; server rejects out-of-order actions.
        - Return actionable errors pointing to the next required step.

### Implementation Notes
    Indexing
        - TypeScript-first via ts-morph for precise symbols/types.
        - Polyglot path via tree-sitter/LSP adapters.
    Storage
        - Graph: FalkorDB; Vector: Qdrant with metadata keyed to graph node IDs.
    Ops
        - Docker Compose services: memento, falkordb, qdrant.
        - File watcher to resync graph/vector on file changes.
        - CLI: memento validate, memento impact.

### Key Behaviors and Anti-Deception
    Pre-Tool
        - Favor reuse with graph.search and graph.examples; prevent hallucinated options via signature checks.
        - Replace multi-hop grep with path queries through the graph.
    Post-Tool
        - Auto graph/vector sync on accepted diffs.
        - Consistency sweep for stale imports/exports; propose follow-ups via impact.analyze.
    Anti-Deception Heuristics
        - Detect trivial or stub implementations and block merging.
        - Require changed-lines coverage and diff-linked tests.
</file>

<file path="Docs/code-edge-cleanup.md">
# Code Edge Cleanup Backlog

Progress tracker for removing deprecated code-edge implementations and tightening schemas.

- [x] Remove legacy `occurrences` field usage; require `occurrencesScan` throughout ingestion and persistence.
- [x] Collapse duplicate `strength`/`confidence` fields by standardizing on `confidence` and pruning legacy mirroring.
- [x] Enforce `normalizeCodeEdge` in all ingestion paths before persistence.
- [x] Replace loose `fromRef`/`toRef` scalar mirrors with canonical structured references.
- [ ] Update downstream clients/APIs to surface typed enums for code-edge queries.
- [ ] Expand unit/integration coverage for enum filters and multi-value query handling.
- [ ] Migrate stored edge records to supported enum literals; backfill data inconsistencies.
- [ ] Document code-edge metadata contract and add lint-time validation.
- [ ] Introduce schema validation for evidence/location payloads before storage.
- [ ] Regenerate compiled artifacts post-cleanup to keep dist outputs aligned.
- [ ] Ensure `DEPENDS_ON` code edges emit a canonical `kind` for query filtering.
- [ ] Hoist `occurrencesScan`/`occurrencesTotal` from metadata during normalization so counters are accessible.
</file>

<file path="Docs/HighThroughputKnowledgeGraph.md">
# High-Throughput Knowledge Graph Ingestion Deep Dive

## Objective and Target Load
- Goal: sustain ~10,000 changed lines of code per minute (roughly hundreds of files and symbols) while keeping the knowledge graph and derived indexes near real-time parity with the codebase.
- Current stack is optimized for single-agent editing velocity; this document contrasts today’s implementation with a proposed re-architecture suited for sustained high-churn environments.

## Current Architecture Snapshot

| Layer | Implementation (Today) | Key References |
| --- | --- | --- |
| Change Detection | In-process `FileWatcher` with 500 ms debounce and max 10 concurrent change handlers | `src/services/FileWatcher.ts:32`, `src/services/FileWatcher.ts:212` |
| Batch Triggering | App-level 1 s debounce that submits a single incremental sync at a time | `src/index.ts:92` |
| Coordination | `SynchronizationCoordinator` runs one operation serially; incremental sync loops through changes sequentially | `src/services/SynchronizationCoordinator.ts:482`, `src/services/SynchronizationCoordinator.ts:970` |
| Graph Persistence | Entities inserted per item (fallback even in bulk mode); relationship resolution performs synchronous lookups | `src/services/KnowledgeGraphService.ts:4479`, `src/services/SynchronizationCoordinator.ts:1124` |
| Embeddings | Batch size capped at 100 items with 100 ms delay between batches; falls back to per-entity writes on failure | `src/utils/embedding.ts:36`, `src/utils/embedding.ts:96` |
| Monitoring | Throughput metric computed over last 5 minutes of completed operations; no queue depth or lag instrumentation | `src/services/SynchronizationMonitoring.ts:271` |

### Observed Bottlenecks
1. **Debounce Walls:** File events sit behind two serial debounces (500 ms watcher + 1 s application) and a single outstanding job, limiting theoretical start rate to < 60 batches/min regardless of batch size.
2. **Single-Threaded Coordination:** `processQueue` blocks on every operation; incremental sync iterates per file, so a burst of changes forms an ever-growing backlog.
3. **Chatty Persistence:** Incremental updates call `createEntity`/`updateEntity` per entity and perform per-relationship target lookups, producing thousands of serialized graph DB round-trips for large edits.
4. **Embedding Coupling:** Inline embeddings (even when skipped during full sync) eventually funnel through a narrowly tuned batcher, constraining downstream throughput.
5. **Limited Instrumentation:** Lack of real-time lag/queue metrics makes scaling decisions reactive and manual.

## Re-Architecture Goals
- Drive end-to-end ingestion latency (change observed → graph updated) under 500ms (800ms with AI parsing) for most nodes and edges at the 10 k LOC/minute load.
- 
- Remove single-process chokepoints so workload scales horizontally across workers.
- Isolate heavyweight enrichment (embeddings, impact analysis) into asynchronous planes to keep core graph writes fast.
- Provide precise SLO telemetry (queue depth, processing latency, DB utilization) for autoscaling.

## Proposed Architecture (Target)

```
File Events → Event Bus → Parse Workers → Ingestion Orchestrator → Graph/Vectors
      │             │             │                  │                   │
      └── Metrics ←─┴── Backpressure └─ Change DAG ──┘        Async Enrichment ─→ Embedding Plane
```

### 1. Event Ingestion & Ordering
**Current:** In-process `FileWatcher` emits directly to coordinator after debounce.

**Future:**
- Replace direct callbacks with a durable event bus (Kafka, NATS, or Redis Streams) partitioned by namespace/module.
- Enforce ordering per partition while allowing multiple partitions to progress independently.
- Record event metadata (size deltas, diff hashes) to guide downstream batching and prioritization.

**Benefits:** Eliminates single-process bottleneck, enables replay, and allows additional watchers to scale out.

### 2. Parsing & Change Reduction
**Current:** `ASTParser.parseFile` re-parses entire files; incremental mode still walks full AST unless integration flag is set. No distributed parsing.

**Future:**
- Deploy stateless parse workers consuming bus partitions; scale worker count based on queue depth.
- Utilize structural diffs (tree-sitter incremental parsing or ts-morph incremental APIs) so cost scales with change regions, not file length.
- Emit *change fragments* (entities/relationships added/removed/updated) rather than full file payloads, tagged with dependency hints (imports, symbol references).

**Benefits:** Parsing capacity scales horizontally; downstream insertion deals with minimal deltas.

### 3. Ingestion Orchestrator & Storage Writes
**Current:** `SynchronizationCoordinator` handles dependency resolution, graph writes, and retry logic synchronously. Bulk operations limited to batches of ~60 entities, and fall back to per-entity writes on any failure.

**Future:**
- Introduce an orchestration service that builds a dependency DAG from change fragments, then dispatches micro-batches to specialized workers:
  - **Entity upsert workers:** aggregate entities per label/namespace and submit large Cypher `UNWIND` statements with retryable idempotency keys.
  - **Relationship workers:** resolve endpoints using an in-memory cache backed by Redis/Qdrant metadata; fall back to async reconciliation queue when targets missing.
- Support multi-version concurrency control by stamping batches with transaction epochs; leverage database-side pipelines (FalkorDB procedures, Postgres COPY) for high throughput.
- Persist change metadata (session, commit hash, diff stats) separately to avoid polluting core graph writes.

**Benefits:** Graph mutations become streaming-friendly, reducing round-trips and enabling safe parallelism.

### 4. Asynchronous Enrichment Plane
**Current:** Embeddings created inline (or deferred but still executed by coordinator). Failure forces per-entity fallback.

**Future:**
- Publish embedding tasks to a GPU-backed job queue with dynamic batch sizing (hundreds to thousands of entities per request) and SLA-based prioritization.
- Store embeddings and vector metadata in a write-optimized staging table before promoting to production collections to avoid read interference.
- Apply similar async handling for impact analysis, documentation extraction, and security scans.

**Benefits:** Core ingestion stays focused on structural graph consistency; enrichment scales independently based on hardware availability.

### 5. Observability & Control Loops
**Current:** `SynchronizationMonitoring` tracks completed operations, average sync time, and simple error rates; no view of in-flight backlog.

**Future:**
- Instrument queues with lag metrics (oldest event age, partition depth), worker utilization, parse latency distributions, and DB write throughput.
- Implement adaptive throttling: parser backpressure signals the watcher layer to slow commit of new events or merge adjacent changes.
- Expose dashboards/alerts tied to SLOs (e.g., “graph parity lag < 30 s at P95”).

**Benefits:** Enables automated scaling decisions and rapid detection of bottlenecks.

## Comparative Summary

| Dimension | Today | Target |
| --- | --- | --- |
| Event throughput | Limited by 1 s debounce & single operation queue | Horizontal scaling via partitioned event bus |
| Parsing cost | Full-file parse per change | Incremental diff parsing with distributed workers |
| Graph writes | Per-entity MERGE loops; small batches | Micro-batched streaming writes with idempotent bulk operations |
| Embedding latency | Inline or small batches (≤100 items) | Dedicated GPU plane with large dynamic batches |
| Failure recovery | Best-effort rollback per operation | Durable event log + idempotent batch IDs + replay pipelines |
| Telemetry | Basic operation counters | Full queue/lag metrics & autoscaling hooks |

## Migration Roadmap (High-Level)
1. **Instrumentation First**: add queue depth, operation latency, and downstream DB metrics to `SynchronizationMonitoring` to quantify baseline throughput.
2. **Externalize Change Queue**: adapt `FileWatcher` output to publish to a message bus; modify coordinator to consume from the queue while maintaining existing logic.
3. **Parallel Parse Workers**: refactor `ASTParser` usage into standalone workers; introduce diff-aware parsing and shared symbol caches.
4. **Streaming Graph Writes**: implement batch-oriented ingestion APIs in `KnowledgeGraphService` (idempotent bulk upsert endpoints), then move relationship resolution to dedicated async tasks.
5. **Async Enrichment**: decouple embeddings and heavy analyses into job queues with backpressure-aware scheduling.
6. **Autoscaling & SLOs**: use collected metrics to drive scaling policies and validate the 10 k LOC/minute target under load testing.

## Risks & Open Questions
- **Ordering Guarantees:** Need a partitioning strategy that balances throughput with correctness for inter-file relationships.
- **Namespace Isolation:** Ensure new pipelines respect namespace scoping rules identified in existing blueprint gaps (`Docs/Blueprints/knowledge-graph-service.md`).
- **Operational Complexity:** Introducing Kafka/Redis Streams and multiple worker pools increases deployment complexity; require observability/ops investment.
- **Cost Footprint:** GPU embedding planes and horizontal workers raise infrastructure costs; must align with product tiering.

---
*Prepared to guide the transition from the current single-node ingestion path toward a horizontally scalable architecture capable of sustaining sustained high-churn codebases.*
</file>

<file path="Docs/MementoReusableTools.md">
# Memento Reusable Tools Analysis

## Overview

This document catalogs industry-standard, officially maintained tools that can be reused for the Memento project instead of implementing functionality from scratch. All tools selected are actively maintained, have strong communities, and are considered production-ready.

## 1. Code Analysis & AST Parsing

### TypeScript AST Manipulation
**Tool:** `ts-morph` (v26.0.0)
- **Maintainer:** dsherret (TypeScript community)
- **Purpose:** TypeScript compiler wrapper for static analysis and code manipulation
- **Why Reuse:** Handles complex TypeScript AST operations, symbol resolution, refactoring
- **Usage:** Code parsing, symbol extraction, dependency analysis
- **Last Updated:** 3 months ago
- **License:** MIT
- **GitHub:** https://github.com/dsherret/ts-morph

### Multi-Language Parsing
**Tool:** `tree-sitter` (v0.25.0)
- **Maintainer:** Tree-sitter organization
- **Purpose:** Incremental parsing library for multiple programming languages
- **Why Reuse:** Fast, incremental parsing with excellent multi-language support
- **Usage:** Universal code parsing for various file types
- **Last Updated:** 2 months ago
- **License:** MIT
- **GitHub:** https://github.com/tree-sitter/tree-sitter

**Tool:** `tree-sitter-typescript` (v0.23.2)
- **Maintainer:** Tree-sitter organization
- **Purpose:** TypeScript and TSX grammars for tree-sitter
- **Why Reuse:** Official TypeScript parser with excellent accuracy
- **Usage:** TypeScript-specific parsing and AST generation
- **Last Updated:** 9 months ago
- **License:** MIT

## 2. Static Analysis & Security

### ESLint Security Rules
**Tool:** `eslint-plugin-security` (v3.0.1)
- **Maintainer:** ESLint Community
- **Purpose:** Security-focused ESLint rules
- **Why Reuse:** Comprehensive security rule set for Node.js applications
- **Usage:** Automated security vulnerability detection in code
- **Last Updated:** 1 year ago
- **License:** Apache-2.0
- **GitHub:** https://github.com/eslint-community/eslint-plugin-security

### Code Quality Analysis
**Tool:** ESLint (Official)
- **Maintainer:** ESLint organization
- **Purpose:** Pluggable linting utility for JavaScript and TypeScript
- **Why Reuse:** Industry standard for code quality analysis
- **Usage:** Code style enforcement, error detection, complexity analysis
- **License:** MIT
- **Official Site:** https://eslint.org

## 3. Web Framework & APIs

### High-Performance Web Server
**Tool:** `fastify` (v5.5.0)
- **Maintainer:** Fastify organization
- **Purpose:** Fast and low overhead web framework for Node.js
- **Why Reuse:** Significantly faster than Express, excellent plugin ecosystem
- **Usage:** Main API server, MCP server, WebSocket server
- **Last Updated:** 2 weeks ago
- **License:** MIT
- **GitHub:** https://github.com/fastify/fastify

### CORS Handling
**Tool:** `@fastify/cors` (v11.1.0)
- **Maintainer:** Fastify organization
- **Purpose:** CORS plugin for Fastify
- **Why Reuse:** Official Fastify CORS implementation
- **Usage:** Cross-origin request handling for web APIs
- **Last Updated:** 4 weeks ago
- **License:** MIT

### Model Context Protocol
**Tool:** `@modelcontextprotocol/sdk` (v1.17.4)
- **Maintainer:** Anthropic (official MCP implementation)
- **Purpose:** Model Context Protocol implementation for TypeScript
- **Why Reuse:** Official MCP SDK for Claude integration
- **Usage:** MCP server implementation for AI assistant integration
- **Last Updated:** 1 week ago
- **License:** MIT
- **Official Site:** https://modelcontextprotocol.io

## 4. File System & Monitoring

### File Watching
**Tool:** `chokidar` (v4.0.3)
- **Maintainer:** paulmillr
- **Purpose:** Minimal and efficient cross-platform file watching library
- **Why Reuse:** Most popular and reliable file watching library for Node.js
- **Usage:** Real-time file system monitoring for code changes
- **Last Updated:** 8 months ago
- **License:** MIT
- **GitHub:** https://github.com/paulmillr/chokidar

## 5. Testing Frameworks

### Unit Testing
**Tool:** `jest` (v30.1.1)
- **Maintainer:** Meta (Facebook) Open Source
- **Purpose:** Delightful JavaScript Testing framework
- **Why Reuse:** Industry standard testing framework with excellent TypeScript support
- **Usage:** Unit tests, integration tests, test coverage analysis
- **Last Updated:** 3 days ago
- **License:** MIT
- **Official Site:** https://jestjs.io

### API Testing
**Tool:** `supertest` (Standard choice)
- **Maintainer:** VisionMedia
- **Purpose:** HTTP endpoint testing library
- **Why Reuse:** De facto standard for testing HTTP APIs
- **Usage:** API endpoint testing, integration testing
- **License:** MIT

## 6. Documentation Processing

### Markdown Parsing
**Tool:** `marked` (v16.2.1)
- **Maintainer:** marked.js organization
- **Purpose:** Fast markdown parser and compiler
- **Why Reuse:** One of the fastest and most popular markdown parsers
- **Usage:** README parsing, documentation analysis, business context extraction
- **Last Updated:** 3 days ago
- **License:** MIT
- **Official Site:** https://marked.js.org

## 7. Databases & Storage

### Graph Database
**Tool:** FalkorDB (Official Docker image)
- **Maintainer:** FalkorDB organization
- **Purpose:** Redis-compatible graph database
- **Why Reuse:** High-performance graph database with Cypher support
- **Usage:** Knowledge graph storage and querying
- **License:** Redis Source Available License
- **Official Site:** https://falkordb.com

### Vector Database
**Tool:** Qdrant (Official Docker image)
- **Maintainer:** Qdrant organization
- **Purpose:** Vector similarity search engine
- **Why Reuse:** Fast, scalable vector search with metadata filtering
- **Usage:** Semantic code search, embedding storage and retrieval
- **License:** Apache-2.0
- **Official Site:** https://qdrant.tech

### Relational Database
**Tool:** PostgreSQL (Official Docker image)
- **Maintainer:** PostgreSQL Global Development Group
- **Purpose:** Advanced open source relational database
- **Why Reuse:** Robust, feature-rich database with JSON support
- **Usage:** Document storage, structured data, metadata storage
- **License:** PostgreSQL License
- **Official Site:** https://postgresql.org

## 8. Containerization & Orchestration

### Container Runtime
**Tool:** Docker (Official)
- **Maintainer:** Docker Inc.
- **Purpose:** Containerization platform
- **Why Reuse:** Industry standard for containerization
- **Usage:** Application containerization, service isolation
- **License:** Apache-2.0
- **Official Site:** https://docker.com

### Container Orchestration
**Tool:** Docker Compose (Official)
- **Maintainer:** Docker Inc.
- **Purpose:** Multi-container application definition and orchestration
- **Why Reuse:** Simple, effective orchestration for development
- **Usage:** Local development environment, service coordination
- **License:** Apache-2.0

## 9. Development Tools

### Package Management
**Tool:** `pnpm` (Official)
- **Maintainer:** pnpm organization
- **Purpose:** Fast, disk-efficient package manager
- **Why Reuse:** Modern alternative to npm with better performance
- **Usage:** Dependency management, workspace management
- **License:** MIT
- **Official Site:** https://pnpm.io

### Type Checking
**Tool:** TypeScript Compiler (Official)
- **Maintainer:** Microsoft
- **Purpose:** TypeScript compilation and type checking
- **Why Reuse:** Official TypeScript compiler
- **Usage:** Type checking, compilation, declaration file generation
- **License:** Apache-2.0
- **Official Site:** https://typescriptlang.org

## 10. Build Tools & Automation

### Task Running
**Tool:** `tsx` (Modern alternative to ts-node)
- **Maintainer:** esbuild organization
- **Purpose:** TypeScript execution and REPL
- **Why Reuse:** Fast TypeScript execution with esbuild
- **Usage:** Development scripts, testing, debugging
- **License:** MIT

## Tool Selection Criteria

All tools were selected based on these criteria:

### ✅ Industry Standard
- Widely adopted in the JavaScript/TypeScript ecosystem
- Recommended by official documentation
- Used by major companies and projects

### ✅ Active Maintenance
- Regular updates and security patches
- Active community and issue resolution
- Long-term support commitment

### ✅ Official & Trusted
- Official packages from maintainers
- Verified publishers and maintainers
- Security audited and trusted

### ✅ Performance & Reliability
- High performance and low resource usage
- Stable APIs and backward compatibility
- Production-ready and battle-tested

### ✅ Ecosystem Integration
- Good integration with other selected tools
- Rich plugin and extension ecosystem
- TypeScript support and type definitions

## Implementation Impact

### What We Can Reuse (Instead of Building):

1. **AST Parsing:** ts-morph + tree-sitter
2. **File Watching:** chokidar
3. **Web Server:** fastify
4. **Security Scanning:** eslint-plugin-security
5. **Testing:** jest + supertest
6. **Documentation Parsing:** marked
7. **MCP Integration:** @modelcontextprotocol/sdk

### What We Still Need to Build:

1. **Knowledge Graph Logic:** Business logic for graph operations
2. **Synchronization Engine:** File change to graph update coordination
3. **Embedding Generation:** Custom embedding logic for code
4. **MCP Tools:** Custom tool implementations for our domain
5. **Integration Orchestration:** Coordinating all components

## Maintenance & Updates

### Tool Monitoring Strategy:
- **Weekly:** Check for security updates via `pnpm audit`
- **Monthly:** Review release notes for major version updates
- **Quarterly:** Evaluate new tools that might provide better functionality

### Update Process:
1. **Security Updates:** Apply immediately when available
2. **Patch Updates:** Apply within 1-2 weeks
3. **Minor Updates:** Apply within 1 month
4. **Major Updates:** Evaluate compatibility and plan migration

This approach ensures we leverage the best of the ecosystem while focusing our development effort on the unique value proposition of Memento.
</file>

<file path="Docs/RateLimitingArchitecture.md">
# Architecture: Rate Limiting Module

This document describes the architecture and design decisions behind the API rate limiting module.
It outlines the components, data flow, and how metrics are collected.

## Key Components

- Token buckets stored per-identity
- Refill scheduler and bucket stores
- Monitoring utilities

## Implementation Notes

- The function `getRateLimitStats` provides a snapshot of current rate limiting statistics
  (total buckets, active buckets, oldest/newest refill timestamps).
- Buckets are refilled based on the configured window and max requests.
- Hooks expose lightweight introspection for health and metrics.

## Dependencies

- Fastify middleware integration
- In-memory stores with optional persistence

## Non-Goals

- This module does not handle authentication or authorization; it focuses solely on rate control.
</file>

<file path="Docs/Blueprints/falkor-query-alignment.md">
# Falkor Query Alignment Blueprint

## 1. Overview
Knowledge graph queries now support richer symbol taxonomy (graph node `type` versus `kind`) and enhanced evidence metadata. The REST layer and unit tests already translate legacy inputs such as `type=function` into the `{ type: 'symbol', kind: 'function' }` shape that `KnowledgeGraphService.listEntities` consumes, so baseline filtering paths have stabilised. The remaining work focuses on consolidating that mapping logic, documenting the contract, and finishing the surrounding consistency fixes (metadata, counters, error contracts) so every Falkor-facing surface behaves predictably.

## 2. Current Gaps
- `KnowledgeGraphService.listEntities` (src/services/KnowledgeGraphService.ts:11438-11516) now accepts `type`/`kind`, but the translation from legacy query params lives in two places: the REST handler (`src/api/routes/graph.ts:1080-1116`) and several test helpers. Without a shared helper or documented mapping table, future changes risk reintroducing drift.
- Relationship merge pipeline (src/services/KnowledgeGraphService.ts:3120-3184) now feeds `mergeEdgeLocations` with concatenated history, so `metadata.locations` returns multiple path/line entries. Legacy callers (e.g., unit test `createRelationship > merges incoming code edges…`) assume a single canonical location (earliest line). We need to define whether metadata should surface primary-only, multi-location, or both representations.
- MCP `graph.examples` handler (src/api/mcp-router.ts:1067-1114) reports `totalExamples` as usage + test counts, but tests assume it only reflects `usageExamples`. Clarify the intended semantics and update clients/tests accordingly.
- Test fixtures for Falkor-adjacent services (graph routes, MCP router, Backup restore dry-run) stub out database interactions with the old resolve/throw contract. Implementation now prefers structured error objects. Without harmonising these expectations, unit tests will continue failing even after code fixes.

## 3. Desired Capabilities
1. A single, documented mapping from external filters (`type`, `entityTypes`, `symbolKind`) to Falkor query parameters that both REST and service-level callers reuse.
2. Stable response metadata for relationships: clearly defined primary location plus optional historical locations so prior consumers remain unaffected.
3. Consistent result counters (`totalExamples`, `totalUsageExamples`, `totalTestExamples`) across MCP and REST APIs.
4. Error handling contract for graph/backup operations that either throws or returns structured errors—chosen once and enforced across tests, mocks, and production code.

## 4. Workstreams
### 4.1 Filter & Parameter Normalisation
- Extract a shared helper (e.g., `mapEntityFilters`) that receives REST/MCP inputs and returns `{ type, kind, language, tags }`, replacing the ad-hoc translation in `graph.ts` and duplicated test utilities.
- Publish a mapping table (in docs and shared types) so SDKs know how legacy inputs such as `type=function` map to `{ type: 'symbol', kind: 'function' }` and keep it versioned.
- Patch mocks (`tests/test-utils/mock-db-service.ts`, etc.) to call the helper so fixtures mirror production behaviour when new taxonomy values land.

### 4.2 Relationship Metadata Strategy
- Define desired shape: (a) keep earliest location as `metadata.primaryLocation`, (b) expose historical array under `metadata.locations`, and (c) keep `evidence` unchanged.
- Implement helper in `KnowledgeGraphService` that derives `primaryLocation` (earliest by line & column) and trims `metadata.locations` if legacy callers require single entry.
- Add regression tests validating merge scenarios (existing + new evidence, same path different line, multiple files).

### 4.3 Result Counting Semantics
- Align `getEntityExamples` return type so `totalExamples` reflects documented behaviour. Decide whether it equals `usageExamples.length` or `usage + test` and update both handler and tests.
- Audit other counters (e.g., `KnowledgeGraphService.listEntities` total) for parity between service and API surfaces.

### 4.4 Error Contract Harmonisation
- Pick a global approach: either throw for unrecoverable states (missing backup) or return `{ success: false, error }`. Update `BackupService.restoreBackup`, graph endpoints, and tests to enforce this contract.
- Extend mock DB/test helpers to mirror new behaviours, preventing drift between implementation and unit expectations.

### 4.5 Test & Tooling Updates
- Update unit tests (`tests/unit/services/KnowledgeGraphService.test.ts`, `tests/unit/api/routes/graph.test.ts`, `tests/unit/api/mcp-router.test.ts`, `tests/unit/services/BackupService.test.ts`) to assert against the normalised payloads.
- Add new test cases covering mixed `type/kind` filtering and multi-location metadata so future changes surface regressions early.

## 5. Risks & Mitigations
| Risk | Mitigation |
| --- | --- |
| Breaking existing clients that rely on legacy `type=function` contract | Keep the translation helper accepting legacy strings, document the new taxonomy, and advertise a deprecation window before removing shims. |
| Confusion over location metadata semantics | Document primary vs historical fields and keep tests asserting both. |
| Touching multiple modules increases regression surface | Incremental workstream rollout with dedicated test coverage per subsystem (service, REST, MCP, backup). |
| Mock/test drift recurring | Centralise shared helpers for mapping and error handling used by both production code and test doubles. |

## 6. Milestones
1. Finalise mapping + helper, update service/API/tests (filter normalisation).
2. Implement metadata strategy + regression tests (relationship merge).
3. Settle counter semantics and align MCP/REST handlers/tests.
4. Harmonise error contract across backup & graph paths.
5. Documentation refresh and announce changes to consuming teams.

## 7. Open Questions
- Should legacy query parameters like `type=function` continue to be accepted indefinitely, or should we introduce a versioned API path?
- Do clients need both `metadata.locations` and a separate `primaryLocation`, or can we retrofit UI components to use the new array?
- Are there downstream analytics relying on `totalExamples === usageExamples.length` that would need migration support?
</file>

<file path="Docs/Blueprints/maintenance-operations.md">
# Maintenance Operations Blueprint

## 1. Overview
Admin maintenance endpoints (backup, restore, housekeeping) now coordinate FalkorDB, Qdrant, PostgreSQL, and pluggable storage providers via a centralized control plane. Backups store structured metadata in PostgreSQL, emit audit events through the shared logging service, and drive a two-step restore workflow (preview token → confirmation) that enforces dry-run validation and optional secondary approval before destructive changes are applied.

## 2. Current Gaps
- Unit coverage now exercises the preview/confirm flow, in-memory storage providers, retention pruning, and approval gating (see 2025-09-19 refresh), but we still lack end-to-end integration runs against real Docker services and remote storage adapters.
- Retention policy logic is only asserted via stubbed PostgreSQL responses; we still need an integration test that verifies pruning against a live `maintenance_backups` table and artifact cleanup.
- Only the local filesystem storage provider ships today. Remote providers (S3/GCS) and streaming restore remain TODO, limiting scalability for large datasets.
- Restore execution still stubs parts of the workflow (e.g., Qdrant snapshot replay merely verifies presence). We need real vendor API integrations to guarantee fidelity.
- Structured logging is in place, but there is no out-of-the-box metrics/alerting pipeline. SREs still have to wire dashboards and alerts manually.

## 3. Desired Capabilities
1. Extend unit/integration tests to cover preview token issuance, integrity failures, approval gating, and confirmation flows; add regression coverage for new admin endpoints.
2. Implement remote storage adapters (S3, GCS) with resumable upload/download and streaming restores for large backups.
3. Replace placeholder restore logic for Qdrant and other subsystems with real API-driven restores and reconciliation diffs.
4. Export maintenance telemetry to metrics/alerting stacks (Prometheus/OpenTelemetry) and document dashboard recipes for ops teams.
5. Broaden dependency readiness checks beyond DatabaseService by probing downstream services and surfacing structured failure codes to tooling/CLI clients.

## 4. Metrics & Observability
- **Endpoints**: Maintenance telemetry is exposed via `GET /maintenance/metrics` (JSON summary) and `GET /maintenance/metrics/prometheus` (Prometheus text format). The existing `/metrics` response now embeds the maintenance summary for consolidated dashboards.
- **Counters**:
  - `maintenance_backup_total{status,provider,type}` tracks backups by outcome, storage provider, and full vs. incremental mode.
  - `maintenance_restore_total{mode,status,requires_approval,provider}` captures both preview and apply phases alongside approval requirements.
  - `maintenance_task_total{task_type,status}` surfaces housekeeping/reindex/validation cadence and failure hotspots.
  - `maintenance_restore_approvals_total{status}` measures human approval throughput for audit/SOX reporting.
- **Histograms**:
  - `maintenance_backup_duration_seconds` and `maintenance_restore_duration_seconds` provide SLO-ready latency buckets (1s → 30m) for burn-rate alerts.
- **Gauges**:
  - `maintenance_metrics_age_seconds` reports exporter freshness; alert if it exceeds the scrape interval.
- **Dashboard recipe**:
  1. Success rate: `sum(rate(maintenance_backup_total{status="success"}[5m])) / sum(rate(maintenance_backup_total[5m]))`.
  2. Restore p95 latency: `histogram_quantile(0.95, sum by (le) (rate(maintenance_restore_duration_seconds_bucket[5m])))`.
  3. Outstanding approvals: `sum(maintenance_restore_total{status="failure",requires_approval="true"}) - sum(maintenance_restore_approvals_total{status="approved"})`.
- **Alerting cues**: Page on backup failure rate >5% over 15m, warn if approvals drop below one per hour during scheduled windows, and ensure exporter age stays <2 scrape intervals.

## 5. Decisions & Open Questions
- **Backup metadata store**: centralized in PostgreSQL (`maintenance_backups`) with transactional updates and queryable history.
- **Destructive restore guardrails**: enforced through RBAC scopes plus the two-step (preview + confirm) workflow; optional dual-approval is controlled via service policy.
- **Structured PostgreSQL artifacts**: backups now emit `*_postgres.json` alongside the schema-only `*_postgres.sql`. The JSON payload captures ordered column metadata, primary keys, and sanitized row data so restores can replay via parameterised inserts rather than brittle SQL strings. Legacy `.sql` dumps are still honoured as a fallback for older artifacts.
- **Legacy migration**: not required yet—the code has not produced production backups. Future schema/storage changes can skip migration of historical payloads.
- **Observability backend**: Standardise on Prometheus for maintenance KPIs (with optional OTLP fan-out downstream). Retention/expiration policies on `maintenance_backups` remain under evaluation as backup cadence scales.
</file>

<file path="Docs/Blueprints/source-control-management.md">
# Source Control Management Blueprint

## 1. Overview
The SCM surface is responsible for automating commit creation, pull request scaffolding, branch management, and diff/log retrieval so AI-driven workflows can land code safely. Today the Fastify router exposes `/api/v1/scm/*` endpoints, but most handlers are stubs that return `501 NOT_IMPLEMENTED`.

## 2. Current State & Gaps (2025-09-19)
- `/api/v1/scm/commit-pr` always responds with `{ success: false, error: { code: "NOT_IMPLEMENTED" } }`. There is no orchestration over git, no commit metadata persistence, and no graph integration beyond the tests seeding entities.
- Related endpoints (`/scm/commit`, `/scm/push`, `/scm/branch`, `/scm/status`, etc.) share the same stub handler. They provide neither validation nor structured responses beyond the generic 501 envelope.
- Integration tests previously assumed successful commits and branch management, masking the fact that no SCM automation exists. The suite now asserts the 501 envelope instead, ensuring we notice when real functionality lands.
- A basic async git wrapper already exists (`src/services/GitService.ts`) and powers diff/stat helpers, but the HTTP routes do not call it yet and there are no provider adapters (GitHub, GitLab) or persistence hooks for commit metadata.

## 3. Interim Behaviour
- Clients should treat the SCM APIs as non-functional placeholders. The only contract guaranteed at the moment is the structured error payload (`code: NOT_IMPLEMENTED`, descriptive `message`, `success: false`).
- Tests exercise the endpoints purely to ensure the gateway wiring and error formatting remain stable.

## 4. Next Steps
1. Layer orchestration on top of the existing `GitService` (staging, commit, diff helpers) and introduce provider adapters that can push branches and open PRs/merge requests.
2. Model commit metadata in Postgres and link produced commits back into the knowledge graph so downstream analytics can reason about code provenance.
3. Introduce request validation schemas for `/scm/commit-pr` to reject malformed payloads before orchestration runs.
4. Replace the 501 stubs with real handlers behind a feature flag, migrate integration tests to exercise the full workflow, then retire the fallback assertions.
</file>

<file path="Docs/Blueprints/websocket-integration.md">
# WebSocket Integration Blueprint

## 1. Overview
The WebSocket router (`src/api/websocket-router.ts`) delivers real-time graph and file system events to connected clients. Integration coverage now exercises connection concurrency, subscription state introspection, and broadcast fan-out across multiple clients. To keep the channel useful for IDEs and background agents we need to tighten the protocol contract and address replay semantics surfaced while hardening the tests.

## 2. Current Gaps
- **Replay ambiguity**: `handleSubscription` immediately replays the last cached event per type. The replay uses the exact same envelope as live updates (no `isReplay`, sequence, or timestamp delta), so consumers cannot distinguish historical payloads from fresh broadcasts. In the new broadcast test this surfaced as stale `entity_created` events being treated as the target notification.
- **Inconsistent envelopes**: Subscription acknowledgements populate both a top-level `event` field (via `@ts-ignore`) and `data.event`. Other messages (errors, list responses) embed payloads under `data` without top-level mirroring. Client implementations must special-case each message type.
- **No general broadcast surface**: The router now exposes `broadcastCustomEvent` for arbitrary fan-out, but we still need to document the API, harden authentication, and wire it through higher-level services so callers don’t have to reach into the router directly.
- **Health routes only expose counts**: `/ws/health` reports counts but not subscription metadata (filters, replay timestamps). Troubleshooting noisy subscriptions or replay storms still requires manual instrumentation.
- **Limited subscription introspection**: Filters are now normalized and stored per subscription so the router can enforce path/type semantics, but we still lack an operator view of the active filters. Neither `/ws/health` nor any admin endpoint surfaces which paths/extensions are hot, making it hard to reason about noisy clients.

## 3. Proposed Enhancements
1. Add a stable envelope that includes a monotonic `sequenceId`, original `emittedAt`, and an `isReplay` flag. Allow subscribers to opt out of warm replays when they intend to handle only future events.
2. Normalize message schemas so every response contains `{ type, id?, data }` and avoid sprinkling top-level protocol extensions. Provide TypeScript types for the wire contract and export them for client SDKs.
3. Introduce a server-side broadcast API (e.g., `broadcastCustomEvent` exposed through the gateway or a privileged websocket message) that forwards arbitrary event types. Gate it behind authentication/role checks.
4. Extend `/ws/health` (or a new `/ws/subscriptions`) to list active subscriptions with event type, filter, and last activity/replay metadata. Now that filters are normalized server-side we can safely expose aggregates (e.g., watched directories, change-type counts) to audit replay pressure and diagnose stuck clients.

## 4. Open Questions
- Should replay behaviour vary per event type (e.g., graph changes replay, file changes skip)?
- Do we need per-connection backpressure metrics to defend against slow clients before exposing general broadcasts?
- What authentication model should govern custom broadcast access (API key scopes vs. internal service tokens)?
</file>

<file path="Docs/MementoArchitecture.md">
# Memento Architecture & Technology Stack

## System Overview

Memento is a **local-first AI coding assistant** that provides comprehensive codebase awareness through a knowledge graph system. Designed primarily for local development environments, it integrates documentation, testing, performance monitoring, and security analysis to enable intelligent code understanding and generation.

### Key Characteristics
- **Local-First**: Runs entirely on developer machines using Docker
- **AI-Native**: Built for AI coding assistants (Claude, GPT, etc.)
- **Knowledge Graph**: Maintains comprehensive codebase understanding
- **Multi-Protocol**: Supports MCP, REST, and WebSocket interfaces
- **Developer-Centric**: Optimized for individual and small team workflows

### Why Local-First Architecture?

#### Benefits for AI Coding Assistants
- **Instant Response**: No network latency for code analysis
- **Privacy**: Code never leaves the developer's machine
- **Offline Capability**: Works without internet connection
- **Full Context**: Direct access to entire codebase and dependencies
- **Cost-Effective**: No cloud infrastructure costs for individual developers

#### Perfect for Development Workflows
- **IDE Integration**: Seamless connection with local editors
- **File Watching**: Real-time updates as you code
- **Git Integration**: Works with local Git repositories
- **Multi-Project**: Easy switching between different projects
- **Resource Efficient**: Uses only necessary resources on local machine

## Core Architecture

```
┌─────────────────────────────────────────────────────────────────────┐
│                          Client Layer                                │
│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐         │
│  │   Claude Code   │ │   OpenAI       │ │   VS Code       │         │
│  │   (MCP)         │ │   (HTTP)       │ │   (WebSocket)   │         │
│  └─────────────────┘ └─────────────────┘ └─────────────────┘         │
└─────────────────────────────────────────────────────────────────────┘
                                   │
                                   ▼
┌─────────────────────────────────────────────────────────────────────┐
│                         API Gateway Layer                           │
│  ┌─────────────────┐ ┌─────────────────┐         │
│  │   MCP Server    │ │   REST API     │         │
│  │   (Local)       │ │   (Local)      │         │
│  └─────────────────┘ └─────────────────┘         │
└─────────────────────────────────────────────────────────────────────┘
                                   │
                                   ▼
┌─────────────────────────────────────────────────────────────────────┐
│                       Service Layer                                 │
│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐         │
│  │   Knowledge     │ │   Test         │ │   Security      │         │
│  │   Graph         │ │   Engine       │ │   Scanner       │         │
│  │   Service       │ │                 │ │                 │         │
│  └─────────────────┘ └─────────────────┘ └─────────────────┘         │
└─────────────────────────────────────────────────────────────────────┘
                                   │
                                   ▼
┌─────────────────────────────────────────────────────────────────────┐
│                      Data Storage Layer                             │
│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐         │
│  │   Graph DB      │ │   Vector DB     │ │   Document      │         │
│  │   (Docker)      │ │   (Docker)      │ │   Store         │         │
│  │   FalkorDB      │ │   Qdrant        │ │   (Docker)      │         │
│  └─────────────────┘ └─────────────────┘ └─────────────────┘         │
└─────────────────────────────────────────────────────────────────────┘
                                   │
                                   ▼
┌─────────────────────────────────────────────────────────────────────┐
│                      Infrastructure Layer                           │
│  ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐         │
│  │   File System   │ │   Git           │ │   Docker        │         │
│  │   (Local)       │ │   (Local)       │ │   Compose       │         │
│  └─────────────────┘ └─────────────────┘ └─────────────────┘         │
└─────────────────────────────────────────────────────────────────────┘
```

## Technology Stack Decisions

### Core Runtime
- **Language**: TypeScript (Node.js)
- **Runtime**: Node.js 18+ (LTS)
- **Package Manager**: pnpm (for workspace management)

### Databases & Storage
- **Graph Database**: FalkorDB (Redis-based graph database)
  - **Why**: High performance, Redis compatibility, Cypher support
  - **Alternatives Considered**: Neo4j, Amazon Neptune
- **Vector Database**: Qdrant
  - **Why**: Fast similarity search, metadata filtering, horizontal scaling
  - **Alternatives Considered**: Pinecone, Weaviate
- **Document Store**: PostgreSQL with JSONB
  - **Why**: Structured data with flexible schemas, ACID compliance
  - **Alternatives Considered**: MongoDB, DynamoDB

### Frameworks & Libraries

#### Backend Framework
- **Web Framework**: Fastify
  - **Why**: High performance, plugin ecosystem, TypeScript support
  - **Alternatives Considered**: Express, Koa

#### Graph & Data Processing
- **Graph Processing**: Cypher (FalkorDB)
- **AST Parsing**: TypeScript Compiler API + ts-morph
- **Vector Embeddings**: OpenAI Ada-002 or local models (Transformers.js)
- **File Watching**: chokidar

#### External Integrations
- **LLM Integration**: OpenAI API + Anthropic Claude API
- **Git Integration**: isomorphic-git
- **Security Scanning**: ESLint security plugin, Semgrep, Snyk
- **Test Frameworks**: Jest, Vitest (auto-detection)

### Infrastructure & Deployment

#### Containerization
- **Container Runtime**: Docker
- **Orchestration**: Docker Compose (development) / Kubernetes (production)
- **Base Images**: Node.js Alpine Linux

#### Deployment Options

##### Primary: Local Development (Recommended)
- **Containerized**: Docker Compose for all services
- **Databases**: Local Docker containers for FalkorDB, Qdrant, PostgreSQL
- **IDE Integration**: Direct WebSocket/HTTP connections
- **File Access**: Direct filesystem access for code analysis

##### Optional: Cloud Deployment (Enterprise/Team Features)
- **Cloud Provider**: AWS/GCP/Azure (when scaling needed)
- **Compute**: ECS Fargate / EKS / Cloud Run
- **Storage**: RDS PostgreSQL, ElastiCache Redis, OpenSearch
- **CDN**: CloudFront / Cloud CDN (for distributed teams)
- **Monitoring**: CloudWatch / Cloud Monitoring

#### Development Environment
- **Local Development**: Docker Compose (primary workflow)
- **IDE Integration**: VS Code extensions, direct API connections
- **Package Management**: pnpm workspaces
- **Hot Reload**: Development mode with file watching

## Component Architecture

### 1. MCP Server Component

```typescript
// src/mcp/server.ts
import { Server } from '@modelcontextprotocol/sdk';

class MementoMCPServer extends Server {
  private knowledgeGraph: KnowledgeGraphService;
  private testEngine: TestEngine;
  private securityScanner: SecurityScanner;

  async handleToolCall(toolName: string, params: any): Promise<any> {
    switch (toolName) {
      case 'design.create_spec':
        return this.knowledgeGraph.createSpec(params);
      case 'tests.plan_and_generate':
        return this.testEngine.planTests(params);
      case 'graph.search':
        return this.knowledgeGraph.search(params);
      // ... other tools
    }
  }
}
```

### 2. Knowledge Graph Service

```typescript
// src/services/KnowledgeGraphService.ts
import { FalkorDB } from 'falkordb';
import { QdrantClient } from '@qdrant/js-client-rest';

class KnowledgeGraphService {
  private graphDb: FalkorDB;
  private vectorDb: QdrantClient;
  private fileWatcher: FileWatcher;

  async initialize(): Promise<void> {
    await this.graphDb.connect(process.env.FALKORDB_URL);
    await this.vectorDb.connect(process.env.QDRANT_URL);
    this.setupFileWatcher();
  }

  async createEntity(entity: CodebaseEntity): Promise<void> {
    // Create graph node
    await this.graphDb.query(`
      CREATE (e:${entity.type} {id: $id, path: $path, ...})
    `, entity);

    // Create vector embedding
    const embedding = await this.generateEmbedding(entity);
    await this.vectorDb.upsert(entity.id, embedding);

    // Update relationships
    await this.updateRelationships(entity);
  }
}
```

### 3. API Gateway

```typescript
// src/api/gateway.ts
import Fastify from 'fastify';
import { MCPRouter } from './mcp-router';
import { RestRouter } from './rest-router';

class APIGateway {
  private app: FastifyInstance;
  private mcpRouter: MCPRouter;
  private restRouter: RestRouter;

  async start(port: number): Promise<void> {
    // MCP endpoint
    this.app.register(this.mcpRouter.register, { prefix: '/mcp' });

    // REST API
    this.app.register(this.restRouter.register, { prefix: '/api/v1' });

    await this.app.listen({ port });
  }
}
```

## Data Flow Architecture

### Code Analysis Pipeline

```
File Change → File Watcher → Queue → Parser → Knowledge Graph → Vector DB
       ↓           ↓           ↓       ↓           ↓           ↓
  Detect Change → Debounce → Prioritize → AST → Create/Update → Embed
```

### Query Processing Flow

```
Query → API Gateway → Service Layer → Graph DB → Vector DB → Response
   ↓         ↓            ↓            ↓         ↓          ↓
Parse → Route → Validate → Cypher → Similarity → Format
```

### Synchronization Flow

```
Code Change → Change Detection → Impact Analysis → Update Graph → Update Vectors → Notify Clients
     ↓              ↓                ↓              ↓             ↓            ↓
File Event → Compare States → Find Affected → Apply Changes → Re-embed → WebSocket
```

## Security Architecture

### Authentication & Authorization
- **JWT Tokens** for API authentication
- **API Keys** for service-to-service communication
- **Role-Based Access Control** (RBAC)
- **OAuth 2.0** integration with GitHub/GitLab

### Data Protection
- **Encryption at Rest**: AES-256 for database storage
- **Encryption in Transit**: TLS 1.3 for all communications
- **Secret Management**: AWS Secrets Manager / HashiCorp Vault
- **Data Sanitization**: Input validation and SQL injection prevention

### Security Scanning Integration
- **SAST**: Static Application Security Testing
- **SCA**: Software Composition Analysis
- **Container Scanning**: Vulnerability scanning for Docker images
- **Secrets Detection**: Automated detection of exposed credentials

## Performance Architecture

### Caching Strategy
- **Application Cache**: Redis for frequently accessed data
- **Query Cache**: Graph query result caching
- **Embedding Cache**: Vector embedding caching
- **CDN**: Static asset caching

### Local Scaling Strategy
- **Resource Allocation**: Adjust Docker container memory/CPU based on project size
- **Database Optimization**: Configure FalkorDB/Qdrant for local hardware specs
- **Caching Strategy**: Optimize Redis caching for local performance
- **Concurrent Processing**: Handle multiple AI assistant sessions efficiently

### Optional Cloud Scaling (Enterprise)
- **Horizontal Scaling**: Multiple service instances across availability zones
- **Database Scaling**: Read replicas and clustering for graph/vector databases
- **Load Balancing**: Application Load Balancer with auto-scaling
- **CDN Integration**: Global distribution for distributed teams

### Performance Targets
- **API Response Time**: < 200ms for simple queries
- **Graph Query Time**: < 500ms for complex traversals
- **Vector Search Time**: < 100ms for similarity searches
- **File Sync Time**: < 5 seconds for typical file changes
- **Concurrent Users**: Support 1000+ simultaneous connections

## Monitoring & Observability

### Metrics Collection
- **Application Metrics**: Response times, error rates, throughput
- **System Metrics**: CPU, memory, disk usage
- **Business Metrics**: API usage, user engagement
- **Graph Metrics**: Node/relationship counts, query performance

### Logging Strategy
- **Structured Logging**: JSON format with correlation IDs
- **Log Levels**: ERROR, WARN, INFO, DEBUG
- **Log Aggregation**: Centralized logging with ELK stack
- **Audit Logging**: Security events and data access

### Alerting
- **Performance Alerts**: Response time degradation
- **Error Alerts**: Increased error rates
- **Security Alerts**: Suspicious activities
- **Capacity Alerts**: Resource utilization thresholds

## Development Workflow

### Local Development
```bash
# Start development environment
docker-compose up -d

# Run tests
pnpm test

# Build and run
pnpm build && pnpm start

# Run with hot reload
pnpm dev
```

### CI/CD Pipeline
```yaml
# .github/workflows/ci.yml
name: CI/CD
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: pnpm/action-setup@v2
      - run: pnpm install
      - run: pnpm test
      - run: pnpm build
      - run: docker build -t memento-local .

  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: pnpm audit
      - run: semgrep --config=auto

  integration-test:
    needs: [test, security]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: docker-compose up -d
      - run: sleep 30  # Wait for services to start
      - run: pnpm test:integration
      - run: docker-compose down

# Optional: Cloud deployment for enterprise use
  deploy-cloud:
    needs: [integration-test]
    if: github.ref == 'refs/heads/main' && contains(github.event.head_commit.message, '[deploy]')
    runs-on: ubuntu-latest
    steps:
      - uses: aws-actions/configure-aws-credentials@v2
      - run: aws ecs update-service --cluster memento --service memento-api --force-new-deployment
```

## Deployment Architecture

### Local Development Environment (Primary)
```yaml
# docker-compose.yml
version: '3.8'
services:
  memento-api:
    build: .
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
    volumes:
      - .:/app
      - /app/node_modules
    depends_on:
      - falkordb
      - qdrant
      - postgres

  falkordb:
    image: falkordb/falkordb:latest
    ports:
      - "6379:6379"

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"

  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=memento
      - POSTGRES_USER=memento
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
```

### Production Environment
```yaml
# Kubernetes deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: memento-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: memento-api
  template:
    metadata:
      labels:
        app: memento-api
    spec:
      containers:
      - name: memento-api
        image: memento/memento-api:latest
        ports:
        - containerPort: 3000
        env:
        - name: NODE_ENV
          value: "production"
        - name: FALKORDB_URL
          valueFrom:
            secretKeyRef:
              name: memento-secrets
              key: falkordb-url
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
```

## Directory Structure

```
memento/
├── src/
│   ├── api/                    # API endpoints and routers
│   │   ├── mcp-router.ts      # MCP server for Claude integration
│   │   ├── rest-router.ts     # REST API endpoints
│   │   ├── websocket-router.ts # Real-time WebSocket connections
│   │   └── middleware/        # Request validation, logging
│   ├── services/              # Core business logic services
│   │   ├── KnowledgeGraphService.ts
│   │   ├── TestEngine.ts
│   │   ├── SecurityScanner.ts
│   │   ├── DocumentationParser.ts
│   │   └── FileWatcher.ts
│   ├── models/                # Data models and schemas
│   │   ├── entities.ts        # Graph node type definitions
│   │   ├── relationships.ts   # Graph relationship definitions
│   │   └── types.ts           # TypeScript type definitions
│   ├── utils/                 # Utility functions and helpers
│   │   ├── ast-parser.ts      # TypeScript AST parsing
│   │   ├── embedding.ts       # Vector embedding generation
│   │   ├── git-integration.ts # Git repository operations
│   │   └── validation.ts      # Input validation helpers
│   └── index.ts               # Application entry point
├── tests/                     # Test suites and fixtures
│   ├── unit/                  # Unit tests
│   ├── integration/           # Integration tests
│   └── fixtures/              # Test data and mocks
├── docs/                      # Documentation and guides
├── docker/                    # Docker configuration
│   ├── Dockerfile             # Main application container
│   ├── docker-compose.yml     # Local development stack
│   └── docker-compose.test.yml # Testing environment
├── scripts/                   # Development and deployment scripts
│   ├── setup-dev.sh          # Development environment setup
│   ├── sync-knowledge-graph.sh # Manual graph synchronization
│   └── health-check.sh       # System health verification
├── config/                    # Configuration files
│   ├── default.json           # Default configuration
│   └── development.json       # Development overrides
├── package.json
├── tsconfig.json
├── eslint.config.js
└── README.md
```

## Technology Selection Rationale

### Why TypeScript?
- **Type Safety**: Prevents runtime errors through compile-time checking
- **Developer Experience**: Excellent IDE support and refactoring tools
- **Ecosystem**: Rich library ecosystem and community support
- **Node.js Compatibility**: Seamless integration with Node.js runtime

### Why FalkorDB over Neo4j?
- **Performance**: Redis-based architecture provides lower latency
- **Resource Usage**: Lighter memory footprint
- **Cypher Support**: Familiar query language
- **Cloud-Native**: Better containerization support

### Why Qdrant for Vector Search?
- **Performance**: Optimized for high-dimensional vector search
- **Metadata Filtering**: Advanced filtering capabilities
- **Horizontal Scaling**: Distributed architecture
- **API Compatibility**: REST and gRPC support

### Why Fastify over Express?
- **Performance**: Significantly faster than Express
- **Plugin Ecosystem**: Rich plugin architecture
- **TypeScript Support**: Built-in TypeScript definitions
- **Validation**: Built-in request/response validation

This architecture provides a solid foundation for building the Memento system, with clear separation of concerns, scalable components, and production-ready infrastructure patterns.
</file>

<file path="Docs/Blueprints/knowledge-graph-service.md">
# Knowledge Graph Service Blueprint

## 1. Overview
`KnowledgeGraphService` orchestrates graph persistence across FalkorDB and Qdrant while maintaining in-memory caches for entity lookups and search. Each instance derives a `NamespaceScope` so every CRUD path consistently applies the active namespace prefix, normalises relationship identifiers, and targets the correct Qdrant collections. Although the wider `DatabaseService` can supply PostgreSQL and Redis clients, the current implementation does not issue calls to those stores; they remain expansion points for future features.

## 2. Current State
- Namespaces are applied automatically on every entity and relationship read/write/delete path. Callers can provide raw IDs or pre-prefixed values and always hit the canonically namespaced records.
- Relationship canonicalisation now threads through FalkorDB operations, event payloads, and vector clean-up, so either legacy `rel_` or temporal `time-rel_` identifiers resolve to the same record.
- Entity upserts assign a common `Entity` label alongside specialised types, enabling FalkorDB queries that span code, documentation, and semantic clusters without schema drift.
- Temporal lifecycle helpers and embedding maintenance reuse the scoped namespace, ensuring Qdrant deletes operate on the same namespaced identifiers as graph mutations.

## 3. Known Gaps
- Redis namespace bindings exist (`GraphNamespaceConfig.redisPrefix` and `NamespaceScope.qualifyRedisKey`), but no cache keys are generated yet. We need either an integration plan for Redis-backed features or a decision to drop the surface area.
- Service consumers still rely on tribal knowledge for namespace expectations. We owe them explicit guidance on valid inputs, emitted IDs, and cache semantics.
- Namespace-aware tests cover entity fetches, but relationship pathways and Qdrant clean-up with mixed ID formats remain thin.

## 4. Desired Capabilities
1. Make namespace behaviour self-service: callers should never care whether IDs are raw or prefixed and should have documentation that reflects that guarantee.
2. Thread namespace metadata through secondary stores (Redis, future PostgreSQL helpers) from a single source of truth, or intentionally trim unused configuration.
3. Keep lifecycle utilities (`initialize`, `shutdown`, scoped helpers) straightforward so harnesses do not need to instantiate multiple services to flush state.
4. Ensure relationship canonicalisation and cache invalidation stay resilient as new edge types, temporal states, or aggregation jobs are introduced.

## 5. Immediate Follow-ups
- Decide on the Redis story: either wire `namespaceScope.qualifyRedisKey()` into the caches we plan to keep warm, or remove the prefix fields and document the rationale.
- Publish namespace contract documentation that explains caller expectations (input shapes, ID normalisation, cache behaviour) and link it from service entry points.
- Add regression coverage for relationship reads/deletes via raw IDs and ensure Qdrant deletions honour both raw and prefixed identifiers.

_(Updated: 2025-09-23 to reflect NamespaceScope rollout and resolved namespace gaps.)_

## 6. Recent Fixes
- **2025-09-19:** All nodes now receive a shared `Entity` label alongside their specific type. Database integration searches rely on this base label when correlating relational fixtures with graph fixtures.
- **2025-09-22:** Introduced `NamespaceScope` and routed every read/update/delete path through it, eliminating the need for callers to pre-prefix IDs.

## 7. Backlog
- [x] **Namespace scope utilities**: expose helpers (`resolveEntityId`, `resolveRelationshipId`, `qualifyRedisKey`, `qdrantCollectionFor`) and consume them across CRUD paths.
- [x] **Relationship operations alignment**: ensure `deleteRelationship`, `getRelationshipById`, cache invalidation, and event payloads use canonical namespaced IDs.
- [x] **Query normalisation sweep**: migrate `getRelationships`, traversal/impact queries, ingestion validation, and bulk upsert code paths to the namespace helpers.
- [ ] **Redis strategy decision**: drive cache key generation off `namespaceScope.qualifyRedisKey()` (including warm-path caches and invalidation) or remove the Redis prefix from `GraphNamespaceConfig` and document the migration.
- [ ] **Consistency guards**: extend unit + integration coverage for relationship reads/deletes and Qdrant payload clean-up using mixed ID formats.
- [ ] **Namespace contract docs**: publish developer guidance outlining caller expectations (inputs, outputs, cache semantics, migration playbook) and reference it from onboarding materials.

## 8. Test Coverage Notes
- Integration tests already validate entity CRUD via both raw and namespaced IDs; reuse those patterns for relationships, embeddings, and cache invalidation events.
- Add unit tests for `NamespaceScope` that cover relationship ID helpers and Redis key qualification, including empty inputs and already-prefixed values.
- Introduce Qdrant integration smoke tests that assert deletes succeed when triggered with raw IDs, proving the namespace helpers resolve to the stored payloads.

## 9. Open Questions
- When Redis-backed features arrive, do we need a cache-busting window for any existing keys, or can we safely assume new namespaces and key derivations? (Currently moot while Redis remains unused.)
- Should we continue deriving canonical relationship IDs from `from/to/type`, or move to hashed identifiers to guarantee stability if namespace prefixes change again?
</file>

<file path="Docs/Blueprints/session-relationships.md">
# Session Relationship Blueprint

## 1. Overview
Session relationships (`SESSION_MODIFIED`, `SESSION_IMPACTED`, `SESSION_CHECKPOINT`, `BROKE_IN`, `FIXED_IN`, `DEPENDS_ON_CHANGE`) model real-time collaborative sessions and incident remediation workflows. They capture ordered events, state transitions, and impacts during an editing or debugging session.

## 2. Current Gaps
- Session edges are buffered in `SynchronizationCoordinator` but persistence ignores session-specific fields such as `sessionId`, `sequenceNumber`, `changeInfo`, and `stateTransition`.
- Canonical IDs collide (based on `from|to|type`), causing multiple events per session to overwrite each other.
- Query APIs provide no way to reconstruct session timelines or filter by session metadata.
- Checkpoint integration is only partially wired—`SynchronizationCoordinator` creates checkpoint nodes inline, but there is no async job, metadata enrichment, or failure handling around `SESSION_CHECKPOINT` edges yet.
- The Source Control REST endpoints (`/api/v1/scm/*`) are placeholders that return HTTP 501. The REST integration suite expects commit/branch data sourced from the session graph, so we need an implementation that pipes session/SCM events into the knowledge graph (or a stubbed fixture) before these endpoints can pass.
- WebSocket session notifications are not wired up; connection keep-alives, file change broadcasts, and subscription teardown events never fire, causing all WebSocket integration tests to hit the "zero assertions" guard. We need a proper WebSocket adapter on top of `SynchronizationCoordinator` so session edges can stream to clients.

## 3. Desired Capabilities
1. Persist ordered session events with metadata capturing change details, state transitions, and impacts.
2. Ensure edges are unique per event (session + sequence) and maintain ordering guarantees.
3. Provide APIs to reconstruct session timelines, impacted entities, and checkpoints.
4. Link session checkpoints to knowledge graph snapshots for recovery and audit.
5. Support integration with history (temporal edges) and change management tools.

## 3.a Checkpoint Processing Lifecycle
- `SessionCheckpointJobRunner` enqueues checkpoint requests from `SynchronizationCoordinator` and processes them asynchronously. Jobs transition through `pending → completed` or `manual_intervention` states, and the current status is recorded on the originating session relationships via `metadata.checkpoint`.
- Successful jobs create the checkpoint node, persist `SESSION_CHECKPOINT` edges with enriched metadata (`reason`, `hopCount`, `jobId`, `seedEntityIds`), and register linkage with `RollbackCapabilities` for future recovery tooling.
- Failures after retry exhaustion mark the affected session edges with `metadata.checkpoint.status = manual_intervention` and surface the error context for operators.
- Job-level metrics (`enqueued`, `completed`, `failed`, `retries`) are exposed through the runner for observability dashboards.

## 4. Inputs & Consumers
- **Inputs**: SynchronizationCoordinator events, IDE integrations, manual annotations, incident response tooling.
- **Consumers**: History/Timeline UI, Rollback service, admin dashboards, analytics on session effectiveness, automation gating (e.g., auto-tests when session impacts critical entities).

## 5. Schema & Metadata Requirements
| Field | Type | Notes |
| --- | --- | --- |
| `sessionId` | string | Unique session identifier (UUID or deterministic). Required.
| `sequenceNumber` | integer | Monotonic order within session.
| `timestamp` | Date | Event time.
| `changeInfo` | object | `{ elementType, elementName, operation, semanticHash, affectedLines }`.
| `stateTransition` | object | `{ from, to, verifiedBy, confidence, criticalChange }`.
| `impact` | object | `{ severity, testsFailed, testsFixed, buildError, performanceImpact }`.
| `eventId` | string | Optional stable ID to dedupe cross-process ingestion.
| `actor` | string | Person/agent performing change.
| `metadata.annotations` | array | Additional labels or comments.

## 6. Normalization Strategy
1. **Helper `normalizeSessionRelationship`**:
   - Require `sessionId` and `sequenceNumber`; convert to canonical formats (lowercase, trim).
   - Ensure `sequenceNumber` is integer ≥ 0; log duplicates or out-of-order events.
   - Convert `timestamp` to Date; default to ingestion time if missing.
   - Validate nested objects (`changeInfo`, `stateTransition`, `impact`) and strip unexpected keys; enforce type constraints.
   - Promote `actor`, `eventId`, `annotations` from metadata; maintain sanitized JSON.
2. Maintain instrumentation tracking sequence violations; provide metrics for session quality.
3. Compute `siteHash` combining `sessionId` + `sequenceNumber` + `changeInfo` to support canonical ID.

## 7. Persistence & Merge Mechanics
1. **Canonical ID**: `rel_session_<sha1(sessionId|sequenceNumber|type)>`. Store `sessionId`/`sequenceNumber` as scalars.
2. **Cypher Projections**: Add columns for `sessionId`, `sequenceNumber`, `timestamp`, `actor`, `eventId`, `changeInfo` JSON, `stateTransition` JSON, `impact` JSON, `annotations` JSON.
3. **Merge Rules**:
   - If duplicate event detected (same session + sequence), compare `eventId`/`timestamp` to decide whether to replace or log conflict.
   - Keep `active` semantics: session edges remain active for analytics but may not need closure; optionally mark `archived` when session completes.
4. **Indexes**: `(sessionId, sequenceNumber)`, `(sessionId, type)`, `(actor)`. Consider partial index for `eventId`.

## 8. Query & API Surface
1. Extend `getRelationships` with filters: `sessionId`, `sequenceNumberRange`, `timestampRange`, `actor`, `impact.severity`, `stateTransition.to`.
2. Provide specialized APIs:
   - `getSessionTimeline(sessionId, { limit, offset })` returning ordered events and metadata.
   - `getSessionImpacts(sessionId)` summarizing impacted entities, severity, and unresolved actions.
   - `getSessionsAffectingEntity(entityId, { since })` for incident triage.
3. Update history routes and admin UI to use these helpers, offering pagination and filtering.

## 9. Integration with Checkpoints & Temporal Data
1. When `SESSION_CHECKPOINT` emitted, trigger asynchronous job creating checkpoint via `createCheckpoint`, linking via `CHECKPOINT_INCLUDES` edges.
2. Store checkpoint metadata (hops, reason) and ensure session timeline references checkpoint ID for quick navigation.
3. Align session edges with temporal pipeline: optionally call `openEdge`/`closeEdge` when sessions start/end, or mark `validFrom/validTo` using `timestamp`.

## 10. Migration & Backfill Plan
1. Add new columns and update canonical ID; ensure feature flag to guard rollout.
2. If historical session logs exist, import them using script that enforces sequence numbers.
3. For existing placeholder edges, generate pseudo sequence numbers (order by ingestion timestamp) to avoid collisions.
4. Validate by reconstructing recent sessions via API and comparing with raw logs.

## 11. Risks & Mitigations
| Risk | Mitigation |
| --- | --- |
| Missing or non-monotonic sequence numbers from clients | Provide server-side auto-increment fallback; log warnings for upstream fixes. |
| High-volume sessions causing large edge counts | Implement retention/archival policy, e.g., compress session details after N days. |
| Checkpoint creation failures leaving dangling edges | Use retry queue with DLQ; mark edges requiring manual intervention. |
| Sensitive metadata (actors, comments) requiring access control | Store access control metadata and ensure APIs enforce permissions. |

## 12. Implementation Milestones
1. Implement normalization/persistence updates with unit tests.
2. Extend query APIs and add timeline helper endpoints.
3. Integrate checkpoint workflow and monitor for successes/failures.
4. Import historical sessions (if available) and validate with stakeholders.
5. Release dashboards/visualizations for session analytics.

## 13. Open Questions
- Should session IDs be namespaced per workspace or globally unique? How to handle concurrent sessions editing same files?
- Do we need to capture undo/redo operations or only final state transitions?
- How to represent collaborative sessions with multiple actors—separate edges per actor or aggregated metadata?
- What retention policy is acceptable for session data given privacy/compliance constraints?
</file>

<file path="Docs/Blueprints/spec-relationships.md">
# Spec Relationship Blueprint

## 1. Overview
Specification edges (`REQUIRES`, `IMPACTS`, `IMPLEMENTS_SPEC`) connect product requirements to implementation artifacts. They enable requirement traceability, impact analysis, and coverage tracking throughout the development lifecycle.

## 2. Current Gaps
- Relationship query APIs (`KnowledgeGraphService.getRelationships`) still lack first-class filters for spec metadata (`impactLevel`, `priority`, `status`, acceptance criteria IDs, owner teams), limiting dashboards and automation even though the metadata is now persisted on each edge.
- Structural graph search omits specification entities when callers provide `entityTypes: ["spec"]` because the mapping table in `KnowledgeGraphService.structuralSearch` never handles the `"spec"` case; specs ingest correctly, but discovery breaks for design and test-planning workflows.
- Spec relationship identity still collapses multiple acceptance-criteria references into a single edge because `canonicalRelationshipId` ignores criterion identifiers; this prevents disambiguating inferred links from confirmed per-criterion edges.

## 3. Desired Capabilities
1. Persist structured metadata for requirement linkage: acceptance criteria IDs, rationale, priority, impact level, owning team, verification status.
2. Distinguish between inferred and confirmed relationships, preserving both without data loss.
3. Allow filtering/grouping by spec attributes (priority, impact, domain) to feed planning dashboards and gating automation.
4. Integrate with versioning to capture requirement evolution and compliance timeline.

## 4. Inputs & Consumers
- **Ingestion Sources**: Design authoring APIs (`src/api/routes/design.ts`), documentation parser (domain docs), manual annotations, potential integrations with project management tools (e.g., Linear, Jira).
- **Consumers**: Impact endpoints, planning dashboards, spec coverage reports, testing automation (identify unimplemented requirements), and compliance audits.

## 5. Schema & Metadata Requirements
| Field | Type | Notes |
| --- | --- | --- |
| `impactLevel` | enum (`high`, `medium`, `low`, `critical`) | Required for prioritization; map free-form inputs.
| `priority` | enum (`critical`, `high`, `medium`, `low`) | Align with backlog tooling.
| `status` | enum (`pending`, `in-progress`, `validated`, `waived`) | Tracks lifecycle of requirement implementation.
| `confidence` | number (0-1) | Confidence in edge accuracy (e.g., heuristic vs. manual).
| `resolutionState` | enum (`inferred`, `confirmed`, `deprecated`) | Distinguish edge origin/validation.
| `acceptanceCriteriaId` | string | Unique identifier for criterion; may come from design doc anchors.
| `rationale` | string | Explanation or summary from spec.
| `ownerTeam` | string | Optional reference to team or squad.
| `metadata.links` | array | URLs or doc references supporting the relationship.
| `validatedAt`, `reviewedAt` | timestamp | Track gating events.

## 6. Normalization Strategy
1. **Spec-edge normalization helper (follow-up)**: build a lightweight wrapper around `KnowledgeGraphService.createRelationship` for `REQUIRES`/`IMPACTS`/`IMPLEMENTS_SPEC` edges that
   - Maps free-form `impactLevel`, `priority`, `status`, and `resolutionState` inputs onto the canonical enums; surface unknown values in diagnostics but keep the metadata persisted.
   - Trims and bounds `acceptanceCriteriaId`, `ownerTeam`, and `rationale` strings (e.g., 128, 120, 2000 chars respectively) before writing them back so downstream queries stay indexable.
   - Normalizes acceptance-criteria metadata into `{ acceptanceCriteriaId, acceptanceCriteriaIds }` while preserving the raw array in `metadata.origins` for auditability.
2. **Evidence Handling**: When design doc references specific text or code blocks, continue capturing them via `EdgeEvidence` entries (`source="docs-parser"`); the helper above should ensure evidence arrays are deduplicated and capped.
3. **Validation**: Enforce presence of base spec ID and target entity, and warn when acceptance criteria are duplicate or missing unique anchors so authors can correct documents before confirmation.

## 7. Persistence & Merge Mechanics
1. **Canonical Identity**:
   - Current behavior hashes `specId|normalizedTarget|type`; follow-up is to append an acceptance-criterion hash when the metadata supplies a concrete criterion so that separate confirmations do not collide.
   - Maintain ability to aggregate heuristics vs. confirmed edges by storing `resolutionState`. When merging, prefer `confirmed` metadata.
2. **Cypher Projection**: Add fields `impactLevel`, `priority`, `status`, `confidence`, `resolutionState`, `acceptanceCriteriaId`, `rationale`, `ownerTeam`, `validatedAt`, `reviewedAt`, `links` JSON.
3. **Conflict Resolution**: If a heuristic edge exists and a confirmed edge arrives, update metadata to reflect confirmation while retaining heuristic evidence (store both in `metadata.origins`).
4. **Indexes**: Create indexes for `(type, impactLevel)`, `(type, priority)`, `(acceptanceCriteriaId)`, and optionally `resolutionState` for efficient queries.

## 8. Query & API Surface
1. Extend `getRelationships` to include filters: `impactLevel`, `priority`, `status`, `ownerTeam`, `resolutionState`, `validatedBefore/After`, and acceptance-criteria IDs. Ship REST/TRPC parameter plumbing alongside the server change so planning and dashboard clients can opt-in immediately.
2. Update structural graph search to map `entityTypes: ["spec"]` onto the stored spec entity shape (type=`"spec"`, kind=`"spec"`), and add an integration test that exercises spec lookup right after ingest.
3. Add helper APIs:
   - `getSpecCoverage(specId)` returning attached entities grouped by status.
   - `getSpecsAffectingEntity(entityId, { statusFilter })` for gating flows.
   - `getHighImpactSpecs({ limit, team })` to power dashboards.
4. Document additional parameters in API docs, providing sample queries for planning tools.
5. Provide GraphQL/trpc schema updates if relevant to front-end clients.

## 9. Temporal & Auditing
1. Integrate with history pipeline: when spec changes, append version snapshot, reopen edges as needed, set `validFrom/validTo` to mirror requirement lifecycle.
2. Track `validatedAt`, `reviewedAt`, and optionally a `statusHistory` array capturing decision events (manual approvals, waivers).
3. Provide timeline queries to show how requirements progressed over time, e.g., `getSpecTimeline(specId)` returning edges with version segments.

## 10. Migration & Backfill Plan
1. **Data Model Update**: Add new columns/properties as needed, and introduce the acceptance-criterion-aware canonical ID behind a feature flag so we can shadow-write and validate before cutting over.
2. **Reingestion**: Run design/doc parsing pipeline to populate new metadata, generating acceptance-criteria IDs if missing (use doc anchor or hashed text).
3. **Manual Data Import**: Provide script to import status/ownership from planning tool exports (CSV/JSON).
4. **Validation Run**: Produce audit report comparing new edges vs. previous counts (delta of heuristics vs. confirmed) to flag drops.

## 11. Risks & Mitigations
| Risk | Mitigation |
| --- | --- |
| Duplicate acceptance-criteria IDs causing collisions | Generate deterministic IDs from doc heading + index; fallback to UUID and store mapping. |
| Overwriting confirmed metadata when heuristics rerun | Merge logic prioritizes `resolutionState='confirmed'`; do not downgrade without explicit signal. |
| Query performance degrade due to new filters | Add indexes and `SKIP/LIMIT` defaults; consider caching aggregated reports. |
| Data staleness when specs change outside pipeline | Integrate with version-control hooks or manual sync command to reingest docs. |

## 12. Implementation Milestones
1. Implement the spec-edge normalization helper and targeted tests covering enum coercion, acceptance-criteria promotion, and evidence deduplication.
2. Introduce the acceptance-criterion-aware canonical ID behind a feature flag, run shadow writes, and flip once collision metrics stay flat.
3. Extend `getRelationships` (and public APIs) with spec metadata filters, update client typings, and document usage.
4. Patch structural search to surface spec entities for `entityTypes: ["spec"]`, and add integration coverage for immediate post-ingest lookup.
5. Rerun reingestion/backfill jobs to populate normalized metadata and acceptance identities, then roll out dashboards/alerts that rely on the richer query surface.

## 13. Open Questions
- Should we model `ownerTeam` as separate entity node to support richer queries (team hierarchies)?
- How do we reconcile spec edges across branches/environments? Do we maintain environment-specific relationships?
- What governance process is needed to manually override `priority`/`impactLevel` and keep history of changes?
- How should archived/retired specs be represented—deactivate edges or move to separate type?
</file>

<file path="Docs/Blueprints/temporal-relationships.md">
# Temporal Relationship Blueprint

## 1. Overview
Temporal relationships (`PREVIOUS_VERSION`, `MODIFIED_BY`, `CREATED_IN`, `MODIFIED_IN`, `REMOVED_IN`, `OF`) provide the timeline backbone of the knowledge graph. They link entities to versions, sessions, and change records, enabling historical reconstruction, auditing, and rollback support.

## 2. Current Gaps
- Synchronization coordination tests still surface pending-state leaks during certain rollback paths; failure propagation work remains outstanding.
- Rollback integration lacks comprehensive assertions for FalkorDB outages and will stay on the backlog until coordinator hardening lands.
- Lightweight fixtures for `clearTestData` are still desirable so history suites can run faster.

## 2.1 Status Snapshot (2025-09-19)
- ✅ `appendVersion`, `openEdge`, and `closeEdge` persist temporal metadata, create change nodes, and link provenance edges when `changeSetId` is provided.
- ✅ Timeline helpers (`getEntityTimeline`, `getRelationshipTimeline`, `getChangesForSession`) are implemented and wired into `src/api/routes/history.ts`; unit coverage exercises the new flows.
- ✅ Temporal writes flow through `runTemporalTransaction`, which now serializes `MULTI/EXEC` usage on the shared Falkor connection to prevent interleaving; the validator job auto-repairs missing `PREVIOUS_VERSION` links.
- ⏳ SynchronizationCoordinator + rollback paths require failure-mode hardening and tests to surface database errors and clear pending states.
- ✅ Blueprint/API documentation now includes transactional guidance, `scripts/backfill-temporal-history.ts` provides a backfill/repair entry point, and integration suites cover the history timelines surface.

> See TODO item “### 10. Operationalize Temporal Relationship Lifecycle & Timelines” for the authoritative task checklist.

## 3. Desired Capabilities
1. Implement transactional versioning and temporal edge lifecycle to accurately reflect when entities/relationships change.
2. Ensure each change event records provenance: which change set or session modified which entities and edges.
3. Provide accessible timeline APIs (`getEntityTimeline`, `getRelationshipTimeline`, `getChangesForSession`).
4. Support checkpointing by linking snapshots to captured subgraphs with consistent temporal metadata.

## 4. Inputs & Consumers
- **Inputs**: SynchronizationCoordinator parsing cycles, SCM commit metadata, manual change annotations, rollback operations, session manager.
- **Consumers**: History APIs (`src/api/routes/history.ts`), admin dashboards, rollback service, impact analysis, compliance auditors.

## 5. Schema & Metadata Requirements
| Field | Type | Notes |
| --- | --- | --- |
| `versionId` | string | Unique identifier for version node (`ver_<entityId>_<hash>`).
| `changeSetId` | string | Links to change entity (commit, PR, session).
| `timestamp` | Date | Precise timestamp for operations.
| `author` | string | Commit author or session agent.
| `description` | string | Optional summary of change.
| `validFrom`, `validTo` | Date | Validity interval for edges.
| `segmentId` | string | Optional identifier for edge lifespan segments (if using same ID across versions).
| `metadata.diffStats` | object | Stats about change (lines added/removed, tokens impacted).

## 6. Temporal Operation Strategy
1. **Version Creation (`appendVersion`)**:
   - Accept entity snapshot (id, hash, metadata). Create/merge `version` node with `hash`, `timestamp`, `changeSetId`.
   - Link via `OF` to entity and via `PREVIOUS_VERSION` to prior version (ordered by timestamp).
   - Optionally link version to change node (`MODIFIED_IN` or `CREATED_IN`).
2. **Edge Lifecycle (`openEdge`/`closeEdge`)**:
   - `openEdge` ensures relationship exists, sets `validFrom`, clears `validTo`, increments `version`, logs `changeSetId` when provided.
   - `closeEdge` sets `validTo`, marks `active=false`, increments `version`.
   - Both operations should run in transactions to avoid partial state (use `falkordbQuery` with multi-statement or explicit transactions).
3. **Change Provenance**:
   - For each change set (commit/session), create `change` entity node capturing metadata and link via `MODIFIED_BY`, `MODIFIED_IN`, `CREATED_IN`, `REMOVED_IN` relationships.
   - Ensure ingestion populates these links when scanning diffs or applying patches.

## 7. Persistence & Consistency Considerations
1. **Transactions**: Wrap version + edge updates per entity/change set to maintain invariants (no version gap). Consider using `FALKORDB` transaction support or emulating via steps with guards.
2. **Canonical IDs vs. Segments**:
   - Keep canonical IDs stable but maintain `version` counter and `segmentId` (if needed) for referencing specific lifespans.
   - Optionally create derived nodes representing relationship segments when multiple active periods exist; link via `HAS_SEGMENT` edges.
3. **Indexing**: Create indexes on `version.id`, `(changeSetId)`, `(validFrom)`, `(validTo)` to accelerate timeline queries.
4. **Concurrency**: Avoid race conditions by checking existing `validTo`; use locking mechanisms or idempotency keys tied to change set.

## 8. Query & API Surface
1. Implement timeline helpers:
   - `getEntityTimeline(entityId, { includeRelationships?, limit? })`: returns ordered versions and change events.
   - `getRelationshipTimeline(relationshipId)`: iterates through version counters and validity intervals.
   - `getChangesForRange({ since, until })`: lists change sets and affected entities.
2. Update History API to leverage these helpers; provide pagination and filtering (author, change type).
3. Expose timeline data to front-end/CLI for diff visualization and audit exports.

## 9. Migration & Backfill Plan
1. **Baseline Snapshot**: For existing edges, set `validFrom = firstSeenAt` (or `created` fallback) and `validTo = null`. For inactive edges, set `validTo = lastSeenAt`.
2. **Version Nodes**: Create initial version node per entity with current hash; link to entity via `OF`.
3. **Change Sets**: Optionally parse SCM history to populate `change` nodes for recent commits (can be phased).
4. **Idempotency**: Use the `history:backfill` script (below) to re-run safely; helper mutations rely on `MERGE` semantics.

### 9.1 Backfill / Validation Runbook
1. Dry-run validation:
   - `pnpm history:backfill --dry-run`
   - Confirms graph readiness, emits issue summaries without mutating data.
2. Auto-repair missing `PREVIOUS_VERSION` links:
   - `pnpm history:backfill --repair`
   - Enables transactional repairs via `TemporalHistoryValidator` (delegates to `KnowledgeGraphService.repairPreviousVersionLink`).
3. Scope tuning:
   - `--batch-size`, `--timeline-limit`, and `--max-entities` throttle long-running audits for huge graphs.
4. Automation note:
   - Schedule the dry-run command as a health check post-sync; pair with `--repair` in maintenance windows to keep gaps closed.

## 10. Risks & Mitigations
| Risk | Mitigation |
| --- | --- |
| Transaction failure mid-update causing inconsistent intervals | Implement retry logic and validation checks (e.g., periodic job verifying no overlapping intervals). |
| Data volume increase due to version nodes | Cap retention (store last N versions per entity or prune old segments with archival). |
| Integration complexity with existing ingestion flows | Introduce feature flag; progressively enable per subsystem (code sync, docs, tests). |
| Backfill from SCM costly | Start with present state baseline; schedule incremental backfill if needed; allow partial coverage. |

## 11. Implementation Milestones
1. Design transaction wrappers & implement `openEdge`/`closeEdge` with tests.
2. Implement `appendVersion` storing version nodes and linking to change sets.
3. Build timeline query helpers and update API routes.
4. Execute baseline backfill; monitor metrics and data volume.
5. Expand ingestion to populate change provenance for all subsystems.

## 12. Open Questions
- Should we maintain multiple history modes (full vs. lightweight) configurable via environment variables?
- How to integrate with SCM metadata (git commit, branch) when changes originate from manual edits outside orchestrated pipeline?
- Do we need to preserve diff snapshots (content) in graph or rely on external storage and link via metadata?
- What retention/archival policy ensures graph remains performant while retaining required audit history?
</file>

<file path="Docs/KnowledgeGraphDesign.md">
# Knowledge Graph Design for Memento

## Overview

This document outlines the comprehensive knowledge graph schema for the Memento system, designed to provide full awareness of codebase changes and prevent context drift in AI coding agents.

## Core Architecture

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Codebase      │    │  Knowledge      │    │   Vector        │
│   Files         │────│   Graph         │────│   Index         │
│                 │    │  (FalkorDB)     │    │  (Qdrant)       │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                              │
                              ▼
                       ┌─────────────────┐
                       │   Validation    │
                       │   Engine        │
                       └─────────────────┘
```

## Node Types (Entities)

### 1. CodebaseEntity
**Base node for all codebase elements**
```
{
  id: string (UUID)
  path: string (relative to project root)
  hash: string (content hash)
  language: string (typescript, javascript, etc.)
  lastModified: timestamp
  created: timestamp
  metadata: object (additional properties)
}
```

### 2. File
**Represents source files**
```
{
  ...CodebaseEntity,
  type: "file"
  extension: string
  size: number (bytes)
  lines: number
  isTest: boolean
  isConfig: boolean
  dependencies: string[] (external packages)
}
```

### 3. Directory
**Represents directory structure**
```
{
  ...CodebaseEntity,
  type: "directory"
  children: string[] (file/directory IDs)
  depth: number (from root)
}
```

### 4. Module
**Represents logical modules/packages**
```
{
  ...CodebaseEntity,
  type: "module"
  name: string
  version: string
  packageJson: object
  entryPoint: string (main file path)
}
```

### 5. Symbol
**Base for code symbols (functions, classes, etc.)**
```
{
  ...CodebaseEntity,
  type: "symbol"
  name: string
  kind: string (function, class, interface, etc.)
  signature: string (full type signature)
  docstring: string
  visibility: string (public, private, protected)
  isExported: boolean
  isDeprecated: boolean
}
```

### 6. Function
**Function definitions**
```
{
  ...Symbol,
  kind: "function"
  parameters: object[] (name, type, defaultValue, optional)
  returnType: string
  isAsync: boolean
  isGenerator: boolean
  complexity: number (cyclomatic complexity)
  calls: string[] (function IDs it calls)
}
```

### 7. Class
**Class definitions**
```
{
  ...Symbol,
  kind: "class"
  extends: string[] (class IDs)
  implements: string[] (interface IDs)
  methods: string[] (method IDs)
  properties: string[] (property IDs)
  isAbstract: boolean
}
```

### 8. Interface
**Interface definitions**
```
{
  ...Symbol,
  kind: "interface"
  extends: string[] (interface IDs)
  methods: string[] (method signatures)
  properties: string[] (property definitions)
}
```

### 9. TypeAlias
**Type alias definitions**
```
{
  ...Symbol,
  kind: "typeAlias"
  aliasedType: string
  isUnion: boolean
  isIntersection: boolean
}
```

### 10. Test
**Test entities**
```
{
  ...CodebaseEntity,
  type: "test"
  testType: string (unit, integration, e2e)
  targetSymbol: string (symbol ID being tested)
  framework: string (jest, vitest, etc.)
  coverage: object (lines, branches, functions)
}
```

### 11. Spec
**Feature specifications**
```
{
  ...CodebaseEntity,
  type: "spec"
  title: string
  description: string
  acceptanceCriteria: string[]
  status: string (draft, approved, implemented)
  priority: string (low, medium, high)
  assignee: string
  created: timestamp
  updated: timestamp
}
```

### 12. Change
**Tracks changes to codebase entities**
```
{
  id: string (UUID)
  type: "change"
  changeType: string (create, update, delete, rename, move)
  entityType: string (file, symbol, spec, etc.)
  entityId: string (ID of entity that changed)
  timestamp: timestamp
  author: string
  commitHash: string (optional)
  diff: string (change details)
  previousState: object (entity state before change)
  newState: object (entity state after change)
  sessionId: string (links to AI agent session)
  specId: string (optional, links to related spec)
}
```

### 13. Session
**Tracks AI agent interaction sessions and development activity**
```
{
  id: string (UUID)
  type: "session"
  startTime: timestamp
  endTime: timestamp
  agentType: string (claude, gpt, etc.)
  userId: string
  changes: string[] (change IDs from this session)
  specs: string[] (spec IDs created in this session)
  status: string (active, completed, failed)
  metadata: object (session context and parameters)
  
  // Event tracking for fine-grained changes
  events: Array<{
    timestamp: Date
    entityId: string
    changeType: 'modified' | 'added' | 'deleted' | 'tested' | 'built'
    elementType?: 'function' | 'class' | 'import' | 'test'
    elementName?: string
    operation?: 'added' | 'modified' | 'deleted' | 'renamed'
    testResult?: 'passed' | 'failed' | 'skipped'
    buildResult?: 'success' | 'failure'
  }>
  
  // Semantic snapshots at key moments
  snapshots: Map<string, {
    trigger: 'test_fail' | 'test_pass' | 'build_fail' | 'build_pass' | 'manual' | 'periodic'
    timestamp: Date
    workingState: boolean
    affectedEntities: Array<{
      entityId: string
      fullImplementation?: string  // Complete function/class at this moment
      context?: {
        imports: string[]
        callers: string[]
        callees: string[]
      }
    }>
  }>
  
  // Session-level state tracking
  lastKnownGoodState?: {
    timestamp: Date
    verifiedBy: 'test' | 'build' | 'manual'
  }
  currentState: 'working' | 'broken' | 'unknown'
}
```

### 14. Version
**Compact snapshot of an entity when its content changes (append-only)**
```
{
  id: string (UUID)
  type: "version"
  entityId: string           // id of the current/live entity node
  path?: string              // helpful for files/symbols
  hash: string               // content hash at this moment
  language?: string
  timestamp: timestamp       // when this snapshot was recorded
  metadata?: object          // minimal additional info (size, lines, metrics)
}
```

### 15. Checkpoint
**Materialized subgraph descriptor for fast, AI-ready retrieval**
```
{
  id: string (UUID)
  type: "checkpoint"
  checkpointId: string       // human-friendly id or derived hash
  timestamp: timestamp
  reason: 'daily' | 'incident' | 'manual'
  hops: number               // K-hop neighborhood captured
  seedEntities: string[]     // entities around which snapshot was built
  metadata?: object
}
```

## Relationship Types (Edges)

### Base Relationship Properties
All relationships include:
```
{
  created: timestamp
  lastModified: timestamp
  version: number (incrementing version for relationship changes)
  metadata: object (additional relationship-specific data)
  
  // Temporal validity interval (when history mode is enabled)
  validFrom?: timestamp      // when this edge became active
  validTo?: timestamp        // when this edge was deactivated (open interval if null)
}
```

### SessionRelationship Interface
**Specific interface for session-based temporal relationships**
```
{
  ...BaseRelationshipProperties,
  type: SESSION_MODIFIED | SESSION_IMPACTED | SESSION_CHECKPOINT | BROKE_IN | FIXED_IN | DEPENDS_ON_CHANGE
  
  // Session tracking
  sessionId: string
  timestamp: Date  // Precise timestamp of the event
  sequenceNumber: number  // Order within session
  
  // Semantic change information (for SESSION_MODIFIED)
  changeInfo?: {
    elementType: 'function' | 'class' | 'import' | 'test'
    elementName: string
    operation: 'added' | 'modified' | 'deleted' | 'renamed'
    semanticHash?: string  // Hash of the semantic unit, not full file
    affectedLines?: number  // Approximate lines changed
  }
  
  // State transition tracking (for BROKE_IN, FIXED_IN, SESSION_CHECKPOINT)
  stateTransition?: {
    from: 'working' | 'broken' | 'unknown'
    to: 'working' | 'broken' | 'unknown'
    verifiedBy: 'test' | 'build' | 'manual'
    confidence: number  // 0-1, confidence in state determination
    criticalChange?: {
      entityId: string
      beforeSnippet?: string  // Just the relevant lines before
      afterSnippet?: string   // Just the relevant lines after
    }
  }
  
  // Impact information (for SESSION_IMPACTED)
  impact?: {
    severity: 'high' | 'medium' | 'low'
    testsFailed?: string[]
    testsFixed?: string[]
    buildError?: string
    performanceImpact?: number  // Performance delta if measurable
  }
}
```

### Structural Relationships
- `BELONGS_TO`: File/Directory → Directory (hierarchy)
- `CONTAINS`: Directory → File/Directory
- `DEFINES`: File → Symbol (symbol defined in file)
- `EXPORTS`: File → Symbol (symbol exported from file)
- `IMPORTS`: File → Symbol (symbol imported by file)

### Code Relationships
- `CALLS`: Function → Function (function calls another)
- `REFERENCES`: Symbol → Symbol (symbol references another)
- `IMPLEMENTS`: Class → Interface (class implements interface)
- `EXTENDS`: Class → Class, Interface → Interface (inheritance)
- `DEPENDS_ON`: Symbol → Symbol (dependency relationship)
- `USES`: Symbol → Symbol (usage relationship)

### Test Relationships
- `TESTS`: Test → Symbol (test covers symbol)
- `VALIDATES`: Test → Spec (test validates spec criteria)

### Spec Relationships
- `REQUIRES`: Spec → Symbol (spec requires symbol implementation)
- `IMPACTS`: Spec → File/Directory (files impacted by spec)

### Temporal Relationships
- `PREVIOUS_VERSION`: Entity → Entity (links to previous version of same entity)
- `CHANGED_AT`: Entity → Timestamp (tracks when entity changed)
- `MODIFIED_BY`: Entity → Change (links entity to change that modified it)
- `CREATED_IN`: Entity → Commit/Session (links entity to creation context)
- `OF`: Version → Entity (version belongs to current/live entity)

### Change Tracking Relationships
- `INTRODUCED_IN`: Entity → Change (when entity was first introduced)
- `MODIFIED_IN`: Entity → Change (all changes that modified entity)
- `REMOVED_IN`: Entity → Change (when entity was removed/deleted)

### Session-Based Temporal Relationships
- `SESSION_MODIFIED`: Session → Entity (entity modified during session with timestamp/sequence)
- `SESSION_IMPACTED`: Session → Test/Build (test/build affected during session)
- `SESSION_CHECKPOINT`: Session → Entity (important state transition marked)
- `BROKE_IN`: Session → Test/Build (when test/build started failing)
- `FIXED_IN`: Session → Test/Build (when test/build was fixed)
- `DEPENDS_ON_CHANGE`: Entity → Entity (change dependency within session)

### Checkpoint Relationships
- `CHECKPOINT_INCLUDES`: Checkpoint → Entity/Version (members of the checkpoint subgraph)

## Graph Constraints and Indexes

### Unique Constraints
- File.path: unique file paths
- Symbol.name + Symbol.file: unique symbol names within files
- Spec.title: unique spec titles
- Module.name: unique module names

### Indexes
- File.path: for fast file lookups
- Symbol.name: for symbol name searches
- Symbol.kind: for filtering by symbol type
- Spec.status: for filtering specs by status

### Temporal Indexes
- Entity.lastModified: for recent changes queries
- Entity.created: for creation time queries
- Change.timestamp: for change history queries
- Session.startTime/endTime: for session queries
- Relationship.created: for relationship creation queries
- Relationship.lastModified: for relationship change queries
- Relationship.validFrom/validTo: for time-scoped graph traversal

### Composite Indexes
- (entityType + timestamp): for entity type change history
- (sessionId + timestamp): for session activity
- (specId + timestamp): for spec-related changes

## Vector Database Integration

### Embedding Strategy
- **Code Embeddings**: Functions, classes, interfaces
- **Documentation Embeddings**: Specs, comments, docstrings
- **Test Embeddings**: Test cases and assertions

### Metadata Mapping
```typescript
interface VectorMetadata {
  nodeId: string;           // Graph node ID
  nodeType: string;         // Entity type
  path: string;             // File path
  symbolName?: string;      // Symbol name if applicable
  symbolKind?: string;      // Symbol type if applicable
  language: string;         // Programming language
  created: timestamp;       // Creation time
  lastModified: timestamp;  // Last modification time
  version: number;          // Entity version
  changeFrequency: number;  // How often entity changes
  sessionId?: string;       // Last modifying session
  author?: string;          // Last modifying author
  tags: string[];           // Additional tags
}
```

### Temporal Search Patterns
- **Recent Code Search**: Find recently modified similar code
- **Temporal Code Similarity**: Compare code at different points in time
- **Change-aware Search**: Weight results by recency and change frequency
- **Session Context Search**: Find code modified in similar sessions

### Search Patterns
- **Semantic Code Search**: Find similar functions/classes
- **API Usage Examples**: Find usage patterns for symbols
- **Test Case Retrieval**: Find relevant tests for symbols
- **Spec Matching**: Find related specifications

## Query Patterns

### 1. Symbol Usage Analysis
```
MATCH (s:Symbol {name: $symbolName})
OPTIONAL MATCH (s)<-[:CALLS|REFERENCES|USES]-(caller:Symbol)
OPTIONAL MATCH (s)-[:CALLS|REFERENCES|USES]->(callee:Symbol)
OPTIONAL MATCH (s)<-[:TESTS]-(t:Test)
RETURN s, collect(caller) as callers, collect(callee) as callees, collect(t) as tests
```

### 2. File Dependency Graph
```
MATCH (f:File {path: $filePath})
OPTIONAL MATCH (f)-[:IMPORTS]->(s:Symbol)<-[:EXPORTS]-(ef:File)
OPTIONAL MATCH (f)-[:EXPORTS]->(es:Symbol)<-[:IMPORTS]-(if:File)
RETURN f, collect(DISTINCT ef) as importedFrom, collect(DISTINCT if) as importedBy
```

### 3. Impact Analysis
```
MATCH (s:Symbol {id: $symbolId})
MATCH (s)<-[:CALLS|REFERENCES|USES|IMPLEMENTS|EXTENDS*1..3]-(dependent:Symbol)
MATCH (dependent)-[:DEFINES]->(f:File)
RETURN DISTINCT f.path as affectedFiles, dependent.name as dependentSymbols
```

### 4. Test Coverage Analysis
```
MATCH (spec:Spec {id: $specId})
OPTIONAL MATCH (spec)<-[:VALIDATES]-(t:Test)
OPTIONAL MATCH (t)-[:TESTS]->(s:Symbol)
OPTIONAL MATCH (s)-[:DEFINES]->(f:File)
RETURN spec, collect(t) as tests, collect(DISTINCT s) as symbols, collect(DISTINCT f) as files
```

## Temporal Query Patterns

### 1. Recent Changes Query
**Find all entities modified within a time window**
```
MATCH (e:CodebaseEntity)
WHERE e.lastModified >= $startTime AND e.lastModified <= $endTime
RETURN e.path, e.type, e.lastModified, e.created
ORDER BY e.lastModified DESC
```

### 2. Entity Evolution History
**Track how a specific entity has changed over time**
```
MATCH (e {id: $entityId})
OPTIONAL MATCH (v:version)-[:OF]->(e)
OPTIONAL MATCH (e)-[:PREVIOUS_VERSION*]->(prev)
OPTIONAL MATCH (e)-[:MODIFIED_IN]->(changes:Change)
RETURN e, collect(DISTINCT v) as versions,
       collect(DISTINCT prev) as previousEntities,
       collect(changes {.*, .timestamp}) as changeHistory
ORDER BY coalesce(v.timestamp, changes.timestamp) DESC
```

### 3. Session Activity Analysis
**Find all changes made in a specific AI agent session**
```
MATCH (session:Session {id: $sessionId})
OPTIONAL MATCH (session)-[:CONTAINS]->(changes:Change)
OPTIONAL MATCH (changes)-[:AFFECTS]->(entities)
RETURN session, collect(changes) as changes,
       collect(DISTINCT entities) as affectedEntities
```

### 4. Temporal Impact Analysis
**Find entities affected by changes within a time window**
```
MATCH (change:Change)
WHERE change.timestamp >= $startTime AND change.timestamp <= $endTime
MATCH (change)-[:AFFECTS]->(affected:CodebaseEntity)
OPTIONAL MATCH (affected)-[:DEPENDS_ON|USES|CALLS*1..2]->(downstream:CodebaseEntity)
RETURN change, collect(DISTINCT affected) as directlyAffected,
       collect(DISTINCT downstream) as indirectlyAffected
```

### 9. Time-Scoped Traversal with Validity Intervals
**Traverse structure as it existed at time T**
```
MATCH (start {id: $entityId})
MATCH path = (start)-[r*1..3]-(connected)
WHERE ALL(rel IN r WHERE rel.validFrom <= $atTime AND (rel.validTo IS NULL OR rel.validTo > $atTime))
RETURN DISTINCT connected
```

### 10. Fetch a Checkpoint Neighborhood
```
MATCH (c:checkpoint {checkpointId: $checkpointId})-[:CHECKPOINT_INCLUDES]->(n)
RETURN c, collect(DISTINCT n) as members
```

### 5. Cascading Change Detection
**Detect breaking changes and their downstream impact**
```
MATCH (changed:Function {name: $functionName})
WHERE changed.lastModified >= $changeTime
// Find all direct callers
MATCH (caller:Function)-[:CALLS]->(changed)
// Find files that import and use this function
MATCH (caller)-[:DEFINES]->(file:File)
MATCH (file)-[:IMPORTS]->(changed)
// Find indirect dependents (functions that call the callers)
OPTIONAL MATCH (indirect:Function)-[:CALLS*1..3]->(caller)
// Find files containing indirect dependents
OPTIONAL MATCH (indirect)-[:DEFINES]->(indirectFile:File)
RETURN changed.name as changedFunction,
       collect(DISTINCT caller.name) as directCallers,
       collect(DISTINCT file.path) as directlyAffectedFiles,
       collect(DISTINCT indirect.name) as indirectCallers,
       collect(DISTINCT indirectFile.path) as indirectlyAffectedFiles
```

### 6. Signature Change Impact
**Analyze impact of function signature changes**
```
MATCH (func:Function {name: $functionName})
MATCH (func)-[:DEFINES]->(definingFile:File)
// Find all files that import this function
MATCH (importingFile:File)-[:IMPORTS]->(func)
// Find all usage sites within those files
MATCH (usage:Function)-[:DEFINES]->(importingFile)
WHERE usage.signature =~ ".*" + func.name + ".*"
// Check if usage matches current function signature
MATCH (func)-[:MODIFIED_IN]->(changes:Change)
WHERE changes.changeType = "signature_change"
RETURN importingFile.path as affectedFile,
       usage.name as usingFunction,
       func.signature as newSignature,
       changes.previousState.signature as oldSignature,
       changes.timestamp as changeTime
```

### 7. Breaking Change Propagation
**Find complete propagation path of breaking changes**
```
MATCH (breakingChange:Change {changeType: "breaking"})
WHERE breakingChange.timestamp >= $sinceTime
MATCH (breakingChange)-[:AFFECTS]->(primaryEntity:Symbol)
// Find cascading dependencies
MATCH path = (primaryEntity)-[:CALLS|REFERENCES|USES|IMPLEMENTS|EXTENDS*1..5]-(dependent:Symbol)
WHERE ALL(rel IN relationships(path) WHERE rel.created < breakingChange.timestamp)
MATCH (dependent)-[:DEFINES]->(affectedFile:File)
// Group by distance from breaking change
RETURN affectedFile.path,
       length(path) as distanceFromChange,
       collect(DISTINCT dependent.name) as affectedSymbols,
       breakingChange.changeType,
       breakingChange.timestamp
ORDER BY distanceFromChange, affectedFile.path
```

### 8. Change Pattern Analysis
**Analyze change patterns for a specific entity type**
```
MATCH (entity:CodebaseEntity)
WHERE entity.type = $entityType
MATCH (entity)-[:MODIFIED_IN]->(changes:Change)
RETURN entity.path, count(changes) as changeCount,
       min(changes.timestamp) as firstModified,
       max(changes.timestamp) as lastModified,
       collect(changes.changeType) as changeTypes
ORDER BY changeCount DESC
```

### 9. Relationship Evolution
**Track how relationships between entities have evolved**
```
MATCH (a:Symbol {name: $symbolA})-[r:CALLS|USES|REFERENCES]-(b:Symbol {name: $symbolB})
WHERE r.created >= $startTime
RETURN a.name, b.name, type(r) as relationshipType,
       r.created, r.lastModified, r.version
ORDER BY r.lastModified DESC
```

### 10. Codebase Age Analysis
**Analyze the age distribution of codebase entities**
```
MATCH (e:CodebaseEntity)
RETURN e.type,
       count(e) as totalCount,
       min(e.created) as oldest,
       max(e.created) as newest,
       avg(duration.between(e.created, datetime()).days) as avgAgeDays
ORDER BY avgAgeDays DESC
```

### 11. Recent Activity by Agent
**Find recent activity by specific AI agent type**
```
MATCH (session:Session {agentType: $agentType})
WHERE session.startTime >= $startTime
OPTIONAL MATCH (session)-[:CONTAINS]->(changes:Change)
OPTIONAL MATCH (changes)-[:AFFECTS]->(entities:CodebaseEntity)
RETURN session, count(changes) as changeCount,
       collect(DISTINCT entities.type) as affectedEntityTypes,
       collect(DISTINCT changes.changeType) as changeTypes
ORDER BY session.startTime DESC
```

## Session-Based Query Patterns

### 1. What Changed in the Last Hour
**Find all changes within a recent time window during active session**
```
MATCH (s:Session)
WHERE s.status = 'active' AND s.startTime > (now() - 1 hour)
MATCH (s)-[m:SESSION_MODIFIED]->(e:CodebaseEntity)
WHERE m.timestamp > (now() - 1 hour)
RETURN e.path, e.type, m.changeInfo.elementName, 
       m.changeInfo.operation, m.timestamp
ORDER BY m.sequenceNumber DESC
```

### 2. Find Last Working State
**Identify the last known good state and what broke it**
```
MATCH (s:Session)
WHERE s.lastKnownGoodState IS NOT NULL
MATCH (s)-[f:FIXED_IN|SESSION_CHECKPOINT]->(entity)
WHERE f.stateTransition.to = 'working'
WITH s, MAX(f.timestamp) as lastWorkingTime
MATCH (s)-[b:BROKE_IN]->(brokenEntity)
WHERE b.timestamp > lastWorkingTime
RETURN lastWorkingTime, brokenEntity.name, 
       b.stateTransition.criticalChange as breakingChange
```

### 3. Session State Transitions
**Track working/broken state transitions within a session**
```
MATCH (s:Session {id: $sessionId})
MATCH (s)-[r:BROKE_IN|FIXED_IN|SESSION_CHECKPOINT]->(entity)
WHERE r.stateTransition IS NOT NULL
RETURN entity.name, r.timestamp, 
       r.stateTransition.from as fromState,
       r.stateTransition.to as toState,
       r.stateTransition.verifiedBy as verification
ORDER BY r.timestamp
```

### 4. Impact Analysis Within Session
**Find cascading impacts of changes during a session**
```
MATCH (s:Session {id: $sessionId})
MATCH (s)-[m:SESSION_MODIFIED]->(changed:CodebaseEntity)
WHERE m.timestamp >= $sinceTime
MATCH (s)-[i:SESSION_IMPACTED]->(impacted:Test|Build)
WHERE i.timestamp > m.timestamp
RETURN changed.name, m.changeInfo.operation,
       impacted.name, i.impact.severity,
       i.impact.testsFailed
ORDER BY m.sequenceNumber, i.timestamp
```

### 5. Semantic Change Recovery
**Reconstruct semantic state at any point in session**
```
MATCH (s:Session {id: $sessionId})
WHERE $targetTime >= s.startTime AND $targetTime <= s.endTime
// Find the nearest snapshot before target time
WITH s, s.snapshots as snapshots
UNWIND snapshots as snapshot
WHERE snapshot.timestamp <= $targetTime
WITH s, snapshot ORDER BY snapshot.timestamp DESC LIMIT 1
// Get all changes between snapshot and target
MATCH (s)-[m:SESSION_MODIFIED]->(e:CodebaseEntity)
WHERE m.timestamp > snapshot.timestamp AND m.timestamp <= $targetTime
RETURN snapshot.affectedEntities as snapshotState,
       collect({
         entity: e.name,
         change: m.changeInfo,
         timestamp: m.timestamp
       }) as changesSinceSnapshot
```

### 6. Breaking Change Detection
**Find what specific change broke the tests**
```
MATCH (s:Session)
MATCH (s)-[broke:BROKE_IN]->(test:Test)
WHERE broke.timestamp >= $startTime
// Find the change that immediately preceded the break
MATCH (s)-[mod:SESSION_MODIFIED]->(entity:CodebaseEntity)
WHERE mod.timestamp < broke.timestamp
AND mod.sequenceNumber = (
  SELECT MAX(m2.sequenceNumber) 
  FROM (s)-[m2:SESSION_MODIFIED]->()
  WHERE m2.timestamp < broke.timestamp
)
RETURN test.name as brokenTest,
       entity.name as suspectEntity,
       mod.changeInfo as suspectChange,
       broke.impact.testsFailed as failedTests
```

### 7. Session Activity Timeline
**Get complete timeline of session activities**
```
MATCH (s:Session {id: $sessionId})
// Collect all session relationships with timestamps
MATCH (s)-[r]->(entity)
WHERE type(r) IN ['SESSION_MODIFIED', 'BROKE_IN', 'FIXED_IN', 
                  'SESSION_CHECKPOINT', 'SESSION_IMPACTED']
RETURN entity.name, type(r) as eventType, r.timestamp,
       CASE type(r)
         WHEN 'SESSION_MODIFIED' THEN r.changeInfo
         WHEN 'BROKE_IN' THEN r.impact
         WHEN 'FIXED_IN' THEN r.stateTransition
         ELSE r.metadata
       END as eventDetails
ORDER BY r.timestamp
```

### 8. Change Dependency Analysis
**Find dependencies between changes in a session**
```
MATCH (s:Session {id: $sessionId})
MATCH (e1:CodebaseEntity)-[d:DEPENDS_ON_CHANGE]->(e2:CodebaseEntity)
WHERE EXISTS((s)-[:SESSION_MODIFIED]->(e1))
AND EXISTS((s)-[:SESSION_MODIFIED]->(e2))
RETURN e1.name as dependent, e2.name as dependency,
       d.metadata.reason as dependencyReason
```

## Lifecycle Integration

### 1. Spec Creation Phase
- Create Spec node
- Link to affected files/symbols via `IMPACTS` relationships
- Generate initial vector embeddings

### 2. Test Generation Phase
- Create/update Test nodes
- Link tests to specs via `VALIDATES`
- Link tests to symbols via `TESTS`
- Update vector index with test embeddings

### 3. Implementation Phase
- Update/create Symbol nodes
- Create relationships (`CALLS`, `REFERENCES`, `IMPLEMENTS`, etc.)
- Update File nodes with new hashes
- Refresh vector embeddings

### 4. Validation Phase
- Query graph for dependency analysis
- Check architectural constraints
- Verify test coverage via graph traversals

### 5. Impact Analysis Phase
- Traverse dependency graph for cascading effects
- Identify stale imports/exports
- Propose follow-up changes

## Anti-Deception Mechanisms

### 1. Implementation Quality Checks
- Detect stub implementations via complexity metrics
- Validate against forbidden patterns
- Check for proper error handling

### 2. Architecture Compliance
- Enforce layer boundaries via graph queries
- Block banned dependency imports
- Verify proper abstraction usage

### 3. Coverage Validation
- Track test-to-code relationships
- Enforce coverage thresholds
- Detect uncovered code paths

## Synchronization Mechanisms

### File System Watcher
**Real-time synchronization with filesystem changes**
```
Event Types:
- File Created/Modified/Deleted
- Directory Created/Deleted
- Git Operations (commit, branch switch, merge)

Watcher Implementation:
- Node.js chokidar for cross-platform file watching
- Debounced events (500ms) to handle rapid changes
- Ignore patterns: node_modules/**, .git/**, *.log, build/**, dist/**
- Queue-based processing to prevent overwhelming the system
```

### Change Detection Strategy
**Incremental vs Full Synchronization**

#### Incremental Sync (Real-time)
```typescript
interface FileChange {
  path: string;
  type: 'create' | 'modify' | 'delete' | 'rename';
  oldPath?: string;
  mtime: Date;
  size: number;
  hash: string;
}

interface SyncContext {
  change: FileChange;
  affectedNodes: string[]; // Graph node IDs
  requiresFullReindex: boolean;
  priority: 'high' | 'medium' | 'low';
}
```

**Change Classification:**
- **High Priority**: Core files (.ts, .js, package.json)
- **Medium Priority**: Config files, documentation
- **Low Priority**: Generated files, logs

#### Full Sync (Scheduled/Batch)
- Weekly full re-indexing
- After major refactors or merges
- On-demand via CLI: `memento sync --full`

### Graph Update Process

#### 1. Change Ingestion
```
File Change Detected → Queue → Prioritize → Process
                                      ↓
                              Create Change Node
                                      ↓
                           Update Affected Entities
                                      ↓
                        Update Relationships
                                      ↓
                     Update Vector Embeddings
                                      ↓
                   Validate & Cache Results
```

#### 2. Entity Updates
```typescript
async function updateEntity(change: FileChange): Promise<void> {
  // 1. Parse file with ts-morph/tree-sitter
  const ast = await parseFile(change.path);

  // 2. Extract symbols and relationships
  const entities = extractEntities(ast);
  const relationships = extractRelationships(ast);

  // 3. Compare with existing graph state
  const diff = await computeGraphDiff(change.path, entities, relationships);

  // 4. Apply changes transactionally
  await applyGraphChanges(diff);

  // 5. Update vector embeddings
  await updateEmbeddings(diff.modifiedEntities);

  // 6. Trigger impact analysis
  await analyzeImpact(diff);
}
```

#### 3. Relationship Synchronization
**Bidirectional Relationship Updates:**
- **Forward**: When A changes, update A→B relationships
- **Reverse**: When A changes, update B→A relationships (imports, references)
- **Cascade**: When A changes, check if B→C relationships need updates

**Relationship Types Requiring Sync:**
- CALLS: Function call sites
- REFERENCES: Symbol references
- IMPORTS/EXPORTS: Module dependencies
- IMPLEMENTS/EXTENDS: Inheritance chains
- TESTS: Test-to-code relationships

### Vector Database Synchronization

#### Embedding Update Strategy
```typescript
interface EmbeddingUpdate {
  nodeId: string;
  content: string;
  metadata: VectorMetadata;
  operation: 'create' | 'update' | 'delete';
  priority: number;
}

Batch Processing:
- Group by priority and operation type
- Process high-priority updates immediately
- Batch low-priority updates (100 items/batch)
- Retry failed updates with exponential backoff
```

#### Content Extraction for Embeddings
- **Functions**: Full function signature + docstring + body summary
- **Classes**: Class definition + method signatures + properties
- **Interfaces**: Interface definition + method signatures
- **Files**: Import statements + key function signatures
- **Tests**: Test descriptions + assertions

### Synchronization Triggers

#### Automatic Triggers
- **File System Events**: Real-time via file watcher
- **Git Operations**: Post-commit, post-merge hooks
- **IDE Actions**: Save events, refactor operations
- **CI/CD Pipeline**: Post-deployment sync

#### Manual Triggers
```bash
# Full sync
memento sync --full

# Sync specific files
memento sync src/components/Button.tsx src/utils/helpers.ts

# Sync by pattern
memento sync --pattern "src/**/*.ts"

# Force re-index vector embeddings
memento sync --embeddings

# Dry run to see what would change
memento sync --dry-run
```

### Conflict Resolution

#### Concurrent Changes
**Optimistic Locking:**
- Each entity has version number
- Changes include expected version
- Conflicts detected and resolved via merge strategies

**Merge Strategies:**
- **Last Write Wins**: For simple metadata updates
- **Manual Resolution**: For conflicting code changes
- **Version Branching**: Create parallel versions for complex conflicts

#### Stale Data Detection
```typescript
interface StaleCheck {
  entityId: string;
  graphVersion: number;
  fileVersion: number;
  lastSync: Date;
  isStale: boolean;
}

// Check staleness every 5 minutes
setInterval(async () => {
  const staleEntities = await findStaleEntities();
  for (const entity of staleEntities) {
    await queueResync(entity);
  }
}, 5 * 60 * 1000);
```

### Performance Optimizations

#### Graph Database
- Use appropriate indexes for common query patterns
- Implement graph partitioning for large codebases
- Cache frequently accessed subgraphs
- Connection pooling for concurrent operations

#### Vector Database
- Batch embedding updates (50-100 items per batch)
- Use approximate nearest neighbor search for performance
- Implement metadata filtering for faster retrieval
- Cache frequently accessed embeddings

#### Caching Strategy
- Cache graph queries for recent changes (TTL: 10 minutes)
- Store computed impact analyses (TTL: 1 hour)
- Cache validation results (TTL: 30 minutes)
- Invalidate caches on related changes

### Monitoring and Observability

#### Synchronization Metrics
- Sync latency (file change to graph update)
- Queue depth and processing rate
- Error rates by sync type
- Vector embedding update success rate

#### Health Checks
- File watcher connectivity
- Graph database responsiveness
- Vector database synchronization status
- Queue processing health

#### Alerts
- Sync failures > 5% error rate
- Queue depth > 1000 items
- Sync latency > 30 seconds
- Vector embedding failures
- Unresolved synchronization alerts should persist across retention cycles; cleanup routines may only purge resolved entries older than the retention window.

### Recovery Mechanisms

#### Automatic Recovery
- Failed sync retries (3 attempts with backoff)
- Partial sync recovery (resume from last successful point)
- Database connection recovery with circuit breaker

#### Manual Recovery
```bash
# Reset sync state
memento sync --reset

# Rebuild from scratch
memento sync --rebuild

# Validate sync consistency
memento sync --validate
```

### Integration Points

#### Development Workflow Integration
- **Pre-commit hooks**: Validate sync state before commits
- **IDE plugins**: Real-time sync status in editor
- **CI/CD integration**: Sync validation in pipelines
- **Git integration**: Sync on branch switches and merges

#### External System Integration
- **Version Control**: Git hooks for sync triggers
- **CI/CD**: Pipeline steps for sync validation
- **Monitoring**: Metrics export to external systems
- **Backup**: Sync state included in backups

## Final System Architecture

### Complete Synchronization Flow
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   File System   │────│  File Watcher   │────│   Queue System  │
│   Changes       │    │   (chokidar)    │    │   (Priority)    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                        │                        │
         │                        │                        │
         ▼                        ▼                        ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│  Change Parser  │────│ Graph Database │────│Vector Database │
│ (ts-morph/tree- │    │  (FalkorDB)    │    │   (Qdrant)     │
│    sitter)      │    │                 │    │                │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                        │                        │
         │                        │                        │
         ▼                        ▼                        ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│ Impact Analysis │────│   Validation    │────│   Caching      │
│                 │    │   Engine        │    │   Layer        │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

### Synchronization Performance Targets
- **File Change Latency**: < 2 seconds from file save to graph update
- **Query Performance**: < 100ms for common graph queries
- **Vector Search**: < 50ms for semantic searches
- **Throughput**: 100+ file changes per minute
- **Reliability**: 99.9% sync success rate

### Disaster Recovery
- **Point-in-time Recovery**: Restore to any previous state
- **Incremental Backups**: Daily graph snapshots
- **Cross-region Replication**: Multi-zone database setup
- **Automated Failover**: Switch to backup systems automatically

## Enhanced Capabilities Integration

### Documentation-Centric Semantic Integration

#### Architecture Integration
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Code Parser   │────│   Doc Parser    │────│ Knowledge Graph │
│  (ts-morph)     │    │ (Markdown, etc) │    │   (FalkorDB)    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                        │                        │
         ▼                        ▼                        ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│ Documentation   │────│   LLM Engine    │────│ Semantic Links  │
│   Extraction    │    │ (Optional)      │    │ & Clustering    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

#### Documentation Node Types

##### DocumentationNode
**Represents documentation files and their content**
```
{
  id: string (UUID)
  type: "documentation"
  filePath: string
  title: string
  content: string (full text content)
  docType: string ("readme", "api-docs", "design-doc", "architecture", "user-guide")
  lastModified: timestamp
  lastIndexed: timestamp
  businessDomains: string[] (extracted business domains)
  stakeholders: string[] (mentioned stakeholders)
  technologies: string[] (mentioned technologies/frameworks)
  status: string ("active", "deprecated", "draft")
}
```

##### BusinessDomain
**Represents business domains extracted from documentation**
```
{
  id: string (UUID)
  type: "businessDomain"
  name: string
  description: string
  parentDomain?: string (for hierarchical domains)
  criticality: string ("core", "supporting", "utility")
  stakeholders: string[] ("users", "admins", "developers", "business")
  keyProcesses: string[] (business processes this domain supports)
  extractedFrom: string[] (documentation source IDs)
}
```

##### SemanticCluster
**Groups related code entities by business functionality**
```
{
  id: string (UUID)
  type: "semanticCluster"
  name: string
  description: string
  businessDomainId: string
  clusterType: string ("feature", "module", "capability", "service")
  cohesionScore: number (0-1, how tightly related entities are)
  lastAnalyzed: timestamp
  memberEntities: string[] (IDs of entities in this cluster)
}
```

#### Documentation Relationship Types

##### DESCRIBES_DOMAIN
- **Source**: DocumentationNode → BusinessDomain
- **Properties**: confidence, extraction method, last validated

##### BELONGS_TO_DOMAIN
- **Source**: CodebaseEntity → BusinessDomain
- **Properties**: strength, inferred vs explicit, last validated

##### DOCUMENTED_BY
- **Source**: CodebaseEntity → DocumentationNode
- **Properties**: documentation quality, coverage completeness, last sync

##### CLUSTER_MEMBER
- **Source**: CodebaseEntity → SemanticCluster
- **Properties**: membership strength, role in cluster, join date

##### DOMAIN_RELATED
- **Source**: BusinessDomain → BusinessDomain
- **Properties**: relationship type, strength, business context

#### Documentation Processing Pipeline

**1. Documentation Discovery:**
```typescript
async function discoverDocumentation(rootPath: string): Promise<DocumentationNode[]> {
  const patterns = [
    'README.md', 'docs/**/*.md', 'api/**/*.md',
    'architecture/**/*.md', 'design/**/*.md'
  ];

  const docs: DocumentationNode[] = [];
  for (const pattern of patterns) {
    const files = await glob(pattern, { cwd: rootPath });
    for (const file of files) {
      const content = await readFile(file);
      const doc = await parseDocumentation(file, content);
      docs.push(doc);
    }
  }

  return docs;
}
```

**2. Domain Extraction (Optional LLM Enhancement):**
```typescript
async function extractBusinessDomains(doc: DocumentationNode): Promise<BusinessDomain[]> {
  // Primary: Extract from explicit sections and patterns
  const explicitDomains = extractExplicitDomains(doc.content);

  // Secondary: LLM-assisted extraction for complex docs
  if (needsLLM(doc)) {
    const llmDomains = await llm.extract(`
      Extract business domains from this documentation:
      ${doc.content}

      Focus on:
      1. Business capabilities described
      2. User roles and stakeholders
      3. Business processes mentioned
      4. Domain boundaries and relationships
    `);
    return [...explicitDomains, ...llmDomains];
  }

  return explicitDomains;
}
```

**3. Semantic Clustering:**
```typescript
async function createSemanticClusters(
  entities: CodebaseEntity[],
  domains: BusinessDomain[]
): Promise<SemanticCluster[]> {
  const clusters: SemanticCluster[] = [];

  // Group entities by shared imports, calls, and domains
  const entityGroups = groupBySharedRelationships(entities);

  for (const group of entityGroups) {
    const cluster = new SemanticCluster({
      name: inferClusterName(group),
      description: inferClusterDescription(group, domains),
      businessDomainId: findRelevantDomain(group, domains),
      memberEntities: group.map(e => e.id),
      cohesionScore: calculateCohesion(group)
    });
    clusters.push(cluster);
  }

  return clusters;
}
```

#### Documentation Synchronization

**1. Documentation Change Detection:**
```typescript
async function syncDocumentation() {
  const docs = await discoverDocumentation(projectRoot);

  for (const doc of docs) {
    const existing = await findDocumentationByPath(doc.filePath);

    if (!existing || existing.lastModified < doc.lastModified) {
      await updateDocumentation(doc);
      await updateDomainRelationships(doc);
      await updateSemanticClusters(doc);
    }
  }
}
```

**2. Cluster Maintenance:**
```typescript
async function maintainClusters() {
  // Remove stale clusters
  const staleClusters = await findClustersWithLowCohesion();
  for (const cluster of staleClusters) {
    await dissolveCluster(cluster);
  }

  // Merge overlapping clusters
  const overlapping = await findOverlappingClusters();
  for (const pair of overlapping) {
    await mergeClusters(pair.cluster1, pair.cluster2);
  }

  // Update cluster descriptions
  const clusters = await getAllClusters();
  for (const cluster of clusters) {
    await updateClusterDescription(cluster);
  }
}
```

#### Query Patterns for Documentation-Centric Analysis

**Find code by business domain:**
```cypher
MATCH (d:BusinessDomain {name: $domainName})
MATCH (c:CodebaseEntity)-[:BELONGS_TO_DOMAIN]->(d)
OPTIONAL MATCH (c)-[:CLUSTER_MEMBER]->(sc:SemanticCluster)
RETURN c, sc.name as clusterName, sc.description as clusterDescription
ORDER BY c.path
```

**Get documentation for a code entity:**
```cypher
MATCH (c:CodebaseEntity {id: $entityId})
MATCH (c)-[:DOCUMENTED_BY]->(doc:DocumentationNode)
MATCH (doc)-[:DESCRIBES_DOMAIN]->(d:BusinessDomain)
RETURN doc.title, doc.content, d.name as businessDomain
ORDER BY doc.lastModified DESC
```

**Find clusters by business capability:**
```cypher
MATCH (d:BusinessDomain)-[:DOMAIN_RELATED*0..2]->(related:BusinessDomain)
WHERE d.name = $capabilityName
MATCH (sc:SemanticCluster)-[:BELONGS_TO_DOMAIN]->(related)
MATCH (c:CodebaseEntity)-[:CLUSTER_MEMBER]->(sc)
RETURN sc.name, sc.description, collect(c.path) as entities
ORDER BY sc.cohesionScore DESC
```

**Business impact analysis:**
```cypher
MATCH (d:BusinessDomain {name: $domainName})
MATCH (d)<-[:BELONGS_TO_DOMAIN]-(c:CodebaseEntity)
MATCH (c)-[:MODIFIED_IN]->(changes:Change)
WHERE changes.timestamp >= $sinceTime
RETURN c.path, count(changes) as changeCount,
       collect(changes.changeType) as changeTypes
ORDER BY changeCount DESC
```

#### Benefits of Documentation-Centric Approach

**1. Staleness Prevention:**
- Documentation updates automatically propagate to code entities
- No need to maintain separate intent annotations
- Changes to docs invalidate and refresh related analyses

**2. Cluster-Level Analysis:**
- Business context lives at the appropriate granularity
- Clusters group related functionality naturally
- Easier to understand system architecture

**3. Maintainability:**
- Documentation is explicitly maintained by teams
- Clear ownership and update processes
- Easier to audit and verify accuracy

**4. Flexibility:**
- Can use simple pattern matching for most docs
- LLM enhancement only where needed
- Gradual adoption possible

#### API Integration Points

```typescript
interface DocumentationCentricAPI {
  // Documentation Management
  syncDocumentation(): Promise<void>;
  getDocumentationForEntity(entityId: string): Promise<DocumentationNode[]>;

  // Domain Analysis
  getBusinessDomains(): Promise<BusinessDomain[]>;
  getEntitiesByDomain(domainName: string): Promise<CodebaseEntity[]>;

  // Clustering
  getSemanticClusters(): Promise<SemanticCluster[]>;
  getClusterMembers(clusterId: string): Promise<CodebaseEntity[]>;

  // Business Intelligence
  getBusinessImpact(domainName: string, since: Date): Promise<BusinessImpact>;
  analyzeDomainDependencies(domainName: string): Promise<DomainDependencies>;
}
```

### Test Performance Integration

#### Test-to-Function Connection Architecture
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Test Runner   │────│  Test Parser    │────│ Knowledge Graph │
│ (Jest/Vitest)   │    │                 │    │   (FalkorDB)    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                        │                        │
         ▼                        ▼                        ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│ Test Execution  │────│ Performance     │────│ Coverage Links  │
│   Results       │    │   Metrics       │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

#### Enhanced Test Node Types

##### PerformanceTest
**Tracks test performance metrics**
```
{
  ...Test,
  type: "performanceTest"
  averageExecutionTime: number (ms)
  p95ExecutionTime: number (ms)
  successRate: number (0-1)
  isFlaky: boolean
  lastFailureReason: string
  performanceTrend: string ("improving", "stable", "degrading")
  benchmarkValue?: number
  benchmarkUnit?: string ("ms", "MB", "ops/sec")
}
```

##### TestCoverageLink
**Connects tests to specific functions/code paths**
```
{
  id: string (UUID)
  type: "testCoverageLink"
  testId: string
  functionId: string
  coverageType: string ("unit", "integration", "e2e")
  codePaths: string[] (specific code paths tested)
  assertions: string[] (what the test validates)
  performanceImpact: number (how much test affects performance)
  lastExecuted: timestamp
  executionCount: number
}
```

#### Performance Relationship Types

##### PERFORMANCE_IMPACT
- **Source**: Function → Test
- **Properties**: execution time impact, resource usage, scalability impact

##### COVERAGE_PROVIDES
- **Source**: Test → Function
- **Properties**: coverage percentage, test quality score, confidence level

##### PERFORMANCE_REGRESSION
- **Source**: Change → Function
- **Properties**: performance delta, regression severity, affected tests

#### Test Performance Queries

**Find performance bottlenecks:**
```cypher
MATCH (f:Function)-[:TESTED_BY]->(t:PerformanceTest)
WHERE t.averageExecutionTime > 1000
RETURN f.name, t.averageExecutionTime, t.successRate
ORDER BY t.averageExecutionTime DESC
```

**Identify flaky tests:**
```cypher
MATCH (t:PerformanceTest)
WHERE t.isFlaky = true AND t.successRate < 0.95
MATCH (t)-[:TESTS]->(f:Function)
RETURN t.name, f.name, t.successRate, t.lastFailureReason
```

**Performance regression detection:**
```cypher
MATCH (c:Change)-[:AFFECTS]->(f:Function)
MATCH (f)-[:TESTED_BY]->(t:PerformanceTest)
WHERE t.performanceTrend = "degrading"
RETURN c.changeType, f.name, t.performanceTrend, c.timestamp
```

### Security Tooling Integration

#### Security Integration Architecture
```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│  Security Tools │────│ Security Parser │────│ Knowledge Graph │
│ (SAST, SCA, etc)│    │                 │    │   (FalkorDB)    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                        │                        │
         ▼                        ▼                        ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│ Vulnerability   │────│ Security Issues │────│ Risk Assessment │
│   Scanner       │    │                 │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

#### Security Node Types

##### SecurityIssue
**Security vulnerabilities and findings**
```
{
  id: string (UUID)
  type: "securityIssue"
  tool: string ("eslint-security", "semgrep", "snyk", "owasp-zap")
  ruleId: string
  severity: string ("critical", "high", "medium", "low", "info")
  title: string
  description: string
  cwe: string (Common Weakness Enumeration)
  owasp: string (OWASP category)
  affectedEntityId: string
  lineNumber: number
  codeSnippet: string
  remediation: string
  status: string ("open", "fixed", "accepted", "false-positive")
  discoveredAt: timestamp
  lastScanned: timestamp
  confidence: number (0-1)
}
```

##### Vulnerability
**Dependency vulnerabilities**
```
{
  id: string (UUID)
  type: "vulnerability"
  packageName: string
  version: string
  vulnerabilityId: string (CVE, GHSA, etc)
  severity: string
  description: string
  cvssScore: number
  affectedVersions: string
  fixedInVersion: string
  publishedAt: timestamp
  lastUpdated: timestamp
  exploitability: string ("high", "medium", "low")
}
```

#### Security Relationship Types

##### HAS_SECURITY_ISSUE
- **Source**: Entity → SecurityIssue
- **Properties**: severity, status, discovered date

##### DEPENDS_ON_VULNERABLE
- **Source**: Module → Vulnerability
- **Properties**: severity, exposure level, remediation status

##### SECURITY_IMPACTS
- **Source**: SecurityIssue → Function/File
- **Properties**: attack vector, potential impact, exploitability

#### Integrated Security Tools

**1. ESLint Security Rules:**
```javascript
// .eslintrc.js
{
  "plugins": ["security"],
  "extends": ["plugin:security/recommended"],
  "rules": {
    "security/detect-object-injection": "error",
    "security/detect-eval-with-expression": "error",
    "security/detect-no-csrf-before-method-override": "error"
  }
}
```

**2. SAST Tools (Semgrep, CodeQL):**
- Pattern-based vulnerability detection
- Custom rules for business-specific security issues
- Integration with CI/CD pipelines

**3. SCA Tools (Snyk, OWASP Dependency Check):**
- Dependency vulnerability scanning
- License compliance checking
- Outdated package detection

**4. Secret Detection:**
- API key detection
- Credential exposure prevention
- Integration with git hooks

#### Security Analysis Queries

**Critical security issues:**
```cypher
MATCH (si:SecurityIssue)
WHERE si.severity = "critical" AND si.status = "open"
MATCH (si)-[:HAS_SECURITY_ISSUE]->(entity)
RETURN si.title, entity.path, si.lineNumber, si.remediation
ORDER BY si.discoveredAt DESC
```

**Vulnerable dependencies:**
```cypher
MATCH (m:Module)-[:DEPENDS_ON_VULNERABLE]->(v:Vulnerability)
WHERE v.severity IN ["critical", "high"]
RETURN m.name, v.vulnerabilityId, v.cvssScore, v.fixedInVersion
ORDER BY v.cvssScore DESC
```

**Security debt by file:**
```cypher
MATCH (f:File)-[:HAS_SECURITY_ISSUE]->(si:SecurityIssue)
WHERE si.status = "open"
RETURN f.path,
       count(si) as issueCount,
       collect(si.severity) as severities,
       collect(si.title) as issues
ORDER BY issueCount DESC
```

### Integration Workflow

#### Combined Enhancement Pipeline
```
Code Change → Parse → LLM Analysis → Test Execution → Security Scan
      ↓            ↓            ↓            ↓            ↓
  Graph Update → Intent Store → Performance Record → Issue Creation → Risk Assessment
      ↓            ↓            ↓            ↓            ↓
  Impact Analysis → Business Context → Performance Alerts → Security Reports → Recommendations
```

#### API Integration Points
```typescript
interface EnhancedKnowledgeGraphAPI {
  // Documentation Integration
  syncDocumentation(): Promise<void>;
  getDocumentationForEntity(entityId: string): Promise<DocumentationNode[]>;

  // Domain Analysis
  getBusinessDomains(): Promise<BusinessDomain[]>;
  getEntitiesByDomain(domainName: string): Promise<CodebaseEntity[]>;

  // Test Integration
  recordTestExecution(testId: string, results: TestResults): Promise<void>;
  getPerformanceMetrics(functionId: string): Promise<PerformanceMetrics>;

  // Security Integration
  scanForSecurityIssues(entityId: string): Promise<SecurityIssue[]>;
  getVulnerabilityReport(): Promise<VulnerabilityReport>;

  // Clustering
  getSemanticClusters(): Promise<SemanticCluster[]>;
  getClusterMembers(clusterId: string): Promise<CodebaseEntity[]>;

  // Combined Analysis
  getEntityInsights(entityId: string): Promise<EntityInsights>;
}
```

#### Performance & Reliability Enhancements

**1. LLM Caching:**
- Cache LLM responses for similar code patterns
- Invalidate cache on code changes
- Fallback to rule-based analysis when LLM unavailable

**2. Test Performance Monitoring:**
- Track test execution times over time
- Alert on performance regressions
- Correlate test performance with code complexity

**3. Security Scan Scheduling:**
- Daily automated security scans
- On-demand scans for critical changes
- Incremental scans for modified files only

This enhanced knowledge graph would provide comprehensive insights into:
- **Business Context**: Why code exists and its business value
- **Performance Characteristics**: How code performs and scales
- **Security Posture**: Vulnerabilities and security issues
- **Quality Metrics**: Beyond structure to include semantic and performance quality

The system would transform from a structural code analyzer into a business-aware, performance-conscious, security-focused development intelligence platform.
</file>

<file path="Docs/Blueprints/performance-relationships.md">
# Performance Relationship Blueprint

## 1. Overview
Performance relationships (`PERFORMANCE_IMPACT`, `PERFORMANCE_REGRESSION`) capture how code changes affect benchmarks, latency budgets, and resource usage. They serve incident analysis, regression prevention, and optimization prioritization.

## 2. Current Gaps
- **Instrumentation depth**: Relationship ingestion now emits timing/backpressure telemetry through `PostgreSQLService.bulkQuery`. Capture `lastBatch`, `slowBatches`, and queue-depth metrics (see §14) so regression hunts remain debuggable under load.
- **Backpressure & sizing**: Batch ingestion exposes tuning knobs (`warnOnLargeBatchSize`, `slowBatchThresholdMs`, `queueDepthWarningThreshold`, `historyLimit`) but we still need adaptive throttling once load characteristics settle. Document the rollout policy and revisit dynamic control loops as production data arrives.
- **Data lifecycle**: Temporal edges now persist with provenance, but we still need archival/retention guidance for high-volume metrics. Define rotation or downsampling strategies before perf dashboards grow beyond manageable storage size.
- **JSON/JSONB contracts**: API responses now surface parsed JSON objects for `metadata` and `metricsHistory`. Document how callers can opt into raw strings (if required) and keep the contract in sync across unit/integration suites.
- **Fixtures & load coverage**: Integration datasets seed 60+ snapshot rows (`tests/integration/services/TestEngine.integration.test.ts`) to reflect load-test behaviour; expand to >100 rows when stress-testing queue length and pagination paths.

## 3. Desired Capabilities
1. Define a robust ingestion contract representing benchmarks, scenarios, and statistical metrics.
2. Persist metrics and context (baseline, current value, delta, unit, sample size, environment) without flattening to generic metadata.
3. Support multiple metrics per entity and scenario, enabling comparisons across benchmarks.
4. Provide query filters for metric IDs, threshold breaches, trend directions, and environment.
5. Integrate with history to track when regressions occurred/resolved and support reporting.

## 4. Inputs & Consumers
- **Ingestion Sources**: Benchmark harnesses, profiling tools, performance CI jobs, production telemetry summarizers.
- **Consumers**: Performance dashboards, regression detection bots, CI gates, release readiness checks, impact analysis features.

## 5. Schema & Metadata Requirements
| Field | Type | Notes |
| --- | --- | --- |
| `metricId` | string | Unique identifier for metric (e.g., `benchmark/api/login-latency`).
| `environment` | enum (`dev`, `staging`, `prod`, `perf-lab`, etc.) | Where measurement was taken.
| `baselineValue` | number | Baseline measurement (ms, MB, ops/s); stored with 4 decimals.
| `currentValue` | number | Current run measurement.
| `unit` | string | Measurement unit; standardize (ms, MB, req/s).
| `delta` | number | `currentValue - baselineValue`; convenience.
| `percentChange` | number | (current - baseline)/baseline * 100.
| `sampleSize` | integer | Number of samples/runs.
| `confidenceInterval` | object | Optional (lower, upper) values.
| `trend` | enum (`regression`, `improvement`, `neutral`) | Derived from delta.
| `severity` | enum (`critical`, `high`, `medium`, `low`) | Based on threshold policy.
| `evidence` | array | Link to benchmark run artifacts, flamegraphs.
| `runId` | string | Reference to run entity.
| `metadata.metrics` | array | Additional metrics (variance, percentiles).
| `detectedAt`, `resolvedAt` | timestamps | Lifecycle markers.

## 6. Normalization Strategy
1. Implement `normalizePerformanceRelationship`:
   - Require `metricId` and sanitize to lowercase with `/` separators; limit length.
   - Validate numeric fields; convert string numbers; clamp to sensible ranges.
   - Compute `delta`, `percentChange`, and `trend` if not provided.
   - Map severity using policy thresholds (config-driven) while forcing improvements/negative deltas to settle at `low` severity so resolved trends no longer surface as regressions. Optionally store `policyId` referencing threshold config.
   - Promote `environment`, `unit`, `sampleSize`, `confidenceInterval`, `runId`, `metrics` array.
   - Merge evidence entries referencing benchmark artifacts.
2. Validate that `baselineValue` is non-zero before computing percent change; handle zero baseline gracefully.
3. Derive `riskScore` to compare regressions; keep in metadata.

## 7. Persistence & Merge Mechanics
1. **Canonical ID**: Extend to include `metricId` and optionally `environment`. Example: `rel_perf_sha1(from|metricId|environment|type)`.
2. **Cypher Projections**: Add scalar fields for `metricId`, `environment`, `baselineValue`, `currentValue`, `delta`, `percentChange`, `unit`, `sampleSize`, `trend`, `severity`, `runId`, `confidenceInterval`, `detectedAt`, `resolvedAt`, `riskScore`, `metadata.metrics` JSON.
3. **Merge Rules**:
   - When new data arrives for same metric + environment, append to `metricsHistory` (bounded array) and update `currentValue`, `trend`, `severity`.
   - Maintain rolling statistics (e.g., exponential moving average) for trending analysis.
   - When regression resolves (values return within threshold), update `status` to `resolved`, set `resolvedAt`, but keep history for reporting.
4. **Auxiliary Entities**: Optionally create `benchmark_run` nodes to store detailed runs and link edges; re-use for dashboards.
5. **Indexes**: `(metricId)`, `(type, severity)`, `(environment)`, `(trend)`, optionally `(type, metricId, environment)` composite.

## 8. Query & API Surface
1. `getRelationships` now accepts `metricId`, `environment`, `severity`, `trend`, `detectedAfter`, `detectedBefore`, and resolution filters. Document combined usage patterns (e.g., “critical regressions in staging last 7 days”).
2. REST + MCP `tests.performance` endpoints return aggregate metrics **and** a `history` array of `performance_metric_snapshots` (see §8). Extend blueprints with concrete payloads so dashboard builders can align on shapes.
3. Follow-up helpers remain desirable: `getPerformanceRegressions`, `getMetricHistory`, and `getEntityPerformanceSummary` should reuse the snapshot storage once prioritised.

## 9. Temporal & Auditing
1. Integrate with history pipeline: open edges when regression detected, close when resolved; maintain `validFrom/validTo` reflecting active regression periods.
2. Store `metricsHistory` or project to version nodes for deeper timeline analytics.
3. Provide timeline queries to show metric performance over time for specific entities or metrics; leverage the persisted `metricsHistory` arrays as the compact historical trace.

## 10. Migration & Backfill Plan
1. Develop ingestion spec and update instrumentation to emit performance edges; ensure compatibility with new schema.
2. Backfill from existing benchmark archives or telemetry (if available) to seed baseline data.
3. Provide script to import historical regression incidents (if tracked in spreadsheets or logs).
4. Validate by comparing new graph data against existing performance dashboards; align thresholds.

## 11. Risks & Mitigations
| Risk | Mitigation |
| --- | --- |
| Missing or inconsistent metric IDs across tools | Define shared catalog of metrics; enforce via ingestion contract. |
| Data volume (frequent benchmarks) causing large history | Cap history arrays; offload raw runs to storage with references. |
| Unit mismatches leading to incorrect trends | Normalize units at ingestion; reject edges with conflicting units. |
| Threshold policy drift | Store `policyId` and version; update edges when policy changes. |

## 12. Implementation Milestones
1. Finalize ingestion contract and update emitter tools.
2. Implement normalization/persistence updates + tests.
3. Extend queries/APIs and add indexes.
4. Roll out ingestion with feature flag; monitor metrics.
5. Build dashboards and alerting pipelines.

## 13. Open Questions
- Should regression detection happen upstream or within graph service (e.g., schedule job comparing metrics)?
- Do we need environment-specific nodes or metadata suffices?
- How will we manage baselines—per branch, per release, per environment?
- What retention/archival policy should apply to historical performance data to keep graph manageable?

## 14. Telemetry & Backpressure Configuration
- `PostgreSQLService.bulkQuery` now records per-batch telemetry (duration, batch size, queue depth, success state) and aggregates (`totalBatches`, `totalQueries`, rolling `history`, `slowBatches`). Access these snapshots via `DatabaseService.getPostgresBulkWriterMetrics()` or directly from the PostgreSQL service for observability tooling.
- Default thresholds: `warnOnLargeBatchSize = 50`, `slowBatchThresholdMs = 750`, `queueDepthWarningThreshold = 3`, `historyLimit = 10`. Override them by supplying `bulkConfig` when constructing `PostgreSQLService` (and therefore `DatabaseService`).
- `slowBatches` captures any batch that exceeds thresholds or fails; use it for alert routing and throttling decisions. `history` is capped to `historyLimit` entries to avoid unbounded growth.
- The high-volume integration scenario (`tests/integration/services/TestEngine.integration.test.ts`) inserts 60 performance snapshots via `postgresBulkQuery`, ensuring telemetry and throttling safeguards stay regression-tested.
</file>

<file path="Docs/Blueprints/security-relationships.md">
# Security Relationship Blueprint

## 1. Overview
Security relationships (`HAS_SECURITY_ISSUE`, `DEPENDS_ON_VULNERABLE`, `SECURITY_IMPACTS`) map code artifacts to vulnerability data, remediation plans, and security posture insights. They should capture severity, status, evidence, and provenance to support risk management workflows.

## 2. Current Gaps
- Security scanner emits severity, status, CVSS score, advisory IDs, and evidence paths but persistence only retains core identifiers.
- Canonical IDs collapse distinct findings affecting the same target, leading to data loss when multiple CVEs apply.
- No lifecycle tracking (open/fixed/accepted) exists; history stubs cannot capture remediation timelines.
- Query interfaces cannot filter by severity, status, or specific vulnerability identifiers, limiting dashboards and automation.
- Dependency scanning routines require outbound calls to OSV during unit tests; lack of a mockable client makes offline runs noisy and potentially flaky. Introduce an injectable advisory provider with deterministic fixtures before claiming hermetic coverage.
- Security scanner now provisions unique constraints through `GRAPH.CONSTRAINT CREATE` with an index preflight and falls back to the legacy Cypher only when running against older engines; continue monitoring for engines that report missing index support despite the preflight.
- OSV batch responses only include `id`/`modified`, forcing the scanner to re-query individual advisories for full metadata. The new fallback restores CVE normalization but doubles external calls per dependency; design caching or mirror strategy before scaling to larger dependency graphs.

## 3. Desired Capabilities
1. Persist vulnerability metadata (severity, status, CVSS, advisory ID, exploitability, CWE) with evidence and remediation references.
2. Track multiple findings per entity without collisions and maintain their lifecycle transitions.
3. Allow filtering by severity, status, vulnerability ID, exploitability, owning team, and evidence freshness.
4. Integrate with temporal history to audit remediation progress and enforce policies.
5. Reference external vulnerability sources (OSV, CVE) while storing local assessment state.

## 4. Inputs & Consumers
- **Ingestion Sources**: Static analysis scanners, dependency scanners (Snyk, OSS Index), manual security reviews, runtime telemetry.
- **Consumers**: Security dashboards, CI/CD gates, incident response tooling, dependency update automation, compliance reporting.

## 5. Schema & Metadata Requirements
| Field | Type | Notes |
| --- | --- | --- |
| `findingId` | string | Stable identifier (CVE, advisory ID, or generated hash); required to disambiguate.
| `severity` | enum (`critical`, `high`, `medium`, `low`, `info`) | Map upstream strings.
| `status` | enum (`open`, `fixed`, `accepted`, `false-positive`, `investigating`) | Lifecycle state.
| `cvssScore` | number (0-10) | Clamp and store with 2 decimals.
| `cvssVector` | string | Optional detailed vector string.
| `cweIds` | string[] | CWE identifiers; optional.
| `source` | enum (`scanner`, `manual`, `imported`) | Origin of finding.
| `evidence` | array | Reuse `EdgeEvidence` with additional context (file path, line, note).
| `remediation` | string | Summary of remediation guidance.
| `ownerTeam` | string | Responsible team.
| `statusHistory` | array | Chronological record of status transitions.
| `detectedAt`, `resolvedAt` | timestamps | Derived from status history.
| `metadata.external` | object | Links to external advisories, patches.

## 6. Normalization Strategy
1. **Helper `normalizeSecurityRelationship`**:
   - Require `findingId`; if missing, generate deterministic hash from advisory data.
   - Normalize `severity`, `status`, `source` to canonical enums; log unknown values.
   - Clamp `cvssScore`, parse `cvssVector` for validation, ensure `cweIds` sanitized.
   - Promote `remediation`, `ownerTeam`, `statusHistory`, `detectedAt`, `resolvedAt` from metadata.
   - Merge evidence arrays, ensuring they include file paths and note context.
2. **Validation**: Reject edges lacking `findingId` or `severity`; track metrics on missing fields. Validate chronological order in `statusHistory`.
3. **Risk Scoring**: Optionally derive `riskScore` based on severity, exploitability, coverage; store for quick sorting.

## 7. Persistence & Merge Mechanics
1. **Canonical ID**: Compute `rel_security_sha1(from|to|type|findingId)` ensuring unique edge per finding. Keep fallback to old scheme during migration.
2. **Cypher Writes**: Add columns for `findingId`, `severity`, `status`, `cvssScore`, `cvssVector`, `cweIds` JSON, `source`, `remediation`, `ownerTeam`, `riskScore`, `statusHistory`, `detectedAt`, `resolvedAt`.
3. **Merge Rules**:
   - When status updates arrive, append to `statusHistory` (dedupe by timestamp) and update `status`, `resolvedAt` accordingly.
   - Keep highest severity when scanners disagree (configurable precedence); preserve manual overrides with flag `isManualOverride`.
   - Maintain `metadata.origins` to track multiple scanners contributing evidence.
4. **Auxiliary Entities**: Model vulnerabilities as nodes (`vulnerability { id: findingId, source }`) linked via `HAS_SECURITY_ISSUE`. Security edge can then connect code -> vulnerability; ensures reusability across repos.
5. **Indexes**: `(findingId)`, `(type, severity)`, `(status)`, `(ownerTeam)`; optionally composite `(type, status, severity)` for dashboards.

## 8. Query & API Surface
1. Enhance `getRelationships` with filters for `findingId`, `severity`, `status`, `cvssScoreMin/Max`, `ownerTeam`, `detectedBefore/After`, `resolved` flag.
2. Provide helper APIs:
   - `getOpenVulnerabilities({ severityMin, team, limit })` returning prioritized list.
   - `getVulnerabilityTimeline(findingId)` showing status history and impacted entities.
   - `getVulnerableDependencies(entityId)` to drive dependency upgrades.
3. Integrate with admin/security routes to display aggregated counts and trending metrics.

## 9. Temporal & Auditing
1. Use `openEdge`/`closeEdge` semantics to reflect vulnerability lifecycle: edge is active while status is `open`/`investigating`; closing sets `validTo` when resolved or accepted.
2. Store `statusHistory` and optionally project events into dedicated timeline nodes for analytics.
3. Provide audit reports enumerating vulnerabilities resolved in time windows, reopens, and accepted exceptions.

## 10. Migration & Backfill Plan
1. **Schema Migration**: Add new properties and indexes; guard canonical ID change behind feature flag.
2. **Reingestion**: Re-run security scanners or ingest stored findings to populate new fields.
3. **Manual Import**: Provide tool to import accepted/waived vulnerabilities from security team spreadsheets.
4. **Cleanup**: Identify duplicates from old canonical ID scheme; merge or retire as appropriate.
5. **Validation**: Compare counts of open vulnerabilities pre/post migration; ensure severity distributions align.

## 11. Risks & Mitigations
| Risk | Mitigation |
| --- | --- |
| Scanner churn causing inconsistent finding IDs | Use deterministic hash fallback and maintain mapping table. |
| Manual overrides overwritten by automated scans | Apply precedence rules; require explicit flag to downgrade manual status. |
| Status history growth | Cap history for storage; move older events to separate timeline nodes if needed. |
| Query cost due to heavy filters | Add indexes and precomputed aggregates; monitor stats. |

## 12. Implementation Milestones
1. Build normalization helper/tests.
2. Update persistence and canonical ID; add migrations.
3. Extend query APIs and update security routes.
4. Reingest security data and validate dashboards.
5. Roll out temporal tracking and compliance reporting.

## 13. Open Questions
- How do we handle environment-specific findings (prod vs. staging)? Separate edges or metadata flag?
- Should we differentiate dependency vulnerabilities vs. code issues via additional relationship types or metadata fields?
- Do we need SLA tracking integrated (due dates per severity) and where should that live?
- How to integrate with external vulnerability management systems (bi-directional sync or read-only import)?
</file>

<file path="Docs/MementoAPIDesign.md">
# Memento API Design

## Overview

The Memento API provides comprehensive access to the knowledge graph system, enabling AI agents and developers to interact with codebase analysis, documentation, testing, and security capabilities. The API is exposed through multiple interfaces:

- **MCP Server** (Claude Code compatible)
- **HTTP REST API** (OpenAI function-calling compatible)
- **WebSocket API** (Real-time updates)
- **MCP API** (Model Context Protocol for AI assistants)

## Core Concepts

### Response Types
```typescript
interface APIResponse<T> {
  success: boolean;
  data?: T;
  error?: {
    code: string;
    message: string;
    details?: any;
  };
  metadata?: {
    requestId: string;
    timestamp: Date;
    executionTime: number;
  };
}

interface PaginatedResponse<T> extends APIResponse<T[]> {
  pagination: {
    page: number;
    pageSize: number;
    total: number;
    hasMore: boolean;
  };
}
```

### Common Parameters
```typescript
interface BaseQueryParams {
  limit?: number;
  offset?: number;
  sortBy?: string;
  sortOrder?: 'asc' | 'desc';
  includeMetadata?: boolean;
}

interface TimeRangeParams {
  since?: Date;
  until?: Date;
  timeRange?: '1h' | '24h' | '7d' | '30d' | '90d';
}
```

---

## 1. Design & Specification Management

### 1.1 Create Specification
**Endpoint:** `POST /api/design/create-spec`
**MCP Tool:** `design.create_spec`

Creates and validates a feature specification with acceptance criteria.

```typescript
interface CreateSpecRequest {
  title: string;
  description: string;
  goals: string[];
  acceptanceCriteria: string[];
  priority?: 'low' | 'medium' | 'high' | 'critical';
  assignee?: string;
  tags?: string[];
  dependencies?: string[]; // Spec IDs this depends on
}

interface CreateSpecResponse {
  specId: string;
  spec: Spec;
  validationResults: {
    isValid: boolean;
    issues: ValidationIssue[];
    suggestions: string[];
  };
}

async function createSpec(params: CreateSpecRequest): Promise<CreateSpecResponse>
```

**Example:**
```typescript
const result = await createSpec({
  title: "User Authentication System",
  description: "Implement secure user login and registration",
  acceptanceCriteria: [
    "Users can register with email/password",
    "Users can login with valid credentials",
    "Invalid login attempts are rejected",
    "Passwords are securely hashed"
  ]
});
```

### 1.2 Get Specification
**Endpoint:** `GET /api/design/specs/{specId}`
**MCP Tool:** `design.get_spec`

Retrieves a specification with full details.

```typescript
interface GetSpecResponse {
  spec: Spec;
  relatedSpecs: Spec[];
  affectedEntities: CodebaseEntity[];
  testCoverage: TestCoverage;
  status: 'draft' | 'approved' | 'implemented' | 'deprecated';
}

async function getSpec(specId: string): Promise<GetSpecResponse>
```

### 1.3 Update Specification
**Endpoint:** `PUT /api/design/specs/{specId}`
**MCP Tool:** `design.update_spec`

Updates an existing specification.

```typescript
interface UpdateSpecRequest {
  title?: string;
  description?: string;
  acceptanceCriteria?: string[];
  status?: 'draft' | 'approved' | 'implemented' | 'deprecated';
  priority?: 'low' | 'medium' | 'high' | 'critical';
}

async function updateSpec(specId: string, updates: UpdateSpecRequest): Promise<Spec>
```

### 1.4 List Specifications
**Endpoint:** `GET /api/design/specs`
**MCP Tool:** `design.list_specs`

Lists specifications with filtering options.

```typescript
interface ListSpecsParams extends BaseQueryParams {
  status?: string[];
  priority?: string[];
  assignee?: string;
  tags?: string[];
  search?: string;
}

async function listSpecs(params?: ListSpecsParams): Promise<PaginatedResponse<Spec>>
```

---

## 2. Test Management

### 2.1 Plan and Generate Tests
**Endpoint:** `POST /api/tests/plan-and-generate`
**MCP Tool:** `tests.plan_and_generate`

Generates test plans and implementations for a specification.

- Leverages knowledge graph relationships (`REQUIRES`, `IMPLEMENTS_SPEC`, `VALIDATES`) to map acceptance criteria to concrete code targets and existing tests.
- Generates `TestSpec` entries per criterion and test type, including assertions, target symbols, and data requirements.
- Estimates coverage uplift based on existing coverage history plus projected impact from newly generated tests.
- Returns a `changedFiles` list derived from impacted code and test artefacts so tooling can stage updates automatically.

```typescript
interface TestPlanRequest {
  specId: string;
  testTypes?: ('unit' | 'integration' | 'e2e')[];
  coverage?: {
    minLines?: number;
    minBranches?: number;
    minFunctions?: number;
  };
  includePerformanceTests?: boolean;
  includeSecurityTests?: boolean;
}

interface TestPlanResponse {
  testPlan: {
    unitTests: TestSpec[];
    integrationTests: TestSpec[];
    e2eTests: TestSpec[];
    performanceTests: TestSpec[];
  };
  estimatedCoverage: CoverageMetrics;
  changedFiles: string[];
}

async function planAndGenerateTests(params: TestPlanRequest): Promise<TestPlanResponse>
```

**Sample Response**

```json
{
  "testPlan": {
    "unitTests": [
      {
        "name": "[AC-1] Unit chargeCustomer happy path",
        "description": "Validate acceptance criterion AC-1 for Checkout Workflow.",
        "type": "unit",
        "targetFunction": "chargeCustomer",
        "assertions": [
          "Implements acceptance criterion AC-1: Order succeeds with valid payment",
          "Covers chargeCustomer core behaviour and edge conditions",
          "Establishes regression harness for new functionality"
        ],
        "dataRequirements": [
          "Include dataset covering valid payment tokens.",
          "Provide negative cases capturing rejection paths."
        ]
      }
    ],
    "integrationTests": [
      {
        "name": "[AC-1] Integration chargeCustomer ↔ ledger",
        "description": "Exercise system collaboration for Checkout Workflow. Cover integration between chargeCustomer, ledger writer, payments API.",
        "type": "integration",
        "targetFunction": "chargeCustomer & ledger",
        "assertions": [
          "Coordinates chargeCustomer, ledger writer, payments API end-to-end",
          "Verifies cross-cutting requirements for AC-1: Order succeeds with valid payment",
          "Document integration contract assumptions and fixtures"
        ],
        "dataRequirements": [
          "Provision upstream and downstream fixtures mirroring production.",
          "Include dataset covering declined card responses."
        ]
      },
      {
        "name": "Checkout Workflow security posture",
        "description": "Validate authentication, authorization, and data handling rules tied to Checkout Workflow.",
        "type": "integration",
        "targetFunction": "Checkout Workflow",
        "assertions": [
          "Rejects requests lacking required claims or tokens",
          "Enforces least privilege access for privileged operations",
          "Scrubs sensitive fields from logs and downstream payloads"
        ],
        "dataRequirements": [
          "Generate signed and tampered tokens",
          "Include role combinations from spec metadata",
          "Verify encryption-in-transit and at-rest paths"
        ]
      }
    ],
    "e2eTests": [
      {
        "name": "Checkout Workflow happy path flow",
        "description": "Exercise the primary workflow covering 2 acceptance criteria for Checkout Workflow.",
        "type": "e2e",
        "targetFunction": "Checkout Workflow",
        "assertions": [
          "Satisfies AC-1: Order succeeds with valid payment",
          "Satisfies AC-2: Order fails with declined card"
        ],
        "dataRequirements": [
          "Mirror production-like happy path data and environment.",
          "Enumerate rollback or recovery steps for failed stages."
        ]
      }
    ],
    "performanceTests": [
      {
        "name": "Checkout Workflow performance guardrail",
        "description": "Protect high priority specification against latency regressions by validating hot paths under load.",
        "type": "performance",
        "targetFunction": "chargeCustomer",
        "assertions": [
          "Throughput remains within baseline for chargeCustomer",
          "P95 latency does not regress beyond 10% of current benchmark",
          "Resource utilization stays below allocated service limits"
        ],
        "dataRequirements": [
          "Replay representative production workload",
          "Include peak load burst scenarios",
          "Capture CPU, memory, and downstream dependency timings"
        ]
      }
    ]
  },
  "estimatedCoverage": {
    "lines": 78,
    "branches": 70,
    "functions": 74,
    "statements": 77
  },
  "changedFiles": [
    "src/services/payments.ts",
    "tests/integration/checkout.test.ts"
  ]
}
```

### 2.2 Record Test Execution
**Endpoint:** `POST /api/tests/record-execution`
**MCP Tool:** `tests.record_execution`

Records test execution results and updates performance metrics.

```typescript
interface TestExecutionResult {
  testId: string;
  testSuite: string;
  testName: string;
  status: 'passed' | 'failed' | 'skipped' | 'error';
  duration: number; // milliseconds
  errorMessage?: string;
  stackTrace?: string;
  coverage?: {
    lines: number;
    branches: number;
    functions: number;
    statements: number;
  };
  performance?: {
    memoryUsage?: number;
    cpuUsage?: number;
    networkRequests?: number;
  };
}

async function recordTestExecution(results: TestExecutionResult[]): Promise<void>
```

### 2.3 Get Performance Metrics
**Endpoint:** `GET /api/tests/performance/{entityId}`
**MCP Tool:** `tests.get_performance`

Retrieves aggregate performance metrics for a specific entity and the persisted snapshot history that backs the calculation.

**Query Parameters**
- `metricId` (optional) — narrow history to a particular metric key (e.g. `test/foo-latency/p95`).
- `environment` (optional) — sanitized automatically (`Production` → `prod`, etc.).
- `severity` (optional) — `critical | high | medium | low`.
- `limit` (optional) — max history rows per entity (default `100`).
- `days` (optional) — only include snapshots detected within the last N days.

```typescript
interface PerformanceMetrics {
  averageExecutionTime: number;
  p95ExecutionTime: number;
  successRate: number;
  trend: 'improving' | 'stable' | 'degrading';
  benchmarkComparisons: {
    benchmark: string;
    value: number;
    status: 'above' | 'below' | 'at';
    threshold: number;
  }[];
  historicalData: {
    timestamp: Date;
    executionTime: number;
    successRate: number;
    coveragePercentage: number;
  }[];
}

interface PerformanceSnapshot {
  metricId: string;
  scenario?: string;
  environment?: string;
  severity?: 'critical' | 'high' | 'medium' | 'low';
  trend?: 'regression' | 'improvement' | 'neutral';
  unit?: string;
  baselineValue?: number | null;
  currentValue?: number | null;
  delta?: number | null;
  percentChange?: number | null;
  sampleSize?: number | null;
  riskScore?: number | null;
  runId?: string;
  detectedAt?: Date | null;
  resolvedAt?: Date | null;
  metricsHistory?: {
    value: number;
    timestamp?: Date;
    runId?: string;
    environment?: string;
    unit?: string;
  }[];
  metadata?: Record<string, any> | null;
  source: 'snapshot' | 'legacy';
}

async function getPerformanceMetrics(
  entityId: string,
  opts?: {
    metricId?: string;
    environment?: string;
    severity?: 'critical' | 'high' | 'medium' | 'low';
    limit?: number;
    days?: number;
  }
): Promise<{ data: PerformanceMetrics; history: PerformanceSnapshot[] }>
```

**Response Example**

```json
{
  "success": true,
  "data": {
    "averageExecutionTime": 245.5,
    "p95ExecutionTime": 320,
    "successRate": 0.92,
    "trend": "improving",
    "benchmarkComparisons": [
      { "benchmark": "team-baseline", "value": 280, "status": "below", "threshold": 300 }
    ],
    "historicalData": [
      { "timestamp": "2024-08-01T00:00:00.000Z", "executionTime": 260, "successRate": 0.88, "coveragePercentage": 82 },
      { "timestamp": "2024-08-05T00:00:00.000Z", "executionTime": 240, "successRate": 0.94, "coveragePercentage": 85 }
    ]
  },
  "history": [
    {
      "metricId": "test/auth-login/latency/p95",
      "environment": "test",
      "severity": "medium",
      "trend": "regression",
      "unit": "ms",
      "baselineValue": 180,
      "currentValue": 215,
      "delta": 35,
      "percentChange": 19.44,
      "sampleSize": 12,
      "riskScore": 1.07,
      "runId": "run-123",
      "detectedAt": "2024-08-05T00:00:00.000Z",
      "metricsHistory": [
        { "value": 180, "timestamp": "2024-07-25T00:00:00.000Z", "environment": "test", "unit": "ms" },
        { "value": 215, "timestamp": "2024-08-05T00:00:00.000Z", "environment": "test", "unit": "ms" }
      ],
      "metadata": {
        "reason": "Latency threshold breached in latest run",
        "testId": "auth-login",
        "framework": "vitest"
      },
      "source": "snapshot"
    }
  ]
}
```

### 2.4 Get Test Coverage
**Endpoint:** `GET /api/tests/coverage/{entityId}`
**MCP Tool:** `tests.get_coverage`

Retrieves test coverage information for an entity.

```typescript
interface TestCoverage {
  entityId: string;
  overallCoverage: CoverageMetrics;
  testBreakdown: {
    unitTests: CoverageMetrics;
    integrationTests: CoverageMetrics;
    e2eTests: CoverageMetrics;
  };
  uncoveredLines: number[];
  uncoveredBranches: number[];
  testCases: {
    testId: string;
    testName: string;
    covers: string[]; // Lines/branches covered
  }[];
}

async function getTestCoverage(entityId: string): Promise<TestCoverage>
```

---

## 3. Graph Operations

### 3.1 Graph Search
**Endpoint:** `POST /api/graph/search`
**MCP Tool:** `graph.search`

Performs semantic and structural searches over the knowledge graph.

```typescript
interface GraphSearchRequest {
  query: string;
  entityTypes?: ('function' | 'class' | 'interface' | 'file' | 'module')[];
  searchType?: 'semantic' | 'structural' | 'usage' | 'dependency';
  filters?: {
    language?: string;
    path?: string;
    tags?: string[];
    lastModified?: TimeRangeParams;
  };
  includeRelated?: boolean;
  limit?: number;
}

interface GraphSearchResult {
  entities: CodebaseEntity[];
  relationships: GraphRelationship[];
  clusters: SemanticCluster[];
  relevanceScore: number;
}

async function graphSearch(params: GraphSearchRequest): Promise<GraphSearchResult>
```

### 3.2 Get Graph Examples
**Endpoint:** `GET /api/graph/examples/{entityId}`
**MCP Tool:** `graph.examples`

Retrieves canonical usage examples and tests for an entity.

```typescript
interface GraphExamples {
  entityId: string;
  signature: string;
  usageExamples: {
    context: string;
    code: string;
    file: string;
    line: number;
  }[];
  testExamples: {
    testId: string;
    testName: string;
    testCode: string;
    assertions: string[];
  }[];
  relatedPatterns: {
    pattern: string;
    frequency: number;
    confidence: number;
  }[];
}

async function getGraphExamples(entityId: string): Promise<GraphExamples>
```

### 3.3 Get Entity Dependencies
**Endpoint:** `GET /api/graph/dependencies/{entityId}`
**MCP Tool:** `graph.get_dependencies`

Analyzes dependency relationships for an entity.

```typescript
interface DependencyAnalysis {
  entityId: string;
  directDependencies: {
    entity: CodebaseEntity;
    relationship: string;
    strength: number;
  }[];
  indirectDependencies: {
    entity: CodebaseEntity;
    path: CodebaseEntity[];
    relationship: string;
    distance: number;
  }[];
  reverseDependencies: {
    entity: CodebaseEntity;
    relationship: string;
    impact: 'high' | 'medium' | 'low';
  }[];
  circularDependencies: {
    cycle: CodebaseEntity[];
    severity: 'critical' | 'warning' | 'info';
  }[];
}

async function getEntityDependencies(entityId: string): Promise<DependencyAnalysis>
```

### 3.4 List Module Children
**Endpoint:** `GET /api/graph/modules/children`
**MCP Tool:** `graph.list_module_children`

Returns the structural children (directories, files, and symbols) beneath a module or directory, preserving structural metadata so clients do not need to rehydrate nodes manually. Results are ordered by entity type and then name.

**Query Parameters**

| Field | Type | Required | Notes |
| --- | --- | --- | --- |
| `modulePath` | string | ✅ | Accepts a normalized path or entity id (`file:src/app.ts:module`). |
| `includeFiles` | boolean | ❌ | Defaults to `true`. When `false`, only non-file children are returned. |
| `includeSymbols` | boolean | ❌ | Defaults to `true`. When `false`, symbol children are omitted. |
| `language` | string &#124; string[] | ❌ | Case-insensitive match on relationship or child language. Comma separated or array input supported. |
| `symbolKind` | string &#124; string[] | ❌ | Filters symbol children by `kind` (e.g., `class`, `function`). |
| `modulePathPrefix` | string | ❌ | Restricts children whose `modulePath`/`path` starts with the prefix. |
| `limit` | number | ❌ | Page size (1-500). Defaults to 50. |

**Response**

```typescript
interface ModuleChildrenResult {
  modulePath: string;
  parentId: string;
  children: Array<{
    entity: Entity;
    relationship: GraphRelationship;
  }>;
}
```

`relationship` objects include structural metadata (`language`, `symbolKind`, `modulePath`, `importAlias`, etc.) when present so the client can render navigation context without additional lookups.

### 3.5 List Imports
**Endpoint:** `GET /api/graph/entity/{entityId}/imports`
**MCP Tool:** `graph.list_imports`

Returns the incoming/outgoing structural import edges for a file or module along with resolved targets. The response mirrors the MCP tool payload.

**Query Parameters**

| Field | Type | Required | Notes |
| --- | --- | --- | --- |
| `resolvedOnly` | boolean | ❌ | When `true`, filters out unresolved imports. |
| `language` | string &#124; string[] | ❌ | Case-insensitive language filter on the relationship or target entity. |
| `symbolKind` | string &#124; string[] | ❌ | Restricts results to specific target symbol kinds. |
| `importAlias` | string &#124; string[] | ❌ | Exact match on alias used in the import (`as AliasName`). |
| `importType` | string &#124; string[] | ❌ | Accepts `default`, `named`, `namespace`, `wildcard`, `side-effect`. |
| `isNamespace` | boolean | ❌ | Matches namespace (`import * as`) relationships. |
| `modulePath` | string &#124; string[] | ❌ | Exact match on the normalized module path. |
| `modulePathPrefix` | string | ❌ | Prefix filter for `modulePath`. |
| `limit` | number | ❌ | Page size (1-1000). Defaults to 200. |

**Response**

```typescript
interface ListImportsResult {
  entityId: string;
  imports: Array<{
    relationship: GraphRelationship;
    target?: Entity | null;
  }>;
}
```

Each relationship carries structural metadata (`importAlias`, `importType`, `language`, `modulePath`, `isNamespace`, timestamps, confidence) so clients can display import details without extra queries. When available, `target` is the resolved entity for the import.

### 3.6 Find Symbol Definition
**Endpoint:** `GET /api/graph/symbol/{symbolId}/definition`
**MCP Tool:** `graph.find_definition`

Resolves the defining entity for a symbol and returns the corresponding `DEFINES` relationship metadata. Useful for jumping to implementation while still surfacing graph confidence and provenance.

```typescript
interface DefinitionLookupResult {
  symbolId: string;
  relationship: GraphRelationship | null;
  source: Entity | null;
}
```

`relationship` is `null` when the symbol has not been linked to a defining entity.

---

## 4. Code Operations

### 4.1 Propose Code Changes
**Endpoint:** `POST /api/code/propose-diff`
**MCP Tool:** `code.propose_diff`

Analyzes proposed code changes and their impact.

```typescript
interface CodeChangeProposal {
  changes: {
    file: string;
    type: 'create' | 'modify' | 'delete' | 'rename';
    oldContent?: string;
    newContent?: string;
    lineStart?: number;
    lineEnd?: number;
  }[];
  description: string;
  relatedSpecId?: string;
}

interface CodeChangeAnalysis {
  affectedEntities: CodebaseEntity[];
  breakingChanges: {
    severity: 'breaking' | 'potentially-breaking' | 'safe';
    description: string;
    affectedEntities: string[];
  }[];
  impactAnalysis: {
    directImpact: CodebaseEntity[];
    indirectImpact: CodebaseEntity[];
    testImpact: Test[];
  };
  recommendations: {
    type: 'warning' | 'suggestion' | 'requirement';
    message: string;
    actions: string[];
  }[];
}

async function proposeCodeChanges(proposal: CodeChangeProposal): Promise<CodeChangeAnalysis>
```

### 4.2 Validate Code
**Endpoint:** `POST /api/code/validate`
**MCP Tool:** `validate.run`

Runs comprehensive validation on code.

```typescript
interface ValidationRequest {
  files?: string[];
  specId?: string;
  includeTypes?: ('typescript' | 'eslint' | 'security' | 'tests' | 'coverage' | 'architecture')[];
  failOnWarnings?: boolean;
}

interface ValidationResult {
  overall: {
    passed: boolean;
    score: number; // 0-100
    duration: number;
  };
  typescript: {
    errors: number;
    warnings: number;
    issues: ValidationIssue[];
  };
  eslint: {
    errors: number;
    warnings: number;
    issues: ValidationIssue[];
  };
  security: {
    critical: number;
    high: number;
    medium: number;
    low: number;
    issues: SecurityIssue[];
  };
  tests: {
    passed: number;
    failed: number;
    skipped: number;
    coverage: CoverageMetrics;
  };
  architecture: {
    violations: number;
    issues: ValidationIssue[];
  };
}

async function validateCode(params: ValidationRequest): Promise<ValidationResult>
```

---

## 5. Impact Analysis

### 5.1 Analyze Change Impact
**Endpoint:** `POST /api/impact/analyze`
**MCP Tool:** `impact.analyze`

Performs cascading impact analysis for proposed changes.

```typescript
interface ImpactAnalysisRequest {
  changes: {
    entityId: string;
    changeType: 'modify' | 'delete' | 'rename';
    newName?: string;
    signatureChange?: boolean;
  }[];
  includeIndirect?: boolean;
  maxDepth?: number;
}

interface ImpactAnalysis {
  directImpact: {
    entities: CodebaseEntity[];
    severity: 'high' | 'medium' | 'low';
    reason: string;
  }[];
  cascadingImpact: {
    level: number;
    entities: CodebaseEntity[];
    relationship: string;
    confidence: number;
  }[];
  testImpact: {
    affectedTests: Test[];
    requiredUpdates: string[];
    coverageImpact: number;
  };
  documentationImpact: {
    staleDocs: DocumentationNode[];
    missingDocs: Array<{
      entityId: string;
      entityName: string;
      reason: string;
    }>;
    requiredUpdates: string[];
    freshnessPenalty: number;
  };
  specImpact: {
    relatedSpecs: Array<{
      specId: string;
      spec?: Pick<Spec, 'id' | 'title' | 'priority' | 'status' | 'assignee' | 'tags'>;
      priority?: 'critical' | 'high' | 'medium' | 'low';
      impactLevel?: 'critical' | 'high' | 'medium' | 'low';
      status?: Spec['status'] | 'unknown';
      ownerTeams: string[];
      acceptanceCriteriaIds: string[];
      relationships: Array<{
        type: RelationshipType;
        impactLevel?: 'critical' | 'high' | 'medium' | 'low';
        priority?: 'critical' | 'high' | 'medium' | 'low';
        acceptanceCriteriaIds?: string[];
        ownerTeam?: string;
        rationale?: string;
        confidence?: number;
        status?: Spec['status'] | 'unknown';
      }>;
    }>;
    requiredUpdates: string[];
    summary: {
      byPriority: Record<'critical' | 'high' | 'medium' | 'low', number>;
      byImpactLevel: Record<'critical' | 'high' | 'medium' | 'low', number>;
      statuses: Record<'draft' | 'approved' | 'implemented' | 'deprecated' | 'unknown', number>;
      acceptanceCriteriaReferences: number;
      pendingSpecs: number;
    };
  };
  deploymentGate: {
    blocked: boolean;
    level: 'none' | 'advisory' | 'required';
    reasons: string[];
    stats: {
      missingDocs: number;
      staleDocs: number;
      freshnessPenalty: number;
    };
  };
  recommendations: {
    priority: 'immediate' | 'planned' | 'optional';
    description: string;
    effort: 'low' | 'medium' | 'high';
    impact: 'breaking' | 'functional' | 'cosmetic';
    type?: 'warning' | 'requirement' | 'suggestion';
    actions?: string[];
  }[];
}

async function analyzeImpact(params: ImpactAnalysisRequest): Promise<ImpactAnalysis>
```

The handler normalises graph traversal depth (`maxDepth` 1–8) and aggregates secondary metrics for consumers (risk scoring, documentation gating). Documentation freshness drives the `deploymentGate` status: any missing documentation pushes the gate to `required`, while stale references escalate to `advisory`. Linked specifications influence both the risk level and recommendation feed—critical/high-priority specs trigger immediate actions, while acceptance-criteria references surface targeted follow-ups. Recommendations surface immediate remediation steps (e.g., update tests, resolve high-risk dependencies, close spec gaps) and include machine-generated context for MCP tooling.

---

## 6. Vector Database Operations

### 6.1 Semantic Search
**Endpoint:** `POST /api/vdb/search`
**MCP Tool:** `vdb.search`

Performs semantic search with vector similarity.

```typescript
interface VectorSearchRequest {
  query: string;
  entityTypes?: string[];
  similarity?: number; // 0-1, minimum similarity threshold
  limit?: number;
  includeMetadata?: boolean;
  filters?: {
    language?: string;
    lastModified?: TimeRangeParams;
    tags?: string[];
  };
}

interface VectorSearchResult {
  results: {
    entity: CodebaseEntity;
    similarity: number;
    context: string;
    highlights: string[];
  }[];
  metadata: {
    totalResults: number;
    searchTime: number;
    indexSize: number;
  };
}

async function vectorSearch(params: VectorSearchRequest): Promise<VectorSearchResult>
```

---

## 7. Source Control Management

### 7.1 Create Commit/PR
**Endpoint:** `POST /api/scm/commit-pr`
**MCP Tool:** `scm.commit_pr`

Creates a commit and/or pull request with links to related artifacts.

```typescript
interface CommitPRRequest {
  title: string;
  description: string;
  changes: string[]; // File paths
  relatedSpecId?: string;
  testResults?: string[]; // Test IDs
  validationResults?: string; // Validation result ID
  createPR?: boolean;
  branchName?: string;
  labels?: string[];
}

interface CommitPRResponse {
  commitHash: string;
  prUrl?: string;
  branch: string;
  relatedArtifacts: {
    spec: Spec;
    tests: Test[];
    validation: ValidationResult;
  };
}

async function createCommitPR(params: CommitPRRequest): Promise<CommitPRResponse>
```

---

## 8. Documentation & Domain Analysis

### 8.1 Sync Documentation
**Endpoint:** `POST /api/docs/sync`
**MCP Tool:** `docs.sync`

Synchronizes documentation with the knowledge graph.

```typescript
interface SyncDocumentationResponse {
  processedFiles: number;
  newDomains: number;
  updatedClusters: number;
  errors: string[];
}

async function syncDocumentation(): Promise<SyncDocumentationResponse>
```

### 8.2 Get Business Domains
**Endpoint:** `GET /api/domains`
**MCP Tool:** `domains.get_business_domains`

Retrieves all business domains.

```typescript
interface BusinessDomain {
  id: string;
  name: string;
  description: string;
  criticality: 'core' | 'supporting' | 'utility';
  stakeholders: string[];
  keyProcesses: string[];
}

async function getBusinessDomains(): Promise<BusinessDomain[]>
```

### 8.3 Get Entities by Domain
**Endpoint:** `GET /api/domains/{domainName}/entities`
**MCP Tool:** `domains.get_entities`

Retrieves all code entities belonging to a business domain.

```typescript
async function getEntitiesByDomain(domainName: string): Promise<CodebaseEntity[]>
```

### 8.4 Get Semantic Clusters
**Endpoint:** `GET /api/clusters`
**MCP Tool:** `clusters.get_semantic_clusters`

Retrieves all semantic clusters.

```typescript
interface SemanticCluster {
  id: string;
  name: string;
  description: string;
  businessDomainId: string;
  clusterType: 'feature' | 'module' | 'capability' | 'service';
  cohesionScore: number;
  memberEntities: string[];
}

async function getSemanticClusters(): Promise<SemanticCluster[]>
```

### 8.5 Get Business Impact
**Endpoint:** `GET /api/business/impact/{domainName}`
**MCP Tool:** `business.get_impact`

Analyzes business impact of recent changes in a domain.

```typescript
interface BusinessImpact {
  domainName: string;
  timeRange: TimeRangeParams;
  changeVelocity: number;
  riskLevel: 'low' | 'medium' | 'high' | 'critical';
  affectedCapabilities: string[];
  mitigationStrategies: string[];
}

async function getBusinessImpact(domainName: string, since?: Date): Promise<BusinessImpact>
```

---

## 9. Security Operations

### 9.1 Scan for Security Issues
**Endpoint:** `POST /api/security/scan`
**MCP Tool:** `security.scan`

Scans entities for security vulnerabilities.

```typescript
interface SecurityScanRequest {
  entityIds?: string[];
  scanTypes?: ('sast' | 'sca' | 'secrets' | 'dependency')[];
  severity?: ('critical' | 'high' | 'medium' | 'low')[];
}

interface SecurityScanResult {
  issues: SecurityIssue[];
  vulnerabilities: Vulnerability[];
  summary: {
    totalIssues: number;
    bySeverity: Record<string, number>;
    byType: Record<string, number>;
  };
}

async function scanForSecurityIssues(params?: SecurityScanRequest): Promise<SecurityScanResult>
```

### 9.2 Get Vulnerability Report
**Endpoint:** `GET /api/security/vulnerabilities`
**MCP Tool:** `security.get_vulnerability_report`

Retrieves vulnerability report for the entire codebase.

```typescript
interface VulnerabilityReport {
  summary: {
    total: number;
    critical: number;
    high: number;
    medium: number;
    low: number;
  };
  vulnerabilities: Vulnerability[];
  byPackage: Record<string, Vulnerability[]>;
  remediation: {
    immediate: string[];
    planned: string[];
    monitoring: string[];
  };
}

async function getVulnerabilityReport(): Promise<VulnerabilityReport>
```

---

## 10. Administration & Monitoring

### 10.1 Get System Health
**Endpoint:** `GET /api/health`
**MCP Tool:** `admin.get_health`

Retrieves system health status.

```typescript
interface SystemHealth {
  overall: 'healthy' | 'degraded' | 'unhealthy';
  components: {
    graphDatabase: ComponentHealth;
    vectorDatabase: ComponentHealth;
    fileWatcher: ComponentHealth;
    apiServer: ComponentHealth;
  };
  metrics: {
    uptime: number;
    totalEntities: number;
    totalRelationships: number;
    syncLatency: number;
    errorRate: number;
  };
}

async function getSystemHealth(): Promise<SystemHealth>
```

### 10.2 Get Sync Status
**Endpoint:** `GET /api/admin/sync-status`
**MCP Tool:** `admin.get_sync_status`

Retrieves synchronization status and queue information.

```typescript
interface SyncStatus {
  isActive: boolean;
  lastSync: Date;
  queueDepth: number;
  processingRate: number;
  errors: {
    count: number;
    recent: string[];
  };
  performance: {
    syncLatency: number;
    throughput: number;
    successRate: number;
  };
}

async function getSyncStatus(): Promise<SyncStatus>
```

### 10.3 Trigger Full Sync
**Endpoint:** `POST /api/admin/sync`
**MCP Tool:** `admin.trigger_sync`

Triggers a full synchronization of the knowledge graph.

```typescript
interface SyncOptions {
  force?: boolean;
  includeEmbeddings?: boolean;
  includeTests?: boolean;
  includeSecurity?: boolean;
}

async function triggerFullSync(options?: SyncOptions): Promise<{ jobId: string }>
```

### 10.4 Get Analytics
**Endpoint:** `GET /api/analytics`
**MCP Tool:** `admin.get_analytics`

Retrieves system analytics and usage metrics.

```typescript
interface SystemAnalytics {
  period: TimeRangeParams;
  usage: {
    apiCalls: number;
    uniqueUsers: number;
    popularEndpoints: Record<string, number>;
  };
  performance: {
    averageResponseTime: number;
    p95ResponseTime: number;
    errorRate: number;
  };
  content: {
    totalEntities: number;
    totalRelationships: number;
    growthRate: number;
    mostActiveDomains: string[];
  };
}

async function getAnalytics(params?: TimeRangeParams): Promise<SystemAnalytics>
```

---

## Error Handling

All API endpoints follow consistent error handling:

```typescript
interface APIError {
  code: 'VALIDATION_ERROR' | 'NOT_FOUND' | 'PERMISSION_DENIED' | 'INTERNAL_ERROR' | 'RATE_LIMITED';
  message: string;
  details?: any;
  requestId: string;
  timestamp: Date;
}

// Common HTTP status codes:
// 200 - Success
// 400 - Bad Request (validation errors)
// 401 - Unauthorized
// 403 - Forbidden
// 404 - Not Found
// 429 - Too Many Requests
// 500 - Internal Server Error
```

## Authentication & Authorization

```typescript
interface AuthenticatedRequest {
  headers: {
    'Authorization': `Bearer ${token}`;
    'X-API-Key'?: string;
    'X-Request-ID'?: string;
  };
}

// Role-based permissions:
// - 'read' - Basic read access
// - 'write' - Create/modify operations
// - 'admin' - Administrative operations
// - 'security' - Security-related operations
```

> Current implementation: when either `JWT_SECRET` or `API_KEY_SECRET` is configured the integration layer enforces authentication on `/api/v1/admin*` routes. Admin endpoints require JWTs carrying the `admin` permission. API keys are limited to read-only scenarios and receive `INSUFFICIENT_PERMISSIONS` on admin routes.

### Token lifecycle

`POST /api/v1/auth/refresh` accepts `{ refreshToken }` and returns a fresh access token plus refresh token. Expired or malformed refresh tokens respond with `TOKEN_EXPIRED` or `INVALID_TOKEN` payloads to mirror the integration test contract.

## Rate Limiting

```typescript
interface RateLimit {
  limit: number;
  remaining: number;
  resetTime: Date;
  retryAfter?: number;
}

// Headers returned:
// X-RateLimit-Limit
// X-RateLimit-Remaining
// X-RateLimit-Reset
// Retry-After (when limit exceeded)
```

## Webhooks & Real-time Updates

```typescript
interface WebhookConfig {
  url: string;
  events: ('sync.completed' | 'validation.failed' | 'security.alert')[];
  secret: string;
}

interface RealTimeSubscription {
  event: string;
  filter?: any;
  callback: (event: any) => void;
}

// WebSocket events:
// sync:update - Real-time sync progress
// validation:result - Validation completion
// security:alert - New security issues
// change:detected - Code changes detected
```

## Versioning

The API follows semantic versioning:

- **Major version** (v1, v2): Breaking changes
- **Minor version** (v1.1, v1.2): New features, backward compatible
- **Patch version** (v1.0.1): Bug fixes

Version is specified in:
- URL path: `/api/v1/design/create-spec`
- Header: `Accept-Version: v1`
- Query parameter: `?version=v1`

## SDKs & Client Libraries

Official client libraries available for:
- **JavaScript/TypeScript**: `npm install @memento-ai/sdk`
- **Python**: `pip install memento-ai`
- **Java**: Maven dependency
- **Go**: `go get github.com/memento-ai/go-sdk`

## Support & Documentation

- **Interactive API Documentation**: Available at `/api/docs`
- **OpenAPI Specification**: Available at `/api/openapi.json`

- **Community Forums**: `https://community.memento.ai`
- **Support**: `support@memento.ai`

---

*This API design provides comprehensive access to all Memento knowledge graph capabilities, enabling seamless integration with AI agents, IDEs, CI/CD pipelines, and development workflows.*
</file>

<file path="Docs/Blueprints/structural-relationships.md">
# Structural Relationship Blueprint

## 1. Overview
Structural relationships (`CONTAINS`, `DEFINES`, `EXPORTS`, `IMPORTS`, optionally `BELONGS_TO`) encode the static architecture of the codebase—module hierarchies, symbol definitions, and import/export dependencies. They serve as the backbone for navigation, dependency analysis, and change impact scoping.

## 2. Status Summary
### Completed Improvements
- Structural metadata is now promoted to first-class Falkor properties (alias, import type, namespace flags, module path, temporal fields) during ingestion via `extractStructuralPersistenceFields` and the write pipeline, instead of living solely in `r.metadata` (`src/services/relationships/structuralPersistence.ts:8`, `src/services/KnowledgeGraphService.ts:5380`).
- `normalizeStructuralRelationship` canonicalizes import/export metadata, resolution state, confidence defaults, and `time-rel_*` IDs while language adapters populate cross-language fields (`src/services/relationships/RelationshipNormalizer.ts:208`, `src/services/relationships/RelationshipNormalizer.ts:348`).
- Graph APIs expose structural filters and navigation helpers—`getRelationships` accepts alias/module filters, `finalizeScan` retires stale edges, `listModuleChildren`/`listImports` power module navigation, and `getModuleHistory` surfaces temporal context (`src/services/KnowledgeGraphService.ts:6740`, `src/services/KnowledgeGraphService.ts:7132`, `src/services/KnowledgeGraphService.ts:11637`, `src/services/KnowledgeGraphService.ts:11803`).
- Falkor command serialization now handles nested objects and arrays without hitting the "primitive types only" error, allowing structured metadata to flow through `SET n += $props` operations (`src/services/database/FalkorDBService.ts:384`).

### Outstanding Gaps
- Guarantee Falkor index bootstrap on first graph creation: `setupGraph` still defers when the graph key is missing, so an initial write can occur without indexes. Add a first-write hook or bootstrap task to create indexes immediately (`src/services/database/FalkorDBService.ts:279`).
- Restore vector search endpoints by wiring the Fastify router to the vector-store implementation; `registerVDBRoutes` remains commented out so `/api/v1/vdb/search` responds 404 (`src/api/APIGateway.ts:27`).
- Codify a canonical symbol-kind taxonomy shared by REST schemas and graph queries—the current lookup tables keep behaviour working but remain ad-hoc (`src/api/routes/graph.ts:20`).
- Ensure dependency ingestion surfaces meaningful results for `graph.dependencies.analyze`; the MCP tool currently replays whatever `CALLS`/`REFERENCES`/`DEPENDS_ON` edges exist, so we still need fixtures and parser coverage to guarantee non-empty responses (`src/services/KnowledgeGraphService.ts:9480`).

## 3. Desired Capabilities
1. Persist structural metadata that supports multi-language ingestion—import alias, import type (default/named/wildcard), namespace flags, re-export targets, symbol kind, and module path.
2. Provide efficient queries to navigate project structure (list children, find definitions, analyze dependencies) with filtering criteria.
3. Integrate with history to track structural changes over time (file moves, API surface modifications).
4. Maintain compatibility with different languages by using generic fields plus language-specific metadata namespaces.

## 4. Inputs & Consumers
- **Inputs**: AST parser (`src/services/ASTParser.ts`), language-specific parsers (future), manual overrides for legacy languages.
- **Consumers**: IDE integrations, dependency graph API, impact analysis, docs linking, code navigation features, build tooling analyzing module graphs.

## 5. Schema & Metadata Requirements
| Field | Type | Notes |
| --- | --- | --- |
| `importAlias` | string | Alias used in import/export; optional.
| `importType` | enum (`default`, `named`, `namespace`, `wildcard`, `side-effect`) | Normalized enum persisted as a first-class relationship property.
| `importDepth` | integer | (Existing field) number of hops for resolved import; maintain for parity.
| `isReExport` | boolean | For exports re-exporting from another module.
| `reExportTarget` | string | Path/name of re-exported symbol.
| `isNamespace` | boolean | Indicates namespace import.
| `language` | string | Language of source file (TS, JS, Python, etc.).
| `symbolKind` | enum (`class`, `function`, `interface`, `module`, etc.) | For `DEFINES` edges.
| `modulePath` | string | Canonical module path normalized across languages; used for prefix filters.
| `metadata.languageSpecific` | object | Namespaced details (e.g., `ts.typeOnly`).

## 6. Normalization Strategy
1. Extend `normalizeRelationship` with `normalizeStructuralRelationship` when `type` is structural:
   - Map import/export types to canonical enums.
   - Normalize path separators and apply case sensitivity rules as needed.
   - Promote metadata fields (`importAlias`, `isReExport`, `reExportTarget`, `language`, `symbolKind`).
   - For languages lacking certain concepts (e.g., `importAlias`), default to `null`.
2. Provide language adapters to fill metadata (`ts`, `py`, `go`). For each adapter, ensure contributions align with generic fields and store language-specific extras in `metadata.languageSpecific`.
3. Validate presence of base node IDs and log when parser emits ambiguous edges (e.g., unresolved imports).

## 7. Persistence & Merge Mechanics
1. Keep canonical ID `from|to|type` (structural edges are unique by definition), but ensure metadata updates merge without losing previous information.
2. Extend Cypher queries to persist new fields: `importAlias`, `importType`, `isReExport`, `reExportTarget`, `isNamespace`, `language`, `symbolKind`, `modulePath`, plus existing location data. Persist the structured properties alongside a stringified `metadata` blob so query filters can operate on primitives without losing richer language-specific context.
3. When edges change (e.g., alias updated), update metadata while preserving history (set `lastSeenAt`, reuse `openEdge`/`closeEdge` logic).
4. For unresolved imports generating placeholders, maintain `resolutionState` metadata to track unresolved vs resolved state.
5. Index `(type, importAlias)`, `(type, modulePath)`, `(language, type)` for navigation queries.

## 8. Query & API Surface
1. Extend `getRelationships` filters for structural-specific fields: `importAlias`, `importType`, `isNamespace`, `symbolKind`, `language`, `modulePath` prefix.
2. Provide helper APIs:
   - `listModuleChildren(modulePath, { includeFiles, includeSymbols })` returning contained nodes sorted by kind.
   - `listImports(fileId, { includeResolved })` showing import details and resolution status.
   - `listExports(fileId)` with alias/re-export info.
   - `findDefinition(symbolId)` retrieving `DEFINES` edge metadata.
3. Update documentation and API design references to reflect new capabilities for code navigation.
4. When adding tests, seed structural fixtures via `tests/test-utils/realistic-kg.ts` (unit) or `tests/integration/services/KnowledgeGraphService.integration.test.ts` (integration) so filter behaviour stays covered.

## 9. Backfill & Operations
- Use `pnpm structural:backfill` to promote legacy structural relationships (where metadata lived solely in `r.metadata`) to the first-class properties described above. The command defaults to a dry run; pass `--apply` to persist updates once the preview matches expectations.
- The backfill normalizes aliases, import types, namespace flags, language/symbol kind casing, and module paths before writing them to Falkor properties. It also rewrites the metadata JSON with the normalized values so ingestion and read paths stay consistent.
- Schedule the migration after deploying parser or normalization changes so existing edges immediately benefit from the richer query filters (`importAlias`, `modulePath`, `symbolKind`, etc.).

## 10. Temporal & History Integration
1. Use history pipeline to track structural changes: when symbol moves, close old `DEFINES` edge and open new one with `validFrom/validTo`.
2. For `IMPORTS`, mark edges inactive when dependency removed; track `lastSeenAt` from parser scans.
3. Provide timeline queries for module evolution (e.g., `getModuleHistory(modulePath)`).

### Module History Helper
- `KnowledgeGraphService.getModuleHistory(modulePath, options?)` now returns a full `ModuleHistoryResult` object that includes:
  - `moduleId`, `moduleType`, and `generatedAt` snapshot metadata.
  - Recent version records (`EntityTimelineEntry[]`) with associated change relationships.
  - `relationships`: structural edges touching the module (incoming and outgoing) enriched with `confidence`, `scope`, temporal segments (`openedAt`/`closedAt`), and resolved entity summaries for both endpoints.
- Segment timelines reflect moves/removals by closing the previous edge and opening a new one; consumers can detect refactors by inspecting `segments` and `direction`.
- The helper accepts `{ includeInactive?: boolean, limit?: number, versionLimit?: number }` so callers can scope the response for dashboards versus deep dives.
- Structural relationship persistence hoists `confidence`/`scope` and keeps `firstSeenAt`/`lastSeenAt` updated, ensuring timeline queries, analytics, and downstream tooling consume consistent metadata.

## 11. Migration & Backfill Plan
1. Expand schema to include new fields and indexes; ensure safe defaults for existing edges.
2. Re-run AST parser to repopulate structural edges with enriched metadata; verify counts remain consistent.
3. For languages beyond TypeScript, document ingestion requirements and plan incremental rollout.
4. Provide audit reports comparing before/after metadata to ensure no regressions in import/export detection.

## 12. Risks & Mitigations
| Risk | Mitigation |
| --- | --- |
| Metadata explosion for multi-language support | Use generic fields plus namespaced metadata; keep core schema lean. |
| Parser inconsistencies between languages | Define adapter interface and validation tests per language. |
| Query performance degrade with new filters | Add targeted indexes and query caching; measure using benchmarks. |
| Temporal updates increasing ingestion workload | Optimize parser diffing to emit only changed edges; batch history updates. |

## 13. Implementation Milestones
1. Implement normalization helper & language adapters.
2. Update persistence/query layers and add indexes.
3. Reingest structural data; validate with integration tests.
4. Integrate temporal tracking and update history APIs.
5. Roll out navigation improvements (IDE, UI) leveraging new metadata.

## 14. Open Questions
- Should we introduce dedicated `MODULE` nodes to represent directories/packages more explicitly?
- How do we model multi-language modules (e.g., TS + JS + wasm) sharing imports/exports?
- Do we need to track wildcard imports’ expanded symbol sets, or rely on runtime resolution? Could this be metadata referencing derived edges?
- What is the retention/archival policy for historical structural data, especially for large codebases with frequent refactors?
</file>

<file path="Docs/Blueprints/tests-relationships.md">
# Tests Relationship Blueprint

## 1. Overview
Testing edges (`TESTS`, `VALIDATES`) connect automated tests to code and specs so that impact analysis, coverage insights, and regression triage can be grounded in graph data. The goal is to elevate these edges to parity with code-edge richness—persisting structured metadata, supporting temporal history, and enabling precise querying.

## 2. Current Gaps
- Persistence now carries `testType` and coverage metadata from `TestEngine`, but suite/run identifiers never reach the graph. Without `suiteId`/`runId` the system still collapses results from different suites and loses provenance—extend ingestion to emit and persist stable suite/run metadata.
- Canonical IDs are purely `from|to|type`, so multiple suites targeting the same implementation overwrite each other, erasing granularity.
- Query helpers (`getRelationships`) expose only code-edge-centric filters, preventing clients from retrieving tests by type, suite, or coverage thresholds.
- Temporal helpers (`openEdge`/`closeEdge`) are implemented, yet the test-ingestion path still writes edges via `createRelationship` only—no `validFrom/validTo` history is recorded when coverage appears or disappears.
- Integration suites (`tests/integration/api/*.integration.test.ts`) still depend on real FalkorDB/Qdrant/PostgreSQL instances via `tests/test-utils/database-helpers.ts`. In sandboxed or containerized runs the bootstrap fails with `EPERM connect ...:6380`, so there is no in-memory test double to validate these relationships offline.
- `tests/integration/models/relationships.integration.test.ts` seeds data with ad-hoc `:Directory` / `:File` labels and issues raw Cypher instead of exercising `RelationshipService` / `KnowledgeGraphService` APIs. The suite never asserts canonical `Entity` schema, ID normalization, or metadata fields, so it can pass while the production ingestion path drifts. Rework the test to hydrate through service helpers and validate normalized edges end-to-end.
- Admin API compatibility for legacy `/api/v1/admin/**` routes is undocumented; when aliases were trimmed, the integration suite started returning `404` and masked the intended `503` signal for critical health failures. Document and enforce the alias contract (including 503 semantics for unhealthy components) so routing refactors keep the admin coverage intact.
- Unit tests referencing legacy paths/response shapes (e.g., `tests/unit/api/routes/impact.test.ts:95-165`) will keep failing until they are updated to reflect the graph-backed APIs described here.
- TestEngine ingestion now persists distinct KG nodes per test and writes `COVERAGE_PROVIDES` edges, but the pipeline still depends on resolving `targetSymbol` to an existing entity. When symbol inference fails we log `⚠️ No target symbol` and skip graph edges, which leaves `/tests/coverage` blind to those suites even though inline coverage metrics exist. Add fallback entity synthesis (or heuristics that anchor to the test path) so coverage analytics stay complete.
- `/api/v1/tests/performance` now merges Postgres history, but coverage endpoints still rely exclusively on KG edges. Until ingestion hydrates the graph from `coverage_history` (or surfaces historical metrics directly), integration tests must seed `TESTS`/`COVERAGE_PROVIDES` edges manually before `/tests/coverage` returns data.
- Flaky analysis persistence now stores richer analytics (`flaky_score`, rates, recommendations), but nothing feeds that data back into knowledge-graph edges or impact reports—wire the stored analytics into query surfaces so flaky insights are discoverable without bespoke SQL.
- `TestResultParser` still extracts suite metadata via regex heuristics and misses canonical names for JUnit inputs; align implementation with XML parsing so suite identifiers persist alongside per-test results.
- Running the MCP protocol/compliance integration suite still assumes a local FalkorDB/Redis instance is listening on `redis://localhost:6380`. When the service is absent the health check in `setupTestDatabase` fails and Vitest skips the entire suite. Document the dependency in the test harness and provide either a lightweight stub or container recipe so the CI/DEV environments can bring FalkorDB up before exercising MCP tests.

## 3. Desired Capabilities
1. Persist test metadata (type, suite, run identifiers, coverage percentages, confidence) both as scalar properties and in structured metadata JSON.
2. Support multiple distinct edges per entity pair (e.g., integration test vs. unit test, different suites) without collisions.
3. Allow consumers to filter by test attributes (`testType`, `suiteId`, `lastSeenAt`, `coverageMin`, `flakyScore`, etc.).
4. Integrate with temporal pipeline: track first/last seen, detect when coverage disappears, and support historical queries.
5. Provide hooks for attaching evidence (test run artifacts, failure traces) and linking to checkpoints or sessions.

## 4. Inputs & Consumers
- **Ingestion Sources**: Test engine execution results, CI integrations, manual annotations for spec validation. Additional future sources include flaky-test detectors and telemetry from external services.
- **Downstream Consumers**: Impact API (`src/api/routes/impact.ts`), tests API (`src/api/routes/tests.ts`), admin dashboards, IDE integrations surfacing coverage, and automation (rerun impacted tests).

## 5. Schema & Metadata Requirements
| Field | Type | Notes |
| --- | --- | --- |
| `testType` | enum (`unit`, `integration`, `e2e`, `snapshot`, `perf`) | Normalize to canonical set; allow extensibility via metadata.
| `suiteId` | string | Stable identifier for test suite or group.
| `runId` | string | Optional reference to test run entity; should map to execution node.
| `coverage` | number (0-1) | Clamp and store two decimal precision; fallback to `null` when unknown.
| `flaky` | boolean | Derived from run history; optional.
| `confidence` | number (0-1) | Quality of mapping between test and entity/spec.
| `evidence` | array | Links to run artifacts, failure traces (reuse `EdgeEvidence`).
| `why` | string | Explanation (e.g., "matched via acceptance criterion A1").
| `metadata.additional` | object | Room for language/framework specifics (e.g., jest test path).

## 6. Normalization Strategy
1. **Dedicated Helper**: Implement `normalizeTestRelationship` and invoke it from `KnowledgeGraphService.normalizeRelationship` (`src/services/KnowledgeGraphService.ts`). Responsibilities:
   - Map incoming `testType` strings to enum; log/collect metrics for unknown values.
   - Sanitize `suiteId`, `runId` to max lengths (e.g., 256 chars), remove whitespace.
   - Clamp `coverage` to `[0,1]`, round to 3 decimals; if missing but `passed=true`, allow null.
   - Promote `confidence`, `flaky`, `runId` from metadata to top-level; maintain `metadata` copy.
   - Ensure `evidence` uses `EdgeEvidence` schema, merging with existing entries via `mergeEdgeEvidence`.
2. **Validation**: If required fields absent (e.g., `suiteId` missing for automated runs), emit warning and supply default (`unknown-suite`). Provide instrumentation counters.
3. **Site Hash Update**: Incorporate `suiteId` and test file path into `siteHash` computation to disambiguate edges coming from same test file but different suites.

## 7. Persistence & Merge Mechanics
1. **Canonical ID**: Extend `canonicalRelationshipId` for tests to include deterministic suffix derived from `suiteId` + `testName` (or upstream `testId`). Optionally guard behind env flag to ease migration.
2. **Cypher Writes**: Augment `createRelationship`/`createRelationshipsBulk` to project test-specific columns: `testType`, `suiteId`, `runId`, `coverage`, `flaky`, `confidence`, `why`, `evidence`, `locations`, `firstSeenAt`, `lastSeenAt`.
3. **Aggregation Rules**:
   - When merging duplicate edges (same canonical id), accumulate `occurrencesScan` per run and update coverage as weighted average (store `coverageSamples` count to compute).
   - Preserve best (max) confidence; maintain `statusHistory` array for test status (pass/fail counts) in metadata.
4. **Auxiliary Nodes**: Optionally dual-write `test_run` nodes to capture run-level metadata; link via `EVIDENCE_OF` edges. This mirrors existing evidence dual write for code edges.
5. **Indexes**: Create indexes on `(type, suiteId)`, `(type, testType)`, `(runId)` to keep queries fast.

## 8. Query & API Surface
1. **`getRelationships` Enhancements**: Add filters for `testType`, `suiteId`, `runId`, `coverageMin/Max`, `confidenceMin/Max`, `flaky`, `lastSeenSince`, `status`.
2. **Helper APIs**:
   - `getTestsForEntity(entityId, { includeSpecs?: boolean, limit?: number })` returning structured objects with test metadata.
   - `getCoverageSummary(entityId)` aggregating coverage values and run counts.
   - `getTestsBySuite(suiteId)` for dashboards.
   - `getFlakyTestAnalysis(entityId)` now retrieves execution history from PostgreSQL, scores flakiness via `TestEngine`, and powers `/api/tests/flaky-analysis/:entityId` responses.
3. **API Documentation**: Update `Docs/MementoAPIDesign.md` to describe parameters and example payloads.
4. **Caching**: Evaluate caching of frequent queries (e.g., impacted tests) with invalidation triggered by new edges.

## 9. Temporal & Auditing
1. On ingestion, call revised `openEdge`/`closeEdge` to maintain valid intervals for tests. When a test stops covering an entity (not emitted during scan), mark inactive and set `validTo`.
2. Keep `firstSeenAt`/`lastSeenAt` accurate using per-run timestamps; store `lastPassAt`, `lastFailAt` scalars for quick health checks.
3. Support timeline queries (`getTestCoverageTimeline`) returning historical `coverage` and status events for analytics.

## 10. Migration & Backfill Plan
1. **Phase 1**: Migrate schema to include new columns. Backfill existing edges with defaults (e.g., `testType='unknown'`, `coverage=null`, `suiteId='legacy'`).
2. **Phase 2**: Trigger re-run of `TestEngine` ingestion to republish edges with full metadata. Provide script to reconcile duplicates by generating suite/test IDs.
3. **Phase 3**: Populate historical data (if available) from CI logs to seed `statusHistory` and `lastPassAt` fields.
4. Provide idempotent scripts and record metrics during migration to catch anomalies.

## 11. Risks & Mitigations
| Risk | Mitigation |
| --- | --- |
| Canonical ID change causes duplicate edges | Use feature flag and cleanup job to retire old ids post-migration. |
| Coverage data missing for older runs | Default to `null`, add reprocessing job when coverage provider integrated. |
| Query complexity impacts performance | Add targeted indexes and caching; monitor query stats post-release. |
| Evidence arrays grow large | Enforce cap (20 items) and compress older artifacts into dedicated nodes. |

## 12. Implementation Milestones
1. **Milestone A**: Implement normalization helper + unit tests.
2. **Milestone B**: Update persistence layer & migrations; deploy behind feature flag.
3. **Milestone C**: Extend queries/APIs and document usage.
4. **Milestone D**: Enable temporal tracking and finalize migration.
5. **Milestone E**: Roll out dashboards and monitor instrumentation.

## 13. Open Questions
- Should manual QA or exploratory tests be represented differently (e.g., `testType='manual'`)?
- Do we need to model flaky probability explicitly (`flakyScore`) or infer from `statusHistory`?
- How should we handle parameterized tests that map to multiple entities—multiple edges or aggregated metadata?
- What retention policy is required for test evidence artifacts to balance storage and auditability?
</file>

</files>
