This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cargo/
  config.toml
.github/
  actions/
    setup-node/
      action.yml
  workflows/
    pre-release.yml
    publish.yml
    test.yml
assets/
  scripts/
    toast-notification.ps1
crates/
  db/
    .sqlx/
      query-003a304a303ff4e55405edab31fe1d3a7b70eb75047fbf7b750a5f72c2401dec.json
      query-00aa2d8701f6b1ed2e84ad00b9b6aaf8d3cce788d2494ff283e2fad71df0a05d.json
      query-01a0f9724e5fce7d3312a742e72cded85605ee540150972e2a8364919f56d5c0.json
      query-024b53c73eda9f79c65997261d5cc3b35ce19c27b22dcc03dbb3fd11ad7bbfe2.json
      query-0923b77d137a29fc54d399a873ff15fc4af894490bc65a4d344a7575cb0d8643.json
      query-09510a7e5927bd5000f6e9e027d4bf1edf6246f1feb575917ed0aff0e6e0f5a1.json
      query-09d997b7b3dcc6bbea9b20c878795f6d13bac6f4f9064c457f2e4847a76214be.json
      query-0bf539bafb9c27cb352b0e08722c59a1cca3b6073517c982e5c08f62bc3ef4e4.json
      query-0cc11bb9acffabc6f173cdbaac3be4c84fb1f2802364ac996285b611cb83c3bf.json
      query-1268afe9ca849daa6722e3df7ca8e9e61f0d37052e782bb5452ab8e1018d9b63.json
      query-1280290c78a1f55b3f0074ebcb61d855cab3e9be9ab0bc8c3f678adf4506b9cc.json
      query-129f898c089030e5ce8c41ff43fd28f213b1c78fc2cf97698da877ff91d6c086.json
      query-1395fe4c3041a4d05e5c3caa068471c8790a67890d6a0566f44bd4e134679095.json
      query-19fcd51ab5368347045ccb0eb39f0bf5dc321c057d01b55151b6ca67f163fc9b.json
      query-1b082630a9622f8667ee7a9aba2c2d3176019a68c6bb83d33008594821415a57.json
      query-1c652bb5d039cdcef8e5cc64e283771b6e49fdf3abea89652d2bc57dafd2c63d.json
      query-1d406258fa90610bddb8973e25fd9dc4f59b0769d943d2cc74d9008e68670f3e.json
      query-1e339e959f8d2cdac13b3e2b452d2f718c0fd6cf6202d5c9139fb1afda123d29.json
      query-216193a63f7b0fb788566b63f56d83ee3d344a5c85e1a5999247b6a44f3ae390.json
      query-216efabcdaa2a6ea166e4468a6ac66d3298666a546e964a509538731ece90c9e.json
      query-2188432c66e9010684b6bb670d19abd77695b05d1dd84ef3102930bc0fe6404f.json
      query-233a016d4de730d203f4120f93daaddd10f3047ae17290c82dbbea1aafd064d1.json
      query-2827716b6501cc3c44ec50ae1a3d90f759cc3940c8fe6ffe383d0f741e3a2a78.json
      query-283a8ef6493346c9ee3bf649e977849eb361d801cdfc8180a8f082269a6bd649.json
      query-290ce5c152be8d36e58ff42570f9157beb07ab9e77a03ec6fc30b4f56f9b8f6b.json
      query-290fb7c65611a73e2b3955383c5881d47b101ffec11415ea6824e390d478921f.json
      query-2d7347baa31214cd9d556c279546bc6625050021dc5b20c0b1f29d54caebf03f.json
      query-2ec7648202fc6f496b97d9486cf9fd3c59fdba73c168628784f0a09488b80528.json
      query-32c9dae46df6480ce1ca07f72b8659e60d9159afcc03a4bb5213f7a2bae537d8.json
      query-32db0a6321ee8a93f0aa31e9b9e128e6e802873f557afa3331bededed403742c.json
      query-36e4ba7bbd81b402d5a20b6005755eafbb174c8dda442081823406ac32809a94.json
      query-3880c8745b172bec8e9f0477ee78339e157531a00dc63d73a31ce554f54e5ca6.json
      query-38d187eeb3ffd442fdf69ae2f1c7e26e7b97622dcfb91fddaff53df62541149d.json
      query-3d6bd16fbce59efe30b7f67ea342e0e4ea6d1432389c02468ad79f1f742d4031.json
      query-417a8b1ff4e51de82aea0159a3b97932224dc325b23476cb84153d690227fd8b.json
      query-417b6e6333eb2164b4cb1d9869cf786f34fa0219b30461234c47a869945c2a79.json
      query-461cc1b0bb6fd909afc9dd2246e8526b3771cfbb0b22ae4b5d17b51af587b9e2.json
      query-4a52af0e7eedb3662a05b23e9a0c74c08d6c255ef598bb8ec3ff9a67f2344ab1.json
      query-4e9e0acca10277c51bb132d71946e3da50286e7873807cc0e96a3243e3c18449.json
      query-56238751ac9cab8bd97ad787143d91f54c47089c8e732ef80c3d1e85dfba1430.json
      query-59d178b298ba60d490a9081a40064a5acb06fecbc0b164c0de2fe502d02b13a7.json
      query-5a886026d75d515c01f347cc203c8d99dd04c61dc468e2e4c5aa548436d13834.json
      query-5ae4dea70309b2aa40d41412f70b200038176dc8c56c49eeaaa65763a1b276eb.json
      query-62836ddbbe22ea720063ac2b8d3f5efa39bf018b01b7a1f5ff6eefc9e4c55445.json
      query-659169a5aedc023fe0aad16dd3f5e07cfa9177808cec69c2034dfaeec3baffec.json
      query-665a24d692fc26e67826ecf0f5487f067e53aa896e475c799dc93763349e49ec.json
      query-69234edbfb4ec9fad3e3411fccae611558bc1940dcec18221657bd3a3ad45aee.json
      query-6a2b3feec049de24d28f87e3a4f570122f78ccdacb140901bf231b5e5c52fbe3.json
      query-71c7befa63391ca211eb69036ff0e4aabe92932fd8bb7ba8c52b2ae8bf411ac8.json
      query-72769cc30de13bb250687b26609ee95660cb4b716615406ecb6f45c4562c3f97.json
      query-75239b2da188f749707d77f3c1544332ca70db3d6d6743b2601dc0d167536437.json
      query-79b35e39d668ad2285b3a05c15d5243cc91d35e03104d222488c7e27b8bbb569.json
      query-7e657b504fb7d8935fcb944f8f4646635f14e6ed9ff77d1c2225ce82e40fa03d.json
      query-821192d8d8a8fba8ce0f144a32e7e500aaa2b6e527b7e7f082a1c73b1f9f9eb8.json
      query-8cc087f95fb55426ee6481bdd0f74b2083ceaf6c5cf82456a7d83c18323c5cec.json
      query-8f01ebd64bdcde6a090479f14810d73ba23020e76fd70854ac57f2da251702c3.json
      query-90d5b39dddf9f5c6c48cd8268f7381a2a772537c3daa1f9d800b1ef1f191f21d.json
      query-929cd77c5aa8015af36aa70ef830d030788b96aaddcef73aed4e8c7d7d5dee46.json
      query-943c19a516ecc2060133457fc8104ad612dfb872f616cd47bb900646b7f5af37.json
      query-96036c4f9e0f48bdc5a4a4588f0c5f288ac7aaa5425cac40fc33f337e1a351f2.json
      query-9778726648c310caa65a00d31e7f9ecc38ca88b7536300143a889eda327ed1a4.json
      query-97e6a03adc1c14e9ecabe7885598dcc0ea273dffea920838fc4dcc837293ba6b.json
      query-9966caaf5d4427190b812b20bd76e5370bc0b0ba877192ac487ff7ba487b0fa1.json
      query-a2a728b99b3bd2dfd2e7d9ce57c10e0b37f187025c1e5ac567ca4fbd43b9d9be.json
      query-a31fff84f3b8e532fd1160447d89d700f06ae08821fee00c9a5b60492b05259c.json
      query-a465d763c8cf09aeb8a48873bdd926b02119a6592fa0df5679ebc54a8ec54dcc.json
      query-a5ba908419fb3e456bdd2daca41ba06cc3212ffffb8520fc7dbbcc8b60ada314.json
      query-ac5247c8d7fb86e4650c4b0eb9420031614c831b7b085083bac20c1af314c538.json
      query-b170ff05e4526f2f97fe132c72a5433a29702e33074bd8c563d9a8eaa78cf9ad.json
      query-b68712f3bcf28184ed0497e4d024b914bb01b545bfa627e5f9ba14e1048f4dfd.json
      query-b95cb59154da69213dea2ded3646d2df2f68293be211cc4f9db0582ea691efee.json
      query-b965523f671c1732b03c24c90f2a66f524e657af536c1e767e82011720e17a9f.json
      query-bbc3a97f21c9b6c60a64cd747843837c3af677ab5d7a1167550ab1393ac07ea9.json
      query-bc4d59205c5ff082e33cbe0c3d32d5915c471ca17ac9b6ff61b75cf7cd9839fc.json
      query-c50d2ff0b12e5bcc81e371089ee2d007e233e7db93aefba4fef08e7aa68f5ab7.json
      query-c952513c0d53c8a15d8b0c212fa754f2817e3a810ebe4199dd0d85eedc640164.json
      query-c98097bb6edac80896cf320ca9f670f18db291bf4d626923b63dde3445fb4a3d.json
      query-ce908743b4ad501211d530c4b25ce8ab99a94962d5aa92117a6039201ffa6c2c.json
      query-d14bc3b05882d31c51c7732b2a46cca37c7feb638791d44a2cc83647d1a73624.json
      query-d30aa5786757f32bf2b9c5fe51a45e506c71c28c5994e430d9b0546adb15ffa2.json
      query-d3bdec518c805d8eeb37c2c7d782ce05f7dd1d4df18dab306e91d83f874efe90.json
      query-d5bb6b9584940367852c3ea74613da570956307d063f4d432ab4e9127e863091.json
      query-dbf12fb4f86a70f59781c533a299ff855581b5842f493a7fb7ebed2618db7af9.json
      query-e45aa1e2282cc62522f66049de7d1d1c47e926000fac7a5c5f28237fdb65a0bb.json
      query-eed92030636e8c992067e3cf899b01f06849a71230a0d2a58963dc0d2930244f.json
      query-f9a448b2fdb1435b78a062e5ea77ab77ce31be2205887185900647b4bf49ea73.json
      query-fb0d69b33ac38ec0b1c818e60269214cdbeaa25e4f892c45cf0a3c22f0f9341a.json
      query-fca50b239a422f932269dea68cebc63735a341ec0a154ad7610ad9d65559aaaa.json
    migrations/
      20250617183714_init.sql
      20250620212427_execution_processes.sql
      20250620214100_remove_stdout_stderr_from_task_attempts.sql
      20250621120000_relate_activities_to_execution_processes.sql
      20250623120000_executor_sessions.sql
      20250623130000_add_executor_type_to_execution_processes.sql
      20250625000000_add_dev_script_to_projects.sql
      20250701000000_add_branch_to_task_attempts.sql
      20250701000001_add_pr_tracking_to_task_attempts.sql
      20250701120000_add_assistant_message_to_executor_sessions.sql
      20250708000000_add_base_branch_to_task_attempts.sql
      20250709000000_add_worktree_deleted_flag.sql
      20250710000000_add_setup_completion.sql
      20250715154859_add_task_templates.sql
      20250716143725_add_default_templates.sql
      20250716161432_update_executor_names_to_kebab_case.sql
      20250716170000_add_parent_task_to_tasks.sql
      20250717000000_drop_task_attempt_activities.sql
      20250719000000_add_cleanup_script_to_projects.sql
      20250720000000_add_cleanupscript_to_process_type_constraint.sql
      20250726182144_update_worktree_path_to_container_ref.sql
      20250726210910_make_branch_optional.sql
      20250727124142_remove_command_from_execution_process.sql
      20250727150349_remove_working_directory.sql
      20250729162941_create_execution_process_logs.sql
      20250729165913_remove_stdout_and_stderr_from_execution_processes.sql
      20250730000000_add_executor_action_to_execution_processes.sql
      20250730000001_rename_process_type_to_run_reason.sql
      20250730124500_add_execution_process_task_attempt_index.sql
      20250805112332_add_executor_action_type_to_task_attempts.sql
      20250805122100_fix_executor_action_type_virtual_column.sql
      20250811000000_add_copy_files_to_projects.sql
      20250813000001_rename_base_coding_agent_to_profile.sql
      20250815100344_migrate_old_executor_actions.sql
      20250818150000_refactor_images_to_junction_tables.sql
      20250819000000_move_merge_commit_to_merges_table.sql
      20250902120000_add_masked_by_restore_to_execution_processes.sql
      20250902184501_rename-profile-to-executor.sql
      20250903091032_executors_to_screaming_snake.sql
      20250905090000_add_after_head_commit_to_execution_processes.sql
      20250906120000_add_follow_up_drafts.sql
      20250910120000_add_before_head_commit_to_execution_processes.sql
      20250917123000_optimize_selects_and_cleanup_indexes.sql
    src/
      models/
        execution_process_logs.rs
        execution_process.rs
        executor_session.rs
        follow_up_draft.rs
        image.rs
        merge.rs
        mod.rs
        project.rs
        task_attempt.rs
        task_template.rs
        task.rs
      lib.rs
    Cargo.toml
  deployment/
    src/
      lib.rs
    Cargo.toml
  executors/
    src/
      actions/
        coding_agent_follow_up.rs
        coding_agent_initial.rs
        mod.rs
        script.rs
      executors/
        codex/
          session.rs
        opencode/
          share_bridge.rs
        amp.rs
        claude.rs
        codex.rs
        cursor.rs
        gemini.rs
        mod.rs
        opencode.rs
        qwen.rs
      logs/
        utils/
          entry_index.rs
          mod.rs
          patch.rs
        mod.rs
        plain_text_processor.rs
        stderr_processor.rs
      command.rs
      lib.rs
      mcp_config.rs
      profile.rs
      stdout_dup.rs
    Cargo.toml
    default_mcp.json
    default_profiles.json
  local-deployment/
    src/
      command.rs
      container.rs
      lib.rs
    Cargo.toml
  server/
    src/
      bin/
        generate_types.rs
        mcp_task_server.rs
      mcp/
        mod.rs
        task_server.rs
      middleware/
        mod.rs
        model_loaders.rs
      routes/
        auth.rs
        config.rs
        containers.rs
        events.rs
        execution_processes.rs
        filesystem.rs
        frontend.rs
        github.rs
        health.rs
        images.rs
        mod.rs
        projects.rs
        task_attempts.rs
        task_templates.rs
        tasks.rs
      error.rs
      lib.rs
      main.rs
    build.rs
    Cargo.toml
  services/
    src/
      services/
        config/
          versions/
            mod.rs
            v1.rs
            v2.rs
            v3.rs
            v4.rs
            v5.rs
            v6.rs
          mod.rs
        analytics.rs
        auth.rs
        container.rs
        events.rs
        file_ranker.rs
        file_search_cache.rs
        filesystem_watcher.rs
        filesystem.rs
        git_cli.rs
        git.rs
        github_service.rs
        image.rs
        mod.rs
        notification.rs
        pr_monitor.rs
        sentry.rs
        worktree_manager.rs
      lib.rs
    tests/
      git_ops_safety.rs
      git_remote_ops.rs
      git_workflow.rs
    Cargo.toml
  utils/
    src/
      assets.rs
      browser.rs
      diff.rs
      lib.rs
      log_msg.rs
      msg_store.rs
      path.rs
      port_file.rs
      response.rs
      sentry.rs
      shell.rs
      stream_ext.rs
      stream_lines.rs
      text.rs
      version.rs
    Cargo.toml
dev_assets_seed/
  config.json
docs/
  configuration-customisation/
    agent-configurations.mdx
    creating-task-templates.mdx
    global-settings.mdx
    keyboard-shortcuts.mdx
  core-features/
    creating-projects.mdx
    creating-task-attempts.mdx
    creating-tasks.mdx
    resolving-rebase-conflicts.mdx
    reviewing-code-changes.mdx
    subtasks.mdx
    task-details-full-screen.mdx
  integrations/
    github-integration.mdx
    mcp-server-configuration.mdx
    vibe-kanban-mcp-server.mdx
    vscode-extension.mdx
  logo/
    dark.svg
    light.svg
  AGENTS.md
  CLAUDE.md
  docs.json
  getting-started.mdx
  index.mdx
  README.md
  supported-coding-agents.mdx
frontend/
  public/
    mcp/
      playwright_logo_icon.svg
    site.webmanifest
    vibe-kanban-logo-dark.svg
    vibe-kanban-logo.svg
  src/
    components/
      common/
        ProfileVariantBadge.tsx
        RawLogText.tsx
      dialogs/
        auth/
          GitHubLoginDialog.tsx
          ProvidePatDialog.tsx
        global/
          DisclaimerDialog.tsx
          OnboardingDialog.tsx
          PrivacyOptInDialog.tsx
          ReleaseNotesDialog.tsx
        projects/
          ProjectEditorSelectionDialog.tsx
          ProjectFormDialog.tsx
        settings/
          CreateConfigurationDialog.tsx
          DeleteConfigurationDialog.tsx
        shared/
          ConfirmDialog.tsx
          FolderPickerDialog.tsx
        tasks/
          CreatePRDialog.tsx
          DeleteTaskConfirmationDialog.tsx
          EditorSelectionDialog.tsx
          RebaseDialog.tsx
          RestoreLogsDialog.tsx
          TaskFormDialog.tsx
          TaskTemplateEditDialog.tsx
        index.ts
      diff/
        CommentWidgetLine.tsx
        ReviewCommentRenderer.tsx
      layout/
        navbar.tsx
      logs/
        VirtualizedList.tsx
      NormalizedConversation/
        DisplayConversationEntry.tsx
        EditDiffRenderer.tsx
        FileChangeRenderer.tsx
        FileContentView.tsx
        ToolDetails.tsx
        UserMessage.tsx
      projects/
        copy-files-field.tsx
        github-repository-picker.tsx
        project-detail.tsx
        project-form-fields.tsx
        project-list.tsx
        ProjectCard.tsx
      rjsf/
        templates/
          ArrayFieldTemplate.tsx
          FieldTemplate.tsx
          FormTemplate.tsx
          index.ts
          ObjectFieldTemplate.tsx
        widgets/
          CheckboxWidget.tsx
          index.ts
          SelectWidget.tsx
          TextareaWidget.tsx
          TextWidget.tsx
        index.ts
        theme.ts
      settings/
        ExecutorProfileSelector.tsx
        index.ts
        TaskSettings.tsx
      tasks/
        follow-up/
          FollowUpConflictSection.tsx
          FollowUpEditorCard.tsx
        TaskDetails/
          DiffTab.tsx
          LogsTab.tsx
          ProcessesTab.tsx
          ProcessLogsViewer.tsx
          TabNavigation.tsx
          TaskTitleDescription.tsx
        Toolbar/
          CreateAttempt.tsx
          CurrentAttempt.tsx
        AttemptHeaderCard.tsx
        BranchSelector.tsx
        ConflictBanner.tsx
        FollowUpStatusRow.tsx
        index.ts
        TaskCard.tsx
        TaskDetailsHeader.tsx
        TaskDetailsPanel.tsx
        TaskDetailsToolbar.tsx
        TaskFollowUpSection.tsx
        TaskKanbanBoard.tsx
        TaskRelationshipCard.tsx
        TaskRelationshipViewer.tsx
        TodoPanel.tsx
        VariantSelector.tsx
      ui/
        shadcn-io/
          kanban/
            index.tsx
        alert.tsx
        auto-expanding-textarea.tsx
        badge.tsx
        button.tsx
        card.tsx
        carousel.tsx
        checkbox.tsx
        dialog.tsx
        dropdown-menu.tsx
        file-search-textarea.tsx
        ImageUploadSection.tsx
        input.tsx
        json-editor.tsx
        label.tsx
        loader.tsx
        markdown-renderer.tsx
        multi-file-search-textarea.tsx
        select.tsx
        tabs.tsx
        textarea.tsx
        tooltip.tsx
      config-provider.tsx
      DevBanner.tsx
      diff-view-switch.tsx
      DiffCard.tsx
      ExecutorConfigForm.tsx
      logo.tsx
      search-bar.tsx
      TaskTemplateManager.tsx
      theme-provider.tsx
    constants/
      processes.ts
    contexts/
      EntriesContext.tsx
      ProcessSelectionContext.tsx
      project-context.tsx
      ReviewProvider.tsx
      search-context.tsx
      TabNavigationContext.tsx
    hooks/
      follow-up/
        useDefaultVariant.ts
        useDraftAutosave.ts
        useDraftEdits.ts
        useDraftQueue.ts
        useDraftStream.ts
        useFollowUpSend.ts
      index.ts
      useAttemptBranch.ts
      useAttemptCreation.ts
      useAttemptExecution.ts
      useBranchStatus.ts
      useConversationHistory.ts
      useCreatePR.ts
      useDevServer.ts
      useDiffEntries.ts
      useDiffStream.ts
      useDiffSummary.ts
      useEventSourceManager.ts
      useExecutionProcesses.ts
      useJsonPatchStream.ts
      useJsonPatchWsStream.ts
      useLogStream.ts
      useMerge.ts
      useOpenInEditor.ts
      useOpenProjectInEditor.ts
      usePinnedTodos.ts
      useProcessRetry.ts
      useProfiles.ts
      useProjectBranches.ts
      useProjectTasks.ts
      usePush.ts
      useRebase.ts
      useTaskMutations.ts
      useTaskViewManager.ts
    i18n/
      locales/
        en/
          common.json
          projects.json
          settings.json
        ja/
          common.json
          projects.json
          settings.json
      config.ts
      index.ts
    lib/
      api.ts
      conflicts.ts
      keyboard-shortcuts.ts
      mcp-strategies.ts
      modals.ts
      openTaskForm.ts
      responsive-config.ts
      types.ts
      utils.ts
    pages/
      settings/
        AgentSettings.tsx
        GeneralSettings.tsx
        index.ts
        McpSettings.tsx
        SettingsLayout.tsx
      project-tasks.tsx
      projects.tsx
    stores/
      useDiffViewStore.ts
      useExpandableStore.ts
      useTaskDetailsUiStore.ts
    styles/
      diff-style-overrides.css
      edit-diff-overrides.css
      index.css
    types/
      logs.ts
      modal-args.d.ts
      modals.ts
      tabs.ts
      virtual-executor-schemas.d.ts
    utils/
      extToLanguage.ts
      script-placeholders.ts
      status-labels.ts
      streamJsonPatchEntries.ts
      string.ts
      style-override.tsx
      theme.ts
    vscode/
      bridge.ts
      ContextMenu.tsx
    App.tsx
    main.tsx
    vite-env.d.ts
  .eslintrc.cjs
  .prettierrc.json
  components.json
  index.html
  package.json
  postcss.config.js
  tailwind.config.js
  tsconfig.json
  tsconfig.node.json
  vite.config.ts
npx-cli/
  bin/
    cli.js
  package.json
  README.md
scripts/
  check-i18n.sh
  prepare-db.js
  setup-dev-environment.js
shared/
  schemas/
    amp.json
    claude_code.json
    codex.json
    cursor.json
    gemini.json
    opencode.json
    qwen_code.json
  types.ts
.dockerignore
.gitignore
.npmrc
AGENTS.md
Cargo.toml
check-both.sh
CLAUDE.md
CODE-OF-CONDUCT.md
Dockerfile
LICENSE
local-build.sh
package.json
pnpm-workspace.yaml
README.md
rust-toolchain.toml
rustfmt.toml
test-npm-package.sh
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cargo/config.toml">
# Set macOS deployment target to avoid linker warnings
# Use Rust project defaults: 10.12 for x86_64, 11.0 for aarch64
[target.x86_64-apple-darwin.env]
MACOSX_DEPLOYMENT_TARGET = "10.12"

[target.aarch64-apple-darwin.env]
MACOSX_DEPLOYMENT_TARGET = "11.0"

[target.x86_64-pc-windows-msvc]
rustflags = ["-C", "link-arg=/DEBUG:FASTLINK"]

[target.aarch64-pc-windows-msvc]
rustflags = ["-C", "link-arg=/DEBUG:FASTLINK"]
</file>

<file path=".github/actions/setup-node/action.yml">
name: 'Setup Node.js and pnpm'
description: 'Sets up Node.js and pnpm with caching'

runs:
  using: 'composite'
  steps:
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        registry-url: 'https://registry.npmjs.org'

    - name: Setup pnpm
      uses: pnpm/action-setup@v4
      with:
        version: ${{ env.PNPM_VERSION }}

    - name: Get pnpm store directory
      shell: bash
      run: |
        echo "STORE_PATH=$(pnpm store path --silent)" >> $GITHUB_ENV

    - name: Setup pnpm cache
      uses: actions/cache@v4
      with:
        path: ${{ env.STORE_PATH }}
        key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
        restore-keys: |
          ${{ runner.os }}-pnpm-store-
</file>

<file path=".github/workflows/pre-release.yml">
name: Create GitHub Pre-Release

on:
  workflow_dispatch:
    inputs:
      version_type:
        description: "Version bump type"
        required: true
        default: "patch"
        type: choice
        options:
          - patch
          - minor
          - major
          - prerelease

concurrency:
  group: release-${{ github.ref_name }} # allow concurrent prerelease from different branches
  cancel-in-progress: true

permissions:
  contents: write
  packages: write
  pull-requests: write

env:
  NODE_VERSION: 22
  PNPM_VERSION: 10.8.1
  RUST_TOOLCHAIN: nightly-2025-05-18

jobs:
  bump-version:
    runs-on: ubuntu-22.04
    outputs:
      new_tag: ${{ steps.version.outputs.new_tag }}
      new_version: ${{ steps.version.outputs.new_version }}
      branch_suffix: ${{ steps.branch.outputs.suffix }}
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          ssh-key: ${{ secrets.DEPLOY_KEY }}

      - name: Setup Node
        uses: ./.github/actions/setup-node

      - name: Cache cargo-edit
        uses: actions/cache@v3
        id: cache-cargo-edit
        with:
          path: ~/.cargo/bin/cargo-set-version
          key: cargo-edit-${{ runner.os }}-${{ env.RUST_TOOLCHAIN }}

      - name: Install cargo-edit
        if: steps.cache-cargo-edit.outputs.cache-hit != 'true'
        run: cargo install cargo-edit

      - name: Generate branch suffix
        id: branch
        run: |
          branch_name="${{ github.ref_name }}"
          # Get last 6 characters of branch name, remove all special chars (including dashes)
          suffix=$(echo "$branch_name" | tail -c 7 | sed 's/[^a-zA-Z0-9]//g' | tr '[:upper:]' '[:lower:]')
          echo "Branch: $branch_name"
          echo "Suffix: $suffix"
          echo "suffix=$suffix" >> $GITHUB_OUTPUT

      - name: Determine and update versions
        id: version
        run: |
          # Get the latest version from npm registry
          latest_npm_version=$(npm view vibe-kanban version 2>/dev/null || echo "0.0.0")
          echo "Latest npm version: $latest_npm_version"

          timestamp=$(date +%Y%m%d%H%M%S)

          # Update root package.json based on npm version, not current package.json
          if [[ "${{ github.event.inputs.version_type }}" == "prerelease" ]]; then
            # For prerelease, use current package.json version and add branch suffix
            npm version prerelease --preid="${{ steps.branch.outputs.suffix }}" --no-git-tag-version

            new_version=$(node -p "require('./package.json').version")
            new_tag="v${new_version}.${timestamp}"
          else
            # For regular releases, use npm version and bump it
            npm version $latest_npm_version --no-git-tag-version --allow-same-version
            npm version ${{ github.event.inputs.version_type }} --no-git-tag-version

            new_version=$(node -p "require('./package.json').version")
            new_tag="v${new_version}-${timestamp}"
          fi

          # Update npx-cli package.json to match
          cd npx-cli
          npm version $new_version --no-git-tag-version --allow-same-version
          cd ..

          cargo set-version --workspace "$new_version"

          echo "New version: $new_version"
          echo "new_version=$new_version" >> $GITHUB_OUTPUT
          echo "new_tag=$new_tag" >> $GITHUB_OUTPUT

      - name: Commit changes and create tag
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add package.json pnpm-lock.yaml npx-cli/package.json 
          git add $(find . -name Cargo.toml) 
          git commit -m "chore: bump version to ${{ steps.version.outputs.new_version }}"
          git tag -a ${{ steps.version.outputs.new_tag }} -m "Release ${{ steps.version.outputs.new_tag }}"
          git push
          git push --tags

  build-frontend:
    needs: bump-version
    runs-on: ubuntu-22.04
    env:
      VITE_PUBLIC_REACT_VIRTUOSO_LICENSE_KEY: ${{ secrets.PUBLIC_REACT_VIRTUOSO_LICENSE_KEY }}
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ needs.bump-version.outputs.new_tag }}

      - name: Setup Node
        uses: ./.github/actions/setup-node

      - name: Install dependencies
        run: pnpm install

      - name: Lint frontend
        run: cd frontend && npm run lint

      - name: Type check frontend
        run: cd frontend && npx tsc --noEmit

      - name: Build frontend
        run: cd frontend && npm run build
        env:
          SENTRY_AUTH_TOKEN: ${{ secrets.SENTRY_AUTH_TOKEN }}

      - name: Create Sentry release
        uses: getsentry/action-release@v3
        env:
          SENTRY_AUTH_TOKEN: ${{ secrets.SENTRY_AUTH_TOKEN }}
          SENTRY_ORG: ${{ secrets.SENTRY_ORG }}
          SENTRY_PROJECT: ${{ secrets.SENTRY_PROJECT }}
        with:
          release: ${{ needs.bump-version.outputs.new_version }}
          environment: production
          sourcemaps: "./frontend/dist"
          ignore_missing: true

      - name: Upload frontend artifact
        uses: actions/upload-artifact@v4
        with:
          name: frontend-dist
          path: frontend/dist/
          retention-days: 1

  build-backend:
    needs: [bump-version, build-frontend]
    runs-on: ${{ matrix.os }}
    strategy:
      # Platform matrix - keep target/name in sync with package-npx-cli job
      matrix:
        include:
          - target: x86_64-unknown-linux-gnu
            os: ubuntu-22.04
            name: linux-x64
          - target: x86_64-pc-windows-msvc
            os: windows-latest-l
            name: windows-x64
          - target: x86_64-apple-darwin
            os: macos-13
            name: macos-x64
          - target: aarch64-apple-darwin
            os: macos-14
            name: macos-arm64
          - target: aarch64-pc-windows-msvc
            os: windows-latest-l
            name: windows-arm64
          - target: aarch64-unknown-linux-gnu
            os: ubuntu-22.04
            name: linux-arm64
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ needs.bump-version.outputs.new_tag }}

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: ${{ env.RUST_TOOLCHAIN }}
          targets: ${{ matrix.target }}
          components: rustfmt, clippy

      - name: Cache Rust dependencies
        uses: Swatinem/rust-cache@v2
        with:
          workspaces: "."
          prefix-key: "cache-v1.0"
          key: ${{ matrix.target }}_${{ matrix.os }}
          cache-on-failure: true
          shared-key: "shared"
          cache-all-crates: true

      - name: Download frontend artifact
        uses: actions/download-artifact@v4
        with:
          name: frontend-dist
          path: frontend/dist/

      - name: Install system dependencies (Linux)
        if: matrix.os == 'ubuntu-22.04'
        run: |
          sudo apt-get update
          sudo apt-get install -y pkg-config libssl-dev

      - name: Install ARM64 cross-compilation dependencies (Linux)
        if: matrix.os == 'ubuntu-22.04' && matrix.target == 'aarch64-unknown-linux-gnu'
        run: |
          sudo apt-get install -y gcc-aarch64-linux-gnu g++-aarch64-linux-gnu libc6-dev-arm64-cross

      - name: Build backend for target
        run: |
          cargo build --release --target ${{ matrix.target }} -p server
          cargo build --release --target ${{ matrix.target }} --bin mcp_task_server
        env:
          CARGO_TARGET_AARCH64_UNKNOWN_LINUX_GNU_LINKER: ${{ matrix.target == 'aarch64-unknown-linux-gnu' && 'aarch64-linux-gnu-gcc' || '' }}
          POSTHOG_API_KEY: ${{ secrets.POSTHOG_API_KEY }}
          POSTHOG_API_ENDPOINT: ${{ secrets.POSTHOG_API_ENDPOINT }}

      - name: Setup Sentry CLI
        uses: matbour/setup-sentry-cli@v2
        with:
          token: ${{ secrets.SENTRY_AUTH_TOKEN }}
          organization: ${{ secrets.SENTRY_ORG }}
          project: ${{ secrets.SENTRY_PROJECT }}
          version: 2.21.2

      - name: Upload source maps to Sentry
        run: sentry-cli debug-files upload --include-sources target/${{ matrix.target }}/release

      - name: Prepare binaries (non-macOS)
        if: runner.os != 'macOS'
        shell: bash
        run: |
          mkdir -p dist
          if [[ "${{ matrix.os }}" == "windows-latest-l" ]]; then
            cp target/${{ matrix.target }}/release/server.exe dist/vibe-kanban-${{ matrix.name }}.exe
            cp target/${{ matrix.target }}/release/mcp_task_server.exe dist/vibe-kanban-mcp-${{ matrix.name }}.exe
          else
            cp target/${{ matrix.target }}/release/server dist/vibe-kanban-${{ matrix.name }}
            cp target/${{ matrix.target }}/release/mcp_task_server dist/vibe-kanban-mcp-${{ matrix.name }}
          fi

      # Code signing for macOS only
      - name: Prepare Apple certificate (macOS)
        if: runner.os == 'macOS'
        run: |
          echo "${{ secrets.APPLE_CERTIFICATE_P12_BASE64 }}" | base64 --decode > certificate.p12

      - name: Write API Key to file
        if: runner.os == 'macOS'
        env:
          API_KEY: ${{ secrets.APP_STORE_API_KEY }}
        run: echo $API_KEY > app_store_key.json

      - name: Sign main binary (macOS)
        if: runner.os == 'macOS'
        uses: indygreg/apple-code-sign-action@v1
        with:
          input_path: target/${{ matrix.target }}/release/server
          output_path: vibe-kanban
          p12_file: certificate.p12
          p12_password: ${{ secrets.APPLE_CERTIFICATE_PASSWORD }}
          sign: true
          sign_args: "--code-signature-flags=runtime"

      - name: Package main binary (macOS)
        if: runner.os == 'macOS'
        run: zip vibe-kanban.zip vibe-kanban

      - name: Notarize signed binary (macOS)
        if: runner.os == 'macOS'
        uses: indygreg/apple-code-sign-action@v1
        continue-on-error: true
        with:
          input_path: vibe-kanban.zip
          sign: false
          notarize: true
          app_store_connect_api_key_json_file: app_store_key.json

      - name: Sign MCP binary (macOS)
        if: runner.os == 'macOS'
        uses: indygreg/apple-code-sign-action@v1
        with:
          input_path: target/${{ matrix.target }}/release/mcp_task_server
          output_path: vibe-kanban-mcp
          p12_file: certificate.p12
          p12_password: ${{ secrets.APPLE_CERTIFICATE_PASSWORD }}
          sign: true
          sign_args: "--code-signature-flags=runtime"

      - name: Package MCP binary (macOS)
        if: runner.os == 'macOS'
        run: zip vibe-kanban-mcp.zip vibe-kanban-mcp

      - name: Notarize signed MCP binary (macOS)
        if: runner.os == 'macOS'
        uses: indygreg/apple-code-sign-action@v1
        continue-on-error: true
        with:
          input_path: vibe-kanban-mcp.zip
          sign: false
          notarize: true
          app_store_connect_api_key_json_file: app_store_key.json

      - name: Prepare signed binaries (macOS)
        if: runner.os == 'macOS'
        run: |
          mkdir -p dist
          cp vibe-kanban.zip dist/vibe-kanban-${{ matrix.name }}.zip
          cp vibe-kanban-mcp.zip dist/vibe-kanban-mcp-${{ matrix.name }}.zip

      - name: Clean up certificates (macOS)
        if: runner.os == 'macOS'
        run: |
          rm -f certificate.p12
          rm -rf private_keys/

      - name: Upload binary artifact
        uses: actions/upload-artifact@v4
        with:
          name: backend-binary-${{ matrix.name }}
          path: dist/
          retention-days: 1

  package-npx-cli:
    needs: [bump-version, build-frontend, build-backend]
    runs-on: ubuntu-22.04
    strategy:
      # NOTE: This matrix must be kept in sync with build-backend job above
      # GitHub Actions doesn't support YAML anchors, so duplication is unavoidable
      matrix:
        include:
          - target: x86_64-unknown-linux-gnu
            name: linux-x64
            binary: vibe-kanban
            mcp_binary: vibe-kanban-mcp
          - target: x86_64-pc-windows-msvc
            name: windows-x64
            binary: vibe-kanban.exe
            mcp_binary: vibe-kanban-mcp.exe
          - target: x86_64-apple-darwin
            name: macos-x64
            binary: vibe-kanban
            mcp_binary: vibe-kanban-mcp
          - target: aarch64-apple-darwin
            name: macos-arm64
            binary: vibe-kanban
            mcp_binary: vibe-kanban-mcp
          - target: aarch64-pc-windows-msvc
            name: windows-arm64
            binary: vibe-kanban.exe
            mcp_binary: vibe-kanban-mcp.exe
          - target: aarch64-unknown-linux-gnu
            name: linux-arm64
            binary: vibe-kanban
            mcp_binary: vibe-kanban-mcp
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ needs.bump-version.outputs.new_tag }}

      - name: Download frontend artifact
        uses: actions/download-artifact@v4
        with:
          name: frontend-dist
          path: frontend/dist/

      - name: Download backend binary artifact
        uses: actions/download-artifact@v4
        with:
          name: backend-binary-${{ matrix.name }}
          path: dist/

      - name: List downloaded artifacts
        run: |
          echo "Downloaded backend binaries:"
          find dist/

      - name: Create platform package
        if: matrix.name != 'macos-arm64' && matrix.name != 'macos-x64'
        run: |
          mkdir -p npx-cli/dist/${{ matrix.name }}
          mkdir vibe-kanban-${{ matrix.name }}
          mkdir vibe-kanban-mcp-${{ matrix.name }}

          cp dist/vibe-kanban-${{ matrix.name }}* vibe-kanban-${{ matrix.name }}/${{ matrix.binary }}
          cp dist/vibe-kanban-mcp-${{ matrix.name }}* vibe-kanban-mcp-${{ matrix.name }}/${{ matrix.mcp_binary }}

          zip -j npx-cli/dist/${{ matrix.name }}/vibe-kanban.zip vibe-kanban-${{ matrix.name }}/${{ matrix.binary }}
          zip -j npx-cli/dist/${{ matrix.name }}/vibe-kanban-mcp.zip vibe-kanban-mcp-${{ matrix.name }}/${{ matrix.mcp_binary }}

      - name: Create platform package (macOS)
        if: matrix.name == 'macos-arm64' || matrix.name == 'macos-x64'
        run: |
          mkdir -p npx-cli/dist/${{ matrix.name }}
          mkdir vibe-kanban-${{ matrix.name }}
          cp dist/vibe-kanban-${{ matrix.name }}* npx-cli/dist/${{ matrix.name }}/vibe-kanban.zip
          cp dist/vibe-kanban-mcp-${{ matrix.name }}* npx-cli/dist/${{ matrix.name }}/vibe-kanban-mcp.zip

      - name: Upload platform package artifact
        uses: actions/upload-artifact@v4
        with:
          name: npx-platform-${{ matrix.name }}
          path: npx-cli/dist/
          retention-days: 1

  create-prerelease:
    needs: [bump-version, build-frontend, build-backend, package-npx-cli]
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ needs.bump-version.outputs.new_tag }}

      - name: Download frontend artifact
        uses: actions/download-artifact@v4
        with:
          name: frontend-dist
          path: frontend/dist/

      - name: Download backend npx-cli zips
        uses: actions/download-artifact@v4
        with:
          pattern: npx-platform-*
          path: npx-cli/dist/
          merge-multiple: true

      - name: List downloaded artifacts
        run: |
          echo "Backend dist:"
          find npx-cli/dist
          echo "Frontend dist:"
          find frontend/dist

      - name: Zip frontend
        run: |
          mkdir vibe-kanban-${{ needs.bump-version.outputs.new_tag }}
          mv frontend/dist vibe-kanban-${{ needs.bump-version.outputs.new_tag }}
          zip -r vibe-kanban-${{ needs.bump-version.outputs.new_tag }}.zip vibe-kanban-${{ needs.bump-version.outputs.new_tag }}

      - name: Setup Node for npm pack
        uses: ./.github/actions/setup-node

      - name: Pack
        run: |
          cd npx-cli
          npm pack

      - name: Create GitHub Pre-Release
        uses: softprops/action-gh-release@v2
        with:
          tag_name: ${{ needs.bump-version.outputs.new_tag }}
          name: Pre-release ${{ needs.bump-version.outputs.new_tag }}
          prerelease: true
          generate_release_notes: true
          files: |
            vibe-kanban-${{ needs.bump-version.outputs.new_tag }}.zip
            npx-cli/vibe-kanban-*.tgz
</file>

<file path=".github/workflows/publish.yml">
name: Publish to npm

on:
  release:
    types: [released]
  workflow_dispatch:
    inputs:
      tag_name:
        description: "Release tag (e.g., v1.2.3)"
        required: true
      release_id:
        description: "GitHub release ID"
        required: true

concurrency:
  group: publish
  cancel-in-progress: true

permissions:
  contents: write
  packages: write

env:
  NODE_VERSION: 22
  PNPM_VERSION: 10.8.1

jobs:
  publish:
    runs-on: ubuntu-latest
    # Only run if this was converted from a pre-release
    if: github.event.release.prerelease == false
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.release.tag_name || inputs.tag_name }}

      - name: Setup Node
        uses: ./.github/actions/setup-node

      - name: Configure npm authentication
        run: |
          echo "//registry.npmjs.org/:_authToken=${{ secrets.NPM_TOKEN }}" > ~/.npmrc

      - name: Download release assets
        uses: actions/github-script@v7
        env:
          RELEASE_ID: ${{ inputs.release_id }}
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            const releaseId = context.payload.release?.id || process.env.RELEASE_ID;
            console.log("releaseId:", releaseId);

            if (!releaseId) {
              core.setFailed('No release ID found.');
              return;
            }

            const release = await github.rest.repos.getRelease({
              owner: context.repo.owner,
              repo: context.repo.repo,
              release_id: releaseId
            });
            
            // Find the .tgz file
            const tgzAsset = release.data.assets.find(asset => asset.name.endsWith('.tgz'));
            
            if (!tgzAsset) {
              core.setFailed('No .tgz file found in release assets');
              return;
            }
            
            // Download the asset
            const response = await github.rest.repos.getReleaseAsset({
              owner: context.repo.owner,
              repo: context.repo.repo,
              asset_id: tgzAsset.id,
              headers: {
                Accept: 'application/octet-stream'
              }
            });
            
            // Save to npx-cli directory
            const filePath = path.join('npx-cli', tgzAsset.name);
            fs.writeFileSync(filePath, Buffer.from(response.data));
            
            console.log(`Downloaded ${tgzAsset.name} to ${filePath}`);
            
            // Set output for next step
            core.setOutput('package-file', filePath);
            core.setOutput('package-name', tgzAsset.name);

      - name: Verify package integrity
        id: verify
        run: |
          cd npx-cli
          
          # List files to confirm download
          ls -la *.tgz
          
          # Verify the package can be read
          npm pack --dry-run || echo "Note: This is expected to show differences since we're using the pre-built package"
          
          # Extract package name from the downloaded file
          PACKAGE_FILE=$(ls *.tgz | head -n1)
          echo "package-file=$PACKAGE_FILE" >> $GITHUB_OUTPUT

      - name: Publish to npm
        run: |
          cd npx-cli
          
          # Publish the exact same package that was tested
          PACKAGE_FILE="${{ steps.verify.outputs.package-file }}"
          
          echo "Publishing $PACKAGE_FILE to npm..."
          npm publish "$PACKAGE_FILE"
          
          echo "✅ Successfully published to npm!"
        env:
          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}

      - name: Update release description
        uses: actions/github-script@v7
        env:
          RELEASE_ID: ${{ inputs.release_id }}
        with:
          script: |
            const releaseId = context.payload.release?.id || process.env.RELEASE_ID;;

            // Fetch the release to get the current body
            const release = await github.rest.repos.getRelease({
              owner: context.repo.owner,
              repo: context.repo.repo,
              release_id: releaseId
            });

            const currentBody = release.data.body || '';
            await github.rest.repos.updateRelease({
              owner: context.repo.owner,
              repo: context.repo.repo,
              release_id: releaseId,
              body: currentBody + '\n\n✅ **Published to npm registry**'
            });
</file>

<file path=".github/workflows/test.yml">
name: Test

on:
  pull_request:
    branches: [ main ]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

env:
  CARGO_TERM_COLOR: always
  NODE_VERSION: 22
  PNPM_VERSION: 10.8.1

jobs:
  test:
    runs-on: buildjet-4vcpu-ubuntu-2204
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node
        uses: ./.github/actions/setup-node

      - name: Install dependencies
        run: pnpm install

      - name: Lint frontend
        run: cd frontend && npm run lint

      - name: Check i18n regressions
        env:
          GITHUB_BASE_REF: ${{ github.base_ref || 'main' }}
        run: ./scripts/check-i18n.sh

      - name: Format check frontend
        run: cd frontend && npm run format:check

      - name: Type check frontend
        run: cd frontend && npx tsc --noEmit

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: nightly-2025-05-18
          components: rustfmt, clippy

      - name: Cache Rust dependencies
        uses: Swatinem/rust-cache@v2
        env:
          RUST_CACHE_DEBUG: true
        with:
          workspaces: "."
          cache-provider: "buildjet"
          cache-on-failure: true
          shared-key: "shared"
          cache-all-crates: true

      - name: Build frontend
        run: cd frontend && npm run build

      - name: Checks
        run: |
          cargo fmt --all -- --check
          npm run generate-types:check
          cargo test --workspace
          cargo clippy --all --all-targets -- -D warnings
</file>

<file path="assets/scripts/toast-notification.ps1">
param(
    [Parameter(Mandatory=$true)]
    [string]$Title,
    
    [Parameter(Mandatory=$true)]
    [string]$Message,
    
    [Parameter(Mandatory=$false)]
    [string]$AppName = "Vibe Kanban"
)

[Windows.UI.Notifications.ToastNotificationManager, Windows.UI.Notifications, ContentType = WindowsRuntime] | Out-Null
$Template = [Windows.UI.Notifications.ToastNotificationManager]::GetTemplateContent([Windows.UI.Notifications.ToastTemplateType]::ToastText02)
$RawXml = [xml] $Template.GetXml()
($RawXml.toast.visual.binding.text|where {$_.id -eq "1"}).AppendChild($RawXml.CreateTextNode($Title)) | Out-Null
($RawXml.toast.visual.binding.text|where {$_.id -eq "2"}).AppendChild($RawXml.CreateTextNode($Message)) | Out-Null
$SerializedXml = New-Object Windows.Data.Xml.Dom.XmlDocument
$SerializedXml.LoadXml($RawXml.OuterXml)
$Toast = [Windows.UI.Notifications.ToastNotification]::new($SerializedXml)
$Toast.Tag = $AppName
$Toast.Group = $AppName
$Notifier = [Windows.UI.Notifications.ToastNotificationManager]::CreateToastNotifier($AppName)
$Notifier.Show($Toast)
</file>

<file path="crates/db/.sqlx/query-003a304a303ff4e55405edab31fe1d3a7b70eb75047fbf7b750a5f72c2401dec.json">
{
  "db_name": "SQLite",
  "query": "SELECT  id                AS \"id!: Uuid\",\n                       task_id           AS \"task_id!: Uuid\",\n                       container_ref,\n                       branch,\n                       base_branch,\n                       executor AS \"executor!\",\n                       worktree_deleted  AS \"worktree_deleted!: bool\",\n                       setup_completed_at AS \"setup_completed_at: DateTime<Utc>\",\n                       created_at        AS \"created_at!: DateTime<Utc>\",\n                       updated_at        AS \"updated_at!: DateTime<Utc>\"\n               FROM    task_attempts\n               WHERE   id = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "container_ref",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "branch",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "base_branch",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "executor!",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "worktree_deleted!: bool",
        "ordinal": 6,
        "type_info": "Bool"
      },
      {
        "name": "setup_completed_at: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Datetime"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      true,
      true,
      false,
      true,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "003a304a303ff4e55405edab31fe1d3a7b70eb75047fbf7b750a5f72c2401dec"
}
</file>

<file path="crates/db/.sqlx/query-00aa2d8701f6b1ed2e84ad00b9b6aaf8d3cce788d2494ff283e2fad71df0a05d.json">
{
  "db_name": "SQLite",
  "query": "UPDATE tasks \n               SET title = $3, description = $4, status = $5, parent_task_attempt = $6 \n               WHERE id = $1 AND project_id = $2 \n               RETURNING id as \"id!: Uuid\", project_id as \"project_id!: Uuid\", title, description, status as \"status!: TaskStatus\", parent_task_attempt as \"parent_task_attempt: Uuid\", created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "status!: TaskStatus",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "parent_task_attempt: Uuid",
        "ordinal": 5,
        "type_info": "Blob"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 6
    },
    "nullable": [
      true,
      false,
      false,
      true,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "00aa2d8701f6b1ed2e84ad00b9b6aaf8d3cce788d2494ff283e2fad71df0a05d"
}
</file>

<file path="crates/db/.sqlx/query-01a0f9724e5fce7d3312a742e72cded85605ee540150972e2a8364919f56d5c0.json">
{
  "db_name": "SQLite",
  "query": "SELECT\n  t.id                            AS \"id!: Uuid\",\n  t.project_id                    AS \"project_id!: Uuid\",\n  t.title,\n  t.description,\n  t.status                        AS \"status!: TaskStatus\",\n  t.parent_task_attempt           AS \"parent_task_attempt: Uuid\",\n  t.created_at                    AS \"created_at!: DateTime<Utc>\",\n  t.updated_at                    AS \"updated_at!: DateTime<Utc>\",\n\n  CASE WHEN EXISTS (\n    SELECT 1\n      FROM task_attempts ta\n      JOIN execution_processes ep\n        ON ep.task_attempt_id = ta.id\n     WHERE ta.task_id       = t.id\n       AND ep.status        = 'running'\n       AND ep.run_reason IN ('setupscript','cleanupscript','codingagent')\n     LIMIT 1\n  ) THEN 1 ELSE 0 END            AS \"has_in_progress_attempt!: i64\",\n  \n  CASE WHEN (\n    SELECT ep.status\n      FROM task_attempts ta\n      JOIN execution_processes ep\n        ON ep.task_attempt_id = ta.id\n     WHERE ta.task_id       = t.id\n     AND ep.run_reason IN ('setupscript','cleanupscript','codingagent')\n     ORDER BY ep.created_at DESC\n     LIMIT 1\n  ) IN ('failed','killed') THEN 1 ELSE 0 END\n                                 AS \"last_attempt_failed!: i64\",\n\n  ( SELECT ta.executor\n      FROM task_attempts ta\n      WHERE ta.task_id = t.id\n     ORDER BY ta.created_at DESC\n      LIMIT 1\n    )                               AS \"executor!: String\"\n\nFROM tasks t\nWHERE t.project_id = $1\nORDER BY t.created_at DESC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "status!: TaskStatus",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "parent_task_attempt: Uuid",
        "ordinal": 5,
        "type_info": "Blob"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      },
      {
        "name": "has_in_progress_attempt!: i64",
        "ordinal": 8,
        "type_info": "Null"
      },
      {
        "name": "last_attempt_failed!: i64",
        "ordinal": 9,
        "type_info": "Null"
      },
      {
        "name": "executor!: String",
        "ordinal": 10,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      false,
      true,
      false,
      false,
      null,
      null,
      true
    ]
  },
  "hash": "01a0f9724e5fce7d3312a742e72cded85605ee540150972e2a8364919f56d5c0"
}
</file>

<file path="crates/db/.sqlx/query-024b53c73eda9f79c65997261d5cc3b35ce19c27b22dcc03dbb3fd11ad7bbfe2.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", project_id as \"project_id!: Uuid\", title, description, status as \"status!: TaskStatus\", parent_task_attempt as \"parent_task_attempt: Uuid\", created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM tasks \n               WHERE parent_task_attempt = $1\n               ORDER BY created_at DESC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "status!: TaskStatus",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "parent_task_attempt: Uuid",
        "ordinal": 5,
        "type_info": "Blob"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "024b53c73eda9f79c65997261d5cc3b35ce19c27b22dcc03dbb3fd11ad7bbfe2"
}
</file>

<file path="crates/db/.sqlx/query-0923b77d137a29fc54d399a873ff15fc4af894490bc65a4d344a7575cb0d8643.json">
{
  "db_name": "SQLite",
  "query": "UPDATE task_attempts SET worktree_deleted = TRUE, updated_at = datetime('now') WHERE id = ?",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 1
    },
    "nullable": []
  },
  "hash": "0923b77d137a29fc54d399a873ff15fc4af894490bc65a4d344a7575cb0d8643"
}
</file>

<file path="crates/db/.sqlx/query-09510a7e5927bd5000f6e9e027d4bf1edf6246f1feb575917ed0aff0e6e0f5a1.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO merges (\n                id, task_attempt_id, merge_type, pr_number, pr_url, pr_status, created_at, target_branch_name\n            ) VALUES ($1, $2, 'pr', $3, $4, 'open', $5, $6)\n            RETURNING \n                id as \"id!: Uuid\",\n                task_attempt_id as \"task_attempt_id!: Uuid\",\n                merge_type as \"merge_type!: MergeType\",\n                merge_commit,\n                pr_number,\n                pr_url,\n                pr_status as \"pr_status?: MergeStatus\",\n                pr_merged_at as \"pr_merged_at?: DateTime<Utc>\",\n                pr_merge_commit_sha,\n                created_at as \"created_at!: DateTime<Utc>\",\n                target_branch_name as \"target_branch_name!: String\"\n            ",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "merge_type!: MergeType",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "merge_commit",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "pr_number",
        "ordinal": 4,
        "type_info": "Integer"
      },
      {
        "name": "pr_url",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "pr_status?: MergeStatus",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "pr_merged_at?: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      },
      {
        "name": "pr_merge_commit_sha",
        "ordinal": 8,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      },
      {
        "name": "target_branch_name!: String",
        "ordinal": 10,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 6
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "09510a7e5927bd5000f6e9e027d4bf1edf6246f1feb575917ed0aff0e6e0f5a1"
}
</file>

<file path="crates/db/.sqlx/query-09d997b7b3dcc6bbea9b20c878795f6d13bac6f4f9064c457f2e4847a76214be.json">
{
  "db_name": "SQLite",
  "query": "SELECT es.session_id\n               FROM execution_processes ep\n               JOIN executor_sessions es ON ep.id = es.execution_process_id  \n               WHERE ep.task_attempt_id = $1\n                 AND ep.run_reason = 'codingagent'\n                 AND ep.dropped = FALSE\n                 AND es.session_id IS NOT NULL\n               ORDER BY ep.created_at DESC\n               LIMIT 1",
  "describe": {
    "columns": [
      {
        "name": "session_id",
        "ordinal": 0,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true
    ]
  },
  "hash": "09d997b7b3dcc6bbea9b20c878795f6d13bac6f4f9064c457f2e4847a76214be"
}
</file>

<file path="crates/db/.sqlx/query-0bf539bafb9c27cb352b0e08722c59a1cca3b6073517c982e5c08f62bc3ef4e4.json">
{
  "db_name": "SQLite",
  "query": "UPDATE tasks SET status = $2, updated_at = CURRENT_TIMESTAMP WHERE id = $1",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 2
    },
    "nullable": []
  },
  "hash": "0bf539bafb9c27cb352b0e08722c59a1cca3b6073517c982e5c08f62bc3ef4e4"
}
</file>

<file path="crates/db/.sqlx/query-0cc11bb9acffabc6f173cdbaac3be4c84fb1f2802364ac996285b611cb83c3bf.json">
{
  "db_name": "SQLite",
  "query": "UPDATE execution_processes \n               SET before_head_commit = $1 \n               WHERE id = $2",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 2
    },
    "nullable": []
  },
  "hash": "0cc11bb9acffabc6f173cdbaac3be4c84fb1f2802364ac996285b611cb83c3bf"
}
</file>

<file path="crates/db/.sqlx/query-1268afe9ca849daa6722e3df7ca8e9e61f0d37052e782bb5452ab8e1018d9b63.json">
{
  "db_name": "SQLite",
  "query": "DELETE FROM execution_processes WHERE task_attempt_id = $1",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 1
    },
    "nullable": []
  },
  "hash": "1268afe9ca849daa6722e3df7ca8e9e61f0d37052e782bb5452ab8e1018d9b63"
}
</file>

<file path="crates/db/.sqlx/query-1280290c78a1f55b3f0074ebcb61d855cab3e9be9ab0bc8c3f678adf4506b9cc.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\",\n                      file_path as \"file_path!\",\n                      original_name as \"original_name!\",\n                      mime_type,\n                      size_bytes as \"size_bytes!\",\n                      hash as \"hash!\",\n                      created_at as \"created_at!: DateTime<Utc>\",\n                      updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM images\n               WHERE id = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "file_path!",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "original_name!",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "mime_type",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "size_bytes!",
        "ordinal": 4,
        "type_info": "Integer"
      },
      {
        "name": "hash!",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "1280290c78a1f55b3f0074ebcb61d855cab3e9be9ab0bc8c3f678adf4506b9cc"
}
</file>

<file path="crates/db/.sqlx/query-129f898c089030e5ce8c41ff43fd28f213b1c78fc2cf97698da877ff91d6c086.json">
{
  "db_name": "SQLite",
  "query": "UPDATE task_attempts SET container_ref = $1, updated_at = $2 WHERE id = $3",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 3
    },
    "nullable": []
  },
  "hash": "129f898c089030e5ce8c41ff43fd28f213b1c78fc2cf97698da877ff91d6c086"
}
</file>

<file path="crates/db/.sqlx/query-1395fe4c3041a4d05e5c3caa068471c8790a67890d6a0566f44bd4e134679095.json">
{
  "db_name": "SQLite",
  "query": "SELECT \n                id as \"id!: Uuid\",\n                task_attempt_id as \"task_attempt_id!: Uuid\",\n                merge_type as \"merge_type!: MergeType\",\n                merge_commit,\n                pr_number,\n                pr_url,\n                pr_status as \"pr_status?: MergeStatus\",\n                pr_merged_at as \"pr_merged_at?: DateTime<Utc>\",\n                pr_merge_commit_sha,\n                target_branch_name as \"target_branch_name!: String\",\n                created_at as \"created_at!: DateTime<Utc>\"\n            FROM merges \n            WHERE task_attempt_id = $1\n            ORDER BY created_at DESC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "merge_type!: MergeType",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "merge_commit",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "pr_number",
        "ordinal": 4,
        "type_info": "Integer"
      },
      {
        "name": "pr_url",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "pr_status?: MergeStatus",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "pr_merged_at?: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      },
      {
        "name": "pr_merge_commit_sha",
        "ordinal": 8,
        "type_info": "Text"
      },
      {
        "name": "target_branch_name!: String",
        "ordinal": 9,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 10,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "1395fe4c3041a4d05e5c3caa068471c8790a67890d6a0566f44bd4e134679095"
}
</file>

<file path="crates/db/.sqlx/query-19fcd51ab5368347045ccb0eb39f0bf5dc321c057d01b55151b6ca67f163fc9b.json">
{
  "db_name": "SQLite",
  "query": "UPDATE merges \n            SET pr_status = $1, \n                pr_merge_commit_sha = $2,\n                pr_merged_at = $3\n            WHERE id = $4",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 4
    },
    "nullable": []
  },
  "hash": "19fcd51ab5368347045ccb0eb39f0bf5dc321c057d01b55151b6ca67f163fc9b"
}
</file>

<file path="crates/db/.sqlx/query-1b082630a9622f8667ee7a9aba2c2d3176019a68c6bb83d33008594821415a57.json">
{
  "db_name": "SQLite",
  "query": "DELETE FROM executor_sessions WHERE task_attempt_id = $1",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 1
    },
    "nullable": []
  },
  "hash": "1b082630a9622f8667ee7a9aba2c2d3176019a68c6bb83d33008594821415a57"
}
</file>

<file path="crates/db/.sqlx/query-1c652bb5d039cdcef8e5cc64e283771b6e49fdf3abea89652d2bc57dafd2c63d.json">
{
  "db_name": "SQLite",
  "query": "SELECT id              as \"id!: Uuid\",\n                      task_attempt_id as \"task_attempt_id!: Uuid\",\n                      run_reason      as \"run_reason!: ExecutionProcessRunReason\",\n                      executor_action as \"executor_action!: sqlx::types::Json<ExecutorActionField>\",\n                      before_head_commit,\n                      after_head_commit,\n                      status          as \"status!: ExecutionProcessStatus\",\n                      exit_code,\n                      dropped,\n                      started_at      as \"started_at!: DateTime<Utc>\",\n                      completed_at    as \"completed_at?: DateTime<Utc>\",\n                      created_at      as \"created_at!: DateTime<Utc>\",\n                      updated_at      as \"updated_at!: DateTime<Utc>\"\n               FROM execution_processes\n               WHERE task_attempt_id = ?\n                 AND (? OR dropped = FALSE)\n               ORDER BY created_at ASC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "run_reason!: ExecutionProcessRunReason",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "executor_action!: sqlx::types::Json<ExecutorActionField>",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "before_head_commit",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "after_head_commit",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "status!: ExecutionProcessStatus",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "exit_code",
        "ordinal": 7,
        "type_info": "Integer"
      },
      {
        "name": "dropped",
        "ordinal": 8,
        "type_info": "Bool"
      },
      {
        "name": "started_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      },
      {
        "name": "completed_at?: DateTime<Utc>",
        "ordinal": 10,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 11,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 12,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 2
    },
    "nullable": [
      true,
      false,
      false,
      false,
      true,
      true,
      false,
      true,
      false,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "1c652bb5d039cdcef8e5cc64e283771b6e49fdf3abea89652d2bc57dafd2c63d"
}
</file>

<file path="crates/db/.sqlx/query-1d406258fa90610bddb8973e25fd9dc4f59b0769d943d2cc74d9008e68670f3e.json">
{
  "db_name": "SQLite",
  "query": "SELECT \n                id               as \"id!: Uuid\",\n                task_attempt_id  as \"task_attempt_id!: Uuid\",\n                prompt           as \"prompt!: String\",\n                queued           as \"queued!: bool\",\n                sending          as \"sending!: bool\",\n                variant,\n                image_ids        as \"image_ids?: String\",\n                created_at       as \"created_at!: DateTime<Utc>\",\n                updated_at       as \"updated_at!: DateTime<Utc>\",\n                version          as \"version!: i64\"\n              FROM follow_up_drafts\n             WHERE rowid = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Text"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "prompt!: String",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "queued!: bool",
        "ordinal": 3,
        "type_info": "Integer"
      },
      {
        "name": "sending!: bool",
        "ordinal": 4,
        "type_info": "Integer"
      },
      {
        "name": "variant",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "image_ids?: String",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Datetime"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Datetime"
      },
      {
        "name": "version!: i64",
        "ordinal": 9,
        "type_info": "Integer"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      false,
      false,
      true,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "1d406258fa90610bddb8973e25fd9dc4f59b0769d943d2cc74d9008e68670f3e"
}
</file>

<file path="crates/db/.sqlx/query-1e339e959f8d2cdac13b3e2b452d2f718c0fd6cf6202d5c9139fb1afda123d29.json">
{
  "db_name": "SQLite",
  "query": "DELETE FROM tasks WHERE id = $1",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 1
    },
    "nullable": []
  },
  "hash": "1e339e959f8d2cdac13b3e2b452d2f718c0fd6cf6202d5c9139fb1afda123d29"
}
</file>

<file path="crates/db/.sqlx/query-216193a63f7b0fb788566b63f56d83ee3d344a5c85e1a5999247b6a44f3ae390.json">
{
  "db_name": "SQLite",
  "query": "\n            SELECT ta.id as \"attempt_id!: Uuid\", ta.container_ref, p.git_repo_path as \"git_repo_path!\"\n            FROM task_attempts ta\n            JOIN tasks t ON ta.task_id = t.id\n            JOIN projects p ON t.project_id = p.id\n            WHERE ta.task_id = $1\n            ",
  "describe": {
    "columns": [
      {
        "name": "attempt_id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "container_ref",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "git_repo_path!",
        "ordinal": 2,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      true,
      false
    ]
  },
  "hash": "216193a63f7b0fb788566b63f56d83ee3d344a5c85e1a5999247b6a44f3ae390"
}
</file>

<file path="crates/db/.sqlx/query-216efabcdaa2a6ea166e4468a6ac66d3298666a546e964a509538731ece90c9e.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", project_id as \"project_id!: Uuid\", title, description, status as \"status!: TaskStatus\", parent_task_attempt as \"parent_task_attempt: Uuid\", created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM tasks \n               WHERE id = $1 AND project_id = $2",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "status!: TaskStatus",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "parent_task_attempt: Uuid",
        "ordinal": 5,
        "type_info": "Blob"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 2
    },
    "nullable": [
      true,
      false,
      false,
      true,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "216efabcdaa2a6ea166e4468a6ac66d3298666a546e964a509538731ece90c9e"
}
</file>

<file path="crates/db/.sqlx/query-2188432c66e9010684b6bb670d19abd77695b05d1dd84ef3102930bc0fe6404f.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", project_id as \"project_id!: Uuid\", title, description, status as \"status!: TaskStatus\", parent_task_attempt as \"parent_task_attempt: Uuid\", created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM tasks \n               WHERE id = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "status!: TaskStatus",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "parent_task_attempt: Uuid",
        "ordinal": 5,
        "type_info": "Blob"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "2188432c66e9010684b6bb670d19abd77695b05d1dd84ef3102930bc0fe6404f"
}
</file>

<file path="crates/db/.sqlx/query-233a016d4de730d203f4120f93daaddd10f3047ae17290c82dbbea1aafd064d1.json">
{
  "db_name": "SQLite",
  "query": "SELECT ta.id as \"attempt_id!: Uuid\",\n                      ta.task_id as \"task_id!: Uuid\",\n                      t.project_id as \"project_id!: Uuid\"\n               FROM task_attempts ta\n               JOIN tasks t ON ta.task_id = t.id\n               WHERE ta.container_ref = ?",
  "describe": {
    "columns": [
      {
        "name": "attempt_id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "project_id!: Uuid",
        "ordinal": 2,
        "type_info": "Blob"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false
    ]
  },
  "hash": "233a016d4de730d203f4120f93daaddd10f3047ae17290c82dbbea1aafd064d1"
}
</file>

<file path="crates/db/.sqlx/query-2827716b6501cc3c44ec50ae1a3d90f759cc3940c8fe6ffe383d0f741e3a2a78.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\",\n                      file_path as \"file_path!\",\n                      original_name as \"original_name!\",\n                      mime_type,\n                      size_bytes as \"size_bytes!\",\n                      hash as \"hash!\",\n                      created_at as \"created_at!: DateTime<Utc>\",\n                      updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM images\n               WHERE hash = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "file_path!",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "original_name!",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "mime_type",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "size_bytes!",
        "ordinal": 4,
        "type_info": "Integer"
      },
      {
        "name": "hash!",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "2827716b6501cc3c44ec50ae1a3d90f759cc3940c8fe6ffe383d0f741e3a2a78"
}
</file>

<file path="crates/db/.sqlx/query-283a8ef6493346c9ee3bf649e977849eb361d801cdfc8180a8f082269a6bd649.json">
{
  "db_name": "SQLite",
  "query": "UPDATE projects SET name = $2, git_repo_path = $3, setup_script = $4, dev_script = $5, cleanup_script = $6, copy_files = $7 WHERE id = $1 RETURNING id as \"id!: Uuid\", name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files, created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "name",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "git_repo_path",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "setup_script",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "dev_script",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "cleanup_script",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "copy_files",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 7
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "283a8ef6493346c9ee3bf649e977849eb361d801cdfc8180a8f082269a6bd649"
}
</file>

<file path="crates/db/.sqlx/query-290ce5c152be8d36e58ff42570f9157beb07ab9e77a03ec6fc30b4f56f9b8f6b.json">
{
  "db_name": "SQLite",
  "query": "UPDATE task_templates \n               SET title = $2, description = $3, template_name = $4, updated_at = datetime('now', 'subsec')\n               WHERE id = $1 \n               RETURNING id as \"id!: Uuid\", project_id as \"project_id?: Uuid\", title, description, template_name, created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id?: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "template_name",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 4
    },
    "nullable": [
      true,
      true,
      false,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "290ce5c152be8d36e58ff42570f9157beb07ab9e77a03ec6fc30b4f56f9b8f6b"
}
</file>

<file path="crates/db/.sqlx/query-290fb7c65611a73e2b3955383c5881d47b101ffec11415ea6824e390d478921f.json">
{
  "db_name": "SQLite",
  "query": "UPDATE execution_processes\n               SET dropped = TRUE\n             WHERE task_attempt_id = $1\n               AND created_at >= (SELECT created_at FROM execution_processes WHERE id = $2)\n               AND dropped = FALSE",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 2
    },
    "nullable": []
  },
  "hash": "290fb7c65611a73e2b3955383c5881d47b101ffec11415ea6824e390d478921f"
}
</file>

<file path="crates/db/.sqlx/query-2d7347baa31214cd9d556c279546bc6625050021dc5b20c0b1f29d54caebf03f.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", task_attempt_id as \"task_attempt_id!: Uuid\", run_reason as \"run_reason!: ExecutionProcessRunReason\", executor_action as \"executor_action!: sqlx::types::Json<ExecutorActionField>\", before_head_commit,\n                      after_head_commit, status as \"status!: ExecutionProcessStatus\", exit_code, dropped, started_at as \"started_at!: DateTime<Utc>\", completed_at as \"completed_at?: DateTime<Utc>\",\n                      created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM execution_processes WHERE id = ?",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "run_reason!: ExecutionProcessRunReason",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "executor_action!: sqlx::types::Json<ExecutorActionField>",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "before_head_commit",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "after_head_commit",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "status!: ExecutionProcessStatus",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "exit_code",
        "ordinal": 7,
        "type_info": "Integer"
      },
      {
        "name": "dropped",
        "ordinal": 8,
        "type_info": "Bool"
      },
      {
        "name": "started_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      },
      {
        "name": "completed_at?: DateTime<Utc>",
        "ordinal": 10,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 11,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 12,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      false,
      true,
      true,
      false,
      true,
      false,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "2d7347baa31214cd9d556c279546bc6625050021dc5b20c0b1f29d54caebf03f"
}
</file>

<file path="crates/db/.sqlx/query-2ec7648202fc6f496b97d9486cf9fd3c59fdba73c168628784f0a09488b80528.json">
{
  "db_name": "SQLite",
  "query": "SELECT \n                execution_id as \"execution_id!: Uuid\",\n                logs,\n                byte_size,\n                inserted_at as \"inserted_at!: DateTime<Utc>\"\n               FROM execution_process_logs \n               WHERE execution_id = $1",
  "describe": {
    "columns": [
      {
        "name": "execution_id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "logs",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "byte_size",
        "ordinal": 2,
        "type_info": "Integer"
      },
      {
        "name": "inserted_at!: DateTime<Utc>",
        "ordinal": 3,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      false
    ]
  },
  "hash": "2ec7648202fc6f496b97d9486cf9fd3c59fdba73c168628784f0a09488b80528"
}
</file>

<file path="crates/db/.sqlx/query-32c9dae46df6480ce1ca07f72b8659e60d9159afcc03a4bb5213f7a2bae537d8.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO merges (\n                id, task_attempt_id, merge_type, merge_commit, created_at, target_branch_name\n            ) VALUES ($1, $2, 'direct', $3, $4, $5)\n            RETURNING \n                id as \"id!: Uuid\",\n                task_attempt_id as \"task_attempt_id!: Uuid\",\n                merge_type as \"merge_type!: MergeType\",\n                merge_commit,\n                pr_number,\n                pr_url,\n                pr_status as \"pr_status?: MergeStatus\",\n                pr_merged_at as \"pr_merged_at?: DateTime<Utc>\",\n                pr_merge_commit_sha,\n                created_at as \"created_at!: DateTime<Utc>\",\n                target_branch_name as \"target_branch_name!: String\"\n            ",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "merge_type!: MergeType",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "merge_commit",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "pr_number",
        "ordinal": 4,
        "type_info": "Integer"
      },
      {
        "name": "pr_url",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "pr_status?: MergeStatus",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "pr_merged_at?: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      },
      {
        "name": "pr_merge_commit_sha",
        "ordinal": 8,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      },
      {
        "name": "target_branch_name!: String",
        "ordinal": 10,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 5
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "32c9dae46df6480ce1ca07f72b8659e60d9159afcc03a4bb5213f7a2bae537d8"
}
</file>

<file path="crates/db/.sqlx/query-32db0a6321ee8a93f0aa31e9b9e128e6e802873f557afa3331bededed403742c.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", task_attempt_id as \"task_attempt_id!: Uuid\", run_reason as \"run_reason!: ExecutionProcessRunReason\", executor_action as \"executor_action!: sqlx::types::Json<ExecutorActionField>\", before_head_commit,\n                      after_head_commit, status as \"status!: ExecutionProcessStatus\", exit_code, dropped, started_at as \"started_at!: DateTime<Utc>\", completed_at as \"completed_at?: DateTime<Utc>\",\n                      created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM execution_processes WHERE status = 'running' ORDER BY created_at ASC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "run_reason!: ExecutionProcessRunReason",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "executor_action!: sqlx::types::Json<ExecutorActionField>",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "before_head_commit",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "after_head_commit",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "status!: ExecutionProcessStatus",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "exit_code",
        "ordinal": 7,
        "type_info": "Integer"
      },
      {
        "name": "dropped",
        "ordinal": 8,
        "type_info": "Bool"
      },
      {
        "name": "started_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      },
      {
        "name": "completed_at?: DateTime<Utc>",
        "ordinal": 10,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 11,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 12,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 0
    },
    "nullable": [
      true,
      false,
      false,
      false,
      true,
      true,
      false,
      true,
      false,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "32db0a6321ee8a93f0aa31e9b9e128e6e802873f557afa3331bededed403742c"
}
</file>

<file path="crates/db/.sqlx/query-36e4ba7bbd81b402d5a20b6005755eafbb174c8dda442081823406ac32809a94.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", project_id as \"project_id?: Uuid\", title, description, template_name, created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM task_templates \n               WHERE id = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id?: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "template_name",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      true,
      false,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "36e4ba7bbd81b402d5a20b6005755eafbb174c8dda442081823406ac32809a94"
}
</file>

<file path="crates/db/.sqlx/query-3880c8745b172bec8e9f0477ee78339e157531a00dc63d73a31ce554f54e5ca6.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", task_attempt_id as \"task_attempt_id!: Uuid\", run_reason as \"run_reason!: ExecutionProcessRunReason\", executor_action as \"executor_action!: sqlx::types::Json<ExecutorActionField>\", before_head_commit,\n                      after_head_commit, status as \"status!: ExecutionProcessStatus\", exit_code, dropped, started_at as \"started_at!: DateTime<Utc>\", completed_at as \"completed_at?: DateTime<Utc>\",\n                      created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM execution_processes\n               WHERE task_attempt_id = ? AND run_reason = ? AND dropped = FALSE\n               ORDER BY created_at DESC LIMIT 1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "run_reason!: ExecutionProcessRunReason",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "executor_action!: sqlx::types::Json<ExecutorActionField>",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "before_head_commit",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "after_head_commit",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "status!: ExecutionProcessStatus",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "exit_code",
        "ordinal": 7,
        "type_info": "Integer"
      },
      {
        "name": "dropped",
        "ordinal": 8,
        "type_info": "Bool"
      },
      {
        "name": "started_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      },
      {
        "name": "completed_at?: DateTime<Utc>",
        "ordinal": 10,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 11,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 12,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 2
    },
    "nullable": [
      true,
      false,
      false,
      false,
      true,
      true,
      false,
      true,
      false,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "3880c8745b172bec8e9f0477ee78339e157531a00dc63d73a31ce554f54e5ca6"
}
</file>

<file path="crates/db/.sqlx/query-38d187eeb3ffd442fdf69ae2f1c7e26e7b97622dcfb91fddaff53df62541149d.json">
{
  "db_name": "SQLite",
  "query": "SELECT COUNT(1) as \"count!:_\" FROM execution_processes\n               WHERE task_attempt_id = $1\n                 AND created_at > (SELECT created_at FROM execution_processes WHERE id = $2)",
  "describe": {
    "columns": [
      {
        "name": "count!:_",
        "ordinal": 0,
        "type_info": "Integer"
      }
    ],
    "parameters": {
      "Right": 2
    },
    "nullable": [
      false
    ]
  },
  "hash": "38d187eeb3ffd442fdf69ae2f1c7e26e7b97622dcfb91fddaff53df62541149d"
}
</file>

<file path="crates/db/.sqlx/query-3d6bd16fbce59efe30b7f67ea342e0e4ea6d1432389c02468ad79f1f742d4031.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO task_templates (id, project_id, title, description, template_name) \n               VALUES ($1, $2, $3, $4, $5) \n               RETURNING id as \"id!: Uuid\", project_id as \"project_id?: Uuid\", title, description, template_name, created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id?: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "template_name",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 5
    },
    "nullable": [
      true,
      true,
      false,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "3d6bd16fbce59efe30b7f67ea342e0e4ea6d1432389c02468ad79f1f742d4031"
}
</file>

<file path="crates/db/.sqlx/query-417a8b1ff4e51de82aea0159a3b97932224dc325b23476cb84153d690227fd8b.json">
{
  "db_name": "SQLite",
  "query": "SELECT \n                id as \"id!: Uuid\", \n                task_attempt_id as \"task_attempt_id!: Uuid\", \n                execution_process_id as \"execution_process_id!: Uuid\", \n                session_id, \n                prompt,\n                summary,\n                created_at as \"created_at!: DateTime<Utc>\", \n                updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM executor_sessions \n               WHERE task_attempt_id = $1 \n               ORDER BY created_at ASC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "execution_process_id!: Uuid",
        "ordinal": 2,
        "type_info": "Blob"
      },
      {
        "name": "session_id",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "prompt",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "summary",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "417a8b1ff4e51de82aea0159a3b97932224dc325b23476cb84153d690227fd8b"
}
</file>

<file path="crates/db/.sqlx/query-417b6e6333eb2164b4cb1d9869cf786f34fa0219b30461234c47a869945c2a79.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO follow_up_drafts (id, task_attempt_id, prompt, queued, variant, image_ids)\n                   VALUES ($1, $2, $3, $4, $5, $6)\n                   ON CONFLICT(task_attempt_id) DO UPDATE SET\n                     prompt = excluded.prompt,\n                     queued = excluded.queued,\n                     variant = excluded.variant,\n                     image_ids = excluded.image_ids\n                   RETURNING \n                    id               as \"id!: Uuid\",\n                    task_attempt_id  as \"task_attempt_id!: Uuid\",\n                    prompt           as \"prompt!: String\",\n                    queued           as \"queued!: bool\",\n                    sending          as \"sending!: bool\",\n                    variant,\n                    image_ids        as \"image_ids?: String\",\n                   created_at       as \"created_at!: DateTime<Utc>\",\n                    updated_at       as \"updated_at!: DateTime<Utc>\",\n                    version          as \"version!: i64\"",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Text"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "prompt!: String",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "queued!: bool",
        "ordinal": 3,
        "type_info": "Integer"
      },
      {
        "name": "sending!: bool",
        "ordinal": 4,
        "type_info": "Integer"
      },
      {
        "name": "variant",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "image_ids?: String",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Datetime"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Datetime"
      },
      {
        "name": "version!: i64",
        "ordinal": 9,
        "type_info": "Integer"
      }
    ],
    "parameters": {
      "Right": 6
    },
    "nullable": [
      true,
      false,
      false,
      false,
      false,
      true,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "417b6e6333eb2164b4cb1d9869cf786f34fa0219b30461234c47a869945c2a79"
}
</file>

<file path="crates/db/.sqlx/query-461cc1b0bb6fd909afc9dd2246e8526b3771cfbb0b22ae4b5d17b51af587b9e2.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", project_id as \"project_id?: Uuid\", title, description, template_name, created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"\n                   FROM task_templates \n                   WHERE project_id IS NULL\n                   ORDER BY template_name ASC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id?: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "template_name",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 0
    },
    "nullable": [
      true,
      true,
      false,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "461cc1b0bb6fd909afc9dd2246e8526b3771cfbb0b22ae4b5d17b51af587b9e2"
}
</file>

<file path="crates/db/.sqlx/query-4a52af0e7eedb3662a05b23e9a0c74c08d6c255ef598bb8ec3ff9a67f2344ab1.json">
{
  "db_name": "SQLite",
  "query": "UPDATE executor_sessions \n               SET summary = $1, updated_at = $2 \n               WHERE execution_process_id = $3",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 3
    },
    "nullable": []
  },
  "hash": "4a52af0e7eedb3662a05b23e9a0c74c08d6c255ef598bb8ec3ff9a67f2344ab1"
}
</file>

<file path="crates/db/.sqlx/query-4e9e0acca10277c51bb132d71946e3da50286e7873807cc0e96a3243e3c18449.json">
{
  "db_name": "SQLite",
  "query": "\n            SELECT ta.id as \"attempt_id!: Uuid\", ta.container_ref, p.git_repo_path as \"git_repo_path!\"\n            FROM task_attempts ta\n            LEFT JOIN execution_processes ep ON ta.id = ep.task_attempt_id AND ep.completed_at IS NOT NULL\n            JOIN tasks t ON ta.task_id = t.id\n            JOIN projects p ON t.project_id = p.id\n            WHERE ta.worktree_deleted = FALSE\n                -- Exclude attempts with any running processes (in progress)\n                AND ta.id NOT IN (\n                    SELECT DISTINCT ep2.task_attempt_id\n                    FROM execution_processes ep2\n                    WHERE ep2.completed_at IS NULL\n                )\n            GROUP BY ta.id, ta.container_ref, p.git_repo_path, ta.updated_at\n            HAVING datetime('now', '-72 hours') > datetime(\n                MAX(\n                    CASE\n                        WHEN ep.completed_at IS NOT NULL THEN ep.completed_at\n                        ELSE ta.updated_at\n                    END\n                )\n            )\n            ORDER BY MAX(\n                CASE\n                    WHEN ep.completed_at IS NOT NULL THEN ep.completed_at\n                    ELSE ta.updated_at\n                END\n            ) ASC\n            ",
  "describe": {
    "columns": [
      {
        "name": "attempt_id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "container_ref",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "git_repo_path!",
        "ordinal": 2,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 0
    },
    "nullable": [
      true,
      true,
      true
    ]
  },
  "hash": "4e9e0acca10277c51bb132d71946e3da50286e7873807cc0e96a3243e3c18449"
}
</file>

<file path="crates/db/.sqlx/query-56238751ac9cab8bd97ad787143d91f54c47089c8e732ef80c3d1e85dfba1430.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO execution_process_logs (execution_id, logs, byte_size, inserted_at)\n               VALUES ($1, $2, $3, datetime('now', 'subsec'))\n               ON CONFLICT (execution_id) DO UPDATE\n               SET logs = logs || $2,\n                   byte_size = byte_size + $3,\n                   inserted_at = datetime('now', 'subsec')",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 3
    },
    "nullable": []
  },
  "hash": "56238751ac9cab8bd97ad787143d91f54c47089c8e732ef80c3d1e85dfba1430"
}
</file>

<file path="crates/db/.sqlx/query-59d178b298ba60d490a9081a40064a5acb06fecbc0b164c0de2fe502d02b13a7.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO projects (id, name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files) VALUES ($1, $2, $3, $4, $5, $6, $7) RETURNING id as \"id!: Uuid\", name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files, created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "name",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "git_repo_path",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "setup_script",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "dev_script",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "cleanup_script",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "copy_files",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 7
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "59d178b298ba60d490a9081a40064a5acb06fecbc0b164c0de2fe502d02b13a7"
}
</file>

<file path="crates/db/.sqlx/query-5a886026d75d515c01f347cc203c8d99dd04c61dc468e2e4c5aa548436d13834.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO executor_sessions (\n                id, task_attempt_id, execution_process_id, session_id, prompt, summary,\n                created_at, updated_at\n               )\n               VALUES ($1, $2, $3, $4, $5, $6, $7, $8)\n               RETURNING\n                id as \"id!: Uuid\",\n                task_attempt_id as \"task_attempt_id!: Uuid\",\n                execution_process_id as \"execution_process_id!: Uuid\",\n                session_id,\n                prompt,\n                summary,\n                created_at as \"created_at!: DateTime<Utc>\",\n                updated_at as \"updated_at!: DateTime<Utc>\"",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "execution_process_id!: Uuid",
        "ordinal": 2,
        "type_info": "Blob"
      },
      {
        "name": "session_id",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "prompt",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "summary",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 8
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "5a886026d75d515c01f347cc203c8d99dd04c61dc468e2e4c5aa548436d13834"
}
</file>

<file path="crates/db/.sqlx/query-5ae4dea70309b2aa40d41412f70b200038176dc8c56c49eeaaa65763a1b276eb.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO tasks (id, project_id, title, description, status, parent_task_attempt) \n               VALUES ($1, $2, $3, $4, $5, $6) \n               RETURNING id as \"id!: Uuid\", project_id as \"project_id!: Uuid\", title, description, status as \"status!: TaskStatus\", parent_task_attempt as \"parent_task_attempt: Uuid\", created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "status!: TaskStatus",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "parent_task_attempt: Uuid",
        "ordinal": 5,
        "type_info": "Blob"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 6
    },
    "nullable": [
      true,
      false,
      false,
      true,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "5ae4dea70309b2aa40d41412f70b200038176dc8c56c49eeaaa65763a1b276eb"
}
</file>

<file path="crates/db/.sqlx/query-62836ddbbe22ea720063ac2b8d3f5efa39bf018b01b7a1f5ff6eefc9e4c55445.json">
{
  "db_name": "SQLite",
  "query": "SELECT EXISTS(SELECT 1 FROM task_attempts WHERE container_ref = ?) as \"exists!: bool\"",
  "describe": {
    "columns": [
      {
        "name": "exists!: bool",
        "ordinal": 0,
        "type_info": "Integer"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      false
    ]
  },
  "hash": "62836ddbbe22ea720063ac2b8d3f5efa39bf018b01b7a1f5ff6eefc9e4c55445"
}
</file>

<file path="crates/db/.sqlx/query-659169a5aedc023fe0aad16dd3f5e07cfa9177808cec69c2034dfaeec3baffec.json">
{
  "db_name": "SQLite",
  "query": "SELECT\n                ep.id                         as \"id!: Uuid\",\n                ep.task_attempt_id            as \"task_attempt_id!: Uuid\",\n                ep.after_head_commit          as after_head_commit,\n                prev.after_head_commit        as prev_after_head_commit,\n                ta.base_branch                as base_branch,\n                p.git_repo_path               as git_repo_path\n            FROM execution_processes ep\n            JOIN task_attempts ta ON ta.id = ep.task_attempt_id\n            JOIN tasks t ON t.id = ta.task_id\n            JOIN projects p ON p.id = t.project_id\n            LEFT JOIN execution_processes prev\n              ON prev.task_attempt_id = ep.task_attempt_id\n             AND prev.created_at = (\n                   SELECT max(created_at) FROM execution_processes\n                     WHERE task_attempt_id = ep.task_attempt_id\n                       AND created_at < ep.created_at\n               )\n            WHERE ep.before_head_commit IS NULL\n              AND ep.after_head_commit IS NOT NULL",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "after_head_commit",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "prev_after_head_commit",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "base_branch",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "git_repo_path",
        "ordinal": 5,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 0
    },
    "nullable": [
      true,
      false,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "659169a5aedc023fe0aad16dd3f5e07cfa9177808cec69c2034dfaeec3baffec"
}
</file>

<file path="crates/db/.sqlx/query-665a24d692fc26e67826ecf0f5487f067e53aa896e475c799dc93763349e49ec.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", project_id as \"project_id?: Uuid\", title, description, template_name, created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"\n                   FROM task_templates \n                   WHERE project_id = ?\n                   ORDER BY template_name ASC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id?: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "template_name",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      true,
      false,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "665a24d692fc26e67826ecf0f5487f067e53aa896e475c799dc93763349e49ec"
}
</file>

<file path="crates/db/.sqlx/query-69234edbfb4ec9fad3e3411fccae611558bc1940dcec18221657bd3a3ad45aee.json">
{
  "db_name": "SQLite",
  "query": "\n            SELECT p.id as \"id!: Uuid\", p.name, p.git_repo_path, p.setup_script, p.dev_script, p.cleanup_script, p.copy_files, \n                   p.created_at as \"created_at!: DateTime<Utc>\", p.updated_at as \"updated_at!: DateTime<Utc>\"\n            FROM projects p\n            WHERE p.id IN (\n                SELECT DISTINCT t.project_id\n                FROM tasks t\n                INNER JOIN task_attempts ta ON ta.task_id = t.id\n                ORDER BY ta.updated_at DESC\n            )\n            LIMIT $1\n            ",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "name",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "git_repo_path",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "setup_script",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "dev_script",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "cleanup_script",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "copy_files",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "69234edbfb4ec9fad3e3411fccae611558bc1940dcec18221657bd3a3ad45aee"
}
</file>

<file path="crates/db/.sqlx/query-6a2b3feec049de24d28f87e3a4f570122f78ccdacb140901bf231b5e5c52fbe3.json">
{
  "db_name": "SQLite",
  "query": "DELETE FROM task_images WHERE task_id = $1",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 1
    },
    "nullable": []
  },
  "hash": "6a2b3feec049de24d28f87e3a4f570122f78ccdacb140901bf231b5e5c52fbe3"
}
</file>

<file path="crates/db/.sqlx/query-71c7befa63391ca211eb69036ff0e4aabe92932fd8bb7ba8c52b2ae8bf411ac8.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files, created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\" FROM projects WHERE git_repo_path = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "name",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "git_repo_path",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "setup_script",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "dev_script",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "cleanup_script",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "copy_files",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "71c7befa63391ca211eb69036ff0e4aabe92932fd8bb7ba8c52b2ae8bf411ac8"
}
</file>

<file path="crates/db/.sqlx/query-72769cc30de13bb250687b26609ee95660cb4b716615406ecb6f45c4562c3f97.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files, created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\" FROM projects ORDER BY created_at DESC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "name",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "git_repo_path",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "setup_script",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "dev_script",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "cleanup_script",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "copy_files",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 0
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "72769cc30de13bb250687b26609ee95660cb4b716615406ecb6f45c4562c3f97"
}
</file>

<file path="crates/db/.sqlx/query-75239b2da188f749707d77f3c1544332ca70db3d6d6743b2601dc0d167536437.json">
{
  "db_name": "SQLite",
  "query": "SELECT\n                id as \"id!: Uuid\",\n                task_attempt_id as \"task_attempt_id!: Uuid\",\n                execution_process_id as \"execution_process_id!: Uuid\",\n                session_id,\n                prompt,\n                summary,\n                created_at as \"created_at!: DateTime<Utc>\",\n                updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM executor_sessions\n               WHERE execution_process_id = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "execution_process_id!: Uuid",
        "ordinal": 2,
        "type_info": "Blob"
      },
      {
        "name": "session_id",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "prompt",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "summary",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "75239b2da188f749707d77f3c1544332ca70db3d6d6743b2601dc0d167536437"
}
</file>

<file path="crates/db/.sqlx/query-79b35e39d668ad2285b3a05c15d5243cc91d35e03104d222488c7e27b8bbb569.json">
{
  "db_name": "SQLite",
  "query": "DELETE FROM images WHERE id = $1",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 1
    },
    "nullable": []
  },
  "hash": "79b35e39d668ad2285b3a05c15d5243cc91d35e03104d222488c7e27b8bbb569"
}
</file>

<file path="crates/db/.sqlx/query-7e657b504fb7d8935fcb944f8f4646635f14e6ed9ff77d1c2225ce82e40fa03d.json">
{
  "db_name": "SQLite",
  "query": "UPDATE execution_processes \n               SET status = $1, exit_code = $2, completed_at = $3\n               WHERE id = $4",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 4
    },
    "nullable": []
  },
  "hash": "7e657b504fb7d8935fcb944f8f4646635f14e6ed9ff77d1c2225ce82e40fa03d"
}
</file>

<file path="crates/db/.sqlx/query-821192d8d8a8fba8ce0f144a32e7e500aaa2b6e527b7e7f082a1c73b1f9f9eb8.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files, created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\" FROM projects WHERE id = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "name",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "git_repo_path",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "setup_script",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "dev_script",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "cleanup_script",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "copy_files",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "821192d8d8a8fba8ce0f144a32e7e500aaa2b6e527b7e7f082a1c73b1f9f9eb8"
}
</file>

<file path="crates/db/.sqlx/query-8cc087f95fb55426ee6481bdd0f74b2083ceaf6c5cf82456a7d83c18323c5cec.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", project_id as \"project_id!: Uuid\", title, description, status as \"status!: TaskStatus\", parent_task_attempt as \"parent_task_attempt: Uuid\", created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM tasks \n               WHERE rowid = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "status!: TaskStatus",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "parent_task_attempt: Uuid",
        "ordinal": 5,
        "type_info": "Blob"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "8cc087f95fb55426ee6481bdd0f74b2083ceaf6c5cf82456a7d83c18323c5cec"
}
</file>

<file path="crates/db/.sqlx/query-8f01ebd64bdcde6a090479f14810d73ba23020e76fd70854ac57f2da251702c3.json">
{
  "db_name": "SQLite",
  "query": "DELETE FROM task_templates WHERE id = $1",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 1
    },
    "nullable": []
  },
  "hash": "8f01ebd64bdcde6a090479f14810d73ba23020e76fd70854ac57f2da251702c3"
}
</file>

<file path="crates/db/.sqlx/query-90d5b39dddf9f5c6c48cd8268f7381a2a772537c3daa1f9d800b1ef1f191f21d.json">
{
  "db_name": "SQLite",
  "query": "UPDATE executor_sessions\n               SET session_id = $1, updated_at = $2\n               WHERE execution_process_id = $3",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 3
    },
    "nullable": []
  },
  "hash": "90d5b39dddf9f5c6c48cd8268f7381a2a772537c3daa1f9d800b1ef1f191f21d"
}
</file>

<file path="crates/db/.sqlx/query-929cd77c5aa8015af36aa70ef830d030788b96aaddcef73aed4e8c7d7d5dee46.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO execution_processes (\n                    id, task_attempt_id, run_reason, executor_action, before_head_commit,\n                    after_head_commit, status, exit_code, started_at, completed_at, created_at, updated_at\n                ) VALUES (?, ?, ?, ?, ?, NULL, ?, ?, ?, ?, ?, ?) RETURNING\n                    id as \"id!: Uuid\", task_attempt_id as \"task_attempt_id!: Uuid\", run_reason as \"run_reason!: ExecutionProcessRunReason\", executor_action as \"executor_action!: sqlx::types::Json<ExecutorActionField>\", before_head_commit,\n                    after_head_commit, status as \"status!: ExecutionProcessStatus\", exit_code, dropped, started_at as \"started_at!: DateTime<Utc>\", completed_at as \"completed_at?: DateTime<Utc>\", created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "run_reason!: ExecutionProcessRunReason",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "executor_action!: sqlx::types::Json<ExecutorActionField>",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "before_head_commit",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "after_head_commit",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "status!: ExecutionProcessStatus",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "exit_code",
        "ordinal": 7,
        "type_info": "Integer"
      },
      {
        "name": "dropped",
        "ordinal": 8,
        "type_info": "Bool"
      },
      {
        "name": "started_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      },
      {
        "name": "completed_at?: DateTime<Utc>",
        "ordinal": 10,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 11,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 12,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 11
    },
    "nullable": [
      true,
      false,
      false,
      false,
      true,
      true,
      false,
      true,
      false,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "929cd77c5aa8015af36aa70ef830d030788b96aaddcef73aed4e8c7d7d5dee46"
}
</file>

<file path="crates/db/.sqlx/query-943c19a516ecc2060133457fc8104ad612dfb872f616cd47bb900646b7f5af37.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO images (id, file_path, original_name, mime_type, size_bytes, hash)\n               VALUES ($1, $2, $3, $4, $5, $6)\n               RETURNING id as \"id!: Uuid\", \n                         file_path as \"file_path!\", \n                         original_name as \"original_name!\", \n                         mime_type,\n                         size_bytes as \"size_bytes!\",\n                         hash as \"hash!\",\n                         created_at as \"created_at!: DateTime<Utc>\", \n                         updated_at as \"updated_at!: DateTime<Utc>\"",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "file_path!",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "original_name!",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "mime_type",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "size_bytes!",
        "ordinal": 4,
        "type_info": "Integer"
      },
      {
        "name": "hash!",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 6
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "943c19a516ecc2060133457fc8104ad612dfb872f616cd47bb900646b7f5af37"
}
</file>

<file path="crates/db/.sqlx/query-96036c4f9e0f48bdc5a4a4588f0c5f288ac7aaa5425cac40fc33f337e1a351f2.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", project_id as \"project_id?: Uuid\", title, description, template_name, created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM task_templates \n               ORDER BY project_id IS NULL DESC, template_name ASC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "project_id?: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "title",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "description",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "template_name",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 0
    },
    "nullable": [
      true,
      true,
      false,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "96036c4f9e0f48bdc5a4a4588f0c5f288ac7aaa5425cac40fc33f337e1a351f2"
}
</file>

<file path="crates/db/.sqlx/query-9778726648c310caa65a00d31e7f9ecc38ca88b7536300143a889eda327ed1a4.json">
{
  "db_name": "SQLite",
  "query": "UPDATE follow_up_drafts\n               SET sending = 1, updated_at = CURRENT_TIMESTAMP, version = version + 1\n             WHERE task_attempt_id = $1\n               AND queued = 1\n               AND sending = 0\n               AND TRIM(prompt) != ''",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 1
    },
    "nullable": []
  },
  "hash": "9778726648c310caa65a00d31e7f9ecc38ca88b7536300143a889eda327ed1a4"
}
</file>

<file path="crates/db/.sqlx/query-97e6a03adc1c14e9ecabe7885598dcc0ea273dffea920838fc4dcc837293ba6b.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO execution_process_logs (execution_id, logs, byte_size, inserted_at)\n               VALUES ($1, $2, $3, $4)\n               ON CONFLICT (execution_id) DO UPDATE\n               SET logs = EXCLUDED.logs, \n                   byte_size = EXCLUDED.byte_size,\n                   inserted_at = EXCLUDED.inserted_at\n               RETURNING \n                execution_id as \"execution_id!: Uuid\",\n                logs,\n                byte_size,\n                inserted_at as \"inserted_at!: DateTime<Utc>\"",
  "describe": {
    "columns": [
      {
        "name": "execution_id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "logs",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "byte_size",
        "ordinal": 2,
        "type_info": "Integer"
      },
      {
        "name": "inserted_at!: DateTime<Utc>",
        "ordinal": 3,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 4
    },
    "nullable": [
      true,
      false,
      false,
      false
    ]
  },
  "hash": "97e6a03adc1c14e9ecabe7885598dcc0ea273dffea920838fc4dcc837293ba6b"
}
</file>

<file path="crates/db/.sqlx/query-9966caaf5d4427190b812b20bd76e5370bc0b0ba877192ac487ff7ba487b0fa1.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO task_images (id, task_id, image_id)\n                   SELECT $1, $2, $3\n                   WHERE NOT EXISTS (\n                       SELECT 1 FROM task_images WHERE task_id = $2 AND image_id = $3\n                   )",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 3
    },
    "nullable": []
  },
  "hash": "9966caaf5d4427190b812b20bd76e5370bc0b0ba877192ac487ff7ba487b0fa1"
}
</file>

<file path="crates/db/.sqlx/query-a2a728b99b3bd2dfd2e7d9ce57c10e0b37f187025c1e5ac567ca4fbd43b9d9be.json">
{
  "db_name": "SQLite",
  "query": "SELECT  id                AS \"id!: Uuid\",\n                       task_id           AS \"task_id!: Uuid\",\n                       container_ref,\n                       branch,\n                       base_branch,\n                       executor AS \"executor!\",\n                       worktree_deleted  AS \"worktree_deleted!: bool\",\n                       setup_completed_at AS \"setup_completed_at: DateTime<Utc>\",\n                       created_at        AS \"created_at!: DateTime<Utc>\",\n                       updated_at        AS \"updated_at!: DateTime<Utc>\"\n               FROM    task_attempts\n               WHERE   rowid = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "container_ref",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "branch",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "base_branch",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "executor!",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "worktree_deleted!: bool",
        "ordinal": 6,
        "type_info": "Bool"
      },
      {
        "name": "setup_completed_at: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Datetime"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      true,
      true,
      false,
      true,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "a2a728b99b3bd2dfd2e7d9ce57c10e0b37f187025c1e5ac567ca4fbd43b9d9be"
}
</file>

<file path="crates/db/.sqlx/query-a31fff84f3b8e532fd1160447d89d700f06ae08821fee00c9a5b60492b05259c.json">
{
  "db_name": "SQLite",
  "query": "SELECT \n                id as \"id!: Uuid\", \n                task_attempt_id as \"task_attempt_id!: Uuid\", \n                execution_process_id as \"execution_process_id!: Uuid\", \n                session_id, \n                prompt,\n                summary,\n                created_at as \"created_at!: DateTime<Utc>\", \n                updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM executor_sessions \n               WHERE id = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "execution_process_id!: Uuid",
        "ordinal": 2,
        "type_info": "Blob"
      },
      {
        "name": "session_id",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "prompt",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "summary",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "a31fff84f3b8e532fd1160447d89d700f06ae08821fee00c9a5b60492b05259c"
}
</file>

<file path="crates/db/.sqlx/query-a465d763c8cf09aeb8a48873bdd926b02119a6592fa0df5679ebc54a8ec54dcc.json">
{
  "db_name": "SQLite",
  "query": "SELECT id AS \"id!: Uuid\",\n                              task_id AS \"task_id!: Uuid\",\n                              container_ref,\n                              branch,\n                              base_branch,\n                              executor AS \"executor!\",\n                              worktree_deleted AS \"worktree_deleted!: bool\",\n                              setup_completed_at AS \"setup_completed_at: DateTime<Utc>\",\n                              created_at AS \"created_at!: DateTime<Utc>\",\n                              updated_at AS \"updated_at!: DateTime<Utc>\"\n                       FROM task_attempts\n                       WHERE task_id = $1\n                       ORDER BY created_at DESC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "container_ref",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "branch",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "base_branch",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "executor!",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "worktree_deleted!: bool",
        "ordinal": 6,
        "type_info": "Bool"
      },
      {
        "name": "setup_completed_at: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Datetime"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      true,
      true,
      false,
      true,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "a465d763c8cf09aeb8a48873bdd926b02119a6592fa0df5679ebc54a8ec54dcc"
}
</file>

<file path="crates/db/.sqlx/query-a5ba908419fb3e456bdd2daca41ba06cc3212ffffb8520fc7dbbcc8b60ada314.json">
{
  "db_name": "SQLite",
  "query": "DELETE FROM projects WHERE id = $1",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 1
    },
    "nullable": []
  },
  "hash": "a5ba908419fb3e456bdd2daca41ba06cc3212ffffb8520fc7dbbcc8b60ada314"
}
</file>

<file path="crates/db/.sqlx/query-ac5247c8d7fb86e4650c4b0eb9420031614c831b7b085083bac20c1af314c538.json">
{
  "db_name": "SQLite",
  "query": "UPDATE task_attempts SET base_branch = $1, updated_at = datetime('now') WHERE id = $2",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 2
    },
    "nullable": []
  },
  "hash": "ac5247c8d7fb86e4650c4b0eb9420031614c831b7b085083bac20c1af314c538"
}
</file>

<file path="crates/db/.sqlx/query-b170ff05e4526f2f97fe132c72a5433a29702e33074bd8c563d9a8eaa78cf9ad.json">
{
  "db_name": "SQLite",
  "query": "UPDATE execution_processes\n               SET dropped = TRUE\n             WHERE task_attempt_id = $1\n               AND created_at > (SELECT created_at FROM execution_processes WHERE id = $2)\n               AND dropped = FALSE\n            ",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 2
    },
    "nullable": []
  },
  "hash": "b170ff05e4526f2f97fe132c72a5433a29702e33074bd8c563d9a8eaa78cf9ad"
}
</file>

<file path="crates/db/.sqlx/query-b68712f3bcf28184ed0497e4d024b914bb01b545bfa627e5f9ba14e1048f4dfd.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO task_images (id, task_id, image_id)\n               VALUES ($1, $2, $3)\n               RETURNING id as \"id!: Uuid\",\n                         task_id as \"task_id!: Uuid\",\n                         image_id as \"image_id!: Uuid\", \n                         created_at as \"created_at!: DateTime<Utc>\"",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "image_id!: Uuid",
        "ordinal": 2,
        "type_info": "Blob"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 3,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 3
    },
    "nullable": [
      true,
      false,
      false,
      false
    ]
  },
  "hash": "b68712f3bcf28184ed0497e4d024b914bb01b545bfa627e5f9ba14e1048f4dfd"
}
</file>

<file path="crates/db/.sqlx/query-b95cb59154da69213dea2ded3646d2df2f68293be211cc4f9db0582ea691efee.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files, created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\" FROM projects WHERE git_repo_path = $1 AND id != $2",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "name",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "git_repo_path",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "setup_script",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "dev_script",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "cleanup_script",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "copy_files",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 2
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "b95cb59154da69213dea2ded3646d2df2f68293be211cc4f9db0582ea691efee"
}
</file>

<file path="crates/db/.sqlx/query-b965523f671c1732b03c24c90f2a66f524e657af536c1e767e82011720e17a9f.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", task_attempt_id as \"task_attempt_id!: Uuid\", run_reason as \"run_reason!: ExecutionProcessRunReason\", executor_action as \"executor_action!: sqlx::types::Json<ExecutorActionField>\", before_head_commit,\n                      after_head_commit, status as \"status!: ExecutionProcessStatus\", exit_code, dropped, started_at as \"started_at!: DateTime<Utc>\", completed_at as \"completed_at?: DateTime<Utc>\",\n                      created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM execution_processes WHERE rowid = ?",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "run_reason!: ExecutionProcessRunReason",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "executor_action!: sqlx::types::Json<ExecutorActionField>",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "before_head_commit",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "after_head_commit",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "status!: ExecutionProcessStatus",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "exit_code",
        "ordinal": 7,
        "type_info": "Integer"
      },
      {
        "name": "dropped",
        "ordinal": 8,
        "type_info": "Bool"
      },
      {
        "name": "started_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      },
      {
        "name": "completed_at?: DateTime<Utc>",
        "ordinal": 10,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 11,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 12,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      false,
      true,
      true,
      false,
      true,
      false,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "b965523f671c1732b03c24c90f2a66f524e657af536c1e767e82011720e17a9f"
}
</file>

<file path="crates/db/.sqlx/query-bbc3a97f21c9b6c60a64cd747843837c3af677ab5d7a1167550ab1393ac07ea9.json">
{
  "db_name": "SQLite",
  "query": "UPDATE executor_sessions \n               SET prompt = $1, updated_at = $2 \n               WHERE id = $3",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 3
    },
    "nullable": []
  },
  "hash": "bbc3a97f21c9b6c60a64cd747843837c3af677ab5d7a1167550ab1393ac07ea9"
}
</file>

<file path="crates/db/.sqlx/query-bc4d59205c5ff082e33cbe0c3d32d5915c471ca17ac9b6ff61b75cf7cd9839fc.json">
{
  "db_name": "SQLite",
  "query": "SELECT i.id as \"id!: Uuid\",\n                      i.file_path as \"file_path!\",\n                      i.original_name as \"original_name!\",\n                      i.mime_type,\n                      i.size_bytes as \"size_bytes!\",\n                      i.hash as \"hash!\",\n                      i.created_at as \"created_at!: DateTime<Utc>\",\n                      i.updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM images i\n               LEFT JOIN task_images ti ON i.id = ti.image_id\n               WHERE ti.task_id IS NULL",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "file_path!",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "original_name!",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "mime_type",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "size_bytes!",
        "ordinal": 4,
        "type_info": "Integer"
      },
      {
        "name": "hash!",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 0
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "bc4d59205c5ff082e33cbe0c3d32d5915c471ca17ac9b6ff61b75cf7cd9839fc"
}
</file>

<file path="crates/db/.sqlx/query-c50d2ff0b12e5bcc81e371089ee2d007e233e7db93aefba4fef08e7aa68f5ab7.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\" FROM tasks WHERE id = $1 AND project_id = $2",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      }
    ],
    "parameters": {
      "Right": 2
    },
    "nullable": [
      true
    ]
  },
  "hash": "c50d2ff0b12e5bcc81e371089ee2d007e233e7db93aefba4fef08e7aa68f5ab7"
}
</file>

<file path="crates/db/.sqlx/query-c952513c0d53c8a15d8b0c212fa754f2817e3a810ebe4199dd0d85eedc640164.json">
{
  "db_name": "SQLite",
  "query": "INSERT INTO task_attempts (id, task_id, container_ref, branch, base_branch, executor, worktree_deleted, setup_completed_at)\n               VALUES ($1, $2, $3, $4, $5, $6, $7, $8)\n               RETURNING id as \"id!: Uuid\", task_id as \"task_id!: Uuid\", container_ref, branch, base_branch, executor as \"executor!\",  worktree_deleted as \"worktree_deleted!: bool\", setup_completed_at as \"setup_completed_at: DateTime<Utc>\", created_at as \"created_at!: DateTime<Utc>\", updated_at as \"updated_at!: DateTime<Utc>\"",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "container_ref",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "branch",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "base_branch",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "executor!",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "worktree_deleted!: bool",
        "ordinal": 6,
        "type_info": "Bool"
      },
      {
        "name": "setup_completed_at: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Datetime"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 8
    },
    "nullable": [
      true,
      false,
      true,
      true,
      false,
      true,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "c952513c0d53c8a15d8b0c212fa754f2817e3a810ebe4199dd0d85eedc640164"
}
</file>

<file path="crates/db/.sqlx/query-c98097bb6edac80896cf320ca9f670f18db291bf4d626923b63dde3445fb4a3d.json">
{
  "db_name": "SQLite",
  "query": "SELECT \n                id               as \"id!: Uuid\",\n                task_attempt_id  as \"task_attempt_id!: Uuid\",\n                prompt           as \"prompt!: String\",\n                queued           as \"queued!: bool\",\n                sending          as \"sending!: bool\",\n                variant,\n                image_ids        as \"image_ids?: String\",\n                created_at       as \"created_at!: DateTime<Utc>\",\n                updated_at       as \"updated_at!: DateTime<Utc>\",\n                version          as \"version!: i64\"\n              FROM follow_up_drafts\n             WHERE task_attempt_id = $1",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Text"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "prompt!: String",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "queued!: bool",
        "ordinal": 3,
        "type_info": "Integer"
      },
      {
        "name": "sending!: bool",
        "ordinal": 4,
        "type_info": "Integer"
      },
      {
        "name": "variant",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "image_ids?: String",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Datetime"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Datetime"
      },
      {
        "name": "version!: i64",
        "ordinal": 9,
        "type_info": "Integer"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      false,
      false,
      true,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "c98097bb6edac80896cf320ca9f670f18db291bf4d626923b63dde3445fb4a3d"
}
</file>

<file path="crates/db/.sqlx/query-ce908743b4ad501211d530c4b25ce8ab99a94962d5aa92117a6039201ffa6c2c.json">
{
  "db_name": "SQLite",
  "query": "UPDATE task_attempts SET branch = $1, updated_at = $2 WHERE id = $3",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 3
    },
    "nullable": []
  },
  "hash": "ce908743b4ad501211d530c4b25ce8ab99a94962d5aa92117a6039201ffa6c2c"
}
</file>

<file path="crates/db/.sqlx/query-d14bc3b05882d31c51c7732b2a46cca37c7feb638791d44a2cc83647d1a73624.json">
{
  "db_name": "SQLite",
  "query": "SELECT  ta.id                AS \"id!: Uuid\",\n                       ta.task_id           AS \"task_id!: Uuid\",\n                       ta.container_ref,\n                       ta.branch,\n                       ta.base_branch,\n                       ta.executor AS \"executor!\",\n                       ta.worktree_deleted  AS \"worktree_deleted!: bool\",\n                       ta.setup_completed_at AS \"setup_completed_at: DateTime<Utc>\",\n                       ta.created_at        AS \"created_at!: DateTime<Utc>\",\n                       ta.updated_at        AS \"updated_at!: DateTime<Utc>\"\n               FROM    task_attempts ta\n               JOIN    tasks t ON ta.task_id = t.id\n               JOIN    projects p ON t.project_id = p.id\n               WHERE   ta.id = $1 AND t.id = $2 AND p.id = $3",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "container_ref",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "branch",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "base_branch",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "executor!",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "worktree_deleted!: bool",
        "ordinal": 6,
        "type_info": "Bool"
      },
      {
        "name": "setup_completed_at: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Datetime"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 3
    },
    "nullable": [
      true,
      false,
      true,
      true,
      false,
      true,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "d14bc3b05882d31c51c7732b2a46cca37c7feb638791d44a2cc83647d1a73624"
}
</file>

<file path="crates/db/.sqlx/query-d30aa5786757f32bf2b9c5fe51a45e506c71c28c5994e430d9b0546adb15ffa2.json">
{
  "db_name": "SQLite",
  "query": "\n                SELECT COUNT(*) as \"count!: i64\"\n                FROM projects\n                WHERE id = $1\n            ",
  "describe": {
    "columns": [
      {
        "name": "count!: i64",
        "ordinal": 0,
        "type_info": "Integer"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      false
    ]
  },
  "hash": "d30aa5786757f32bf2b9c5fe51a45e506c71c28c5994e430d9b0546adb15ffa2"
}
</file>

<file path="crates/db/.sqlx/query-d3bdec518c805d8eeb37c2c7d782ce05f7dd1d4df18dab306e91d83f874efe90.json">
{
  "db_name": "SQLite",
  "query": "UPDATE follow_up_drafts \n               SET prompt = '', queued = 0, sending = 0, image_ids = NULL, updated_at = CURRENT_TIMESTAMP, version = version + 1\n             WHERE task_attempt_id = $1",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 1
    },
    "nullable": []
  },
  "hash": "d3bdec518c805d8eeb37c2c7d782ce05f7dd1d4df18dab306e91d83f874efe90"
}
</file>

<file path="crates/db/.sqlx/query-d5bb6b9584940367852c3ea74613da570956307d063f4d432ab4e9127e863091.json">
{
  "db_name": "SQLite",
  "query": "UPDATE execution_processes \n               SET after_head_commit = $1 \n               WHERE id = $2",
  "describe": {
    "columns": [],
    "parameters": {
      "Right": 2
    },
    "nullable": []
  },
  "hash": "d5bb6b9584940367852c3ea74613da570956307d063f4d432ab4e9127e863091"
}
</file>

<file path="crates/db/.sqlx/query-dbf12fb4f86a70f59781c533a299ff855581b5842f493a7fb7ebed2618db7af9.json">
{
  "db_name": "SQLite",
  "query": "SELECT id AS \"id!: Uuid\",\n                              task_id AS \"task_id!: Uuid\",\n                              container_ref,\n                              branch,\n                              base_branch,\n                              executor AS \"executor!\",\n                              worktree_deleted AS \"worktree_deleted!: bool\",\n                              setup_completed_at AS \"setup_completed_at: DateTime<Utc>\",\n                              created_at AS \"created_at!: DateTime<Utc>\",\n                              updated_at AS \"updated_at!: DateTime<Utc>\"\n                       FROM task_attempts\n                       ORDER BY created_at DESC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "container_ref",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "branch",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "base_branch",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "executor!",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "worktree_deleted!: bool",
        "ordinal": 6,
        "type_info": "Bool"
      },
      {
        "name": "setup_completed_at: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Datetime"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 8,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 0
    },
    "nullable": [
      true,
      false,
      true,
      true,
      false,
      true,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "dbf12fb4f86a70f59781c533a299ff855581b5842f493a7fb7ebed2618db7af9"
}
</file>

<file path="crates/db/.sqlx/query-e45aa1e2282cc62522f66049de7d1d1c47e926000fac7a5c5f28237fdb65a0bb.json">
{
  "db_name": "SQLite",
  "query": "SELECT \n                id as \"id!: Uuid\",\n                task_attempt_id as \"task_attempt_id!: Uuid\",\n                merge_type as \"merge_type!: MergeType\",\n                merge_commit,\n                pr_number,\n                pr_url,\n                pr_status as \"pr_status?: MergeStatus\",\n                pr_merged_at as \"pr_merged_at?: DateTime<Utc>\",\n                pr_merge_commit_sha,\n                created_at as \"created_at!: DateTime<Utc>\",\n                target_branch_name as \"target_branch_name!: String\"\n               FROM merges \n               WHERE merge_type = 'pr' AND pr_status = 'open'\n               ORDER BY created_at DESC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "merge_type!: MergeType",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "merge_commit",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "pr_number",
        "ordinal": 4,
        "type_info": "Integer"
      },
      {
        "name": "pr_url",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "pr_status?: MergeStatus",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "pr_merged_at?: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      },
      {
        "name": "pr_merge_commit_sha",
        "ordinal": 8,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      },
      {
        "name": "target_branch_name!: String",
        "ordinal": 10,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 0
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      true,
      false,
      true,
      true,
      false,
      false
    ]
  },
  "hash": "e45aa1e2282cc62522f66049de7d1d1c47e926000fac7a5c5f28237fdb65a0bb"
}
</file>

<file path="crates/db/.sqlx/query-eed92030636e8c992067e3cf899b01f06849a71230a0d2a58963dc0d2930244f.json">
{
  "db_name": "SQLite",
  "query": "SELECT COUNT(*) as \"count!: i64\" FROM projects",
  "describe": {
    "columns": [
      {
        "name": "count!: i64",
        "ordinal": 0,
        "type_info": "Integer"
      }
    ],
    "parameters": {
      "Right": 0
    },
    "nullable": [
      false
    ]
  },
  "hash": "eed92030636e8c992067e3cf899b01f06849a71230a0d2a58963dc0d2930244f"
}
</file>

<file path="crates/db/.sqlx/query-f9a448b2fdb1435b78a062e5ea77ab77ce31be2205887185900647b4bf49ea73.json">
{
  "db_name": "SQLite",
  "query": "SELECT id as \"id!: Uuid\", container_ref FROM task_attempts WHERE worktree_deleted = FALSE",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "container_ref",
        "ordinal": 1,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 0
    },
    "nullable": [
      true,
      true
    ]
  },
  "hash": "f9a448b2fdb1435b78a062e5ea77ab77ce31be2205887185900647b4bf49ea73"
}
</file>

<file path="crates/db/.sqlx/query-fb0d69b33ac38ec0b1c818e60269214cdbeaa25e4f892c45cf0a3c22f0f9341a.json">
{
  "db_name": "SQLite",
  "query": "SELECT i.id as \"id!: Uuid\",\n                      i.file_path as \"file_path!\",\n                      i.original_name as \"original_name!\",\n                      i.mime_type,\n                      i.size_bytes as \"size_bytes!\",\n                      i.hash as \"hash!\",\n                      i.created_at as \"created_at!: DateTime<Utc>\",\n                      i.updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM images i\n               JOIN task_images ti ON i.id = ti.image_id\n               WHERE ti.task_id = $1\n               ORDER BY ti.created_at",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "file_path!",
        "ordinal": 1,
        "type_info": "Text"
      },
      {
        "name": "original_name!",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "mime_type",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "size_bytes!",
        "ordinal": 4,
        "type_info": "Integer"
      },
      {
        "name": "hash!",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 7,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      true,
      true,
      false,
      false,
      false
    ]
  },
  "hash": "fb0d69b33ac38ec0b1c818e60269214cdbeaa25e4f892c45cf0a3c22f0f9341a"
}
</file>

<file path="crates/db/.sqlx/query-fca50b239a422f932269dea68cebc63735a341ec0a154ad7610ad9d65559aaaa.json">
{
  "db_name": "SQLite",
  "query": "SELECT ep.id as \"id!: Uuid\", ep.task_attempt_id as \"task_attempt_id!: Uuid\", ep.run_reason as \"run_reason!: ExecutionProcessRunReason\", ep.executor_action as \"executor_action!: sqlx::types::Json<ExecutorActionField>\",\n                      ep.before_head_commit, ep.after_head_commit, ep.status as \"status!: ExecutionProcessStatus\", ep.exit_code,\n                      ep.dropped, ep.started_at as \"started_at!: DateTime<Utc>\", ep.completed_at as \"completed_at?: DateTime<Utc>\", ep.created_at as \"created_at!: DateTime<Utc>\", ep.updated_at as \"updated_at!: DateTime<Utc>\"\n               FROM execution_processes ep\n               JOIN task_attempts ta ON ep.task_attempt_id = ta.id\n               JOIN tasks t ON ta.task_id = t.id\n               WHERE ep.status = 'running' AND ep.run_reason = 'devserver' AND t.project_id = ?\n               ORDER BY ep.created_at ASC",
  "describe": {
    "columns": [
      {
        "name": "id!: Uuid",
        "ordinal": 0,
        "type_info": "Blob"
      },
      {
        "name": "task_attempt_id!: Uuid",
        "ordinal": 1,
        "type_info": "Blob"
      },
      {
        "name": "run_reason!: ExecutionProcessRunReason",
        "ordinal": 2,
        "type_info": "Text"
      },
      {
        "name": "executor_action!: sqlx::types::Json<ExecutorActionField>",
        "ordinal": 3,
        "type_info": "Text"
      },
      {
        "name": "before_head_commit",
        "ordinal": 4,
        "type_info": "Text"
      },
      {
        "name": "after_head_commit",
        "ordinal": 5,
        "type_info": "Text"
      },
      {
        "name": "status!: ExecutionProcessStatus",
        "ordinal": 6,
        "type_info": "Text"
      },
      {
        "name": "exit_code",
        "ordinal": 7,
        "type_info": "Integer"
      },
      {
        "name": "dropped",
        "ordinal": 8,
        "type_info": "Bool"
      },
      {
        "name": "started_at!: DateTime<Utc>",
        "ordinal": 9,
        "type_info": "Text"
      },
      {
        "name": "completed_at?: DateTime<Utc>",
        "ordinal": 10,
        "type_info": "Text"
      },
      {
        "name": "created_at!: DateTime<Utc>",
        "ordinal": 11,
        "type_info": "Text"
      },
      {
        "name": "updated_at!: DateTime<Utc>",
        "ordinal": 12,
        "type_info": "Text"
      }
    ],
    "parameters": {
      "Right": 1
    },
    "nullable": [
      true,
      false,
      false,
      false,
      true,
      true,
      false,
      true,
      false,
      false,
      true,
      false,
      false
    ]
  },
  "hash": "fca50b239a422f932269dea68cebc63735a341ec0a154ad7610ad9d65559aaaa"
}
</file>

<file path="crates/db/migrations/20250617183714_init.sql">
PRAGMA foreign_keys = ON;

CREATE TABLE projects (
    id            BLOB PRIMARY KEY,
    name          TEXT NOT NULL,
    git_repo_path TEXT NOT NULL DEFAULT '' UNIQUE,
    setup_script  TEXT DEFAULT '',
    created_at    TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    updated_at    TEXT NOT NULL DEFAULT (datetime('now', 'subsec'))
);

CREATE TABLE tasks (
    id          BLOB PRIMARY KEY,
    project_id  BLOB NOT NULL,
    title       TEXT NOT NULL,
    description TEXT,
    status      TEXT NOT NULL DEFAULT 'todo'
                   CHECK (status IN ('todo','inprogress','done','cancelled','inreview')),
    created_at  TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    updated_at  TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE
);

CREATE TABLE task_attempts (
    id            BLOB PRIMARY KEY,
    task_id       BLOB NOT NULL,
    worktree_path TEXT NOT NULL,
    merge_commit  TEXT,
    executor      TEXT,
    stdout        TEXT,
    stderr        TEXT,
    created_at    TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    updated_at    TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE CASCADE
);

CREATE TABLE task_attempt_activities (
    id              BLOB PRIMARY KEY,
    task_attempt_id BLOB NOT NULL,
    status          TEXT NOT NULL DEFAULT 'init'
                       CHECK (status IN ('init','setuprunning','setupcomplete','setupfailed','executorrunning','executorcomplete','executorfailed','paused')),    note            TEXT,
    created_at      TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    FOREIGN KEY (task_attempt_id) REFERENCES task_attempts(id) ON DELETE CASCADE
);
</file>

<file path="crates/db/migrations/20250620212427_execution_processes.sql">
PRAGMA foreign_keys = ON;

CREATE TABLE execution_processes (
    id                BLOB PRIMARY KEY,
    task_attempt_id   BLOB NOT NULL,
    process_type      TEXT NOT NULL DEFAULT 'setupscript'
                         CHECK (process_type IN ('setupscript','codingagent','devserver')),
    status            TEXT NOT NULL DEFAULT 'running'
                         CHECK (status IN ('running','completed','failed','killed')),
    command           TEXT NOT NULL,
    args              TEXT,  -- JSON array of arguments
    working_directory TEXT NOT NULL,
    stdout            TEXT,
    stderr            TEXT,
    exit_code         INTEGER,
    started_at        TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    completed_at      TEXT,
    created_at        TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    updated_at        TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    FOREIGN KEY (task_attempt_id) REFERENCES task_attempts(id) ON DELETE CASCADE
);

CREATE INDEX idx_execution_processes_task_attempt_id ON execution_processes(task_attempt_id);
CREATE INDEX idx_execution_processes_status ON execution_processes(status);
CREATE INDEX idx_execution_processes_type ON execution_processes(process_type);
</file>

<file path="crates/db/migrations/20250620214100_remove_stdout_stderr_from_task_attempts.sql">
PRAGMA foreign_keys = ON;

-- Remove stdout and stderr columns from task_attempts table
-- These are now tracked in the execution_processes table for better granularity

-- SQLite doesn't support DROP COLUMN directly, so we need to recreate the table
-- First, create a new table without stdout and stderr
CREATE TABLE task_attempts_new (
    id            BLOB PRIMARY KEY,
    task_id       BLOB NOT NULL,
    worktree_path TEXT NOT NULL,
    merge_commit  TEXT,
    executor      TEXT,
    created_at    TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    updated_at    TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE CASCADE
);

-- Copy data from old table to new table (excluding stdout and stderr)
INSERT INTO task_attempts_new (id, task_id, worktree_path, merge_commit, executor, created_at, updated_at)
SELECT id, task_id, worktree_path, merge_commit, executor, created_at, updated_at
FROM task_attempts;

-- Drop the old table
DROP TABLE task_attempts;

-- Rename the new table to the original name
ALTER TABLE task_attempts_new RENAME TO task_attempts;
</file>

<file path="crates/db/migrations/20250621120000_relate_activities_to_execution_processes.sql">
-- Migration to relate task_attempt_activities to execution_processes instead of task_attempts
-- This migration will:
-- 1. Drop and recreate the task_attempt_activities table with execution_process_id
-- 2. Clear existing data as it cannot be migrated meaningfully

-- Drop the existing table (this will wipe existing activity data)
DROP TABLE IF EXISTS task_attempt_activities;

-- Create the new table structure with execution_process_id foreign key
CREATE TABLE task_attempt_activities (
    id TEXT PRIMARY KEY,
    execution_process_id TEXT NOT NULL REFERENCES execution_processes(id) ON DELETE CASCADE,
    status TEXT NOT NULL,
    note TEXT,
    created_at DATETIME NOT NULL DEFAULT (datetime('now')),
    FOREIGN KEY (execution_process_id) REFERENCES execution_processes(id) ON DELETE CASCADE
);

-- Create index for efficient lookups by execution_process_id
CREATE INDEX idx_task_attempt_activities_execution_process_id ON task_attempt_activities(execution_process_id);

-- Create index for efficient lookups by created_at for ordering
CREATE INDEX idx_task_attempt_activities_created_at ON task_attempt_activities(created_at);
</file>

<file path="crates/db/migrations/20250623120000_executor_sessions.sql">
PRAGMA foreign_keys = ON;

CREATE TABLE executor_sessions (
    id                    BLOB PRIMARY KEY,
    task_attempt_id       BLOB NOT NULL,
    execution_process_id  BLOB NOT NULL,
    session_id            TEXT,  -- External session ID from Claude/Amp
    prompt                TEXT,  -- The prompt sent to the executor
    created_at            TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    updated_at            TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    FOREIGN KEY (task_attempt_id) REFERENCES task_attempts(id) ON DELETE CASCADE,
    FOREIGN KEY (execution_process_id) REFERENCES execution_processes(id) ON DELETE CASCADE
);

CREATE INDEX idx_executor_sessions_task_attempt_id ON executor_sessions(task_attempt_id);
CREATE INDEX idx_executor_sessions_execution_process_id ON executor_sessions(execution_process_id);
CREATE INDEX idx_executor_sessions_session_id ON executor_sessions(session_id);
</file>

<file path="crates/db/migrations/20250623130000_add_executor_type_to_execution_processes.sql">
PRAGMA foreign_keys = ON;

-- Add executor_type column to execution_processes table
ALTER TABLE execution_processes ADD COLUMN executor_type TEXT;
</file>

<file path="crates/db/migrations/20250625000000_add_dev_script_to_projects.sql">
PRAGMA foreign_keys = ON;

-- Add dev_script column to projects table
ALTER TABLE projects ADD COLUMN dev_script TEXT DEFAULT '';
</file>

<file path="crates/db/migrations/20250701000000_add_branch_to_task_attempts.sql">
-- Add branch column to task_attempts table
ALTER TABLE task_attempts ADD COLUMN branch TEXT NOT NULL DEFAULT '';
</file>

<file path="crates/db/migrations/20250701000001_add_pr_tracking_to_task_attempts.sql">
-- Add PR tracking fields to task_attempts table
ALTER TABLE task_attempts ADD COLUMN pr_url TEXT;
ALTER TABLE task_attempts ADD COLUMN pr_number INTEGER;
ALTER TABLE task_attempts ADD COLUMN pr_status TEXT; -- open, closed, merged
ALTER TABLE task_attempts ADD COLUMN pr_merged_at DATETIME;
</file>

<file path="crates/db/migrations/20250701120000_add_assistant_message_to_executor_sessions.sql">
-- Add summary column to executor_sessions table
ALTER TABLE executor_sessions ADD COLUMN summary TEXT;
</file>

<file path="crates/db/migrations/20250708000000_add_base_branch_to_task_attempts.sql">
-- Add base_branch column to task_attempts table with default value
ALTER TABLE task_attempts ADD COLUMN base_branch TEXT NOT NULL DEFAULT 'main';
</file>

<file path="crates/db/migrations/20250709000000_add_worktree_deleted_flag.sql">
-- Add worktree_deleted flag to track when worktrees are cleaned up
ALTER TABLE task_attempts ADD COLUMN worktree_deleted BOOLEAN NOT NULL DEFAULT FALSE;
</file>

<file path="crates/db/migrations/20250710000000_add_setup_completion.sql">
-- Add setup completion tracking to task_attempts table
-- This enables automatic setup script execution for recreated worktrees
ALTER TABLE task_attempts ADD COLUMN setup_completed_at DATETIME;
</file>

<file path="crates/db/migrations/20250715154859_add_task_templates.sql">
-- Add task templates tables
CREATE TABLE task_templates (
    id            BLOB PRIMARY KEY,
    project_id    BLOB,  -- NULL for global templates
    title         TEXT NOT NULL,
    description   TEXT,
    template_name TEXT NOT NULL,  -- Display name for the template
    created_at    TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    updated_at    TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE
);

-- Add index for faster queries
CREATE INDEX idx_task_templates_project_id ON task_templates(project_id);

-- Add unique constraints to prevent duplicate template names within same scope
-- For project-specific templates: unique within each project
CREATE UNIQUE INDEX idx_task_templates_unique_name_project 
ON task_templates(project_id, template_name) 
WHERE project_id IS NOT NULL;

-- For global templates: unique across all global templates
CREATE UNIQUE INDEX idx_task_templates_unique_name_global 
ON task_templates(template_name) 
WHERE project_id IS NULL;
</file>

<file path="crates/db/migrations/20250716143725_add_default_templates.sql">
-- Add default global templates

-- 1. Bug Analysis template
INSERT INTO task_templates (
    id,
    project_id,
    title,
    description,
    template_name,
    created_at,
    updated_at
) VALUES (
    randomblob(16),
    NULL, -- Global template
    'Analyze codebase for potential bugs and issues',
    'Perform a comprehensive analysis of the project codebase to identify potential bugs, code smells, and areas of improvement.

## Analysis Checklist:

### 1. Static Code Analysis
- [ ] Run linting tools to identify syntax and style issues
- [ ] Check for unused variables, imports, and dead code
- [ ] Identify potential type errors or mismatches
- [ ] Look for deprecated API usage

### 2. Common Bug Patterns
- [ ] Check for null/undefined reference errors
- [ ] Identify potential race conditions
- [ ] Look for improper error handling
- [ ] Check for resource leaks (memory, file handles, connections)
- [ ] Identify potential security vulnerabilities (XSS, SQL injection, etc.)

### 3. Code Quality Issues
- [ ] Identify overly complex functions (high cyclomatic complexity)
- [ ] Look for code duplication
- [ ] Check for missing or inadequate input validation
- [ ] Identify hardcoded values that should be configurable

### 4. Testing Gaps
- [ ] Identify untested code paths
- [ ] Check for missing edge case tests
- [ ] Look for inadequate error scenario testing

### 5. Performance Concerns
- [ ] Identify potential performance bottlenecks
- [ ] Check for inefficient algorithms or data structures
- [ ] Look for unnecessary database queries or API calls

## Deliverables:
1. Prioritized list of identified issues
2. Recommendations for fixes
3. Estimated effort for addressing each issue',
    'Bug Analysis',
    datetime('now', 'subsec'),
    datetime('now', 'subsec')
);

-- 2. Unit Test template
INSERT INTO task_templates (
    id,
    project_id,
    title,
    description,
    template_name,
    created_at,
    updated_at
) VALUES (
    randomblob(16),
    NULL, -- Global template
    'Add unit tests for [component/function]',
    'Write unit tests to improve code coverage and ensure reliability.

## Unit Testing Checklist

### 1. Identify What to Test
- [ ] Run coverage report to find untested functions
- [ ] List the specific functions/methods to test
- [ ] Note current coverage percentage

### 2. Write Tests
- [ ] Test the happy path (expected behavior)
- [ ] Test edge cases (empty inputs, boundaries)
- [ ] Test error cases (invalid inputs, exceptions)
- [ ] Mock external dependencies
- [ ] Use descriptive test names

### 3. Test Quality
- [ ] Each test focuses on one behavior
- [ ] Tests can run independently
- [ ] No hardcoded values that might change
- [ ] Clear assertions that verify the behavior

## Examples to Cover:
- Normal inputs → Expected outputs
- Empty/null inputs → Proper handling
- Invalid inputs → Error cases
- Boundary values → Edge case behavior

## Goal
Achieve at least 80% coverage for the target component

## Deliverables
1. New test file(s) with comprehensive unit tests
2. Updated coverage report
3. All tests passing',
    'Add Unit Tests',
    datetime('now', 'subsec'),
    datetime('now', 'subsec')
);

-- 3. Code Refactoring template
INSERT INTO task_templates (
    id,
    project_id,
    title,
    description,
    template_name,
    created_at,
    updated_at
) VALUES (
    randomblob(16),
    NULL, -- Global template
    'Refactor [component/module] for better maintainability',
    'Improve code structure and maintainability without changing functionality.

## Refactoring Checklist

### 1. Identify Refactoring Targets
- [ ] Run code analysis tools (linters, complexity analyzers)
- [ ] Identify code smells (long methods, duplicate code, large classes)
- [ ] Check for outdated patterns or deprecated approaches
- [ ] Review areas with frequent bugs or changes

### 2. Plan the Refactoring
- [ ] Define clear goals (what to improve and why)
- [ ] Ensure tests exist for current functionality
- [ ] Create a backup branch
- [ ] Break down into small, safe steps

### 3. Common Refactoring Actions
- [ ] Extract methods from long functions
- [ ] Remove duplicate code (DRY principle)
- [ ] Rename variables/functions for clarity
- [ ] Simplify complex conditionals
- [ ] Extract constants from magic numbers/strings
- [ ] Group related functionality into modules
- [ ] Remove dead code

### 4. Maintain Functionality
- [ ] Run tests after each change
- [ ] Keep changes small and incremental
- [ ] Commit frequently with clear messages
- [ ] Verify no behavior has changed

### 5. Code Quality Improvements
- [ ] Apply consistent formatting
- [ ] Update to modern syntax/features
- [ ] Improve error handling
- [ ] Add type annotations (if applicable)

## Success Criteria
- All tests still pass
- Code is more readable and maintainable
- No new bugs introduced
- Performance not degraded

## Deliverables
1. Refactored code with improved structure
2. All tests passing
3. Brief summary of changes made',
    'Code Refactoring',
    datetime('now', 'subsec'),
    datetime('now', 'subsec')
);
</file>

<file path="crates/db/migrations/20250716161432_update_executor_names_to_kebab_case.sql">
-- Migration to update executor type names from snake_case/camelCase to kebab-case
-- This handles the change from charmopencode -> charm-opencode and setup_script -> setup-script

-- Update task_attempts.executor column
UPDATE task_attempts 
SET executor = 'charm-opencode' 
WHERE executor = 'charmopencode';

UPDATE task_attempts 
SET executor = 'setup-script' 
WHERE executor = 'setup_script';

-- Update execution_processes.executor_type column
UPDATE execution_processes 
SET executor_type = 'charm-opencode' 
WHERE executor_type = 'charmopencode';

UPDATE execution_processes 
SET executor_type = 'setup-script' 
WHERE executor_type = 'setup_script';
</file>

<file path="crates/db/migrations/20250716170000_add_parent_task_to_tasks.sql">
PRAGMA foreign_keys = ON;

-- Add parent_task_attempt column to tasks table
ALTER TABLE tasks ADD COLUMN parent_task_attempt BLOB REFERENCES task_attempts(id);

-- Create index for parent_task_attempt lookups
CREATE INDEX idx_tasks_parent_task_attempt ON tasks(parent_task_attempt);
</file>

<file path="crates/db/migrations/20250717000000_drop_task_attempt_activities.sql">
-- Migration to drop task_attempt_activities table
-- This removes the task attempt activity tracking functionality

-- Drop indexes first
DROP INDEX IF EXISTS idx_task_attempt_activities_execution_process_id;
DROP INDEX IF EXISTS idx_task_attempt_activities_created_at;

-- Drop the table
DROP TABLE IF EXISTS task_attempt_activities;
</file>

<file path="crates/db/migrations/20250719000000_add_cleanup_script_to_projects.sql">
-- Add cleanup_script column to projects table
ALTER TABLE projects ADD COLUMN cleanup_script TEXT;
</file>

<file path="crates/db/migrations/20250720000000_add_cleanupscript_to_process_type_constraint.sql">
-- 1. Add the replacement column with the wider CHECK
ALTER TABLE execution_processes
  ADD COLUMN process_type_new TEXT NOT NULL DEFAULT 'setupscript'
    CHECK (process_type_new IN ('setupscript',
                                'cleanupscript',   -- new value 🎉
                                'codingagent',
                                'devserver'));

-- 2. Copy existing values across
UPDATE execution_processes
  SET process_type_new = process_type;

-- 3. Drop any indexes that mention the old column
DROP INDEX IF EXISTS idx_execution_processes_type;

-- 4. Remove the old column (requires 3.35+)
ALTER TABLE execution_processes DROP COLUMN process_type;

-- 5. Rename the new column back to the canonical name
ALTER TABLE execution_processes
  RENAME COLUMN process_type_new TO process_type;

-- 6. Re-create the index
CREATE INDEX idx_execution_processes_type
        ON execution_processes(process_type);
</file>

<file path="crates/db/migrations/20250726182144_update_worktree_path_to_container_ref.sql">
-- Add migration script here

ALTER TABLE task_attempts ADD COLUMN container_ref TEXT;  -- nullable
UPDATE task_attempts SET container_ref = worktree_path;

-- If you might have triggers or indexes on worktree_path, drop them before this step.

ALTER TABLE task_attempts DROP COLUMN worktree_path;
</file>

<file path="crates/db/migrations/20250726210910_make_branch_optional.sql">
-- Add migration script here

-- 1) Create replacement column (nullable TEXT)
ALTER TABLE task_attempts ADD COLUMN branch_new TEXT;  -- nullable

-- 2) Copy existing values
UPDATE task_attempts SET branch_new = branch;

-- If you have indexes/triggers/constraints that reference "branch",
-- drop them before the next two steps and recreate them afterwards.

-- 3) Remove the old non-nullable column
ALTER TABLE task_attempts DROP COLUMN branch;

-- 4) Keep the original column name
ALTER TABLE task_attempts RENAME COLUMN branch_new TO branch;
</file>

<file path="crates/db/migrations/20250727124142_remove_command_from_execution_process.sql">
-- Add migration script here

ALTER TABLE execution_processes DROP COLUMN command;
ALTER TABLE execution_processes DROP COLUMN args;
</file>

<file path="crates/db/migrations/20250727150349_remove_working_directory.sql">
-- Add migration script here

ALTER TABLE execution_processes DROP COLUMN working_directory;
</file>

<file path="crates/db/migrations/20250729162941_create_execution_process_logs.sql">
PRAGMA foreign_keys = ON;

CREATE TABLE execution_process_logs (
    execution_id      BLOB PRIMARY KEY,
    logs              TEXT NOT NULL,      -- JSONL format (one LogMsg per line)
    byte_size         INTEGER NOT NULL,
    inserted_at       TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    FOREIGN KEY (execution_id) REFERENCES execution_processes(id) ON DELETE CASCADE
);

CREATE INDEX idx_execution_process_logs_inserted_at ON execution_process_logs(inserted_at);
</file>

<file path="crates/db/migrations/20250729165913_remove_stdout_and_stderr_from_execution_processes.sql">
-- Add migration script here

ALTER TABLE execution_processes DROP COLUMN stdout;
ALTER TABLE execution_processes DROP COLUMN stderr;
</file>

<file path="crates/db/migrations/20250730000000_add_executor_action_to_execution_processes.sql">
PRAGMA foreign_keys = ON;

-- Clear existing execution_processes records since we can't meaningfully migrate them
-- (old records lack the actual script content and prompts needed for ExecutorActions)
DELETE FROM execution_processes;

-- Add executor_action column to execution_processes table for storing full ExecutorActions JSON
ALTER TABLE execution_processes ADD COLUMN executor_action TEXT NOT NULL DEFAULT '';
</file>

<file path="crates/db/migrations/20250730000001_rename_process_type_to_run_reason.sql">
PRAGMA foreign_keys = ON;

-- Rename process_type column to run_reason for better semantic clarity
ALTER TABLE execution_processes RENAME COLUMN process_type TO run_reason;
</file>

<file path="crates/db/migrations/20250730124500_add_execution_process_task_attempt_index.sql">
ALTER TABLE execution_processes
ADD COLUMN executor_action_type TEXT
  GENERATED ALWAYS AS (json_extract(executor_action, '$.type')) VIRTUAL;

CREATE INDEX idx_execution_processes_task_attempt_type_created
ON execution_processes (task_attempt_id, executor_action_type, created_at DESC);
</file>

<file path="crates/db/migrations/20250805112332_add_executor_action_type_to_task_attempts.sql">
-- Remove unused executor_type column from execution_processes
ALTER TABLE execution_processes DROP COLUMN executor_type;

ALTER TABLE task_attempts RENAME COLUMN executor TO base_coding_agent;
</file>

<file path="crates/db/migrations/20250805122100_fix_executor_action_type_virtual_column.sql">
-- Drop the existing virtual column and index
DROP INDEX IF EXISTS idx_execution_processes_task_attempt_type_created;
ALTER TABLE execution_processes DROP COLUMN executor_action_type;

-- Recreate the virtual column with the correct JSON path
ALTER TABLE execution_processes
ADD COLUMN executor_action_type TEXT
  GENERATED ALWAYS AS (json_extract(executor_action, '$.typ.type')) VIRTUAL;

-- Recreate the index
CREATE INDEX idx_execution_processes_task_attempt_type_created
ON execution_processes (task_attempt_id, executor_action_type, created_at DESC);
</file>

<file path="crates/db/migrations/20250811000000_add_copy_files_to_projects.sql">
-- Add copy_files column to projects table
-- This field stores comma-separated file paths to copy from the original project directory to the worktree
ALTER TABLE projects ADD COLUMN copy_files TEXT;
</file>

<file path="crates/db/migrations/20250813000001_rename_base_coding_agent_to_profile.sql">
PRAGMA foreign_keys = ON;

-- Rename base_coding_agent column to profile_label for better semantic clarity
ALTER TABLE task_attempts RENAME COLUMN base_coding_agent TO profile;
-- best effort attempt to not break older task attempts by mapping to profiles
UPDATE task_attempts
SET profile = CASE profile
    WHEN 'CLAUDE_CODE' THEN 'claude-code'
    WHEN 'CODEX' THEN 'codex'
    WHEN 'GEMINI' THEN 'gemini'
    WHEN 'AMP' THEN 'amp'
    WHEN 'OPENCODE' THEN 'opencode'
END
WHERE profile IS NOT NULL
  AND profile IN ('CLAUDE_CODE', 'CODEX', 'GEMINI', 'AMP', 'OPENCODE');
</file>

<file path="crates/db/migrations/20250815100344_migrate_old_executor_actions.sql">
-- JSON format changed, means you can access logs from old execution_processes

UPDATE execution_processes
SET executor_action = json_set(
  json_remove(executor_action, '$.typ.profile'),
  '$.typ.profile_variant_label',
  json_object(
    'profile', json_extract(executor_action, '$.typ.profile'),
    'variant', json('null')
  )
)
WHERE json_type(executor_action, '$.typ') IS NOT NULL
  AND json_type(executor_action, '$.typ.profile') = 'text';
</file>

<file path="crates/db/migrations/20250818150000_refactor_images_to_junction_tables.sql">
PRAGMA foreign_keys = ON;

-- Refactor images table to use junction tables for many-to-many relationships
-- This allows images to be associated with multiple tasks and execution processes
-- No data migration needed as there are no existing users of the image system

CREATE TABLE images (
    id                    BLOB PRIMARY KEY,
    file_path             TEXT NOT NULL,  -- relative path within cache/images/
    original_name         TEXT NOT NULL,
    mime_type             TEXT,
    size_bytes            INTEGER,
    hash                  TEXT NOT NULL UNIQUE,  -- SHA256 for deduplication
    created_at            TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    updated_at            TEXT NOT NULL DEFAULT (datetime('now', 'subsec'))
);

-- Create junction table for task-image associations
CREATE TABLE task_images (
    id                    BLOB PRIMARY KEY,
    task_id               BLOB NOT NULL,
    image_id              BLOB NOT NULL,
    created_at            TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    FOREIGN KEY (task_id) REFERENCES tasks(id) ON DELETE CASCADE,
    FOREIGN KEY (image_id) REFERENCES images(id) ON DELETE CASCADE,
    UNIQUE(task_id, image_id)  -- Prevent duplicate associations
);


-- Create indexes for efficient querying
CREATE INDEX idx_images_hash ON images(hash);
CREATE INDEX idx_task_images_task_id ON task_images(task_id);
CREATE INDEX idx_task_images_image_id ON task_images(image_id);
</file>

<file path="crates/db/migrations/20250819000000_move_merge_commit_to_merges_table.sql">
-- Create enhanced merges table with type-specific columns
CREATE TABLE merges (
    id              BLOB PRIMARY KEY,
    task_attempt_id BLOB NOT NULL,
    merge_type      TEXT NOT NULL CHECK (merge_type IN ('direct', 'pr')),
    
    -- Direct merge fields (NULL for PR merges)
    merge_commit    TEXT,
    
    -- PR merge fields (NULL for direct merges)
    pr_number       INTEGER,
    pr_url          TEXT,
    pr_status       TEXT CHECK (pr_status IN ('open', 'merged', 'closed')),
    pr_merged_at    TEXT,
    pr_merge_commit_sha TEXT,
    
    created_at      TEXT NOT NULL DEFAULT (datetime('now', 'subsec')),
    target_branch_name TEXT NOT NULL,

    -- Data integrity constraints
    CHECK (
        (merge_type = 'direct' AND merge_commit IS NOT NULL 
         AND pr_number IS NULL AND pr_url IS NULL) 
        OR 
        (merge_type = 'pr' AND pr_number IS NOT NULL AND pr_url IS NOT NULL 
         AND pr_status IS NOT NULL AND merge_commit IS NULL)
    ),
    
    FOREIGN KEY (task_attempt_id) REFERENCES task_attempts(id) ON DELETE CASCADE
);

-- Create general index for all task_attempt_id queries
CREATE INDEX idx_merges_task_attempt_id ON merges(task_attempt_id);

-- Create index for finding open PRs quickly
CREATE INDEX idx_merges_open_pr ON merges(task_attempt_id, pr_status) 
WHERE merge_type = 'pr' AND pr_status = 'open';

-- Migrate existing merge_commit data to new table as direct merges
INSERT INTO merges (id, task_attempt_id, merge_type, merge_commit, created_at, target_branch_name)
SELECT 
    randomblob(16),
    id,
    'direct',
    merge_commit,
    updated_at,
    base_branch
FROM task_attempts
WHERE merge_commit IS NOT NULL;

-- Migrate existing PR data from task_attempts to merges
INSERT INTO merges (id, task_attempt_id, merge_type, pr_number, pr_url, pr_status, pr_merged_at, pr_merge_commit_sha, created_at, target_branch_name)
SELECT 
    randomblob(16),
    id,
    'pr',
    pr_number,
    pr_url,
    CASE 
        WHEN pr_status = 'merged' THEN 'merged'
        WHEN pr_status = 'closed' THEN 'closed'
        ELSE 'open'
    END,
    pr_merged_at,
    NULL, -- We don't have merge_commit for PRs in task_attempts
    COALESCE(pr_merged_at, updated_at),
    base_branch
FROM task_attempts
WHERE pr_number IS NOT NULL;

-- Drop merge_commit column from task_attempts
ALTER TABLE task_attempts DROP COLUMN merge_commit;

-- Drop PR columns from task_attempts
ALTER TABLE task_attempts DROP COLUMN pr_url;
ALTER TABLE task_attempts DROP COLUMN pr_number;
ALTER TABLE task_attempts DROP COLUMN pr_status;
ALTER TABLE task_attempts DROP COLUMN pr_merged_at;
</file>

<file path="crates/db/migrations/20250902120000_add_masked_by_restore_to_execution_processes.sql">
-- Add a boolean flag to mark processes as dropped (excluded from timeline/logs)
ALTER TABLE execution_processes
    ADD COLUMN dropped BOOLEAN NOT NULL DEFAULT 0;
</file>

<file path="crates/db/migrations/20250902184501_rename-profile-to-executor.sql">
-- Add migration script here

ALTER TABLE task_attempts RENAME COLUMN profile TO executor;
</file>

<file path="crates/db/migrations/20250903091032_executors_to_screaming_snake.sql">
-- Converts pascal/camel to SCREAMING_SNAKE
UPDATE task_attempts
SET executor = (
  WITH RECURSIVE
    x(s, i, out) AS (
      SELECT executor, 1, ''
      UNION ALL
      SELECT s, i+1,
             out ||
             CASE
               WHEN i = 1 THEN substr(s,1,1)
               WHEN (substr(s,i,1) BETWEEN 'A' AND 'Z') AND (
                      (substr(s,i-1,1) BETWEEN 'a' AND 'z') OR
                      (substr(s,i-1,1) BETWEEN '0' AND '9') OR
                      ((substr(s,i-1,1) BETWEEN 'A' AND 'Z')
                        AND i < length(s) AND substr(s,i+1,1) BETWEEN 'a' AND 'z')
                    )
                    THEN '_' || substr(s,i,1)
               ELSE substr(s,i,1)
             END
      FROM x
      WHERE i <= length(s)
    )
  SELECT UPPER(out) FROM x WHERE i = length(s) + 1
);
</file>

<file path="crates/db/migrations/20250905090000_add_after_head_commit_to_execution_processes.sql">
-- Add after_head_commit column to store commit OID after a process ends
ALTER TABLE execution_processes
    ADD COLUMN after_head_commit TEXT;
</file>

<file path="crates/db/migrations/20250906120000_add_follow_up_drafts.sql">
-- Follow-up drafts per task attempt
-- Stores a single draft prompt that can be queued for the next available run

CREATE TABLE IF NOT EXISTS follow_up_drafts (
    id               TEXT PRIMARY KEY,
    task_attempt_id  TEXT NOT NULL UNIQUE,
    prompt           TEXT NOT NULL DEFAULT '',
    queued           INTEGER NOT NULL DEFAULT 0,
    sending          INTEGER NOT NULL DEFAULT 0,
    version          INTEGER NOT NULL DEFAULT 0,
    variant          TEXT NULL,
    image_ids        TEXT NULL, -- JSON array of UUID strings
    created_at       DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at       DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY(task_attempt_id) REFERENCES task_attempts(id) ON DELETE CASCADE
);

CREATE INDEX IF NOT EXISTS idx_follow_up_drafts_task_attempt_id
    ON follow_up_drafts(task_attempt_id);

-- Trigger to keep updated_at current
CREATE TRIGGER IF NOT EXISTS trg_follow_up_drafts_updated_at
AFTER UPDATE ON follow_up_drafts
FOR EACH ROW
BEGIN
    UPDATE follow_up_drafts SET updated_at = CURRENT_TIMESTAMP WHERE id = OLD.id;
END;
</file>

<file path="crates/db/migrations/20250910120000_add_before_head_commit_to_execution_processes.sql">
-- Add before_head_commit column to store commit OID before a process starts
ALTER TABLE execution_processes
    ADD COLUMN before_head_commit TEXT;

-- Backfill before_head_commit for legacy rows using the previous process's after_head_commit
UPDATE execution_processes AS ep
SET before_head_commit = (
  SELECT prev.after_head_commit
  FROM execution_processes prev
  WHERE prev.task_attempt_id = ep.task_attempt_id
    AND prev.created_at = (
      SELECT max(created_at) FROM execution_processes
      WHERE task_attempt_id = ep.task_attempt_id AND created_at < ep.created_at
    )
)
WHERE ep.before_head_commit IS NULL
  AND ep.after_head_commit IS NOT NULL;
</file>

<file path="crates/db/migrations/20250917123000_optimize_selects_and_cleanup_indexes.sql">
PRAGMA foreign_keys = ON;

-- 1) task_attempts: filter by task_id and sort by created_at DESC
CREATE INDEX IF NOT EXISTS idx_task_attempts_task_id_created_at
ON task_attempts (task_id, created_at DESC);

-- Global listing ordered by created_at DESC
CREATE INDEX IF NOT EXISTS idx_task_attempts_created_at
ON task_attempts (created_at DESC);

-- 2) execution_processes: filter by task_attempt_id and sort by created_at ASC
CREATE INDEX IF NOT EXISTS idx_execution_processes_task_attempt_created_at
ON execution_processes (task_attempt_id, created_at ASC);

-- Drop redundant single-column index superseded by the composite above
DROP INDEX IF EXISTS idx_execution_processes_task_attempt_id;

-- 3) tasks: list by project ordered by created_at DESC
CREATE INDEX IF NOT EXISTS idx_tasks_project_created_at
ON tasks (project_id, created_at DESC);
</file>

<file path="crates/db/src/models/execution_process_logs.rs">
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use sqlx::{FromRow, SqlitePool};
use ts_rs::TS;
use utils::log_msg::LogMsg;
use uuid::Uuid;

#[derive(Debug, Clone, FromRow, Serialize, Deserialize, TS)]
pub struct ExecutionProcessLogs {
    pub execution_id: Uuid,
    pub logs: String, // JSONL format
    pub byte_size: i64,
    pub inserted_at: DateTime<Utc>,
}

#[derive(Debug, Deserialize, TS)]
pub struct CreateExecutionProcessLogs {
    pub execution_id: Uuid,
    pub logs: String,
    pub byte_size: i64,
}

impl ExecutionProcessLogs {
    /// Find logs by execution process ID
    pub async fn find_by_execution_id(
        pool: &SqlitePool,
        execution_id: Uuid,
    ) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            ExecutionProcessLogs,
            r#"SELECT 
                execution_id as "execution_id!: Uuid",
                logs,
                byte_size,
                inserted_at as "inserted_at!: DateTime<Utc>"
               FROM execution_process_logs 
               WHERE execution_id = $1"#,
            execution_id
        )
        .fetch_optional(pool)
        .await
    }

    /// Create or update execution process logs
    pub async fn upsert(
        pool: &SqlitePool,
        data: &CreateExecutionProcessLogs,
    ) -> Result<Self, sqlx::Error> {
        let now = Utc::now();

        sqlx::query_as!(
            ExecutionProcessLogs,
            r#"INSERT INTO execution_process_logs (execution_id, logs, byte_size, inserted_at)
               VALUES ($1, $2, $3, $4)
               ON CONFLICT (execution_id) DO UPDATE
               SET logs = EXCLUDED.logs, 
                   byte_size = EXCLUDED.byte_size,
                   inserted_at = EXCLUDED.inserted_at
               RETURNING 
                execution_id as "execution_id!: Uuid",
                logs,
                byte_size,
                inserted_at as "inserted_at!: DateTime<Utc>""#,
            data.execution_id,
            data.logs,
            data.byte_size,
            now
        )
        .fetch_one(pool)
        .await
    }

    /// Parse JSONL logs back into Vec<LogMsg>
    pub fn parse_logs(&self) -> Result<Vec<LogMsg>, serde_json::Error> {
        let mut messages = Vec::new();
        for line in self.logs.lines() {
            if !line.trim().is_empty() {
                let msg: LogMsg = serde_json::from_str(line)?;
                messages.push(msg);
            }
        }
        Ok(messages)
    }

    /// Convert Vec<LogMsg> to JSONL format
    pub fn serialize_logs(messages: &[LogMsg]) -> Result<String, serde_json::Error> {
        let mut jsonl = String::new();
        for msg in messages {
            let line = serde_json::to_string(msg)?;
            jsonl.push_str(&line);
            jsonl.push('\n');
        }
        Ok(jsonl)
    }

    /// Append a JSONL line to the logs for an execution process
    pub async fn append_log_line(
        pool: &SqlitePool,
        execution_id: Uuid,
        jsonl_line: &str,
    ) -> Result<(), sqlx::Error> {
        let byte_size = jsonl_line.len() as i64;
        sqlx::query!(
            r#"INSERT INTO execution_process_logs (execution_id, logs, byte_size, inserted_at)
               VALUES ($1, $2, $3, datetime('now', 'subsec'))
               ON CONFLICT (execution_id) DO UPDATE
               SET logs = logs || $2,
                   byte_size = byte_size + $3,
                   inserted_at = datetime('now', 'subsec')"#,
            execution_id,
            jsonl_line,
            byte_size
        )
        .execute(pool)
        .await?;

        Ok(())
    }
}
</file>

<file path="crates/db/src/models/execution_process.rs">
use chrono::{DateTime, Utc};
use executors::actions::ExecutorAction;
use serde::{Deserialize, Serialize};
use serde_json::Value;
use sqlx::{FromRow, SqlitePool, Type};
use thiserror::Error;
use ts_rs::TS;
use uuid::Uuid;

use super::{task::Task, task_attempt::TaskAttempt};

#[derive(Debug, Error)]
pub enum ExecutionProcessError {
    #[error(transparent)]
    Database(#[from] sqlx::Error),
    #[error("Execution process not found")]
    ExecutionProcessNotFound,
    #[error("Failed to create execution process: {0}")]
    CreateFailed(String),
    #[error("Failed to update execution process: {0}")]
    UpdateFailed(String),
    #[error("Invalid executor action format")]
    InvalidExecutorAction,
}

#[derive(Debug, Clone, Type, Serialize, Deserialize, PartialEq, TS)]
#[sqlx(type_name = "execution_process_status", rename_all = "lowercase")]
#[serde(rename_all = "lowercase")]
pub enum ExecutionProcessStatus {
    Running,
    Completed,
    Failed,
    Killed,
}

#[derive(Debug, Clone, Type, Serialize, Deserialize, PartialEq, TS)]
#[sqlx(type_name = "execution_process_run_reason", rename_all = "lowercase")]
#[serde(rename_all = "lowercase")]
pub enum ExecutionProcessRunReason {
    SetupScript,
    CleanupScript,
    CodingAgent,
    DevServer,
}

#[derive(Debug, Clone, FromRow, Serialize, Deserialize, TS)]
pub struct ExecutionProcess {
    pub id: Uuid,
    pub task_attempt_id: Uuid,
    pub run_reason: ExecutionProcessRunReason,
    #[ts(type = "ExecutorAction")]
    pub executor_action: sqlx::types::Json<ExecutorActionField>,
    /// Git HEAD commit OID captured before the process starts
    pub before_head_commit: Option<String>,
    /// Git HEAD commit OID captured after the process ends
    pub after_head_commit: Option<String>,
    pub status: ExecutionProcessStatus,
    pub exit_code: Option<i64>,
    /// dropped: true if this process is excluded from the current
    /// history view (due to restore/trimming). Hidden from logs/timeline;
    /// still listed in the Processes tab.
    pub dropped: bool,
    pub started_at: DateTime<Utc>,
    pub completed_at: Option<DateTime<Utc>>,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

#[derive(Debug, Deserialize, TS)]
pub struct CreateExecutionProcess {
    pub task_attempt_id: Uuid,
    pub executor_action: ExecutorAction,
    pub run_reason: ExecutionProcessRunReason,
}

#[derive(Debug, Deserialize, TS)]
#[allow(dead_code)]
pub struct UpdateExecutionProcess {
    pub status: Option<ExecutionProcessStatus>,
    pub exit_code: Option<i64>,
    pub completed_at: Option<DateTime<Utc>>,
}

#[derive(Debug)]
pub struct ExecutionContext {
    pub execution_process: ExecutionProcess,
    pub task_attempt: TaskAttempt,
    pub task: Task,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(untagged)]
pub enum ExecutorActionField {
    ExecutorAction(ExecutorAction),
    Other(Value),
}

#[derive(Debug, Clone)]
pub struct MissingBeforeContext {
    pub id: Uuid,
    pub task_attempt_id: Uuid,
    pub prev_after_head_commit: Option<String>,
    pub base_branch: String,
    pub git_repo_path: Option<String>,
}

impl ExecutionProcess {
    /// Find execution process by ID
    pub async fn find_by_id(pool: &SqlitePool, id: Uuid) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            ExecutionProcess,
            r#"SELECT id as "id!: Uuid", task_attempt_id as "task_attempt_id!: Uuid", run_reason as "run_reason!: ExecutionProcessRunReason", executor_action as "executor_action!: sqlx::types::Json<ExecutorActionField>", before_head_commit,
                      after_head_commit, status as "status!: ExecutionProcessStatus", exit_code, dropped, started_at as "started_at!: DateTime<Utc>", completed_at as "completed_at?: DateTime<Utc>",
                      created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>"
               FROM execution_processes WHERE id = ?"#,
            id
        )
        .fetch_optional(pool)
        .await
    }

    /// Context for backfilling before_head_commit for legacy rows
    /// List processes that have after_head_commit set but missing before_head_commit, with join context
    pub async fn list_missing_before_context(
        pool: &SqlitePool,
    ) -> Result<Vec<MissingBeforeContext>, sqlx::Error> {
        let rows = sqlx::query!(
            r#"SELECT
                ep.id                         as "id!: Uuid",
                ep.task_attempt_id            as "task_attempt_id!: Uuid",
                ep.after_head_commit          as after_head_commit,
                prev.after_head_commit        as prev_after_head_commit,
                ta.base_branch                as base_branch,
                p.git_repo_path               as git_repo_path
            FROM execution_processes ep
            JOIN task_attempts ta ON ta.id = ep.task_attempt_id
            JOIN tasks t ON t.id = ta.task_id
            JOIN projects p ON p.id = t.project_id
            LEFT JOIN execution_processes prev
              ON prev.task_attempt_id = ep.task_attempt_id
             AND prev.created_at = (
                   SELECT max(created_at) FROM execution_processes
                     WHERE task_attempt_id = ep.task_attempt_id
                       AND created_at < ep.created_at
               )
            WHERE ep.before_head_commit IS NULL
              AND ep.after_head_commit IS NOT NULL"#
        )
        .fetch_all(pool)
        .await?;

        let result = rows
            .into_iter()
            .map(|r| MissingBeforeContext {
                id: r.id,
                task_attempt_id: r.task_attempt_id,
                prev_after_head_commit: r.prev_after_head_commit,
                base_branch: r.base_branch,
                git_repo_path: Some(r.git_repo_path),
            })
            .collect();
        Ok(result)
    }

    /// Count processes created after the given boundary process
    pub async fn count_later_than(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
        boundary_process_id: Uuid,
    ) -> Result<i64, sqlx::Error> {
        let cnt = sqlx::query_scalar!(
            r#"SELECT COUNT(1) as "count!:_" FROM execution_processes
               WHERE task_attempt_id = $1
                 AND created_at > (SELECT created_at FROM execution_processes WHERE id = $2)"#,
            task_attempt_id,
            boundary_process_id
        )
        .fetch_one(pool)
        .await
        .unwrap_or(0i64);
        Ok(cnt)
    }

    /// Find execution process by rowid
    pub async fn find_by_rowid(pool: &SqlitePool, rowid: i64) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            ExecutionProcess,
            r#"SELECT id as "id!: Uuid", task_attempt_id as "task_attempt_id!: Uuid", run_reason as "run_reason!: ExecutionProcessRunReason", executor_action as "executor_action!: sqlx::types::Json<ExecutorActionField>", before_head_commit,
                      after_head_commit, status as "status!: ExecutionProcessStatus", exit_code, dropped, started_at as "started_at!: DateTime<Utc>", completed_at as "completed_at?: DateTime<Utc>",
                      created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>"
               FROM execution_processes WHERE rowid = ?"#,
            rowid
        )
        .fetch_optional(pool)
        .await
    }

    /// Find all execution processes for a task attempt (optionally include soft-deleted)
    pub async fn find_by_task_attempt_id(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
        show_soft_deleted: bool,
    ) -> Result<Vec<Self>, sqlx::Error> {
        sqlx::query_as!(
            ExecutionProcess,
            r#"SELECT id              as "id!: Uuid",
                      task_attempt_id as "task_attempt_id!: Uuid",
                      run_reason      as "run_reason!: ExecutionProcessRunReason",
                      executor_action as "executor_action!: sqlx::types::Json<ExecutorActionField>",
                      before_head_commit,
                      after_head_commit,
                      status          as "status!: ExecutionProcessStatus",
                      exit_code,
                      dropped,
                      started_at      as "started_at!: DateTime<Utc>",
                      completed_at    as "completed_at?: DateTime<Utc>",
                      created_at      as "created_at!: DateTime<Utc>",
                      updated_at      as "updated_at!: DateTime<Utc>"
               FROM execution_processes
               WHERE task_attempt_id = ?
                 AND (? OR dropped = FALSE)
               ORDER BY created_at ASC"#,
            task_attempt_id,
            show_soft_deleted
        )
        .fetch_all(pool)
        .await
    }

    /// Find running execution processes
    pub async fn find_running(pool: &SqlitePool) -> Result<Vec<Self>, sqlx::Error> {
        sqlx::query_as!(
            ExecutionProcess,
            r#"SELECT id as "id!: Uuid", task_attempt_id as "task_attempt_id!: Uuid", run_reason as "run_reason!: ExecutionProcessRunReason", executor_action as "executor_action!: sqlx::types::Json<ExecutorActionField>", before_head_commit,
                      after_head_commit, status as "status!: ExecutionProcessStatus", exit_code, dropped, started_at as "started_at!: DateTime<Utc>", completed_at as "completed_at?: DateTime<Utc>",
                      created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>"
               FROM execution_processes WHERE status = 'running' ORDER BY created_at ASC"#,
        )
        .fetch_all(pool)
        .await
    }

    /// Find running dev servers for a specific project
    pub async fn find_running_dev_servers_by_project(
        pool: &SqlitePool,
        project_id: Uuid,
    ) -> Result<Vec<Self>, sqlx::Error> {
        sqlx::query_as!(
            ExecutionProcess,
            r#"SELECT ep.id as "id!: Uuid", ep.task_attempt_id as "task_attempt_id!: Uuid", ep.run_reason as "run_reason!: ExecutionProcessRunReason", ep.executor_action as "executor_action!: sqlx::types::Json<ExecutorActionField>",
                      ep.before_head_commit, ep.after_head_commit, ep.status as "status!: ExecutionProcessStatus", ep.exit_code,
                      ep.dropped, ep.started_at as "started_at!: DateTime<Utc>", ep.completed_at as "completed_at?: DateTime<Utc>", ep.created_at as "created_at!: DateTime<Utc>", ep.updated_at as "updated_at!: DateTime<Utc>"
               FROM execution_processes ep
               JOIN task_attempts ta ON ep.task_attempt_id = ta.id
               JOIN tasks t ON ta.task_id = t.id
               WHERE ep.status = 'running' AND ep.run_reason = 'devserver' AND t.project_id = ?
               ORDER BY ep.created_at ASC"#,
            project_id
        )
        .fetch_all(pool)
        .await
    }

    /// Find latest session_id by task attempt (simple scalar query)
    pub async fn find_latest_session_id_by_task_attempt(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
    ) -> Result<Option<String>, sqlx::Error> {
        tracing::info!(
            "Finding latest session id for task attempt {}",
            task_attempt_id
        );
        let row = sqlx::query!(
            r#"SELECT es.session_id
               FROM execution_processes ep
               JOIN executor_sessions es ON ep.id = es.execution_process_id  
               WHERE ep.task_attempt_id = $1
                 AND ep.run_reason = 'codingagent'
                 AND ep.dropped = FALSE
                 AND es.session_id IS NOT NULL
               ORDER BY ep.created_at DESC
               LIMIT 1"#,
            task_attempt_id
        )
        .fetch_optional(pool)
        .await?;

        tracing::info!("Latest session id: {:?}", row);

        Ok(row.and_then(|r| r.session_id))
    }

    /// Find latest execution process by task attempt and run reason
    pub async fn find_latest_by_task_attempt_and_run_reason(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
        run_reason: &ExecutionProcessRunReason,
    ) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            ExecutionProcess,
            r#"SELECT id as "id!: Uuid", task_attempt_id as "task_attempt_id!: Uuid", run_reason as "run_reason!: ExecutionProcessRunReason", executor_action as "executor_action!: sqlx::types::Json<ExecutorActionField>", before_head_commit,
                      after_head_commit, status as "status!: ExecutionProcessStatus", exit_code, dropped, started_at as "started_at!: DateTime<Utc>", completed_at as "completed_at?: DateTime<Utc>",
                      created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>"
               FROM execution_processes
               WHERE task_attempt_id = ? AND run_reason = ? AND dropped = FALSE
               ORDER BY created_at DESC LIMIT 1"#,
            task_attempt_id,
            run_reason
        )
        .fetch_optional(pool)
        .await
    }

    /// Create a new execution process
    pub async fn create(
        pool: &SqlitePool,
        data: &CreateExecutionProcess,
        process_id: Uuid,
        before_head_commit: Option<&str>,
    ) -> Result<Self, sqlx::Error> {
        let now = Utc::now();
        let executor_action_json = sqlx::types::Json(&data.executor_action);

        sqlx::query_as!(
            ExecutionProcess,
            r#"INSERT INTO execution_processes (
                    id, task_attempt_id, run_reason, executor_action, before_head_commit,
                    after_head_commit, status, exit_code, started_at, completed_at, created_at, updated_at
                ) VALUES (?, ?, ?, ?, ?, NULL, ?, ?, ?, ?, ?, ?) RETURNING
                    id as "id!: Uuid", task_attempt_id as "task_attempt_id!: Uuid", run_reason as "run_reason!: ExecutionProcessRunReason", executor_action as "executor_action!: sqlx::types::Json<ExecutorActionField>", before_head_commit,
                    after_head_commit, status as "status!: ExecutionProcessStatus", exit_code, dropped, started_at as "started_at!: DateTime<Utc>", completed_at as "completed_at?: DateTime<Utc>", created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>""#,
            process_id,
            data.task_attempt_id,
            data.run_reason,
            executor_action_json,
            before_head_commit,
            ExecutionProcessStatus::Running,
            None::<i64>,
            now,
            None::<DateTime<Utc>>,
            now,
            now
        )
        .fetch_one(pool)
        .await
    }
    pub async fn was_killed(pool: &SqlitePool, id: Uuid) -> bool {
        if let Ok(exp_process) = Self::find_by_id(pool, id).await
            && exp_process.is_some_and(|ep| ep.status == ExecutionProcessStatus::Killed)
        {
            return true;
        }
        false
    }

    /// Update execution process status and completion info
    pub async fn update_completion(
        pool: &SqlitePool,
        id: Uuid,
        status: ExecutionProcessStatus,
        exit_code: Option<i64>,
    ) -> Result<(), sqlx::Error> {
        let completed_at = if matches!(status, ExecutionProcessStatus::Running) {
            None
        } else {
            Some(Utc::now())
        };

        sqlx::query!(
            r#"UPDATE execution_processes 
               SET status = $1, exit_code = $2, completed_at = $3
               WHERE id = $4"#,
            status,
            exit_code,
            completed_at,
            id
        )
        .execute(pool)
        .await?;

        Ok(())
    }

    /// Update the "after" commit oid for the process
    pub async fn update_after_head_commit(
        pool: &SqlitePool,
        id: Uuid,
        after_head_commit: &str,
    ) -> Result<(), sqlx::Error> {
        sqlx::query!(
            r#"UPDATE execution_processes 
               SET after_head_commit = $1 
               WHERE id = $2"#,
            after_head_commit,
            id
        )
        .execute(pool)
        .await?;
        Ok(())
    }

    /// Update the "before" commit oid for the process
    pub async fn update_before_head_commit(
        pool: &SqlitePool,
        id: Uuid,
        before_head_commit: &str,
    ) -> Result<(), sqlx::Error> {
        sqlx::query!(
            r#"UPDATE execution_processes 
               SET before_head_commit = $1 
               WHERE id = $2"#,
            before_head_commit,
            id
        )
        .execute(pool)
        .await?;
        Ok(())
    }

    pub async fn delete_by_task_attempt_id(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
    ) -> Result<(), sqlx::Error> {
        sqlx::query!(
            "DELETE FROM execution_processes WHERE task_attempt_id = $1",
            task_attempt_id
        )
        .execute(pool)
        .await?;
        Ok(())
    }

    pub fn executor_action(&self) -> Result<&ExecutorAction, anyhow::Error> {
        match &self.executor_action.0 {
            ExecutorActionField::ExecutorAction(action) => Ok(action),
            ExecutorActionField::Other(_) => Err(anyhow::anyhow!(
                "Executor action is not a valid ExecutorAction JSON object"
            )),
        }
    }

    /// Set restore boundary: drop processes newer than the specified process, undrop older/equal
    pub async fn set_restore_boundary(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
        boundary_process_id: Uuid,
    ) -> Result<(), sqlx::Error> {
        // Monotonic drop: only mark newer records as dropped; never undrop.
        sqlx::query!(
            r#"UPDATE execution_processes
               SET dropped = TRUE
             WHERE task_attempt_id = $1
               AND created_at > (SELECT created_at FROM execution_processes WHERE id = $2)
               AND dropped = FALSE
            "#,
            task_attempt_id,
            boundary_process_id
        )
        .execute(pool)
        .await?;
        Ok(())
    }

    /// Soft-drop processes at and after the specified boundary (inclusive)
    pub async fn drop_at_and_after(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
        boundary_process_id: Uuid,
    ) -> Result<i64, sqlx::Error> {
        let result = sqlx::query!(
            r#"UPDATE execution_processes
               SET dropped = TRUE
             WHERE task_attempt_id = $1
               AND created_at >= (SELECT created_at FROM execution_processes WHERE id = $2)
               AND dropped = FALSE"#,
            task_attempt_id,
            boundary_process_id
        )
        .execute(pool)
        .await?;
        Ok(result.rows_affected() as i64)
    }

    /// Find the previous process's after_head_commit before the given boundary process
    pub async fn find_prev_after_head_commit(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
        boundary_process_id: Uuid,
    ) -> Result<Option<String>, sqlx::Error> {
        let res = sqlx::query_scalar(
            r#"SELECT after_head_commit FROM execution_processes
               WHERE task_attempt_id = ?
                 AND created_at < (SELECT created_at FROM execution_processes WHERE id = ?)
               ORDER BY created_at DESC
               LIMIT 1"#,
        )
        .bind(task_attempt_id)
        .bind(boundary_process_id)
        .fetch_optional(pool)
        .await?;
        Ok(res)
    }

    /// Get the parent TaskAttempt for this execution process
    pub async fn parent_task_attempt(
        &self,
        pool: &SqlitePool,
    ) -> Result<Option<TaskAttempt>, sqlx::Error> {
        TaskAttempt::find_by_id(pool, self.task_attempt_id).await
    }

    /// Load execution context with related task attempt and task
    pub async fn load_context(
        pool: &SqlitePool,
        exec_id: Uuid,
    ) -> Result<ExecutionContext, sqlx::Error> {
        let execution_process = Self::find_by_id(pool, exec_id)
            .await?
            .ok_or(sqlx::Error::RowNotFound)?;

        let task_attempt = TaskAttempt::find_by_id(pool, execution_process.task_attempt_id)
            .await?
            .ok_or(sqlx::Error::RowNotFound)?;

        let task = Task::find_by_id(pool, task_attempt.task_id)
            .await?
            .ok_or(sqlx::Error::RowNotFound)?;

        Ok(ExecutionContext {
            execution_process,
            task_attempt,
            task,
        })
    }
}
</file>

<file path="crates/db/src/models/executor_session.rs">
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use sqlx::{FromRow, SqlitePool};
use ts_rs::TS;
use uuid::Uuid;

#[derive(Debug, Clone, FromRow, Serialize, Deserialize, TS)]
pub struct ExecutorSession {
    pub id: Uuid,
    pub task_attempt_id: Uuid,
    pub execution_process_id: Uuid,
    pub session_id: Option<String>, // External session ID from Claude/Amp
    pub prompt: Option<String>,     // The prompt sent to the executor
    pub summary: Option<String>,    // Final assistant message/summary
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

#[derive(Debug, Deserialize, TS)]
pub struct CreateExecutorSession {
    pub task_attempt_id: Uuid,
    pub execution_process_id: Uuid,
    pub prompt: Option<String>,
}

#[derive(Debug, Deserialize, TS)]
#[allow(dead_code)]
pub struct UpdateExecutorSession {
    pub session_id: Option<String>,
    pub prompt: Option<String>,
    pub summary: Option<String>,
}

impl ExecutorSession {
    /// Find executor session by ID
    #[allow(dead_code)]
    pub async fn find_by_id(pool: &SqlitePool, id: Uuid) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            ExecutorSession,
            r#"SELECT 
                id as "id!: Uuid", 
                task_attempt_id as "task_attempt_id!: Uuid", 
                execution_process_id as "execution_process_id!: Uuid", 
                session_id, 
                prompt,
                summary,
                created_at as "created_at!: DateTime<Utc>", 
                updated_at as "updated_at!: DateTime<Utc>"
               FROM executor_sessions 
               WHERE id = $1"#,
            id
        )
        .fetch_optional(pool)
        .await
    }

    /// Find executor session by execution process ID
    pub async fn find_by_execution_process_id(
        pool: &SqlitePool,
        execution_process_id: Uuid,
    ) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            ExecutorSession,
            r#"SELECT
                id as "id!: Uuid",
                task_attempt_id as "task_attempt_id!: Uuid",
                execution_process_id as "execution_process_id!: Uuid",
                session_id,
                prompt,
                summary,
                created_at as "created_at!: DateTime<Utc>",
                updated_at as "updated_at!: DateTime<Utc>"
               FROM executor_sessions
               WHERE execution_process_id = $1"#,
            execution_process_id
        )
        .fetch_optional(pool)
        .await
    }

    /// Find all executor sessions for a task attempt
    #[allow(dead_code)]
    pub async fn find_by_task_attempt_id(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
    ) -> Result<Vec<Self>, sqlx::Error> {
        sqlx::query_as!(
            ExecutorSession,
            r#"SELECT 
                id as "id!: Uuid", 
                task_attempt_id as "task_attempt_id!: Uuid", 
                execution_process_id as "execution_process_id!: Uuid", 
                session_id, 
                prompt,
                summary,
                created_at as "created_at!: DateTime<Utc>", 
                updated_at as "updated_at!: DateTime<Utc>"
               FROM executor_sessions 
               WHERE task_attempt_id = $1 
               ORDER BY created_at ASC"#,
            task_attempt_id
        )
        .fetch_all(pool)
        .await
    }

    /// Create a new executor session
    pub async fn create(
        pool: &SqlitePool,
        data: &CreateExecutorSession,
        session_id: Uuid,
    ) -> Result<Self, sqlx::Error> {
        let now = Utc::now();

        tracing::debug!(
            "Creating executor session: id={}, task_attempt_id={}, execution_process_id={}, external_session_id=None (will be set later)",
            session_id,
            data.task_attempt_id,
            data.execution_process_id
        );

        sqlx::query_as!(
            ExecutorSession,
            r#"INSERT INTO executor_sessions (
                id, task_attempt_id, execution_process_id, session_id, prompt, summary,
                created_at, updated_at
               )
               VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
               RETURNING
                id as "id!: Uuid",
                task_attempt_id as "task_attempt_id!: Uuid",
                execution_process_id as "execution_process_id!: Uuid",
                session_id,
                prompt,
                summary,
                created_at as "created_at!: DateTime<Utc>",
                updated_at as "updated_at!: DateTime<Utc>""#,
            session_id,
            data.task_attempt_id,
            data.execution_process_id,
            None::<String>, // session_id initially None until parsed from output
            data.prompt,
            None::<String>, // summary initially None
            now,            // created_at
            now             // updated_at
        )
        .fetch_one(pool)
        .await
    }

    /// Update executor session with external session ID
    pub async fn update_session_id(
        pool: &SqlitePool,
        execution_process_id: Uuid,
        external_session_id: &str,
    ) -> Result<(), sqlx::Error> {
        let now = Utc::now();
        sqlx::query!(
            r#"UPDATE executor_sessions
               SET session_id = $1, updated_at = $2
               WHERE execution_process_id = $3"#,
            external_session_id,
            now,
            execution_process_id
        )
        .execute(pool)
        .await?;

        Ok(())
    }

    /// Update executor session prompt
    #[allow(dead_code)]
    pub async fn update_prompt(
        pool: &SqlitePool,
        id: Uuid,
        prompt: &str,
    ) -> Result<(), sqlx::Error> {
        let now = Utc::now();
        sqlx::query!(
            r#"UPDATE executor_sessions 
               SET prompt = $1, updated_at = $2 
               WHERE id = $3"#,
            prompt,
            now,
            id
        )
        .execute(pool)
        .await?;

        Ok(())
    }

    /// Update executor session summary
    pub async fn update_summary(
        pool: &SqlitePool,
        execution_process_id: Uuid,
        summary: &str,
    ) -> Result<(), sqlx::Error> {
        let now = Utc::now();
        sqlx::query!(
            r#"UPDATE executor_sessions 
               SET summary = $1, updated_at = $2 
               WHERE execution_process_id = $3"#,
            summary,
            now,
            execution_process_id
        )
        .execute(pool)
        .await?;

        Ok(())
    }

    /// Delete executor sessions for a task attempt (cleanup)
    pub async fn delete_by_task_attempt_id(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
    ) -> Result<(), sqlx::Error> {
        sqlx::query!(
            "DELETE FROM executor_sessions WHERE task_attempt_id = $1",
            task_attempt_id
        )
        .execute(pool)
        .await?;

        Ok(())
    }
}
</file>

<file path="crates/db/src/models/follow_up_draft.rs">
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use sqlx::{FromRow, SqlitePool};
use ts_rs::TS;
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct FollowUpDraft {
    pub id: Uuid,
    pub task_attempt_id: Uuid,
    pub prompt: String,
    pub queued: bool,
    pub sending: bool,
    pub variant: Option<String>,
    // Stored as JSON in the DB; serde handles Uuid <-> string in JSON
    #[serde(skip_serializing_if = "Option::is_none")]
    pub image_ids: Option<Vec<Uuid>>,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub version: i64,
}

#[derive(Debug, Clone, FromRow)]
struct FollowUpDraftRow {
    pub id: Uuid,
    pub task_attempt_id: Uuid,
    pub prompt: String,
    pub queued: bool,
    pub sending: bool,
    pub variant: Option<String>,
    pub image_ids: Option<String>,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
    pub version: i64,
}

impl From<FollowUpDraftRow> for FollowUpDraft {
    fn from(r: FollowUpDraftRow) -> Self {
        let image_ids = r
            .image_ids
            .as_deref()
            .and_then(|s| serde_json::from_str::<Vec<Uuid>>(s).ok());
        FollowUpDraft {
            id: r.id,
            task_attempt_id: r.task_attempt_id,
            prompt: r.prompt,
            queued: r.queued,
            sending: r.sending,
            variant: r.variant,
            image_ids,
            created_at: r.created_at,
            updated_at: r.updated_at,
            version: r.version,
        }
    }
}

#[derive(Debug, Deserialize, TS)]
pub struct UpsertFollowUpDraft {
    pub task_attempt_id: Uuid,
    pub prompt: String,
    pub queued: bool,
    pub variant: Option<String>,
    pub image_ids: Option<Vec<Uuid>>,
}

impl FollowUpDraft {
    pub async fn find_by_rowid(pool: &SqlitePool, rowid: i64) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            FollowUpDraftRow,
            r#"SELECT 
                id               as "id!: Uuid",
                task_attempt_id  as "task_attempt_id!: Uuid",
                prompt           as "prompt!: String",
                queued           as "queued!: bool",
                sending          as "sending!: bool",
                variant,
                image_ids        as "image_ids?: String",
                created_at       as "created_at!: DateTime<Utc>",
                updated_at       as "updated_at!: DateTime<Utc>",
                version          as "version!: i64"
              FROM follow_up_drafts
             WHERE rowid = $1"#,
            rowid
        )
        .fetch_optional(pool)
        .await
        .map(|opt| opt.map(FollowUpDraft::from))
    }
    pub async fn find_by_task_attempt_id(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
    ) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            FollowUpDraftRow,
            r#"SELECT 
                id               as "id!: Uuid",
                task_attempt_id  as "task_attempt_id!: Uuid",
                prompt           as "prompt!: String",
                queued           as "queued!: bool",
                sending          as "sending!: bool",
                variant,
                image_ids        as "image_ids?: String",
                created_at       as "created_at!: DateTime<Utc>",
                updated_at       as "updated_at!: DateTime<Utc>",
                version          as "version!: i64"
              FROM follow_up_drafts
             WHERE task_attempt_id = $1"#,
            task_attempt_id
        )
        .fetch_optional(pool)
        .await
        .map(|opt| opt.map(FollowUpDraft::from))
    }

    pub async fn upsert(
        pool: &SqlitePool,
        data: &UpsertFollowUpDraft,
    ) -> Result<Self, sqlx::Error> {
        let id = Uuid::new_v4();
        {
            let image_ids_json = data
                .image_ids
                .as_ref()
                .map(|ids| serde_json::to_string(ids).unwrap_or_else(|_| "[]".to_string()));

            sqlx::query_as!(
                FollowUpDraftRow,
                r#"INSERT INTO follow_up_drafts (id, task_attempt_id, prompt, queued, variant, image_ids)
                   VALUES ($1, $2, $3, $4, $5, $6)
                   ON CONFLICT(task_attempt_id) DO UPDATE SET
                     prompt = excluded.prompt,
                     queued = excluded.queued,
                     variant = excluded.variant,
                     image_ids = excluded.image_ids
                   RETURNING 
                    id               as "id!: Uuid",
                    task_attempt_id  as "task_attempt_id!: Uuid",
                    prompt           as "prompt!: String",
                    queued           as "queued!: bool",
                    sending          as "sending!: bool",
                    variant,
                    image_ids        as "image_ids?: String",
                   created_at       as "created_at!: DateTime<Utc>",
                    updated_at       as "updated_at!: DateTime<Utc>",
                    version          as "version!: i64""#,
                id,
                data.task_attempt_id,
                data.prompt,
                data.queued,
                data.variant,
                image_ids_json
            )
            .fetch_one(pool)
            .await
            .map(FollowUpDraft::from)
        }
    }

    pub async fn clear_after_send(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
    ) -> Result<(), sqlx::Error> {
        sqlx::query!(
            r#"UPDATE follow_up_drafts 
               SET prompt = '', queued = 0, sending = 0, image_ids = NULL, updated_at = CURRENT_TIMESTAMP, version = version + 1
             WHERE task_attempt_id = $1"#,
            task_attempt_id
        )
        .execute(pool)
        .await?;
        Ok(())
    }

    /// Attempt to atomically mark this draft as "sending" if it's currently queued and non-empty.
    /// Returns true if the row was updated (we acquired the send lock), false otherwise.
    pub async fn try_mark_sending(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
    ) -> Result<bool, sqlx::Error> {
        let result = sqlx::query!(
            r#"UPDATE follow_up_drafts
               SET sending = 1, updated_at = CURRENT_TIMESTAMP, version = version + 1
             WHERE task_attempt_id = $1
               AND queued = 1
               AND sending = 0
               AND TRIM(prompt) != ''"#,
            task_attempt_id
        )
        .execute(pool)
        .await?;

        Ok(result.rows_affected() > 0)
    }
}
</file>

<file path="crates/db/src/models/image.rs">
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use sqlx::{FromRow, SqlitePool};
use ts_rs::TS;
use uuid::Uuid;

#[derive(Debug, Clone, FromRow, Serialize, Deserialize, TS)]
pub struct Image {
    pub id: Uuid,
    pub file_path: String, // relative path within cache/images/
    pub original_name: String,
    pub mime_type: Option<String>,
    pub size_bytes: i64,
    pub hash: String, // SHA256 hash for deduplication
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

#[derive(Debug, Deserialize, TS)]
pub struct CreateImage {
    pub file_path: String,
    pub original_name: String,
    pub mime_type: Option<String>,
    pub size_bytes: i64,
    pub hash: String,
}

#[derive(Debug, Clone, FromRow, Serialize, Deserialize, TS)]
pub struct TaskImage {
    pub id: Uuid,
    pub task_id: Uuid,
    pub image_id: Uuid,
    pub created_at: DateTime<Utc>,
}

#[derive(Debug, Deserialize, TS)]
pub struct CreateTaskImage {
    pub task_id: Uuid,
    pub image_id: Uuid,
}

impl Image {
    pub async fn create(pool: &SqlitePool, data: &CreateImage) -> Result<Self, sqlx::Error> {
        let id = Uuid::new_v4();
        sqlx::query_as!(
            Image,
            r#"INSERT INTO images (id, file_path, original_name, mime_type, size_bytes, hash)
               VALUES ($1, $2, $3, $4, $5, $6)
               RETURNING id as "id!: Uuid", 
                         file_path as "file_path!", 
                         original_name as "original_name!", 
                         mime_type,
                         size_bytes as "size_bytes!",
                         hash as "hash!",
                         created_at as "created_at!: DateTime<Utc>", 
                         updated_at as "updated_at!: DateTime<Utc>""#,
            id,
            data.file_path,
            data.original_name,
            data.mime_type,
            data.size_bytes,
            data.hash,
        )
        .fetch_one(pool)
        .await
    }

    pub async fn find_by_hash(pool: &SqlitePool, hash: &str) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            Image,
            r#"SELECT id as "id!: Uuid",
                      file_path as "file_path!",
                      original_name as "original_name!",
                      mime_type,
                      size_bytes as "size_bytes!",
                      hash as "hash!",
                      created_at as "created_at!: DateTime<Utc>",
                      updated_at as "updated_at!: DateTime<Utc>"
               FROM images
               WHERE hash = $1"#,
            hash
        )
        .fetch_optional(pool)
        .await
    }

    pub async fn find_by_id(pool: &SqlitePool, id: Uuid) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            Image,
            r#"SELECT id as "id!: Uuid",
                      file_path as "file_path!",
                      original_name as "original_name!",
                      mime_type,
                      size_bytes as "size_bytes!",
                      hash as "hash!",
                      created_at as "created_at!: DateTime<Utc>",
                      updated_at as "updated_at!: DateTime<Utc>"
               FROM images
               WHERE id = $1"#,
            id
        )
        .fetch_optional(pool)
        .await
    }

    pub async fn find_by_task_id(
        pool: &SqlitePool,
        task_id: Uuid,
    ) -> Result<Vec<Self>, sqlx::Error> {
        sqlx::query_as!(
            Image,
            r#"SELECT i.id as "id!: Uuid",
                      i.file_path as "file_path!",
                      i.original_name as "original_name!",
                      i.mime_type,
                      i.size_bytes as "size_bytes!",
                      i.hash as "hash!",
                      i.created_at as "created_at!: DateTime<Utc>",
                      i.updated_at as "updated_at!: DateTime<Utc>"
               FROM images i
               JOIN task_images ti ON i.id = ti.image_id
               WHERE ti.task_id = $1
               ORDER BY ti.created_at"#,
            task_id
        )
        .fetch_all(pool)
        .await
    }

    pub async fn delete(pool: &SqlitePool, id: Uuid) -> Result<(), sqlx::Error> {
        sqlx::query!(r#"DELETE FROM images WHERE id = $1"#, id)
            .execute(pool)
            .await?;
        Ok(())
    }

    pub async fn find_orphaned_images(pool: &SqlitePool) -> Result<Vec<Self>, sqlx::Error> {
        sqlx::query_as!(
            Image,
            r#"SELECT i.id as "id!: Uuid",
                      i.file_path as "file_path!",
                      i.original_name as "original_name!",
                      i.mime_type,
                      i.size_bytes as "size_bytes!",
                      i.hash as "hash!",
                      i.created_at as "created_at!: DateTime<Utc>",
                      i.updated_at as "updated_at!: DateTime<Utc>"
               FROM images i
               LEFT JOIN task_images ti ON i.id = ti.image_id
               WHERE ti.task_id IS NULL"#
        )
        .fetch_all(pool)
        .await
    }
}

impl TaskImage {
    pub async fn create(pool: &SqlitePool, data: &CreateTaskImage) -> Result<Self, sqlx::Error> {
        let id = Uuid::new_v4();
        sqlx::query_as!(
            TaskImage,
            r#"INSERT INTO task_images (id, task_id, image_id)
               VALUES ($1, $2, $3)
               RETURNING id as "id!: Uuid",
                         task_id as "task_id!: Uuid",
                         image_id as "image_id!: Uuid", 
                         created_at as "created_at!: DateTime<Utc>""#,
            id,
            data.task_id,
            data.image_id,
        )
        .fetch_one(pool)
        .await
    }

    pub async fn associate_many(
        pool: &SqlitePool,
        task_id: Uuid,
        image_ids: &[Uuid],
    ) -> Result<(), sqlx::Error> {
        for &image_id in image_ids {
            let task_image = CreateTaskImage { task_id, image_id };
            TaskImage::create(pool, &task_image).await?;
        }
        Ok(())
    }

    /// Associate multiple images with a task, skipping duplicates.
    pub async fn associate_many_dedup(
        pool: &SqlitePool,
        task_id: Uuid,
        image_ids: &[Uuid],
    ) -> Result<(), sqlx::Error> {
        for &image_id in image_ids {
            let id = Uuid::new_v4();
            sqlx::query!(
                r#"INSERT INTO task_images (id, task_id, image_id)
                   SELECT $1, $2, $3
                   WHERE NOT EXISTS (
                       SELECT 1 FROM task_images WHERE task_id = $2 AND image_id = $3
                   )"#,
                id,
                task_id,
                image_id
            )
            .execute(pool)
            .await?;
        }
        Ok(())
    }

    pub async fn delete_by_task_id(pool: &SqlitePool, task_id: Uuid) -> Result<(), sqlx::Error> {
        sqlx::query!(r#"DELETE FROM task_images WHERE task_id = $1"#, task_id)
            .execute(pool)
            .await?;
        Ok(())
    }
}
</file>

<file path="crates/db/src/models/merge.rs">
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use sqlx::{FromRow, SqlitePool, Type};
use ts_rs::TS;
use uuid::Uuid;

#[derive(Debug, Clone, Serialize, Deserialize, TS, Type)]
#[sqlx(type_name = "merge_status", rename_all = "snake_case")]
#[serde(rename_all = "snake_case")]
pub enum MergeStatus {
    Open,
    Merged,
    Closed,
    Unknown,
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum Merge {
    Direct(DirectMerge),
    Pr(PrMerge),
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct DirectMerge {
    pub id: Uuid,
    pub task_attempt_id: Uuid,
    pub merge_commit: String,
    pub target_branch_name: String,
    pub created_at: DateTime<Utc>,
}

/// PR merge - represents a pull request merge
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct PrMerge {
    pub id: Uuid,
    pub task_attempt_id: Uuid,
    pub created_at: DateTime<Utc>,
    pub target_branch_name: String,
    pub pr_info: PullRequestInfo,
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct PullRequestInfo {
    pub number: i64,
    pub url: String,
    pub status: MergeStatus,
    pub merged_at: Option<chrono::DateTime<chrono::Utc>>,
    pub merge_commit_sha: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, Type)]
#[sqlx(type_name = "TEXT", rename_all = "snake_case")]
pub enum MergeType {
    Direct,
    Pr,
}

#[derive(FromRow)]
struct MergeRow {
    id: Uuid,
    task_attempt_id: Uuid,
    merge_type: MergeType,
    merge_commit: Option<String>,
    target_branch_name: String,
    pr_number: Option<i64>,
    pr_url: Option<String>,
    pr_status: Option<MergeStatus>,
    pr_merged_at: Option<DateTime<Utc>>,
    pr_merge_commit_sha: Option<String>,
    created_at: DateTime<Utc>,
}

impl Merge {
    pub fn merge_commit(&self) -> Option<String> {
        match self {
            Merge::Direct(direct) => Some(direct.merge_commit.clone()),
            Merge::Pr(pr) => pr.pr_info.merge_commit_sha.clone(),
        }
    }

    /// Create a direct merge record
    pub async fn create_direct(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
        target_branch_name: &str,
        merge_commit: &str,
    ) -> Result<DirectMerge, sqlx::Error> {
        let id = Uuid::new_v4();
        let now = Utc::now();

        sqlx::query_as!(
            MergeRow,
            r#"INSERT INTO merges (
                id, task_attempt_id, merge_type, merge_commit, created_at, target_branch_name
            ) VALUES ($1, $2, 'direct', $3, $4, $5)
            RETURNING 
                id as "id!: Uuid",
                task_attempt_id as "task_attempt_id!: Uuid",
                merge_type as "merge_type!: MergeType",
                merge_commit,
                pr_number,
                pr_url,
                pr_status as "pr_status?: MergeStatus",
                pr_merged_at as "pr_merged_at?: DateTime<Utc>",
                pr_merge_commit_sha,
                created_at as "created_at!: DateTime<Utc>",
                target_branch_name as "target_branch_name!: String"
            "#,
            id,
            task_attempt_id,
            merge_commit,
            now,
            target_branch_name
        )
        .fetch_one(pool)
        .await
        .map(Into::into)
    }
    /// Create a new PR record (when PR is opened)
    pub async fn create_pr(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
        target_branch_name: &str,
        pr_number: i64,
        pr_url: &str,
    ) -> Result<PrMerge, sqlx::Error> {
        let id = Uuid::new_v4();
        let now = Utc::now();

        sqlx::query_as!(
            MergeRow,
            r#"INSERT INTO merges (
                id, task_attempt_id, merge_type, pr_number, pr_url, pr_status, created_at, target_branch_name
            ) VALUES ($1, $2, 'pr', $3, $4, 'open', $5, $6)
            RETURNING 
                id as "id!: Uuid",
                task_attempt_id as "task_attempt_id!: Uuid",
                merge_type as "merge_type!: MergeType",
                merge_commit,
                pr_number,
                pr_url,
                pr_status as "pr_status?: MergeStatus",
                pr_merged_at as "pr_merged_at?: DateTime<Utc>",
                pr_merge_commit_sha,
                created_at as "created_at!: DateTime<Utc>",
                target_branch_name as "target_branch_name!: String"
            "#,
            id,
            task_attempt_id,
            pr_number,
            pr_url,
            now,
            target_branch_name
        )
        .fetch_one(pool)
        .await
        .map(Into::into)
    }

    /// Get all open PRs for monitoring
    pub async fn get_open_prs(pool: &SqlitePool) -> Result<Vec<PrMerge>, sqlx::Error> {
        let rows = sqlx::query_as!(
            MergeRow,
            r#"SELECT 
                id as "id!: Uuid",
                task_attempt_id as "task_attempt_id!: Uuid",
                merge_type as "merge_type!: MergeType",
                merge_commit,
                pr_number,
                pr_url,
                pr_status as "pr_status?: MergeStatus",
                pr_merged_at as "pr_merged_at?: DateTime<Utc>",
                pr_merge_commit_sha,
                created_at as "created_at!: DateTime<Utc>",
                target_branch_name as "target_branch_name!: String"
               FROM merges 
               WHERE merge_type = 'pr' AND pr_status = 'open'
               ORDER BY created_at DESC"#,
        )
        .fetch_all(pool)
        .await?;

        Ok(rows.into_iter().map(Into::into).collect())
    }

    /// Update PR status for a task attempt
    pub async fn update_status(
        pool: &SqlitePool,
        merge_id: Uuid,
        pr_status: MergeStatus,
        merge_commit_sha: Option<String>,
    ) -> Result<(), sqlx::Error> {
        let merged_at = if matches!(pr_status, MergeStatus::Merged) {
            Some(Utc::now())
        } else {
            None
        };

        sqlx::query!(
            r#"UPDATE merges 
            SET pr_status = $1, 
                pr_merge_commit_sha = $2,
                pr_merged_at = $3
            WHERE id = $4"#,
            pr_status,
            merge_commit_sha,
            merged_at,
            merge_id
        )
        .execute(pool)
        .await?;

        Ok(())
    }
    /// Find all merges for a task attempt (returns both direct and PR merges)
    pub async fn find_by_task_attempt_id(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
    ) -> Result<Vec<Self>, sqlx::Error> {
        // Get raw data from database
        let rows = sqlx::query_as!(
            MergeRow,
            r#"SELECT 
                id as "id!: Uuid",
                task_attempt_id as "task_attempt_id!: Uuid",
                merge_type as "merge_type!: MergeType",
                merge_commit,
                pr_number,
                pr_url,
                pr_status as "pr_status?: MergeStatus",
                pr_merged_at as "pr_merged_at?: DateTime<Utc>",
                pr_merge_commit_sha,
                target_branch_name as "target_branch_name!: String",
                created_at as "created_at!: DateTime<Utc>"
            FROM merges 
            WHERE task_attempt_id = $1
            ORDER BY created_at DESC"#,
            task_attempt_id
        )
        .fetch_all(pool)
        .await?;

        // Convert to appropriate types based on merge_type
        Ok(rows.into_iter().map(Into::into).collect())
    }

    /// Find the most recent merge for a task attempt
    pub async fn find_latest_by_task_attempt_id(
        pool: &SqlitePool,
        task_attempt_id: Uuid,
    ) -> Result<Option<Self>, sqlx::Error> {
        Self::find_by_task_attempt_id(pool, task_attempt_id)
            .await
            .map(|mut merges| merges.pop())
    }
}

// Conversion implementations
impl From<MergeRow> for DirectMerge {
    fn from(row: MergeRow) -> Self {
        DirectMerge {
            id: row.id,
            task_attempt_id: row.task_attempt_id,
            merge_commit: row
                .merge_commit
                .expect("direct merge must have merge_commit"),
            target_branch_name: row.target_branch_name,
            created_at: row.created_at,
        }
    }
}

impl From<MergeRow> for PrMerge {
    fn from(row: MergeRow) -> Self {
        PrMerge {
            id: row.id,
            task_attempt_id: row.task_attempt_id,
            target_branch_name: row.target_branch_name,
            pr_info: PullRequestInfo {
                number: row.pr_number.expect("pr merge must have pr_number"),
                url: row.pr_url.expect("pr merge must have pr_url"),
                status: row.pr_status.expect("pr merge must have status"),
                merged_at: row.pr_merged_at,
                merge_commit_sha: row.pr_merge_commit_sha,
            },
            created_at: row.created_at,
        }
    }
}

impl From<MergeRow> for Merge {
    fn from(row: MergeRow) -> Self {
        match row.merge_type {
            MergeType::Direct => Merge::Direct(DirectMerge::from(row)),
            MergeType::Pr => Merge::Pr(PrMerge::from(row)),
        }
    }
}
</file>

<file path="crates/db/src/models/mod.rs">
pub mod execution_process;
pub mod execution_process_logs;
pub mod executor_session;
pub mod follow_up_draft;
pub mod image;
pub mod merge;
pub mod project;
pub mod task;
pub mod task_attempt;
pub mod task_template;
</file>

<file path="crates/db/src/models/project.rs">
use std::path::PathBuf;

use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use sqlx::{FromRow, SqlitePool};
use thiserror::Error;
use ts_rs::TS;
use uuid::Uuid;

#[derive(Debug, Error)]
pub enum ProjectError {
    #[error(transparent)]
    Database(#[from] sqlx::Error),
    #[error("Project not found")]
    ProjectNotFound,
    #[error("Project with git repository path already exists")]
    GitRepoPathExists,
    #[error("Failed to check existing git repository path: {0}")]
    GitRepoCheckFailed(String),
    #[error("Failed to create project: {0}")]
    CreateFailed(String),
}

#[derive(Debug, Clone, FromRow, Serialize, Deserialize, TS)]
pub struct Project {
    pub id: Uuid,
    pub name: String,
    pub git_repo_path: PathBuf,
    pub setup_script: Option<String>,
    pub dev_script: Option<String>,
    pub cleanup_script: Option<String>,
    pub copy_files: Option<String>,

    #[ts(type = "Date")]
    pub created_at: DateTime<Utc>,
    #[ts(type = "Date")]
    pub updated_at: DateTime<Utc>,
}

#[derive(Debug, Deserialize, TS)]
pub struct CreateProject {
    pub name: String,
    pub git_repo_path: String,
    pub use_existing_repo: bool,
    pub setup_script: Option<String>,
    pub dev_script: Option<String>,
    pub cleanup_script: Option<String>,
    pub copy_files: Option<String>,
}

#[derive(Debug, Deserialize, TS)]
pub struct UpdateProject {
    pub name: Option<String>,
    pub git_repo_path: Option<String>,
    pub setup_script: Option<String>,
    pub dev_script: Option<String>,
    pub cleanup_script: Option<String>,
    pub copy_files: Option<String>,
}

#[derive(Debug, Serialize, TS)]
pub struct SearchResult {
    pub path: String,
    pub is_file: bool,
    pub match_type: SearchMatchType,
}

#[derive(Debug, Clone, Serialize, TS)]
pub enum SearchMatchType {
    FileName,
    DirectoryName,
    FullPath,
}

impl Project {
    pub async fn count(pool: &SqlitePool) -> Result<i64, sqlx::Error> {
        sqlx::query_scalar!(r#"SELECT COUNT(*) as "count!: i64" FROM projects"#)
            .fetch_one(pool)
            .await
    }

    pub async fn find_all(pool: &SqlitePool) -> Result<Vec<Self>, sqlx::Error> {
        sqlx::query_as!(
            Project,
            r#"SELECT id as "id!: Uuid", name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files, created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>" FROM projects ORDER BY created_at DESC"#
        )
        .fetch_all(pool)
        .await
    }

    /// Find the most actively used projects based on recent task activity
    pub async fn find_most_active(pool: &SqlitePool, limit: i32) -> Result<Vec<Self>, sqlx::Error> {
        sqlx::query_as!(
            Project,
            r#"
            SELECT p.id as "id!: Uuid", p.name, p.git_repo_path, p.setup_script, p.dev_script, p.cleanup_script, p.copy_files, 
                   p.created_at as "created_at!: DateTime<Utc>", p.updated_at as "updated_at!: DateTime<Utc>"
            FROM projects p
            WHERE p.id IN (
                SELECT DISTINCT t.project_id
                FROM tasks t
                INNER JOIN task_attempts ta ON ta.task_id = t.id
                ORDER BY ta.updated_at DESC
            )
            LIMIT $1
            "#,
            limit
        )
        .fetch_all(pool)
        .await
    }

    pub async fn find_by_id(pool: &SqlitePool, id: Uuid) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            Project,
            r#"SELECT id as "id!: Uuid", name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files, created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>" FROM projects WHERE id = $1"#,
            id
        )
        .fetch_optional(pool)
        .await
    }

    pub async fn find_by_git_repo_path(
        pool: &SqlitePool,
        git_repo_path: &str,
    ) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            Project,
            r#"SELECT id as "id!: Uuid", name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files, created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>" FROM projects WHERE git_repo_path = $1"#,
            git_repo_path
        )
        .fetch_optional(pool)
        .await
    }

    pub async fn find_by_git_repo_path_excluding_id(
        pool: &SqlitePool,
        git_repo_path: &str,
        exclude_id: Uuid,
    ) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            Project,
            r#"SELECT id as "id!: Uuid", name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files, created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>" FROM projects WHERE git_repo_path = $1 AND id != $2"#,
            git_repo_path,
            exclude_id
        )
        .fetch_optional(pool)
        .await
    }

    pub async fn create(
        pool: &SqlitePool,
        data: &CreateProject,
        project_id: Uuid,
    ) -> Result<Self, sqlx::Error> {
        sqlx::query_as!(
            Project,
            r#"INSERT INTO projects (id, name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files) VALUES ($1, $2, $3, $4, $5, $6, $7) RETURNING id as "id!: Uuid", name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files, created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>""#,
            project_id,
            data.name,
            data.git_repo_path,
            data.setup_script,
            data.dev_script,
            data.cleanup_script,
            data.copy_files
        )
        .fetch_one(pool)
        .await
    }

    #[allow(clippy::too_many_arguments)]
    pub async fn update(
        pool: &SqlitePool,
        id: Uuid,
        name: String,
        git_repo_path: String,
        setup_script: Option<String>,
        dev_script: Option<String>,
        cleanup_script: Option<String>,
        copy_files: Option<String>,
    ) -> Result<Self, sqlx::Error> {
        sqlx::query_as!(
            Project,
            r#"UPDATE projects SET name = $2, git_repo_path = $3, setup_script = $4, dev_script = $5, cleanup_script = $6, copy_files = $7 WHERE id = $1 RETURNING id as "id!: Uuid", name, git_repo_path, setup_script, dev_script, cleanup_script, copy_files, created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>""#,
            id,
            name,
            git_repo_path,
            setup_script,
            dev_script,
            cleanup_script,
            copy_files
        )
        .fetch_one(pool)
        .await
    }

    pub async fn delete(pool: &SqlitePool, id: Uuid) -> Result<u64, sqlx::Error> {
        let result = sqlx::query!("DELETE FROM projects WHERE id = $1", id)
            .execute(pool)
            .await?;
        Ok(result.rows_affected())
    }

    pub async fn exists(pool: &SqlitePool, id: Uuid) -> Result<bool, sqlx::Error> {
        let result = sqlx::query!(
            r#"
                SELECT COUNT(*) as "count!: i64"
                FROM projects
                WHERE id = $1
            "#,
            id
        )
        .fetch_one(pool)
        .await?;

        Ok(result.count > 0)
    }
}
</file>

<file path="crates/db/src/models/task_attempt.rs">
use chrono::{DateTime, Utc};
use executors::executors::BaseCodingAgent;
use serde::{Deserialize, Serialize};
use sqlx::{FromRow, SqlitePool, Type};
use thiserror::Error;
use ts_rs::TS;
use uuid::Uuid;

use super::{project::Project, task::Task};

#[derive(Debug, Error)]
pub enum TaskAttemptError {
    #[error(transparent)]
    Database(#[from] sqlx::Error),
    #[error("Task not found")]
    TaskNotFound,
    #[error("Project not found")]
    ProjectNotFound,
    #[error("Validation error: {0}")]
    ValidationError(String),
    #[error("Branch not found: {0}")]
    BranchNotFound(String),
}

#[derive(Debug, Clone, Type, Serialize, Deserialize, PartialEq, TS)]
#[sqlx(type_name = "task_attempt_status", rename_all = "lowercase")]
#[serde(rename_all = "lowercase")]
pub enum TaskAttemptStatus {
    SetupRunning,
    SetupComplete,
    SetupFailed,
    ExecutorRunning,
    ExecutorComplete,
    ExecutorFailed,
}

#[derive(Debug, Clone, FromRow, Serialize, Deserialize, TS)]
pub struct TaskAttempt {
    pub id: Uuid,
    pub task_id: Uuid,                 // Foreign key to Task
    pub container_ref: Option<String>, // Path to a worktree (local), or cloud container id
    pub branch: Option<String>,        // Git branch name for this task attempt
    pub base_branch: String,           // Base branch this attempt is based on
    pub executor: String, // Name of the base coding agent to use ("AMP", "CLAUDE_CODE",
    // "GEMINI", etc.)
    pub worktree_deleted: bool, // Flag indicating if worktree has been cleaned up
    pub setup_completed_at: Option<DateTime<Utc>>, // When setup script was last completed
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

/// GitHub PR creation parameters
pub struct CreatePrParams<'a> {
    pub attempt_id: Uuid,
    pub task_id: Uuid,
    pub project_id: Uuid,
    pub github_token: &'a str,
    pub title: &'a str,
    pub body: Option<&'a str>,
    pub base_branch: Option<&'a str>,
}

#[derive(Debug, Deserialize, TS)]
pub struct CreateFollowUpAttempt {
    pub prompt: String,
}

/// Context data for resume operations (simplified)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AttemptResumeContext {
    pub execution_history: String,
    pub cumulative_diffs: String,
}

#[derive(Debug)]
pub struct TaskAttemptContext {
    pub task_attempt: TaskAttempt,
    pub task: Task,
    pub project: Project,
}

#[derive(Debug, Deserialize, TS)]
pub struct CreateTaskAttempt {
    pub executor: BaseCodingAgent,
    pub base_branch: String,
}

impl TaskAttempt {
    pub async fn parent_task(&self, pool: &SqlitePool) -> Result<Option<Task>, sqlx::Error> {
        Task::find_by_id(pool, self.task_id).await
    }

    /// Fetch all task attempts, optionally filtered by task_id. Newest first.
    pub async fn fetch_all(
        pool: &SqlitePool,
        task_id: Option<Uuid>,
    ) -> Result<Vec<Self>, TaskAttemptError> {
        let attempts = match task_id {
            Some(tid) => sqlx::query_as!(
                TaskAttempt,
                r#"SELECT id AS "id!: Uuid",
                              task_id AS "task_id!: Uuid",
                              container_ref,
                              branch,
                              base_branch,
                              executor AS "executor!",
                              worktree_deleted AS "worktree_deleted!: bool",
                              setup_completed_at AS "setup_completed_at: DateTime<Utc>",
                              created_at AS "created_at!: DateTime<Utc>",
                              updated_at AS "updated_at!: DateTime<Utc>"
                       FROM task_attempts
                       WHERE task_id = $1
                       ORDER BY created_at DESC"#,
                tid
            )
            .fetch_all(pool)
            .await
            .map_err(TaskAttemptError::Database)?,
            None => sqlx::query_as!(
                TaskAttempt,
                r#"SELECT id AS "id!: Uuid",
                              task_id AS "task_id!: Uuid",
                              container_ref,
                              branch,
                              base_branch,
                              executor AS "executor!",
                              worktree_deleted AS "worktree_deleted!: bool",
                              setup_completed_at AS "setup_completed_at: DateTime<Utc>",
                              created_at AS "created_at!: DateTime<Utc>",
                              updated_at AS "updated_at!: DateTime<Utc>"
                       FROM task_attempts
                       ORDER BY created_at DESC"#
            )
            .fetch_all(pool)
            .await
            .map_err(TaskAttemptError::Database)?,
        };

        Ok(attempts)
    }

    /// Load task attempt with full validation - ensures task_attempt belongs to task and task belongs to project
    pub async fn load_context(
        pool: &SqlitePool,
        attempt_id: Uuid,
        task_id: Uuid,
        project_id: Uuid,
    ) -> Result<TaskAttemptContext, TaskAttemptError> {
        // Single query with JOIN validation to ensure proper relationships
        let task_attempt = sqlx::query_as!(
            TaskAttempt,
            r#"SELECT  ta.id                AS "id!: Uuid",
                       ta.task_id           AS "task_id!: Uuid",
                       ta.container_ref,
                       ta.branch,
                       ta.base_branch,
                       ta.executor AS "executor!",
                       ta.worktree_deleted  AS "worktree_deleted!: bool",
                       ta.setup_completed_at AS "setup_completed_at: DateTime<Utc>",
                       ta.created_at        AS "created_at!: DateTime<Utc>",
                       ta.updated_at        AS "updated_at!: DateTime<Utc>"
               FROM    task_attempts ta
               JOIN    tasks t ON ta.task_id = t.id
               JOIN    projects p ON t.project_id = p.id
               WHERE   ta.id = $1 AND t.id = $2 AND p.id = $3"#,
            attempt_id,
            task_id,
            project_id
        )
        .fetch_optional(pool)
        .await?
        .ok_or(TaskAttemptError::TaskNotFound)?;

        // Load task and project (we know they exist due to JOIN validation)
        let task = Task::find_by_id(pool, task_id)
            .await?
            .ok_or(TaskAttemptError::TaskNotFound)?;

        let project = Project::find_by_id(pool, project_id)
            .await?
            .ok_or(TaskAttemptError::ProjectNotFound)?;

        Ok(TaskAttemptContext {
            task_attempt,
            task,
            project,
        })
    }

    /// Update container reference
    pub async fn update_container_ref(
        pool: &SqlitePool,
        attempt_id: Uuid,
        container_ref: &str,
    ) -> Result<(), sqlx::Error> {
        let now = Utc::now();
        sqlx::query!(
            "UPDATE task_attempts SET container_ref = $1, updated_at = $2 WHERE id = $3",
            container_ref,
            now,
            attempt_id
        )
        .execute(pool)
        .await?;
        Ok(())
    }

    pub async fn update_branch(
        pool: &SqlitePool,
        attempt_id: Uuid,
        branch: &str,
    ) -> Result<(), sqlx::Error> {
        let now = Utc::now();
        sqlx::query!(
            "UPDATE task_attempts SET branch = $1, updated_at = $2 WHERE id = $3",
            branch,
            now,
            attempt_id
        )
        .execute(pool)
        .await?;
        Ok(())
    }

    /// Helper function to mark a worktree as deleted in the database
    pub async fn mark_worktree_deleted(
        pool: &SqlitePool,
        attempt_id: Uuid,
    ) -> Result<(), sqlx::Error> {
        sqlx::query!(
            "UPDATE task_attempts SET worktree_deleted = TRUE, updated_at = datetime('now') WHERE id = ?",
            attempt_id
        )
        .execute(pool)
        .await?;
        Ok(())
    }

    pub async fn find_by_id(pool: &SqlitePool, id: Uuid) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            TaskAttempt,
            r#"SELECT  id                AS "id!: Uuid",
                       task_id           AS "task_id!: Uuid",
                       container_ref,
                       branch,
                       base_branch,
                       executor AS "executor!",
                       worktree_deleted  AS "worktree_deleted!: bool",
                       setup_completed_at AS "setup_completed_at: DateTime<Utc>",
                       created_at        AS "created_at!: DateTime<Utc>",
                       updated_at        AS "updated_at!: DateTime<Utc>"
               FROM    task_attempts
               WHERE   id = $1"#,
            id
        )
        .fetch_optional(pool)
        .await
    }

    pub async fn find_by_rowid(pool: &SqlitePool, rowid: i64) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            TaskAttempt,
            r#"SELECT  id                AS "id!: Uuid",
                       task_id           AS "task_id!: Uuid",
                       container_ref,
                       branch,
                       base_branch,
                       executor AS "executor!",
                       worktree_deleted  AS "worktree_deleted!: bool",
                       setup_completed_at AS "setup_completed_at: DateTime<Utc>",
                       created_at        AS "created_at!: DateTime<Utc>",
                       updated_at        AS "updated_at!: DateTime<Utc>"
               FROM    task_attempts
               WHERE   rowid = $1"#,
            rowid
        )
        .fetch_optional(pool)
        .await
    }

    /// Find task attempts by task_id with project git repo path for cleanup operations
    pub async fn find_by_task_id_with_project(
        pool: &SqlitePool,
        task_id: Uuid,
    ) -> Result<Vec<(Uuid, Option<String>, String)>, sqlx::Error> {
        let records = sqlx::query!(
            r#"
            SELECT ta.id as "attempt_id!: Uuid", ta.container_ref, p.git_repo_path as "git_repo_path!"
            FROM task_attempts ta
            JOIN tasks t ON ta.task_id = t.id
            JOIN projects p ON t.project_id = p.id
            WHERE ta.task_id = $1
            "#,
            task_id
        )
        .fetch_all(pool)
        .await?;

        Ok(records
            .into_iter()
            .map(|r| (r.attempt_id, r.container_ref, r.git_repo_path))
            .collect())
    }

    pub async fn find_by_worktree_deleted(
        pool: &SqlitePool,
    ) -> Result<Vec<(Uuid, String)>, sqlx::Error> {
        let records = sqlx::query!(
        r#"SELECT id as "id!: Uuid", container_ref FROM task_attempts WHERE worktree_deleted = FALSE"#,
        )
        .fetch_all(pool).await?;
        Ok(records
            .into_iter()
            .filter_map(|r| r.container_ref.map(|path| (r.id, path)))
            .collect())
    }

    pub async fn container_ref_exists(
        pool: &SqlitePool,
        container_ref: &str,
    ) -> Result<bool, sqlx::Error> {
        let result = sqlx::query!(
            r#"SELECT EXISTS(SELECT 1 FROM task_attempts WHERE container_ref = ?) as "exists!: bool""#,
            container_ref
        )
        .fetch_one(pool)
        .await?;

        Ok(result.exists)
    }

    /// Find task attempts that are expired (72+ hours since last activity) and eligible for worktree cleanup
    /// Activity includes: execution completion, task attempt updates (including worktree recreation),
    /// and any attempts that are currently in progress
    pub async fn find_expired_for_cleanup(
        pool: &SqlitePool,
    ) -> Result<Vec<(Uuid, String, String)>, sqlx::Error> {
        let records = sqlx::query!(
            r#"
            SELECT ta.id as "attempt_id!: Uuid", ta.container_ref, p.git_repo_path as "git_repo_path!"
            FROM task_attempts ta
            LEFT JOIN execution_processes ep ON ta.id = ep.task_attempt_id AND ep.completed_at IS NOT NULL
            JOIN tasks t ON ta.task_id = t.id
            JOIN projects p ON t.project_id = p.id
            WHERE ta.worktree_deleted = FALSE
                -- Exclude attempts with any running processes (in progress)
                AND ta.id NOT IN (
                    SELECT DISTINCT ep2.task_attempt_id
                    FROM execution_processes ep2
                    WHERE ep2.completed_at IS NULL
                )
            GROUP BY ta.id, ta.container_ref, p.git_repo_path, ta.updated_at
            HAVING datetime('now', '-72 hours') > datetime(
                MAX(
                    CASE
                        WHEN ep.completed_at IS NOT NULL THEN ep.completed_at
                        ELSE ta.updated_at
                    END
                )
            )
            ORDER BY MAX(
                CASE
                    WHEN ep.completed_at IS NOT NULL THEN ep.completed_at
                    ELSE ta.updated_at
                END
            ) ASC
            "#
        )
        .fetch_all(pool)
        .await?;

        Ok(records
            .into_iter()
            .filter_map(|r| {
                r.container_ref
                    .map(|path| (r.attempt_id, path, r.git_repo_path))
            })
            .collect())
    }

    pub async fn create(
        pool: &SqlitePool,
        data: &CreateTaskAttempt,
        task_id: Uuid,
    ) -> Result<Self, TaskAttemptError> {
        let attempt_id = Uuid::new_v4();
        // let prefixed_id = format!("vibe-kanban-{}", attempt_id);
        // Insert the record into the database
        Ok(sqlx::query_as!(
            TaskAttempt,
            r#"INSERT INTO task_attempts (id, task_id, container_ref, branch, base_branch, executor, worktree_deleted, setup_completed_at)
               VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
               RETURNING id as "id!: Uuid", task_id as "task_id!: Uuid", container_ref, branch, base_branch, executor as "executor!",  worktree_deleted as "worktree_deleted!: bool", setup_completed_at as "setup_completed_at: DateTime<Utc>", created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>""#,
            attempt_id,
            task_id,
            Option::<String>::None, // Container isn't known yet
            Option::<String>::None, // branch name isn't known yet
            data.base_branch,
            data.executor,
            false, // worktree_deleted is false during creation
            Option::<DateTime<Utc>>::None // setup_completed_at is None during creation
        )
        .fetch_one(pool)
        .await?)
    }

    pub async fn update_base_branch(
        pool: &SqlitePool,
        attempt_id: Uuid,
        new_base_branch: &str,
    ) -> Result<(), TaskAttemptError> {
        sqlx::query!(
            "UPDATE task_attempts SET base_branch = $1, updated_at = datetime('now') WHERE id = $2",
            new_base_branch,
            attempt_id,
        )
        .execute(pool)
        .await?;

        Ok(())
    }

    pub async fn resolve_container_ref(
        pool: &SqlitePool,
        container_ref: &str,
    ) -> Result<(Uuid, Uuid, Uuid), sqlx::Error> {
        let result = sqlx::query!(
            r#"SELECT ta.id as "attempt_id!: Uuid",
                      ta.task_id as "task_id!: Uuid",
                      t.project_id as "project_id!: Uuid"
               FROM task_attempts ta
               JOIN tasks t ON ta.task_id = t.id
               WHERE ta.container_ref = ?"#,
            container_ref
        )
        .fetch_optional(pool)
        .await?
        .ok_or(sqlx::Error::RowNotFound)?;

        Ok((result.attempt_id, result.task_id, result.project_id))
    }
}
</file>

<file path="crates/db/src/models/task_template.rs">
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use sqlx::{FromRow, SqlitePool};
use ts_rs::TS;
use uuid::Uuid;

#[derive(Debug, Clone, FromRow, Serialize, Deserialize, TS)]
pub struct TaskTemplate {
    pub id: Uuid,
    pub project_id: Option<Uuid>, // None for global templates
    pub title: String,
    pub description: Option<String>,
    pub template_name: String,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

#[derive(Debug, Deserialize, TS)]
pub struct CreateTaskTemplate {
    pub project_id: Option<Uuid>,
    pub title: String,
    pub description: Option<String>,
    pub template_name: String,
}

#[derive(Debug, Deserialize, TS)]
pub struct UpdateTaskTemplate {
    pub title: Option<String>,
    pub description: Option<String>,
    pub template_name: Option<String>,
}

impl TaskTemplate {
    pub async fn find_all(pool: &SqlitePool) -> Result<Vec<Self>, sqlx::Error> {
        sqlx::query_as!(
            TaskTemplate,
            r#"SELECT id as "id!: Uuid", project_id as "project_id?: Uuid", title, description, template_name, created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>"
               FROM task_templates 
               ORDER BY project_id IS NULL DESC, template_name ASC"#
        )
        .fetch_all(pool)
        .await
    }

    pub async fn find_by_project_id(
        pool: &SqlitePool,
        project_id: Option<Uuid>,
    ) -> Result<Vec<Self>, sqlx::Error> {
        if let Some(pid) = project_id {
            // Return only project-specific templates
            sqlx::query_as!(
                TaskTemplate,
                r#"SELECT id as "id!: Uuid", project_id as "project_id?: Uuid", title, description, template_name, created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>"
                   FROM task_templates 
                   WHERE project_id = ?
                   ORDER BY template_name ASC"#,
                pid
            )
            .fetch_all(pool)
            .await
        } else {
            // Return only global templates
            sqlx::query_as!(
                TaskTemplate,
                r#"SELECT id as "id!: Uuid", project_id as "project_id?: Uuid", title, description, template_name, created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>"
                   FROM task_templates 
                   WHERE project_id IS NULL
                   ORDER BY template_name ASC"#
            )
            .fetch_all(pool)
            .await
        }
    }

    pub async fn find_by_id(pool: &SqlitePool, id: Uuid) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            TaskTemplate,
            r#"SELECT id as "id!: Uuid", project_id as "project_id?: Uuid", title, description, template_name, created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>"
               FROM task_templates 
               WHERE id = $1"#,
            id
        )
        .fetch_optional(pool)
        .await
    }

    pub async fn create(pool: &SqlitePool, data: &CreateTaskTemplate) -> Result<Self, sqlx::Error> {
        let id = Uuid::new_v4();
        sqlx::query_as!(
            TaskTemplate,
            r#"INSERT INTO task_templates (id, project_id, title, description, template_name) 
               VALUES ($1, $2, $3, $4, $5) 
               RETURNING id as "id!: Uuid", project_id as "project_id?: Uuid", title, description, template_name, created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>""#,
            id,
            data.project_id,
            data.title,
            data.description,
            data.template_name
        )
        .fetch_one(pool)
        .await
    }

    pub async fn update(
        pool: &SqlitePool,
        id: Uuid,
        data: &UpdateTaskTemplate,
    ) -> Result<Self, sqlx::Error> {
        // Get existing template first
        let existing = Self::find_by_id(pool, id)
            .await?
            .ok_or(sqlx::Error::RowNotFound)?;

        // Use let bindings to create longer-lived values
        let title = data.title.as_ref().unwrap_or(&existing.title);
        let description = data.description.as_ref().or(existing.description.as_ref());
        let template_name = data
            .template_name
            .as_ref()
            .unwrap_or(&existing.template_name);

        sqlx::query_as!(
            TaskTemplate,
            r#"UPDATE task_templates 
               SET title = $2, description = $3, template_name = $4, updated_at = datetime('now', 'subsec')
               WHERE id = $1 
               RETURNING id as "id!: Uuid", project_id as "project_id?: Uuid", title, description, template_name, created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>""#,
            id,
            title,
            description,
            template_name
        )
        .fetch_one(pool)
        .await
    }

    pub async fn delete(pool: &SqlitePool, id: Uuid) -> Result<u64, sqlx::Error> {
        let result = sqlx::query!("DELETE FROM task_templates WHERE id = $1", id)
            .execute(pool)
            .await?;
        Ok(result.rows_affected())
    }
}
</file>

<file path="crates/db/src/models/task.rs">
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use sqlx::{FromRow, SqlitePool, Type};
use ts_rs::TS;
use uuid::Uuid;

use super::{project::Project, task_attempt::TaskAttempt};

#[derive(Debug, Clone, Type, Serialize, Deserialize, PartialEq, TS)]
#[sqlx(type_name = "task_status", rename_all = "lowercase")]
#[serde(rename_all = "lowercase")]
pub enum TaskStatus {
    Todo,
    InProgress,
    InReview,
    Done,
    Cancelled,
}

#[derive(Debug, Clone, FromRow, Serialize, Deserialize, TS)]
pub struct Task {
    pub id: Uuid,
    pub project_id: Uuid, // Foreign key to Project
    pub title: String,
    pub description: Option<String>,
    pub status: TaskStatus,
    pub parent_task_attempt: Option<Uuid>, // Foreign key to parent TaskAttempt
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct TaskWithAttemptStatus {
    #[serde(flatten)]
    #[ts(flatten)]
    pub task: Task,
    pub has_in_progress_attempt: bool,
    pub has_merged_attempt: bool,
    pub last_attempt_failed: bool,
    pub executor: String,
}

impl std::ops::Deref for TaskWithAttemptStatus {
    type Target = Task;
    fn deref(&self) -> &Self::Target {
        &self.task
    }
}

impl std::ops::DerefMut for TaskWithAttemptStatus {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.task
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct TaskRelationships {
    pub parent_task: Option<Task>,    // The task that owns this attempt
    pub current_attempt: TaskAttempt, // The attempt we're viewing
    pub children: Vec<Task>,          // Tasks created by this attempt
}

#[derive(Debug, Deserialize, TS)]
pub struct CreateTask {
    pub project_id: Uuid,
    pub title: String,
    pub description: Option<String>,
    pub parent_task_attempt: Option<Uuid>,
    pub image_ids: Option<Vec<Uuid>>,
}

#[derive(Debug, Deserialize, TS)]
pub struct UpdateTask {
    pub title: Option<String>,
    pub description: Option<String>,
    pub status: Option<TaskStatus>,
    pub parent_task_attempt: Option<Uuid>,
    pub image_ids: Option<Vec<Uuid>>,
}

impl Task {
    pub fn to_prompt(&self) -> String {
        if let Some(description) = &self.description {
            format!("Title: {}\n\nDescription:{}", &self.title, description)
        } else {
            self.title.clone()
        }
    }

    pub async fn parent_project(&self, pool: &SqlitePool) -> Result<Option<Project>, sqlx::Error> {
        Project::find_by_id(pool, self.project_id).await
    }

    pub async fn find_by_project_id_with_attempt_status(
        pool: &SqlitePool,
        project_id: Uuid,
    ) -> Result<Vec<TaskWithAttemptStatus>, sqlx::Error> {
        let records = sqlx::query!(
            r#"SELECT
  t.id                            AS "id!: Uuid",
  t.project_id                    AS "project_id!: Uuid",
  t.title,
  t.description,
  t.status                        AS "status!: TaskStatus",
  t.parent_task_attempt           AS "parent_task_attempt: Uuid",
  t.created_at                    AS "created_at!: DateTime<Utc>",
  t.updated_at                    AS "updated_at!: DateTime<Utc>",

  CASE WHEN EXISTS (
    SELECT 1
      FROM task_attempts ta
      JOIN execution_processes ep
        ON ep.task_attempt_id = ta.id
     WHERE ta.task_id       = t.id
       AND ep.status        = 'running'
       AND ep.run_reason IN ('setupscript','cleanupscript','codingagent')
     LIMIT 1
  ) THEN 1 ELSE 0 END            AS "has_in_progress_attempt!: i64",
  
  CASE WHEN (
    SELECT ep.status
      FROM task_attempts ta
      JOIN execution_processes ep
        ON ep.task_attempt_id = ta.id
     WHERE ta.task_id       = t.id
     AND ep.run_reason IN ('setupscript','cleanupscript','codingagent')
     ORDER BY ep.created_at DESC
     LIMIT 1
  ) IN ('failed','killed') THEN 1 ELSE 0 END
                                 AS "last_attempt_failed!: i64",

  ( SELECT ta.executor
      FROM task_attempts ta
      WHERE ta.task_id = t.id
     ORDER BY ta.created_at DESC
      LIMIT 1
    )                               AS "executor!: String"

FROM tasks t
WHERE t.project_id = $1
ORDER BY t.created_at DESC"#,
            project_id
        )
        .fetch_all(pool)
        .await?;

        let tasks = records
            .into_iter()
            .map(|rec| TaskWithAttemptStatus {
                task: Task {
                    id: rec.id,
                    project_id: rec.project_id,
                    title: rec.title,
                    description: rec.description,
                    status: rec.status,
                    parent_task_attempt: rec.parent_task_attempt,
                    created_at: rec.created_at,
                    updated_at: rec.updated_at,
                },
                has_in_progress_attempt: rec.has_in_progress_attempt != 0,
                has_merged_attempt: false, // TODO use merges table
                last_attempt_failed: rec.last_attempt_failed != 0,
                executor: rec.executor,
            })
            .collect();

        Ok(tasks)
    }

    pub async fn find_by_id(pool: &SqlitePool, id: Uuid) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            Task,
            r#"SELECT id as "id!: Uuid", project_id as "project_id!: Uuid", title, description, status as "status!: TaskStatus", parent_task_attempt as "parent_task_attempt: Uuid", created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>"
               FROM tasks 
               WHERE id = $1"#,
            id
        )
        .fetch_optional(pool)
        .await
    }

    pub async fn find_by_rowid(pool: &SqlitePool, rowid: i64) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            Task,
            r#"SELECT id as "id!: Uuid", project_id as "project_id!: Uuid", title, description, status as "status!: TaskStatus", parent_task_attempt as "parent_task_attempt: Uuid", created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>"
               FROM tasks 
               WHERE rowid = $1"#,
            rowid
        )
        .fetch_optional(pool)
        .await
    }

    pub async fn find_by_id_and_project_id(
        pool: &SqlitePool,
        id: Uuid,
        project_id: Uuid,
    ) -> Result<Option<Self>, sqlx::Error> {
        sqlx::query_as!(
            Task,
            r#"SELECT id as "id!: Uuid", project_id as "project_id!: Uuid", title, description, status as "status!: TaskStatus", parent_task_attempt as "parent_task_attempt: Uuid", created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>"
               FROM tasks 
               WHERE id = $1 AND project_id = $2"#,
            id,
            project_id
        )
        .fetch_optional(pool)
        .await
    }

    pub async fn create(
        pool: &SqlitePool,
        data: &CreateTask,
        task_id: Uuid,
    ) -> Result<Self, sqlx::Error> {
        sqlx::query_as!(
            Task,
            r#"INSERT INTO tasks (id, project_id, title, description, status, parent_task_attempt) 
               VALUES ($1, $2, $3, $4, $5, $6) 
               RETURNING id as "id!: Uuid", project_id as "project_id!: Uuid", title, description, status as "status!: TaskStatus", parent_task_attempt as "parent_task_attempt: Uuid", created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>""#,
            task_id,
            data.project_id,
            data.title,
            data.description,
            TaskStatus::Todo as TaskStatus,
            data.parent_task_attempt
        )
        .fetch_one(pool)
        .await
    }

    pub async fn update(
        pool: &SqlitePool,
        id: Uuid,
        project_id: Uuid,
        title: String,
        description: Option<String>,
        status: TaskStatus,
        parent_task_attempt: Option<Uuid>,
    ) -> Result<Self, sqlx::Error> {
        sqlx::query_as!(
            Task,
            r#"UPDATE tasks 
               SET title = $3, description = $4, status = $5, parent_task_attempt = $6 
               WHERE id = $1 AND project_id = $2 
               RETURNING id as "id!: Uuid", project_id as "project_id!: Uuid", title, description, status as "status!: TaskStatus", parent_task_attempt as "parent_task_attempt: Uuid", created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>""#,
            id,
            project_id,
            title,
            description,
            status,
            parent_task_attempt
        )
        .fetch_one(pool)
        .await
    }

    pub async fn update_status(
        pool: &SqlitePool,
        id: Uuid,
        status: TaskStatus,
    ) -> Result<(), sqlx::Error> {
        sqlx::query!(
            "UPDATE tasks SET status = $2, updated_at = CURRENT_TIMESTAMP WHERE id = $1",
            id,
            status
        )
        .execute(pool)
        .await?;
        Ok(())
    }

    pub async fn delete(pool: &SqlitePool, id: Uuid) -> Result<u64, sqlx::Error> {
        let result = sqlx::query!("DELETE FROM tasks WHERE id = $1", id)
            .execute(pool)
            .await?;
        Ok(result.rows_affected())
    }

    pub async fn exists(
        pool: &SqlitePool,
        id: Uuid,
        project_id: Uuid,
    ) -> Result<bool, sqlx::Error> {
        let result = sqlx::query!(
            "SELECT id as \"id!: Uuid\" FROM tasks WHERE id = $1 AND project_id = $2",
            id,
            project_id
        )
        .fetch_optional(pool)
        .await?;
        Ok(result.is_some())
    }

    pub async fn find_children_by_attempt_id(
        pool: &SqlitePool,
        attempt_id: Uuid,
    ) -> Result<Vec<Self>, sqlx::Error> {
        // Find only child tasks that have this attempt as their parent
        sqlx::query_as!(
            Task,
            r#"SELECT id as "id!: Uuid", project_id as "project_id!: Uuid", title, description, status as "status!: TaskStatus", parent_task_attempt as "parent_task_attempt: Uuid", created_at as "created_at!: DateTime<Utc>", updated_at as "updated_at!: DateTime<Utc>"
               FROM tasks 
               WHERE parent_task_attempt = $1
               ORDER BY created_at DESC"#,
            attempt_id,
        )
        .fetch_all(pool)
        .await
    }

    pub async fn find_relationships_for_attempt(
        pool: &SqlitePool,
        task_attempt: &TaskAttempt,
    ) -> Result<TaskRelationships, sqlx::Error> {
        // 1. Get the current task (task that owns this attempt)
        let current_task = Self::find_by_id(pool, task_attempt.task_id)
            .await?
            .ok_or(sqlx::Error::RowNotFound)?;

        // 2. Get parent task (if current task was created by another task's attempt)
        let parent_task = if let Some(parent_attempt_id) = current_task.parent_task_attempt {
            // Find the attempt that created the current task
            if let Ok(Some(parent_attempt)) = TaskAttempt::find_by_id(pool, parent_attempt_id).await
            {
                // Find the task that owns that parent attempt - THAT's the real parent
                Self::find_by_id(pool, parent_attempt.task_id).await?
            } else {
                None
            }
        } else {
            None
        };

        // 3. Get children tasks (created by this attempt)
        let children = Self::find_children_by_attempt_id(pool, task_attempt.id).await?;

        Ok(TaskRelationships {
            parent_task,
            current_attempt: task_attempt.clone(),
            children,
        })
    }
}
</file>

<file path="crates/db/src/lib.rs">
use std::{str::FromStr, sync::Arc};

use sqlx::{
    Error, Pool, Sqlite, SqlitePool,
    sqlite::{SqliteConnectOptions, SqliteConnection, SqlitePoolOptions},
};
use utils::assets::asset_dir;

pub mod models;

#[derive(Clone)]
pub struct DBService {
    pub pool: Pool<Sqlite>,
}

impl DBService {
    pub async fn new() -> Result<DBService, Error> {
        let database_url = format!(
            "sqlite://{}",
            asset_dir().join("db.sqlite").to_string_lossy()
        );
        let options = SqliteConnectOptions::from_str(&database_url)?.create_if_missing(true);
        let pool = SqlitePool::connect_with(options).await?;
        sqlx::migrate!("./migrations").run(&pool).await?;
        Ok(DBService { pool })
    }

    pub async fn new_with_after_connect<F>(after_connect: F) -> Result<DBService, Error>
    where
        F: for<'a> Fn(
                &'a mut SqliteConnection,
            ) -> std::pin::Pin<
                Box<dyn std::future::Future<Output = Result<(), Error>> + Send + 'a>,
            > + Send
            + Sync
            + 'static,
    {
        let pool = Self::create_pool(Some(Arc::new(after_connect))).await?;
        Ok(DBService { pool })
    }

    async fn create_pool<F>(after_connect: Option<Arc<F>>) -> Result<Pool<Sqlite>, Error>
    where
        F: for<'a> Fn(
                &'a mut SqliteConnection,
            ) -> std::pin::Pin<
                Box<dyn std::future::Future<Output = Result<(), Error>> + Send + 'a>,
            > + Send
            + Sync
            + 'static,
    {
        let database_url = format!(
            "sqlite://{}",
            asset_dir().join("db.sqlite").to_string_lossy()
        );
        let options = SqliteConnectOptions::from_str(&database_url)?.create_if_missing(true);

        let pool = if let Some(hook) = after_connect {
            SqlitePoolOptions::new()
                .after_connect(move |conn, _meta| {
                    let hook = hook.clone();
                    Box::pin(async move {
                        hook(conn).await?;
                        Ok(())
                    })
                })
                .connect_with(options)
                .await?
        } else {
            SqlitePool::connect_with(options).await?
        };

        sqlx::migrate!("./migrations").run(&pool).await?;
        Ok(pool)
    }
}
</file>

<file path="crates/db/Cargo.toml">
[package]
name = "db"
version = "0.0.94"
edition = "2024"

[dependencies]
utils = { path = "../utils" }
executors = { path = "../executors" }
tokio = { workspace = true }
tokio-util = { version = "0.7", features = ["io"] }
thiserror = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
anyhow = { workspace = true }
tracing = { workspace = true }
tracing-subscriber = { workspace = true }
sqlx = { version = "0.8.6", features = ["runtime-tokio-rustls", "sqlite", "chrono", "uuid"] }
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1.0", features = ["v4", "serde"] }
ts-rs = { workspace = true }
async-trait = "0.1"
regex = "1.11.1"
sentry-tracing = { version = "0.41.0", features = ["backtrace"] }
futures-util = "0.3"
</file>

<file path="crates/deployment/src/lib.rs">
use std::{collections::HashMap, sync::Arc};

use anyhow::Error as AnyhowError;
use async_trait::async_trait;
use axum::response::sse::Event;
use db::{
    DBService,
    models::{
        execution_process::{ExecutionProcess, ExecutionProcessRunReason, ExecutionProcessStatus},
        project::{CreateProject, Project},
        task::{Task, TaskStatus},
        task_attempt::{TaskAttempt, TaskAttemptError},
    },
};
use executors::executors::ExecutorError;
use futures::{StreamExt, TryStreamExt};
use git2::Error as Git2Error;
use serde_json::Value;
use services::services::{
    analytics::AnalyticsService,
    auth::{AuthError, AuthService},
    config::{Config, ConfigError},
    container::{ContainerError, ContainerService},
    events::{EventError, EventService},
    file_search_cache::FileSearchCache,
    filesystem::{FilesystemError, FilesystemService},
    filesystem_watcher::FilesystemWatcherError,
    git::{GitService, GitServiceError},
    image::{ImageError, ImageService},
    pr_monitor::PrMonitorService,
    sentry::SentryService,
    worktree_manager::WorktreeError,
};
use sqlx::{Error as SqlxError, types::Uuid};
use thiserror::Error;
use tokio::sync::RwLock;
use utils::msg_store::MsgStore;

#[derive(Debug, Error)]
pub enum DeploymentError {
    #[error(transparent)]
    Io(#[from] std::io::Error),
    #[error(transparent)]
    Sqlx(#[from] SqlxError),
    #[error(transparent)]
    Git2(#[from] Git2Error),
    #[error(transparent)]
    GitServiceError(#[from] GitServiceError),
    #[error(transparent)]
    FilesystemWatcherError(#[from] FilesystemWatcherError),
    #[error(transparent)]
    TaskAttempt(#[from] TaskAttemptError),
    #[error(transparent)]
    Container(#[from] ContainerError),
    #[error(transparent)]
    Executor(#[from] ExecutorError),
    #[error(transparent)]
    Auth(#[from] AuthError),
    #[error(transparent)]
    Image(#[from] ImageError),
    #[error(transparent)]
    Filesystem(#[from] FilesystemError),
    #[error(transparent)]
    Worktree(#[from] WorktreeError),
    #[error(transparent)]
    Event(#[from] EventError),
    #[error(transparent)]
    Config(#[from] ConfigError),
    #[error(transparent)]
    Other(#[from] AnyhowError),
}

#[async_trait]
pub trait Deployment: Clone + Send + Sync + 'static {
    async fn new() -> Result<Self, DeploymentError>;

    fn user_id(&self) -> &str;

    fn shared_types() -> Vec<String>;

    fn config(&self) -> &Arc<RwLock<Config>>;

    fn sentry(&self) -> &SentryService;

    fn db(&self) -> &DBService;

    fn analytics(&self) -> &Option<AnalyticsService>;

    fn container(&self) -> &impl ContainerService;

    fn auth(&self) -> &AuthService;

    fn git(&self) -> &GitService;

    fn image(&self) -> &ImageService;

    fn filesystem(&self) -> &FilesystemService;

    fn msg_stores(&self) -> &Arc<RwLock<HashMap<Uuid, Arc<MsgStore>>>>;

    fn events(&self) -> &EventService;

    fn file_search_cache(&self) -> &Arc<FileSearchCache>;

    async fn update_sentry_scope(&self) -> Result<(), DeploymentError> {
        let user_id = self.user_id();
        let config = self.config().read().await;
        let username = config.github.username.as_deref();
        let email = config.github.primary_email.as_deref();

        self.sentry().update_scope(user_id, username, email).await;

        Ok(())
    }

    async fn spawn_pr_monitor_service(&self) -> tokio::task::JoinHandle<()> {
        let db = self.db().clone();
        let config = self.config().clone();
        PrMonitorService::spawn(db, config).await
    }

    async fn track_if_analytics_allowed(&self, event_name: &str, properties: Value) {
        let analytics_enabled = self.config().read().await.analytics_enabled;
        // Only skip tracking if user explicitly opted out (Some(false))
        // Send for None (undecided) and Some(true) (opted in)
        if analytics_enabled != Some(false)
            && let Some(analytics) = self.analytics()
        {
            analytics.track_event(self.user_id(), event_name, Some(properties.clone()));
        }
    }

    /// Cleanup executions marked as running in the db, call at startup
    async fn cleanup_orphan_executions(&self) -> Result<(), DeploymentError> {
        let running_processes = ExecutionProcess::find_running(&self.db().pool).await?;
        for process in running_processes {
            tracing::info!(
                "Found orphaned execution process {} for task attempt {}",
                process.id,
                process.task_attempt_id
            );
            // Update the execution process status first
            if let Err(e) = ExecutionProcess::update_completion(
                &self.db().pool,
                process.id,
                ExecutionProcessStatus::Failed,
                None, // No exit code for orphaned processes
            )
            .await
            {
                tracing::error!(
                    "Failed to update orphaned execution process {} status: {}",
                    process.id,
                    e
                );
                continue;
            }
            // Capture after-head commit OID (best-effort)
            if let Ok(Some(task_attempt)) =
                TaskAttempt::find_by_id(&self.db().pool, process.task_attempt_id).await
                && let Some(container_ref) = task_attempt.container_ref
            {
                let wt = std::path::PathBuf::from(container_ref);
                if let Ok(head) = self.git().get_head_info(&wt) {
                    let _ = ExecutionProcess::update_after_head_commit(
                        &self.db().pool,
                        process.id,
                        &head.oid,
                    )
                    .await;
                }
            }
            // Process marked as failed
            tracing::info!("Marked orphaned execution process {} as failed", process.id);
            // Update task status to InReview for coding agent and setup script failures
            if matches!(
                process.run_reason,
                ExecutionProcessRunReason::CodingAgent
                    | ExecutionProcessRunReason::SetupScript
                    | ExecutionProcessRunReason::CleanupScript
            ) && let Ok(Some(task_attempt)) =
                TaskAttempt::find_by_id(&self.db().pool, process.task_attempt_id).await
                && let Ok(Some(task)) = task_attempt.parent_task(&self.db().pool).await
                && let Err(e) =
                    Task::update_status(&self.db().pool, task.id, TaskStatus::InReview).await
            {
                tracing::error!(
                    "Failed to update task status to InReview for orphaned attempt: {}",
                    e
                );
            }
        }
        Ok(())
    }

    /// Backfill before_head_commit for legacy execution processes.
    /// Rules:
    /// - If a process has after_head_commit and missing before_head_commit,
    ///   then set before_head_commit to the previous process's after_head_commit.
    /// - If there is no previous process, set before_head_commit to the base branch commit.
    async fn backfill_before_head_commits(&self) -> Result<(), DeploymentError> {
        let pool = &self.db().pool;
        let rows = ExecutionProcess::list_missing_before_context(pool).await?;
        for row in rows {
            // Skip if no after commit at all (shouldn't happen due to WHERE)
            // Prefer previous process after-commit if present
            let mut before = row.prev_after_head_commit.clone();

            // Fallback to base branch commit OID
            if before.is_none() {
                let repo_path =
                    std::path::Path::new(row.git_repo_path.as_deref().unwrap_or_default());
                match self
                    .git()
                    .get_branch_oid(repo_path, row.base_branch.as_str())
                {
                    Ok(oid) => before = Some(oid),
                    Err(e) => {
                        tracing::warn!(
                            "Backfill: Failed to resolve base branch OID for attempt {} (branch {}): {}",
                            row.task_attempt_id,
                            row.base_branch,
                            e
                        );
                    }
                }
            }

            if let Some(before_oid) = before
                && let Err(e) =
                    ExecutionProcess::update_before_head_commit(pool, row.id, &before_oid).await
            {
                tracing::warn!(
                    "Backfill: Failed to update before_head_commit for process {}: {}",
                    row.id,
                    e
                );
            }
        }

        Ok(())
    }

    /// Trigger background auto-setup of default projects for new users
    async fn trigger_auto_project_setup(&self) {
        // soft timeout to give the filesystem search a chance to complete
        let soft_timeout_ms = 800;
        // hard timeout to ensure the background task doesn't run indefinitely
        let hard_timeout_ms = 1_000;
        let project_count = Project::count(&self.db().pool).await.unwrap_or(0);

        // Only proceed if no projects exist
        if project_count == 0 {
            // Discover local git repositories
            if let Ok(repos) = self
                .filesystem()
                .list_common_git_repos(soft_timeout_ms, hard_timeout_ms, Some(3))
                .await
            {
                // Take first 3 repositories and create projects
                for repo in repos.into_iter().take(3) {
                    // Generate clean project name from path
                    let project_name = repo.name;

                    let create_data = CreateProject {
                        name: project_name,
                        git_repo_path: repo.path.to_string_lossy().to_string(),
                        use_existing_repo: true,
                        setup_script: None,
                        dev_script: None,
                        cleanup_script: None,
                        copy_files: None,
                    };
                    // Ensure existing repo has a main branch if it's empty
                    if let Err(e) = self.git().ensure_main_branch_exists(&repo.path) {
                        tracing::error!("Failed to ensure main branch exists: {}", e);
                        continue;
                    }

                    // Create project (ignore individual failures)
                    let project_id = Uuid::new_v4();
                    if let Err(e) = Project::create(&self.db().pool, &create_data, project_id).await
                    {
                        tracing::warn!(
                            "Failed to auto-create project '{}': {}",
                            create_data.name,
                            e
                        );
                    } else {
                        tracing::info!(
                            "Auto-created project '{}' from {}",
                            create_data.name,
                            create_data.git_repo_path
                        );
                    }
                }
            }
        }
    }

    async fn stream_events(
        &self,
    ) -> futures::stream::BoxStream<'static, Result<Event, std::io::Error>> {
        self.events()
            .msg_store()
            .history_plus_stream()
            .map_ok(|m| m.to_sse_event())
            .boxed()
    }
}
</file>

<file path="crates/deployment/Cargo.toml">
[package]
name = "deployment"
version = "0.0.94"
edition = "2024"

[dependencies]
db = { path = "../db" }
utils = { path = "../utils" }
services = { path = "../services" }
executors = { path = "../executors" }
async-trait = "0.1"
thiserror = { workspace = true } 
anyhow = { workspace = true }
tokio = { workspace = true }
sqlx = "0.8.6"
serde_json = { workspace = true }
tracing = { workspace = true }
git2 = "^0.18.1"
futures = "0.3.31"
axum = { workspace = true }
</file>

<file path="crates/executors/src/actions/coding_agent_follow_up.rs">
use std::path::Path;

use async_trait::async_trait;
use command_group::AsyncGroupChild;
use serde::{Deserialize, Serialize};
use ts_rs::TS;

use crate::{
    actions::Executable,
    executors::{ExecutorError, StandardCodingAgentExecutor},
    profile::{ExecutorConfigs, ExecutorProfileId},
};

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS)]
pub struct CodingAgentFollowUpRequest {
    pub prompt: String,
    pub session_id: String,
    /// Executor profile specification
    #[serde(alias = "profile_variant_label")]
    // Backwards compatability with ProfileVariantIds, esp stored in DB under ExecutorAction
    pub executor_profile_id: ExecutorProfileId,
}

impl CodingAgentFollowUpRequest {
    /// Get the executor profile ID
    pub fn get_executor_profile_id(&self) -> ExecutorProfileId {
        self.executor_profile_id.clone()
    }
}

#[async_trait]
impl Executable for CodingAgentFollowUpRequest {
    async fn spawn(&self, current_dir: &Path) -> Result<AsyncGroupChild, ExecutorError> {
        let executor_profile_id = self.get_executor_profile_id();
        let agent = ExecutorConfigs::get_cached()
            .get_coding_agent(&executor_profile_id)
            .ok_or(ExecutorError::UnknownExecutorType(
                executor_profile_id.to_string(),
            ))?;

        agent
            .spawn_follow_up(current_dir, &self.prompt, &self.session_id)
            .await
    }
}
</file>

<file path="crates/executors/src/actions/coding_agent_initial.rs">
use std::path::Path;

use async_trait::async_trait;
use command_group::AsyncGroupChild;
use serde::{Deserialize, Serialize};
use ts_rs::TS;

use crate::{
    actions::Executable,
    executors::{ExecutorError, StandardCodingAgentExecutor},
    profile::{ExecutorConfigs, ExecutorProfileId},
};

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS)]
pub struct CodingAgentInitialRequest {
    pub prompt: String,
    /// Executor profile specification
    #[serde(alias = "profile_variant_label")]
    // Backwards compatability with ProfileVariantIds, esp stored in DB under ExecutorAction
    pub executor_profile_id: ExecutorProfileId,
}

#[async_trait]
impl Executable for CodingAgentInitialRequest {
    async fn spawn(&self, current_dir: &Path) -> Result<AsyncGroupChild, ExecutorError> {
        let executor_profile_id = self.executor_profile_id.clone();
        let agent = ExecutorConfigs::get_cached()
            .get_coding_agent(&executor_profile_id)
            .ok_or(ExecutorError::UnknownExecutorType(
                executor_profile_id.to_string(),
            ))?;

        agent.spawn(current_dir, &self.prompt).await
    }
}
</file>

<file path="crates/executors/src/actions/mod.rs">
use std::path::Path;

use async_trait::async_trait;
use command_group::AsyncGroupChild;
use enum_dispatch::enum_dispatch;
use serde::{Deserialize, Serialize};
use ts_rs::TS;

use crate::{
    actions::{
        coding_agent_follow_up::CodingAgentFollowUpRequest,
        coding_agent_initial::CodingAgentInitialRequest, script::ScriptRequest,
    },
    executors::ExecutorError,
};
pub mod coding_agent_follow_up;
pub mod coding_agent_initial;
pub mod script;

#[enum_dispatch]
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS)]
#[serde(tag = "type")]
pub enum ExecutorActionType {
    CodingAgentInitialRequest,
    CodingAgentFollowUpRequest,
    ScriptRequest,
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct ExecutorAction {
    pub typ: ExecutorActionType,
    pub next_action: Option<Box<ExecutorAction>>,
}

impl ExecutorAction {
    pub fn new(typ: ExecutorActionType, next_action: Option<Box<ExecutorAction>>) -> Self {
        Self { typ, next_action }
    }

    pub fn typ(&self) -> &ExecutorActionType {
        &self.typ
    }

    pub fn next_action(&self) -> Option<&ExecutorAction> {
        self.next_action.as_deref()
    }
}

#[async_trait]
#[enum_dispatch(ExecutorActionType)]
pub trait Executable {
    async fn spawn(&self, current_dir: &Path) -> Result<AsyncGroupChild, ExecutorError>;
}

#[async_trait]
impl Executable for ExecutorAction {
    async fn spawn(&self, current_dir: &Path) -> Result<AsyncGroupChild, ExecutorError> {
        self.typ.spawn(current_dir).await
    }
}
</file>

<file path="crates/executors/src/actions/script.rs">
use std::path::Path;

use async_trait::async_trait;
use command_group::{AsyncCommandGroup, AsyncGroupChild};
use serde::{Deserialize, Serialize};
use tokio::process::Command;
use ts_rs::TS;
use utils::shell::get_shell_command;

use crate::{actions::Executable, executors::ExecutorError};

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS)]
pub enum ScriptRequestLanguage {
    Bash,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS)]
pub enum ScriptContext {
    SetupScript,
    CleanupScript,
    DevServer,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS)]
pub struct ScriptRequest {
    pub script: String,
    pub language: ScriptRequestLanguage,
    pub context: ScriptContext,
}

#[async_trait]
impl Executable for ScriptRequest {
    async fn spawn(&self, current_dir: &Path) -> Result<AsyncGroupChild, ExecutorError> {
        let (shell_cmd, shell_arg) = get_shell_command();
        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdout(std::process::Stdio::piped())
            .stderr(std::process::Stdio::piped())
            .arg(shell_arg)
            .arg(&self.script)
            .current_dir(current_dir);

        let child = command.group_spawn()?;

        Ok(child)
    }
}
</file>

<file path="crates/executors/src/executors/codex/session.rs">
use std::{path::PathBuf, sync::Arc};

use futures::StreamExt;
use regex::Regex;
use utils::msg_store::MsgStore;

/// Handles session management for Codex
pub struct SessionHandler;

impl SessionHandler {
    /// Start monitoring stderr lines for session ID extraction
    pub fn start_session_id_extraction(msg_store: Arc<MsgStore>) {
        tokio::spawn(async move {
            let mut stderr_lines_stream = msg_store.stderr_lines_stream();

            while let Some(Ok(line)) = stderr_lines_stream.next().await {
                if let Some(session_id) = Self::extract_session_id_from_line(&line) {
                    msg_store.push_session_id(session_id);
                }
            }
        });
    }

    /// Extract session ID from codex stderr output. Supports:
    /// - Old:  session_id: <uuid>
    /// - New:  session_id: ConversationId(<uuid>)
    pub fn extract_session_id_from_line(line: &str) -> Option<String> {
        static SESSION_ID_REGEX: std::sync::OnceLock<Regex> = std::sync::OnceLock::new();
        let regex = SESSION_ID_REGEX.get_or_init(|| {
            Regex::new(r"session_id:\s*(?:ConversationId\()?(?P<id>[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})\)?").unwrap()
        });

        regex
            .captures(line)
            .and_then(|cap| cap.name("id"))
            .map(|m| m.as_str().to_string())
    }

    /// Find codex rollout file path for given session_id. Used during follow-up execution.
    pub fn find_rollout_file_path(session_id: &str) -> Result<PathBuf, String> {
        let home_dir = dirs::home_dir().ok_or("Could not determine home directory")?;
        let sessions_dir = home_dir.join(".codex").join("sessions");

        // Scan the sessions directory recursively for rollout files matching the session_id
        // Pattern: rollout-{YYYY}-{MM}-{DD}T{HH}-{mm}-{ss}-{session_id}.jsonl
        Self::scan_directory(&sessions_dir, session_id)
    }

    // Recursively scan directory for rollout files matching the session_id
    fn scan_directory(dir: &PathBuf, session_id: &str) -> Result<PathBuf, String> {
        if !dir.exists() {
            return Err(format!(
                "Sessions directory does not exist: {}",
                dir.display()
            ));
        }

        let entries = std::fs::read_dir(dir)
            .map_err(|e| format!("Failed to read directory {}: {}", dir.display(), e))?;

        for entry in entries {
            let entry = entry.map_err(|e| format!("Failed to read directory entry: {e}"))?;
            let path = entry.path();

            if path.is_dir() {
                // Recursively search subdirectories
                if let Ok(found) = Self::scan_directory(&path, session_id) {
                    return Ok(found);
                }
            } else if path.is_file()
                && let Some(filename) = path.file_name()
                && let Some(filename_str) = filename.to_str()
                && filename_str.contains(session_id)
                && filename_str.starts_with("rollout-")
                && filename_str.ends_with(".jsonl")
            {
                return Ok(path);
            }
        }

        Err(format!(
            "Could not find rollout file for session_id: {session_id}"
        ))
    }

    /// Fork a Codex rollout file by copying it to a temp location and assigning a new session id.
    /// Returns (new_rollout_path, new_session_id).
    ///
    /// Migration behavior:
    /// - If the original header is old format, it is converted to new format on write.
    /// - Subsequent lines:
    ///   - If already new RolloutLine, pass through unchanged.
    ///   - If object contains "record_type", skip it (ignored in old impl).
    ///   - Otherwise, wrap as RolloutLine of type "response_item" with payload = original JSON.
    pub fn fork_rollout_file(session_id: &str) -> Result<(PathBuf, String), String> {
        use std::io::{BufRead, BufReader, Write};

        let original = Self::find_rollout_file_path(session_id)?;

        let file = std::fs::File::open(&original)
            .map_err(|e| format!("Failed to open rollout file {}: {e}", original.display()))?;
        let mut reader = BufReader::new(file);

        let mut first_line = String::new();
        reader
            .read_line(&mut first_line)
            .map_err(|e| format!("Failed to read first line from {}: {e}", original.display()))?;

        let mut meta: serde_json::Value = serde_json::from_str(first_line.trim()).map_err(|e| {
            format!(
                "Failed to parse first line JSON in {}: {e}",
                original.display()
            )
        })?;

        // Generate new UUID for forked session
        let new_id = uuid::Uuid::new_v4().to_string();
        Self::set_session_id_in_rollout_meta(&mut meta, &new_id)?;

        // Prepare destination path in the same directory, following Codex rollout naming convention.
        // Always create a fresh filename: rollout-<YYYY>-<MM>-<DD>T<HH>-<mm>-<ss>-<session_id>.jsonl
        let parent_dir = original
            .parent()
            .ok_or_else(|| format!("Unexpected path with no parent: {}", original.display()))?;
        let new_filename = Self::new_rollout_filename(&new_id);
        let dest = parent_dir.join(new_filename);

        // Write new file with modified first line and copy the rest with migration as needed
        let mut writer = std::fs::File::create(&dest)
            .map_err(|e| format!("Failed to create forked rollout {}: {e}", dest.display()))?;
        let meta_line = serde_json::to_string(&meta)
            .map_err(|e| format!("Failed to serialize modified meta: {e}"))?;
        writeln!(writer, "{meta_line}")
            .map_err(|e| format!("Failed to write meta to {}: {e}", dest.display()))?;

        // Wrap subsequent lines
        for line in reader.lines() {
            let line =
                line.map_err(|e| format!("I/O error reading {}: {e}", original.display()))?;
            let trimmed = line.trim();
            if trimmed.is_empty() {
                continue;
            }

            // Try parse as JSON
            let parsed: Result<serde_json::Value, _> = serde_json::from_str(trimmed);
            let value = match parsed {
                Ok(v) => v,
                Err(_) => {
                    // Skip invalid JSON lines during migration
                    continue;
                }
            };

            // If already a RolloutLine (has timestamp + type/payload or flattened item), pass through
            let is_rollout_line = value.get("timestamp").is_some()
                && (value.get("type").is_some() || value.get("payload").is_some());
            if is_rollout_line {
                writeln!(writer, "{value}")
                    .map_err(|e| format!("Failed to write to {}: {e}", dest.display()))?;
                continue;
            }

            // Ignore legacy bookkeeping lines like {"record_type": ...}
            if value.get("record_type").is_some() {
                continue;
            }

            // Otherwise, wrap as a new RolloutLine containing a ResponseItem payload
            let timestamp = chrono::Utc::now()
                .format("%Y-%m-%dT%H:%M:%S%.3fZ")
                .to_string();
            let envelope = serde_json::json!({
                "timestamp": timestamp,
                "type": "response_item",
                "payload": value,
            });
            writeln!(writer, "{envelope}")
                .map_err(|e| format!("Failed to write to {}: {e}", dest.display()))?;
        }

        Ok((dest, new_id))
    }

    // Update session id inside the first-line JSON meta, supporting both old and new formats.
    // - Old format: top-level { "id": "<uuid>", ... } -> convert to new format
    // - New format: { "type": "session_meta", "payload": { "id": "<uuid>", ... }, ... }
    // If both are somehow present, new format takes precedence.
    pub(crate) fn set_session_id_in_rollout_meta(
        meta: &mut serde_json::Value,
        new_id: &str,
    ) -> Result<(), String> {
        match meta {
            serde_json::Value::Object(map) => {
                // If already new format, update payload.id and return
                if let Some(serde_json::Value::Object(payload)) = map.get_mut("payload") {
                    payload.insert(
                        "id".to_string(),
                        serde_json::Value::String(new_id.to_string()),
                    );
                    return Ok(());
                }

                // Convert old format to new format header
                let top_timestamp = map.get("timestamp").cloned();
                let instructions = map.get("instructions").cloned();
                let git = map.get("git").cloned();

                let mut new_top = serde_json::Map::new();
                if let Some(ts) = top_timestamp.clone() {
                    new_top.insert("timestamp".to_string(), ts);
                }
                new_top.insert(
                    "type".to_string(),
                    serde_json::Value::String("session_meta".to_string()),
                );

                let mut payload = serde_json::Map::new();
                payload.insert(
                    "id".to_string(),
                    serde_json::Value::String(new_id.to_string()),
                );
                if let Some(ts) = top_timestamp {
                    payload.insert("timestamp".to_string(), ts);
                }
                if let Some(instr) = instructions {
                    payload.insert("instructions".to_string(), instr);
                }
                if let Some(git_val) = git {
                    payload.insert("git".to_string(), git_val);
                }
                // Required fields in new format: cwd, originator, cli_version
                if !payload.contains_key("cwd") {
                    payload.insert(
                        "cwd".to_string(),
                        serde_json::Value::String(".".to_string()),
                    );
                }
                if !payload.contains_key("originator") {
                    payload.insert(
                        "originator".to_string(),
                        serde_json::Value::String("vibe_kanban_migrated".to_string()),
                    );
                }
                if !payload.contains_key("cli_version") {
                    payload.insert(
                        "cli_version".to_string(),
                        serde_json::Value::String("0.0.0-migrated".to_string()),
                    );
                }

                new_top.insert("payload".to_string(), serde_json::Value::Object(payload));

                *map = new_top; // replace the old map with the new-format one
                Ok(())
            }
            _ => Err("First line of rollout file is not a JSON object".to_string()),
        }
    }

    // Build a new rollout filename, ignoring any original name.
    // Always returns: rollout-<timestamp>-<id>.jsonl
    fn new_rollout_filename(new_id: &str) -> String {
        let now_ts = chrono::Local::now().format("%Y-%m-%dT%H-%M-%S").to_string();
        format!("rollout-{now_ts}-{new_id}.jsonl")
    }
}

#[cfg(test)]
mod tests {
    use super::SessionHandler;

    #[test]
    fn test_new_rollout_filename_pattern() {
        let id = "ID-123";
        let out = SessionHandler::new_rollout_filename(id);
        // rollout-YYYY-MM-DDTHH-MM-SS-ID-123.jsonl
        let re = regex::Regex::new(r"^rollout-\d{4}-\d{2}-\d{2}T\d{2}-\d{2}-\d{2}-ID-123\.jsonl$")
            .unwrap();
        assert!(re.is_match(&out), "Unexpected filename: {out}");
    }
}
</file>

<file path="crates/executors/src/executors/opencode/share_bridge.rs">
use std::{collections::HashMap, net::SocketAddr, sync::Arc};

use axum::{
    Json, Router, body::Bytes, extract::State, http::StatusCode, response::IntoResponse,
    routing::post,
};
use serde::{Deserialize, Serialize};
use tokio::{
    net::TcpListener,
    sync::{Mutex, RwLock, broadcast},
    task::JoinHandle,
};

/// Minimal subset of OpenCode share API that we need to ingest structured events locally.
///
/// We run a lightweight HTTP server on 127.0.0.1 with an ephemeral port and point
/// OpenCode to it by setting OPENCODE_API and enabling auto-share. The CLI then POSTs
/// tool/message updates to /share_sync which we rebroadcast to interested consumers.

#[derive(Debug)]
pub struct Bridge {
    pub base_url: String,
    tx: broadcast::Sender<ShareEvent>,
    #[allow(dead_code)]
    secrets: Arc<RwLock<HashMap<String, String>>>,
    shutdown_tx: Arc<Mutex<Option<tokio::sync::oneshot::Sender<()>>>>,
    _server_task: JoinHandle<()>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ShareCreateReq {
    #[serde(rename = "sessionID")]
    pub session_id: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ShareCreateResp {
    pub url: String,
    pub secret: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ShareSyncReq {
    #[serde(rename = "sessionID")]
    pub session_id: String,
    pub secret: String,
    pub key: String,
    pub content: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmptyResp {}

#[derive(Debug, Clone)]
pub enum ShareEvent {
    Sync(ShareSyncReq),
}

#[derive(Clone)]
struct AppState {
    base_url: String,
    tx: broadcast::Sender<ShareEvent>,
    secrets: Arc<RwLock<HashMap<String, String>>>,
}

impl Bridge {
    /// Start a new, isolated bridge server bound to localhost on an ephemeral port.
    pub async fn start() -> std::io::Result<Arc<Bridge>> {
        let (tx, _rx) = broadcast::channel(10_000);
        let secrets = Arc::new(RwLock::new(HashMap::new()));

        // Bind to localhost:0 to get an ephemeral port
        let listener = TcpListener::bind((std::net::Ipv4Addr::LOCALHOST, 0)).await?;
        let addr: SocketAddr = listener.local_addr()?;
        let base_url = format!("http://{}:{}", addr.ip(), addr.port());
        tracing::debug!(
            "OpenCode share bridge started: base_url={}, port={}",
            base_url,
            addr.port()
        );

        let (shutdown_tx, shutdown_rx) = tokio::sync::oneshot::channel::<()>();
        let shutdown_tx = Arc::new(Mutex::new(Some(shutdown_tx)));

        let app_state = AppState {
            base_url: base_url.clone(),
            tx: tx.clone(),
            secrets: secrets.clone(),
        };

        let server_task = tokio::spawn(async move {
            let app = Router::new()
                .route("/share_create", post(share_create))
                .route("/share_delete", post(share_delete))
                .route("/share_sync", post(share_sync))
                .with_state(app_state);

            // Serve with graceful shutdown
            if let Err(e) = axum::serve(listener, app)
                .with_graceful_shutdown(async move {
                    // wait for shutdown signal
                    let _ = shutdown_rx.await;
                })
                .await
            {
                tracing::error!("opencode share bridge server error: {}", e);
            }
        });

        Ok(Arc::new(Bridge {
            base_url,
            tx,
            secrets,
            shutdown_tx,
            _server_task: server_task,
        }))
    }

    /// Subscribe to events from this bridge instance.
    pub fn subscribe(&self) -> broadcast::Receiver<ShareEvent> {
        self.tx.subscribe()
    }

    /// Trigger graceful shutdown of this bridge server.
    pub async fn shutdown(&self) {
        tracing::debug!("Shutting down OpenCode share bridge: {}", self.base_url);
        if let Some(tx) = self.shutdown_tx.lock().await.take() {
            let _ = tx.send(());
        }
    }
}

async fn share_create(State(state): State<AppState>, body: Bytes) -> impl IntoResponse {
    // accept JSON regardless of content-type
    let payload: ShareCreateReq = match serde_json::from_slice(&body) {
        Ok(v) => v,
        Err(_) => ShareCreateReq {
            session_id: "".into(),
        },
    };
    // Generate a simple secret and store against session id
    let secret = uuid::Uuid::new_v4().to_string();
    {
        let mut map = state.secrets.write().await;
        map.insert(payload.session_id.clone(), secret.clone());
    }
    (
        StatusCode::OK,
        Json(ShareCreateResp {
            secret,
            url: format!("{}/s/{}", state.base_url, short(&payload.session_id)),
        }),
    )
}

async fn share_delete(_state: State<AppState>, _body: Bytes) -> impl IntoResponse {
    (StatusCode::OK, Json(EmptyResp {}))
}

async fn share_sync(State(state): State<AppState>, body: Bytes) -> impl IntoResponse {
    let payload: ShareSyncReq = match serde_json::from_slice(&body) {
        Ok(v) => v,
        Err(_) => {
            return (StatusCode::BAD_REQUEST, Json(EmptyResp {}));
        }
    };
    // Validate secret (best-effort)
    let ok = {
        let map = state.secrets.read().await;
        map.get(&payload.session_id)
            .map(|expected| expected == &payload.secret)
            .unwrap_or(false)
    };

    if !ok {
        // Still emit for debugging but warn
        tracing::debug!(
            "share_sync with invalid secret for session {}",
            payload.session_id
        );
    }

    // Broadcast event
    let _ = state.tx.send(ShareEvent::Sync(payload));
    (StatusCode::OK, Json(EmptyResp {}))
}

fn short(id: &str) -> String {
    id.chars()
        .rev()
        .take(8)
        .collect::<String>()
        .chars()
        .rev()
        .collect()
}
</file>

<file path="crates/executors/src/executors/amp.rs">
use std::{path::Path, process::Stdio, sync::Arc};

use async_trait::async_trait;
use command_group::{AsyncCommandGroup, AsyncGroupChild};
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use tokio::{io::AsyncWriteExt, process::Command};
use ts_rs::TS;
use utils::{msg_store::MsgStore, shell::get_shell_command};

use crate::{
    command::{CmdOverrides, CommandBuilder, apply_overrides},
    executors::{
        AppendPrompt, ExecutorError, StandardCodingAgentExecutor,
        claude::{ClaudeLogProcessor, HistoryStrategy},
    },
    logs::{stderr_processor::normalize_stderr_logs, utils::EntryIndexProvider},
};

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema)]
pub struct Amp {
    #[serde(default)]
    pub append_prompt: AppendPrompt,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    #[schemars(
        title = "Dangerously Allow All",
        description = "Allow all commands to be executed, even if they are not safe."
    )]
    pub dangerously_allow_all: Option<bool>,
    #[serde(flatten)]
    pub cmd: CmdOverrides,
}

impl Amp {
    fn build_command_builder(&self) -> CommandBuilder {
        let mut builder = CommandBuilder::new("npx -y @sourcegraph/amp@latest")
            .params(["--execute", "--stream-json"]);
        if self.dangerously_allow_all.unwrap_or(false) {
            builder = builder.extend_params(["--dangerously-allow-all"]);
        }
        apply_overrides(builder, &self.cmd)
    }
}

#[async_trait]
impl StandardCodingAgentExecutor for Amp {
    async fn spawn(
        &self,
        current_dir: &Path,
        prompt: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        let (shell_cmd, shell_arg) = get_shell_command();
        let amp_command = self.build_command_builder().build_initial();

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(&amp_command);

        let mut child = command.group_spawn()?;

        // Feed the prompt in, then close the pipe so amp sees EOF
        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        Ok(child)
    }

    async fn spawn_follow_up(
        &self,
        current_dir: &Path,
        prompt: &str,
        session_id: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        // Use shell command for cross-platform compatibility
        let (shell_cmd, shell_arg) = get_shell_command();

        // 1) Fork the thread synchronously to obtain new thread id
        let fork_cmd = self.build_command_builder().build_follow_up(&[
            "threads".to_string(),
            "fork".to_string(),
            session_id.to_string(),
        ]);
        let fork_output = Command::new(shell_cmd)
            .kill_on_drop(true)
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(&fork_cmd)
            .output()
            .await?;
        let stdout_str = String::from_utf8_lossy(&fork_output.stdout);
        let new_thread_id = stdout_str
            .lines()
            .rev()
            .find(|l| !l.trim().is_empty())
            .unwrap_or("")
            .trim()
            .to_string();
        if new_thread_id.is_empty() {
            return Err(ExecutorError::Io(std::io::Error::other(
                "AMP threads fork did not return a thread id",
            )));
        }

        tracing::debug!("AMP threads fork -> new thread id: {}", new_thread_id);

        // 2) Continue using the new thread id
        let continue_cmd = self.build_command_builder().build_follow_up(&[
            "threads".to_string(),
            "continue".to_string(),
            new_thread_id.clone(),
        ]);

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(&continue_cmd);

        let mut child = command.group_spawn()?;

        // Feed the prompt in, then close the pipe so amp sees EOF
        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        Ok(child)
    }

    fn normalize_logs(&self, msg_store: Arc<MsgStore>, current_dir: &Path) {
        let entry_index_provider = EntryIndexProvider::start_from(&msg_store);

        // Process stdout logs (Amp's stream JSON output) using Claude's log processor
        ClaudeLogProcessor::process_logs(
            msg_store.clone(),
            current_dir,
            entry_index_provider.clone(),
            HistoryStrategy::AmpResume,
        );

        // Process stderr logs using the standard stderr processor
        normalize_stderr_logs(msg_store, entry_index_provider);
    }

    // MCP configuration methods
    fn default_mcp_config_path(&self) -> Option<std::path::PathBuf> {
        dirs::home_dir().map(|home| home.join(".config").join("amp").join("settings.json"))
    }
}
</file>

<file path="crates/executors/src/executors/claude.rs">
use std::{path::Path, process::Stdio, sync::Arc};

use async_trait::async_trait;
use command_group::{AsyncCommandGroup, AsyncGroupChild};
use futures::StreamExt;
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use tokio::{io::AsyncWriteExt, process::Command};
use ts_rs::TS;
use utils::{
    diff::{concatenate_diff_hunks, create_unified_diff, create_unified_diff_hunk},
    log_msg::LogMsg,
    msg_store::MsgStore,
    path::make_path_relative,
    shell::get_shell_command,
};

use crate::{
    command::{CmdOverrides, CommandBuilder, apply_overrides},
    executors::{AppendPrompt, ExecutorError, StandardCodingAgentExecutor},
    logs::{
        ActionType, FileChange, NormalizedEntry, NormalizedEntryType, TodoItem,
        stderr_processor::normalize_stderr_logs,
        utils::{EntryIndexProvider, patch::ConversationPatch},
    },
};

fn base_command(claude_code_router: bool) -> &'static str {
    if claude_code_router {
        "npx -y @musistudio/claude-code-router code"
    } else {
        "npx -y @anthropic-ai/claude-code@latest"
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema)]
pub struct ClaudeCode {
    #[serde(default)]
    pub append_prompt: AppendPrompt,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub claude_code_router: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub plan: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub model: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub dangerously_skip_permissions: Option<bool>,
    #[serde(flatten)]
    pub cmd: CmdOverrides,
}

impl ClaudeCode {
    fn build_command_builder(&self) -> CommandBuilder {
        // If base_command_override is provided and claude_code_router is also set, log a warning
        if self.cmd.base_command_override.is_some() && self.claude_code_router.is_some() {
            tracing::warn!(
                "base_command_override is set, this will override the claude_code_router setting"
            );
        }

        let mut builder =
            CommandBuilder::new(base_command(self.claude_code_router.unwrap_or(false)))
                .params(["-p"]);

        if self.plan.unwrap_or(false) {
            builder = builder.extend_params(["--permission-mode=plan"]);
        }
        if self.dangerously_skip_permissions.unwrap_or(false) {
            builder = builder.extend_params(["--dangerously-skip-permissions"]);
        }
        if let Some(model) = &self.model {
            builder = builder.extend_params(["--model", model]);
        }
        builder = builder.extend_params(["--verbose", "--output-format=stream-json"]);

        apply_overrides(builder, &self.cmd)
    }
}

#[async_trait]
impl StandardCodingAgentExecutor for ClaudeCode {
    async fn spawn(
        &self,
        current_dir: &Path,
        prompt: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        let (shell_cmd, shell_arg) = get_shell_command();
        let command_builder = self.build_command_builder();
        let base_command = command_builder.build_initial();
        let claude_command = if self.plan.unwrap_or(false) {
            create_watchkill_script(&base_command)
        } else {
            base_command
        };

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(&claude_command);

        let mut child = command.group_spawn()?;

        // Feed the prompt in, then close the pipe so Claude sees EOF
        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        Ok(child)
    }

    async fn spawn_follow_up(
        &self,
        current_dir: &Path,
        prompt: &str,
        session_id: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        let (shell_cmd, shell_arg) = get_shell_command();
        let command_builder = self.build_command_builder();
        // Build follow-up command with --resume {session_id}
        let base_command =
            command_builder.build_follow_up(&["--resume".to_string(), session_id.to_string()]);
        let claude_command = if self.plan.unwrap_or(false) {
            create_watchkill_script(&base_command)
        } else {
            base_command
        };

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(&claude_command);

        let mut child = command.group_spawn()?;

        // Feed the followup prompt in, then close the pipe
        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        Ok(child)
    }

    fn normalize_logs(&self, msg_store: Arc<MsgStore>, current_dir: &Path) {
        let entry_index_provider = EntryIndexProvider::start_from(&msg_store);

        // Process stdout logs (Claude's JSON output)
        ClaudeLogProcessor::process_logs(
            msg_store.clone(),
            current_dir,
            entry_index_provider.clone(),
            HistoryStrategy::Default,
        );

        // Process stderr logs using the standard stderr processor
        normalize_stderr_logs(msg_store, entry_index_provider);
    }

    // MCP configuration methods
    fn default_mcp_config_path(&self) -> Option<std::path::PathBuf> {
        dirs::home_dir().map(|home| home.join(".claude.json"))
    }
}

fn create_watchkill_script(command: &str) -> String {
    let claude_plan_stop_indicator = concat!("Exit ", "plan mode?");
    format!(
        r#"#!/usr/bin/env bash
set -euo pipefail

word="{claude_plan_stop_indicator}"
command="{command}"

exit_code=0
while IFS= read -r line; do
    printf '%s\n' "$line"
    if [[ $line == *"$word"* ]]; then
        exit 0
    fi
done < <($command <&0 2>&1)

exit_code=${{PIPESTATUS[0]}}
exit "$exit_code"
"#
    )
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum HistoryStrategy {
    // Claude-code format
    Default,
    // Amp threads format which includes logs from previous executions
    AmpResume,
}

/// Handles log processing and interpretation for Claude executor
pub struct ClaudeLogProcessor {
    model_name: Option<String>,
    // Map tool_use_id -> structured info for follow-up ToolResult replacement
    tool_map: std::collections::HashMap<String, ClaudeToolCallInfo>,
    // Strategy controlling how to handle history and user messages
    strategy: HistoryStrategy,
}

impl ClaudeLogProcessor {
    #[cfg(test)]
    fn new() -> Self {
        Self::new_with_strategy(HistoryStrategy::Default)
    }

    fn new_with_strategy(strategy: HistoryStrategy) -> Self {
        Self {
            model_name: None,
            tool_map: std::collections::HashMap::new(),
            strategy,
        }
    }

    /// Process raw logs and convert them to normalized entries with patches
    pub fn process_logs(
        msg_store: Arc<MsgStore>,
        current_dir: &Path,
        entry_index_provider: EntryIndexProvider,
        strategy: HistoryStrategy,
    ) {
        let current_dir_clone = current_dir.to_owned();
        tokio::spawn(async move {
            let mut stream = msg_store.history_plus_stream();
            let mut buffer = String::new();
            let worktree_path = current_dir_clone.to_string_lossy().to_string();
            let mut session_id_extracted = false;
            let mut processor = Self::new_with_strategy(strategy);

            while let Some(Ok(msg)) = stream.next().await {
                let chunk = match msg {
                    LogMsg::Stdout(x) => x,
                    LogMsg::JsonPatch(_) | LogMsg::SessionId(_) | LogMsg::Stderr(_) => continue,
                    LogMsg::Finished => break,
                };

                buffer.push_str(&chunk);

                // Process complete JSON lines
                for line in buffer
                    .split_inclusive('\n')
                    .filter(|l| l.ends_with('\n'))
                    .map(str::to_owned)
                    .collect::<Vec<_>>()
                {
                    let trimmed = line.trim();
                    if trimmed.is_empty() {
                        continue;
                    }

                    // Filter out claude-code-router service messages
                    if trimmed.starts_with("Service not running, starting service")
                        || trimmed
                            .contains("claude code router service has been successfully stopped")
                    {
                        continue;
                    }

                    match serde_json::from_str::<ClaudeJson>(trimmed) {
                        Ok(claude_json) => {
                            // Extract session ID if present
                            if !session_id_extracted
                                && let Some(session_id) = Self::extract_session_id(&claude_json)
                            {
                                msg_store.push_session_id(session_id);
                                session_id_extracted = true;
                            }

                            // Special handling to capture tool_use ids and replace with results later
                            match &claude_json {
                                ClaudeJson::Assistant { message, .. } => {
                                    // Inject system init with model if first time
                                    if processor.model_name.is_none()
                                        && let Some(model) = message.model.as_ref()
                                    {
                                        processor.model_name = Some(model.clone());
                                        let entry = NormalizedEntry {
                                            timestamp: None,
                                            entry_type: NormalizedEntryType::SystemMessage,
                                            content: format!(
                                                "System initialized with model: {model}"
                                            ),
                                            metadata: None,
                                        };
                                        let id = entry_index_provider.next();
                                        msg_store.push_patch(
                                            ConversationPatch::add_normalized_entry(id, entry),
                                        );
                                    }

                                    for item in &message.content {
                                        match item {
                                            ClaudeContentItem::ToolUse { id, tool_data } => {
                                                let tool_name = tool_data.get_name().to_string();
                                                let action_type = Self::extract_action_type(
                                                    tool_data,
                                                    &worktree_path,
                                                );
                                                let content_text = Self::generate_concise_content(
                                                    tool_data,
                                                    &action_type,
                                                    &worktree_path,
                                                );
                                                let entry = NormalizedEntry {
                                                    timestamp: None,
                                                    entry_type: NormalizedEntryType::ToolUse {
                                                        tool_name: tool_name.clone(),
                                                        action_type,
                                                    },
                                                    content: content_text.clone(),
                                                    metadata: Some(
                                                        serde_json::to_value(item)
                                                            .unwrap_or(serde_json::Value::Null),
                                                    ),
                                                };
                                                let id_num = entry_index_provider.next();
                                                processor.tool_map.insert(
                                                    id.clone(),
                                                    ClaudeToolCallInfo {
                                                        entry_index: id_num,
                                                        tool_name: tool_name.clone(),
                                                        tool_data: tool_data.clone(),
                                                        content: content_text.clone(),
                                                    },
                                                );
                                                msg_store.push_patch(
                                                    ConversationPatch::add_normalized_entry(
                                                        id_num, entry,
                                                    ),
                                                );
                                            }
                                            ClaudeContentItem::Text { .. }
                                            | ClaudeContentItem::Thinking { .. } => {
                                                if let Some(entry) =
                                                    Self::content_item_to_normalized_entry(
                                                        item,
                                                        "assistant",
                                                        &worktree_path,
                                                    )
                                                {
                                                    let id = entry_index_provider.next();
                                                    msg_store.push_patch(
                                                        ConversationPatch::add_normalized_entry(
                                                            id, entry,
                                                        ),
                                                    );
                                                }
                                            }
                                            ClaudeContentItem::ToolResult { .. } => {
                                                // handled via User or Assistant ToolResult messages below
                                            }
                                        }
                                    }
                                }
                                ClaudeJson::User { message, .. } => {
                                    // Amp resume hack: if AmpResume and the user message contains plain text,
                                    // clear all previous entries so UI shows only fresh context, and emit user text.
                                    if matches!(processor.strategy, HistoryStrategy::AmpResume)
                                        && message
                                            .content
                                            .iter()
                                            .any(|c| matches!(c, ClaudeContentItem::Text { .. }))
                                    {
                                        let cur = entry_index_provider.current();
                                        if cur > 0 {
                                            for _ in 0..cur {
                                                msg_store.push_patch(
                                                    ConversationPatch::remove_diff(0.to_string()),
                                                );
                                            }
                                            entry_index_provider.reset();
                                            // Also reset tool map to avoid mismatches with re-streamed tool_use/tool_result ids
                                            processor.tool_map.clear();
                                        }
                                        // Emit user text messages after clearing
                                        for item in &message.content {
                                            if let ClaudeContentItem::Text { text } = item {
                                                let entry = NormalizedEntry {
                                                    timestamp: None,
                                                    entry_type: NormalizedEntryType::UserMessage,
                                                    content: text.clone(),
                                                    metadata: Some(
                                                        serde_json::to_value(item)
                                                            .unwrap_or(serde_json::Value::Null),
                                                    ),
                                                };
                                                let id = entry_index_provider.next();
                                                msg_store.push_patch(
                                                    ConversationPatch::add_normalized_entry(
                                                        id, entry,
                                                    ),
                                                );
                                            }
                                        }
                                    }
                                    for item in &message.content {
                                        if let ClaudeContentItem::ToolResult {
                                            tool_use_id,
                                            content,
                                            is_error,
                                        } = item
                                            && let Some(info) =
                                                processor.tool_map.get(tool_use_id).cloned()
                                        {
                                            let is_command = matches!(
                                                info.tool_data,
                                                ClaudeToolData::Bash { .. }
                                            );
                                            if is_command {
                                                // For bash commands, attach result as CommandRun output where possible
                                                // Prefer parsing Amp's claude-compatible Bash format: {"output":"...","exitCode":0}
                                                let content_str = if let Some(s) = content.as_str()
                                                {
                                                    s.to_string()
                                                } else {
                                                    content.to_string()
                                                };

                                                let result = if let Ok(result) =
                                                    serde_json::from_str::<AmpBashResult>(
                                                        &content_str,
                                                    ) {
                                                    Some(crate::logs::CommandRunResult {

                                                        exit_status : Some(
                                                            crate::logs::CommandExitStatus::ExitCode {
                                                                code: result.exit_code,
                                                            },
                                                        ),
                                                        output: Some(result.output)
                                                    })
                                                } else {
                                                    Some(crate::logs::CommandRunResult {
                                                        exit_status: (*is_error).map(|is_error| {
                                                            crate::logs::CommandExitStatus::Success { success: !is_error }
                                                        }),
                                                        output: Some(content_str)
                                                    })
                                                };

                                                let entry = NormalizedEntry {
                                                    timestamp: None,
                                                    entry_type: NormalizedEntryType::ToolUse {
                                                        tool_name: info.tool_name.clone(),
                                                        action_type: ActionType::CommandRun {
                                                            command: info.content.clone(),
                                                            result,
                                                        },
                                                    },
                                                    content: info.content.clone(),
                                                    metadata: None,
                                                };
                                                msg_store.push_patch(ConversationPatch::replace(
                                                    info.entry_index,
                                                    entry,
                                                ));
                                            } else {
                                                // Show args and results for NotebookEdit and MCP tools
                                                let tool_name =
                                                    info.tool_data.get_name().to_string();
                                                if matches!(
                                                    info.tool_data,
                                                    ClaudeToolData::Unknown { .. }
                                                        | ClaudeToolData::Oracle { .. }
                                                        | ClaudeToolData::Mermaid { .. }
                                                        | ClaudeToolData::CodebaseSearchAgent { .. }
                                                        | ClaudeToolData::NotebookEdit { .. }
                                                ) {
                                                    let (res_type, res_value) =
                                                        Self::normalize_claude_tool_result_value(
                                                            content,
                                                        );

                                                    // Arguments: prefer input for MCP unknown, else full struct
                                                    // Arguments: prefer `input` field if present, derived from tool_data
                                                    let args_to_show =
                                                        serde_json::to_value(&info.tool_data)
                                                            .ok()
                                                            .and_then(|v| {
                                                                serde_json::from_value::<
                                                                    ClaudeToolWithInput,
                                                                >(
                                                                    v
                                                                )
                                                                .ok()
                                                            })
                                                            .map(|w| w.input)
                                                            .unwrap_or(serde_json::Value::Null);

                                                    // Normalize MCP label
                                                    let is_mcp = tool_name.starts_with("mcp__");
                                                    let label = if is_mcp {
                                                        let parts: Vec<&str> =
                                                            tool_name.split("__").collect();
                                                        if parts.len() >= 3 {
                                                            format!("mcp:{}:{}", parts[1], parts[2])
                                                        } else {
                                                            tool_name.clone()
                                                        }
                                                    } else {
                                                        tool_name.clone()
                                                    };

                                                    let entry = NormalizedEntry {
                                                        timestamp: None,
                                                        entry_type: NormalizedEntryType::ToolUse {
                                                            tool_name: label.clone(),
                                                            action_type: ActionType::Tool {
                                                                tool_name: label,
                                                                arguments: Some(args_to_show),
                                                                result: Some(
                                                                    crate::logs::ToolResult {
                                                                        r#type: res_type,
                                                                        value: res_value,
                                                                    },
                                                                ),
                                                            },
                                                        },
                                                        content: info.content.clone(),
                                                        metadata: None,
                                                    };
                                                    msg_store.push_patch(
                                                        ConversationPatch::replace(
                                                            info.entry_index,
                                                            entry,
                                                        ),
                                                    );
                                                }
                                            }
                                        }
                                    }
                                }
                                _ => {
                                    // Convert to normalized entries and create patches for other kinds
                                    for entry in
                                        processor.normalize_entries(&claude_json, &worktree_path)
                                    {
                                        let patch_id = entry_index_provider.next();
                                        let patch = ConversationPatch::add_normalized_entry(
                                            patch_id, entry,
                                        );
                                        msg_store.push_patch(patch);
                                    }
                                }
                            }
                        }
                        Err(_) => {
                            // Handle non-JSON output as raw system message
                            if !trimmed.is_empty() {
                                let entry = NormalizedEntry {
                                    timestamp: None,
                                    entry_type: NormalizedEntryType::SystemMessage,
                                    content: trimmed.to_string(),
                                    metadata: None,
                                };

                                let patch_id = entry_index_provider.next();
                                let patch =
                                    ConversationPatch::add_normalized_entry(patch_id, entry);
                                msg_store.push_patch(patch);
                            }
                        }
                    }
                }

                // Keep the partial line in the buffer
                buffer = buffer.rsplit('\n').next().unwrap_or("").to_owned();
            }

            // Handle any remaining content in buffer
            if !buffer.trim().is_empty() {
                let entry = NormalizedEntry {
                    timestamp: None,
                    entry_type: NormalizedEntryType::SystemMessage,
                    content: buffer.trim().to_string(),
                    metadata: None,
                };

                let patch_id = entry_index_provider.next();
                let patch = ConversationPatch::add_normalized_entry(patch_id, entry);
                msg_store.push_patch(patch);
            }
        });
    }

    /// Extract session ID from Claude JSON
    fn extract_session_id(claude_json: &ClaudeJson) -> Option<String> {
        match claude_json {
            ClaudeJson::System { session_id, .. } => session_id.clone(),
            ClaudeJson::Assistant { session_id, .. } => session_id.clone(),
            ClaudeJson::User { session_id, .. } => session_id.clone(),
            ClaudeJson::ToolUse { session_id, .. } => session_id.clone(),
            ClaudeJson::ToolResult { session_id, .. } => session_id.clone(),
            ClaudeJson::Result { .. } => None,
            ClaudeJson::Unknown { .. } => None,
        }
    }

    /// Generate warning entry if API key source is ANTHROPIC_API_KEY
    fn warn_if_unmanaged_key(src: &Option<String>) -> Option<NormalizedEntry> {
        match src.as_deref() {
            Some("ANTHROPIC_API_KEY") => {
                tracing::warn!(
                    "ANTHROPIC_API_KEY env variable detected, your Anthropic subscription is not being used"
                );
                Some(NormalizedEntry {
                    timestamp: None,
                    entry_type: NormalizedEntryType::SystemMessage,
                    content: "⚠️ ANTHROPIC_API_KEY env variable detected, your Anthropic subscription is not being used".to_string(),
                    metadata: None,
                })
            }
            _ => None,
        }
    }

    /// Convert Claude JSON to normalized entries
    fn normalize_entries(
        &mut self,
        claude_json: &ClaudeJson,
        worktree_path: &str,
    ) -> Vec<NormalizedEntry> {
        match claude_json {
            ClaudeJson::System {
                subtype,
                api_key_source,
                ..
            } => {
                let mut entries = Vec::new();

                // 1) emit billing warning if required
                if let Some(warning) = Self::warn_if_unmanaged_key(api_key_source) {
                    entries.push(warning);
                }

                // 2) keep the existing behaviour for the normal system message
                match subtype.as_deref() {
                    Some("init") => {
                        // Skip system init messages because it doesn't contain the actual model that will be used in assistant messages in case of claude-code-router.
                        // We'll send system initialized message with first assistant message that has a model field.
                        return entries; // only the warning (if any)
                    }
                    Some(subtype) => {
                        let content = format!("System: {subtype}");
                        entries.push(NormalizedEntry {
                            timestamp: None,
                            entry_type: NormalizedEntryType::SystemMessage,
                            content,
                            metadata: Some(
                                serde_json::to_value(claude_json)
                                    .unwrap_or(serde_json::Value::Null),
                            ),
                        });
                    }
                    None => {
                        let content = "System message".to_string();
                        entries.push(NormalizedEntry {
                            timestamp: None,
                            entry_type: NormalizedEntryType::SystemMessage,
                            content,
                            metadata: Some(
                                serde_json::to_value(claude_json)
                                    .unwrap_or(serde_json::Value::Null),
                            ),
                        });
                    }
                }

                entries
            }
            ClaudeJson::Assistant { message, .. } => {
                let mut entries = Vec::new();

                if self.model_name.is_none()
                    && let Some(model) = message.model.as_ref()
                {
                    self.model_name = Some(model.clone());
                    entries.push(NormalizedEntry {
                        timestamp: None,
                        entry_type: NormalizedEntryType::SystemMessage,
                        content: format!("System initialized with model: {model}"),
                        metadata: None,
                    });
                }

                for content_item in &message.content {
                    if let Some(entry) = Self::content_item_to_normalized_entry(
                        content_item,
                        "assistant",
                        worktree_path,
                    ) {
                        entries.push(entry);
                    }
                }
                entries
            }
            ClaudeJson::User { .. } => {
                vec![]
            }
            ClaudeJson::ToolUse { tool_data, .. } => {
                let tool_name = tool_data.get_name();
                let action_type = Self::extract_action_type(tool_data, worktree_path);
                let content =
                    Self::generate_concise_content(tool_data, &action_type, worktree_path);

                vec![NormalizedEntry {
                    timestamp: None,
                    entry_type: NormalizedEntryType::ToolUse {
                        tool_name: tool_name.to_string(),
                        action_type,
                    },
                    content,
                    metadata: Some(
                        serde_json::to_value(claude_json).unwrap_or(serde_json::Value::Null),
                    ),
                }]
            }
            ClaudeJson::ToolResult { .. } => {
                // TODO: Add proper ToolResult support to NormalizedEntry when the type system supports it
                vec![]
            }
            ClaudeJson::Result { .. } => {
                // Skip result messages
                vec![]
            }
            ClaudeJson::Unknown { data } => {
                vec![NormalizedEntry {
                    timestamp: None,
                    entry_type: NormalizedEntryType::SystemMessage,
                    content: format!(
                        "Unrecognized JSON message: {}",
                        serde_json::to_value(data).unwrap_or_default()
                    ),
                    metadata: None,
                }]
            }
        }
    }

    /// Normalize Claude tool_result content to either Markdown string or parsed JSON.
    /// - If content is a string that parses as JSON, return Json with parsed value.
    /// - If content is a string (non-JSON), return Markdown with the raw string.
    /// - If content is an array of { text: string }, join texts as Markdown.
    /// - Otherwise return Json with the original value.
    fn normalize_claude_tool_result_value(
        content: &serde_json::Value,
    ) -> (crate::logs::ToolResultValueType, serde_json::Value) {
        if let Some(s) = content.as_str() {
            if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(s) {
                return (crate::logs::ToolResultValueType::Json, parsed);
            }
            return (
                crate::logs::ToolResultValueType::Markdown,
                serde_json::Value::String(s.to_string()),
            );
        }

        if let Ok(items) = serde_json::from_value::<Vec<ClaudeToolResultTextItem>>(content.clone())
            && !items.is_empty()
        {
            let joined = items
                .into_iter()
                .map(|i| i.text)
                .collect::<Vec<_>>()
                .join("\n\n");
            if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(&joined) {
                return (crate::logs::ToolResultValueType::Json, parsed);
            }
            return (
                crate::logs::ToolResultValueType::Markdown,
                serde_json::Value::String(joined),
            );
        }

        (crate::logs::ToolResultValueType::Json, content.clone())
    }

    /// Convert Claude content item to normalized entry
    fn content_item_to_normalized_entry(
        content_item: &ClaudeContentItem,
        role: &str,
        worktree_path: &str,
    ) -> Option<NormalizedEntry> {
        match content_item {
            ClaudeContentItem::Text { text } => {
                let entry_type = match role {
                    "assistant" => NormalizedEntryType::AssistantMessage,
                    _ => return None,
                };
                Some(NormalizedEntry {
                    timestamp: None,
                    entry_type,
                    content: text.clone(),
                    metadata: Some(
                        serde_json::to_value(content_item).unwrap_or(serde_json::Value::Null),
                    ),
                })
            }
            ClaudeContentItem::Thinking { thinking } => Some(NormalizedEntry {
                timestamp: None,
                entry_type: NormalizedEntryType::Thinking,
                content: thinking.clone(),
                metadata: Some(
                    serde_json::to_value(content_item).unwrap_or(serde_json::Value::Null),
                ),
            }),
            ClaudeContentItem::ToolUse { tool_data, .. } => {
                let name = tool_data.get_name();
                let action_type = Self::extract_action_type(tool_data, worktree_path);
                let content =
                    Self::generate_concise_content(tool_data, &action_type, worktree_path);

                Some(NormalizedEntry {
                    timestamp: None,
                    entry_type: NormalizedEntryType::ToolUse {
                        tool_name: name.to_string(),
                        action_type,
                    },
                    content,
                    metadata: Some(
                        serde_json::to_value(content_item).unwrap_or(serde_json::Value::Null),
                    ),
                })
            }
            ClaudeContentItem::ToolResult { .. } => {
                // TODO: Add proper ToolResult support to NormalizedEntry when the type system supports it
                None
            }
        }
    }

    /// Extract action type from structured tool data
    fn extract_action_type(tool_data: &ClaudeToolData, worktree_path: &str) -> ActionType {
        match tool_data {
            ClaudeToolData::Read { file_path } => ActionType::FileRead {
                path: make_path_relative(file_path, worktree_path),
            },
            ClaudeToolData::Edit {
                file_path,
                old_string,
                new_string,
            } => {
                let changes = if old_string.is_some() || new_string.is_some() {
                    vec![FileChange::Edit {
                        unified_diff: create_unified_diff(
                            file_path,
                            &old_string.clone().unwrap_or_default(),
                            &new_string.clone().unwrap_or_default(),
                        ),
                        has_line_numbers: false,
                    }]
                } else {
                    vec![]
                };
                ActionType::FileEdit {
                    path: make_path_relative(file_path, worktree_path),
                    changes,
                }
            }
            ClaudeToolData::MultiEdit { file_path, edits } => {
                let hunks: Vec<String> = edits
                    .iter()
                    .filter_map(|edit| {
                        if edit.old_string.is_some() || edit.new_string.is_some() {
                            Some(create_unified_diff_hunk(
                                &edit.old_string.clone().unwrap_or_default(),
                                &edit.new_string.clone().unwrap_or_default(),
                            ))
                        } else {
                            None
                        }
                    })
                    .collect();
                ActionType::FileEdit {
                    path: make_path_relative(file_path, worktree_path),
                    changes: vec![FileChange::Edit {
                        unified_diff: concatenate_diff_hunks(file_path, &hunks),
                        has_line_numbers: false,
                    }],
                }
            }
            ClaudeToolData::Write { file_path, content } => {
                let diffs = vec![FileChange::Write {
                    content: content.clone(),
                }];
                ActionType::FileEdit {
                    path: make_path_relative(file_path, worktree_path),
                    changes: diffs,
                }
            }
            ClaudeToolData::Bash { command, .. } => ActionType::CommandRun {
                command: command.clone(),
                result: None,
            },
            ClaudeToolData::Grep { pattern, .. } => ActionType::Search {
                query: pattern.clone(),
            },
            ClaudeToolData::WebFetch { url, .. } => ActionType::WebFetch { url: url.clone() },
            ClaudeToolData::WebSearch { query, .. } => ActionType::WebFetch { url: query.clone() },
            ClaudeToolData::Task {
                description,
                prompt,
                ..
            } => {
                let task_description = if let Some(desc) = description {
                    desc.clone()
                } else {
                    prompt.clone().unwrap_or_default()
                };
                ActionType::TaskCreate {
                    description: task_description,
                }
            }
            ClaudeToolData::ExitPlanMode { plan } => {
                ActionType::PlanPresentation { plan: plan.clone() }
            }
            ClaudeToolData::NotebookEdit { .. } => ActionType::Tool {
                tool_name: "NotebookEdit".to_string(),
                arguments: Some(serde_json::to_value(tool_data).unwrap_or(serde_json::Value::Null)),
                result: None,
            },
            ClaudeToolData::TodoWrite { todos } => ActionType::TodoManagement {
                todos: todos
                    .iter()
                    .map(|t| TodoItem {
                        content: t.content.clone(),
                        status: t.status.clone(),
                        priority: t.priority.clone(),
                    })
                    .collect(),
                operation: "write".to_string(),
            },
            ClaudeToolData::TodoRead { .. } => ActionType::TodoManagement {
                todos: vec![],
                operation: "read".to_string(),
            },
            ClaudeToolData::Glob { pattern, .. } => ActionType::Search {
                query: pattern.clone(),
            },
            ClaudeToolData::LS { .. } => ActionType::Other {
                description: "List directory".to_string(),
            },
            ClaudeToolData::Oracle { .. } => ActionType::Other {
                description: "Oracle".to_string(),
            },
            ClaudeToolData::Mermaid { .. } => ActionType::Other {
                description: "Mermaid diagram".to_string(),
            },
            ClaudeToolData::CodebaseSearchAgent { .. } => ActionType::Other {
                description: "Codebase search".to_string(),
            },
            ClaudeToolData::UndoEdit { .. } => ActionType::Other {
                description: "Undo edit".to_string(),
            },
            ClaudeToolData::Unknown { .. } => {
                // Surface MCP tools as generic Tool with args
                let name = tool_data.get_name();
                if name.starts_with("mcp__") {
                    let parts: Vec<&str> = name.split("__").collect();
                    let label = if parts.len() >= 3 {
                        format!("mcp:{}:{}", parts[1], parts[2])
                    } else {
                        name.to_string()
                    };
                    // Extract `input` if present by serializing then deserializing to a tiny struct
                    let args = serde_json::to_value(tool_data)
                        .ok()
                        .and_then(|v| serde_json::from_value::<ClaudeToolWithInput>(v).ok())
                        .map(|w| w.input)
                        .unwrap_or(serde_json::Value::Null);
                    ActionType::Tool {
                        tool_name: label,
                        arguments: Some(args),
                        result: None,
                    }
                } else {
                    ActionType::Other {
                        description: format!("Tool: {}", tool_data.get_name()),
                    }
                }
            }
        }
    }

    /// Generate concise, readable content for tool usage using structured data
    fn generate_concise_content(
        tool_data: &ClaudeToolData,
        action_type: &ActionType,
        worktree_path: &str,
    ) -> String {
        match action_type {
            ActionType::FileRead { path } => format!("`{path}`"),
            ActionType::FileEdit { path, .. } => format!("`{path}`"),
            ActionType::CommandRun { command, .. } => format!("`{command}`"),
            ActionType::Search { query } => format!("`{query}`"),
            ActionType::WebFetch { url } => format!("`{url}`"),
            ActionType::TaskCreate { description } => {
                if description.is_empty() {
                    "Task".to_string()
                } else {
                    format!("Task: `{description}`")
                }
            }
            ActionType::Tool { .. } => match tool_data {
                ClaudeToolData::NotebookEdit { notebook_path, .. } => {
                    format!("`{}`", make_path_relative(notebook_path, worktree_path))
                }
                ClaudeToolData::Unknown { .. } => {
                    let name = tool_data.get_name();
                    if name.starts_with("mcp__") {
                        let parts: Vec<&str> = name.split("__").collect();
                        if parts.len() >= 3 {
                            return format!("mcp:{}:{}", parts[1], parts[2]);
                        }
                    }
                    name.to_string()
                }
                _ => tool_data.get_name().to_string(),
            },
            ActionType::PlanPresentation { plan } => plan.clone(),
            ActionType::TodoManagement { .. } => "TODO list updated".to_string(),
            ActionType::Other { description: _ } => match tool_data {
                ClaudeToolData::LS { path } => {
                    let relative_path = make_path_relative(path, worktree_path);
                    if relative_path.is_empty() {
                        "List directory".to_string()
                    } else {
                        format!("List directory: `{relative_path}`")
                    }
                }
                ClaudeToolData::Glob { pattern, path, .. } => {
                    if let Some(search_path) = path {
                        format!(
                            "Find files: `{}` in `{}`",
                            pattern,
                            make_path_relative(search_path, worktree_path)
                        )
                    } else {
                        format!("Find files: `{pattern}`")
                    }
                }
                ClaudeToolData::Oracle { task, .. } => {
                    if let Some(t) = task {
                        format!("Oracle: `{t}`")
                    } else {
                        "Oracle".to_string()
                    }
                }
                ClaudeToolData::Mermaid { .. } => "Mermaid diagram".to_string(),
                ClaudeToolData::CodebaseSearchAgent { query, path, .. } => {
                    match (query.as_ref(), path.as_ref()) {
                        (Some(q), Some(p)) if !q.is_empty() && !p.is_empty() => format!(
                            "Codebase search: `{}` in `{}`",
                            q,
                            make_path_relative(p, worktree_path)
                        ),
                        (Some(q), _) if !q.is_empty() => format!("Codebase search: `{q}`"),
                        _ => "Codebase search".to_string(),
                    }
                }
                ClaudeToolData::UndoEdit { path, .. } => {
                    if let Some(p) = path.as_ref() {
                        let rel = make_path_relative(p, worktree_path);
                        if rel.is_empty() {
                            "Undo edit".to_string()
                        } else {
                            format!("Undo edit: `{rel}`")
                        }
                    } else {
                        "Undo edit".to_string()
                    }
                }
                _ => tool_data.get_name().to_string(),
            },
        }
    }
}

// Data structures for parsing Claude's JSON output format
#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
#[serde(tag = "type")]
pub enum ClaudeJson {
    #[serde(rename = "system")]
    System {
        subtype: Option<String>,
        session_id: Option<String>,
        cwd: Option<String>,
        tools: Option<Vec<serde_json::Value>>,
        model: Option<String>,
        #[serde(default, rename = "apiKeySource")]
        api_key_source: Option<String>,
    },
    #[serde(rename = "assistant")]
    Assistant {
        message: ClaudeMessage,
        session_id: Option<String>,
    },
    #[serde(rename = "user")]
    User {
        message: ClaudeMessage,
        session_id: Option<String>,
    },
    #[serde(rename = "tool_use")]
    ToolUse {
        tool_name: String,
        #[serde(flatten)]
        tool_data: ClaudeToolData,
        session_id: Option<String>,
    },
    #[serde(rename = "tool_result")]
    ToolResult {
        result: serde_json::Value,
        is_error: Option<bool>,
        session_id: Option<String>,
    },
    #[serde(rename = "result")]
    Result {
        subtype: Option<String>,
        is_error: Option<bool>,
        duration_ms: Option<u64>,
        result: Option<serde_json::Value>,
    },
    // Catch-all for unknown message types
    #[serde(untagged)]
    Unknown {
        #[serde(flatten)]
        data: std::collections::HashMap<String, serde_json::Value>,
    },
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct ClaudeMessage {
    pub id: Option<String>,
    #[serde(rename = "type")]
    pub message_type: Option<String>,
    pub role: String,
    pub model: Option<String>,
    pub content: Vec<ClaudeContentItem>,
    pub stop_reason: Option<String>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
#[serde(tag = "type")]
pub enum ClaudeContentItem {
    #[serde(rename = "text")]
    Text { text: String },
    #[serde(rename = "thinking")]
    Thinking { thinking: String },
    #[serde(rename = "tool_use")]
    ToolUse {
        id: String,
        #[serde(flatten)]
        tool_data: ClaudeToolData,
    },
    #[serde(rename = "tool_result")]
    ToolResult {
        tool_use_id: String,
        content: serde_json::Value,
        is_error: Option<bool>,
    },
}

/// Structured tool data for Claude tools based on real samples
#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
#[serde(tag = "name", content = "input")]
pub enum ClaudeToolData {
    #[serde(rename = "TodoWrite", alias = "todo_write")]
    TodoWrite {
        todos: Vec<ClaudeTodoItem>,
    },
    #[serde(rename = "Task", alias = "task")]
    Task {
        subagent_type: Option<String>,
        description: Option<String>,
        prompt: Option<String>,
    },
    #[serde(rename = "Glob", alias = "glob")]
    Glob {
        #[serde(alias = "filePattern")]
        pattern: String,
        #[serde(default)]
        path: Option<String>,
        #[serde(default)]
        limit: Option<u32>,
    },
    #[serde(rename = "LS", alias = "list_directory", alias = "ls")]
    LS {
        path: String,
    },
    #[serde(rename = "Read", alias = "read")]
    Read {
        #[serde(alias = "path")]
        file_path: String,
    },
    #[serde(rename = "Bash", alias = "bash")]
    Bash {
        #[serde(alias = "cmd", alias = "command_line")]
        command: String,
        #[serde(default)]
        description: Option<String>,
    },
    #[serde(rename = "Grep", alias = "grep")]
    Grep {
        pattern: String,
        #[serde(default)]
        output_mode: Option<String>,
        #[serde(default)]
        path: Option<String>,
    },
    ExitPlanMode {
        plan: String,
    },
    #[serde(rename = "Edit", alias = "edit_file")]
    Edit {
        #[serde(alias = "path")]
        file_path: String,
        #[serde(alias = "old_str")]
        old_string: Option<String>,
        #[serde(alias = "new_str")]
        new_string: Option<String>,
    },
    #[serde(rename = "MultiEdit", alias = "multi_edit")]
    MultiEdit {
        #[serde(alias = "path")]
        file_path: String,
        edits: Vec<ClaudeEditItem>,
    },
    #[serde(rename = "Write", alias = "create_file", alias = "write_file")]
    Write {
        #[serde(alias = "path")]
        file_path: String,
        content: String,
    },
    #[serde(rename = "NotebookEdit", alias = "notebook_edit")]
    NotebookEdit {
        notebook_path: String,
        new_source: String,
        edit_mode: String,
        #[serde(default)]
        cell_id: Option<String>,
    },
    #[serde(rename = "WebFetch", alias = "read_web_page")]
    WebFetch {
        url: String,
        #[serde(default)]
        prompt: Option<String>,
    },
    #[serde(rename = "WebSearch", alias = "web_search")]
    WebSearch {
        query: String,
        #[serde(default)]
        num_results: Option<u32>,
    },
    // Amp-only utilities for better UX
    #[serde(rename = "Oracle", alias = "oracle")]
    Oracle {
        #[serde(default)]
        task: Option<String>,
        #[serde(default)]
        files: Option<Vec<String>>,
        #[serde(default)]
        context: Option<String>,
    },
    #[serde(rename = "Mermaid", alias = "mermaid")]
    Mermaid {
        code: String,
    },
    #[serde(rename = "CodebaseSearchAgent", alias = "codebase_search_agent")]
    CodebaseSearchAgent {
        #[serde(default)]
        query: Option<String>,
        #[serde(default)]
        path: Option<String>,
        #[serde(default)]
        include: Option<Vec<String>>,
        #[serde(default)]
        exclude: Option<Vec<String>>,
        #[serde(default)]
        limit: Option<u32>,
    },
    #[serde(rename = "UndoEdit", alias = "undo_edit")]
    UndoEdit {
        #[serde(default, alias = "file_path")]
        path: Option<String>,
        #[serde(default)]
        steps: Option<u32>,
    },
    #[serde(rename = "TodoRead", alias = "todo_read")]
    TodoRead {},
    #[serde(untagged)]
    Unknown {
        #[serde(flatten)]
        data: std::collections::HashMap<String, serde_json::Value>,
    },
}

// Helper structs for parsing tool_result content and generic tool input
#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
struct ClaudeToolResultTextItem {
    text: String,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
struct ClaudeToolWithInput {
    #[serde(default)]
    input: serde_json::Value,
}

// Amp's claude-compatible Bash tool_result content format
// Example content (often delivered as a JSON string):
//   {"output":"...","exitCode":0}
#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
struct AmpBashResult {
    #[serde(default)]
    output: String,
    #[serde(rename = "exitCode")]
    exit_code: i32,
}

#[derive(Debug, Clone)]
struct ClaudeToolCallInfo {
    entry_index: usize,
    tool_name: String,
    tool_data: ClaudeToolData,
    content: String,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct ClaudeTodoItem {
    #[serde(default)]
    pub id: Option<String>,
    pub content: String,
    pub status: String,
    #[serde(default)]
    pub priority: Option<String>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct ClaudeEditItem {
    pub old_string: Option<String>,
    pub new_string: Option<String>,
}

impl ClaudeToolData {
    pub fn get_name(&self) -> &str {
        match self {
            ClaudeToolData::TodoWrite { .. } => "TodoWrite",
            ClaudeToolData::Task { .. } => "Task",
            ClaudeToolData::Glob { .. } => "Glob",
            ClaudeToolData::LS { .. } => "LS",
            ClaudeToolData::Read { .. } => "Read",
            ClaudeToolData::Bash { .. } => "Bash",
            ClaudeToolData::Grep { .. } => "Grep",
            ClaudeToolData::ExitPlanMode { .. } => "ExitPlanMode",
            ClaudeToolData::Edit { .. } => "Edit",
            ClaudeToolData::MultiEdit { .. } => "MultiEdit",
            ClaudeToolData::Write { .. } => "Write",
            ClaudeToolData::NotebookEdit { .. } => "NotebookEdit",
            ClaudeToolData::WebFetch { .. } => "WebFetch",
            ClaudeToolData::WebSearch { .. } => "WebSearch",
            ClaudeToolData::TodoRead { .. } => "TodoRead",
            ClaudeToolData::Oracle { .. } => "Oracle",
            ClaudeToolData::Mermaid { .. } => "Mermaid",
            ClaudeToolData::CodebaseSearchAgent { .. } => "CodebaseSearchAgent",
            ClaudeToolData::UndoEdit { .. } => "UndoEdit",
            ClaudeToolData::Unknown { data } => data
                .get("name")
                .and_then(|v| v.as_str())
                .unwrap_or("unknown"),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_claude_json_parsing() {
        let system_json =
            r#"{"type":"system","subtype":"init","session_id":"abc123","model":"claude-sonnet-4"}"#;
        let parsed: ClaudeJson = serde_json::from_str(system_json).unwrap();

        assert_eq!(
            ClaudeLogProcessor::extract_session_id(&parsed),
            Some("abc123".to_string())
        );

        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "");
        assert_eq!(entries.len(), 0);

        let assistant_json = r#"
        {"type":"assistant","message":{"type":"message","role":"assistant","model":"claude-sonnet-4-20250514","content":[{"type":"text","text":"Hi! I'm Claude Code."}]}}"#;
        let parsed: ClaudeJson = serde_json::from_str(assistant_json).unwrap();
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "");

        assert_eq!(entries.len(), 2);
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::SystemMessage
        ));
        assert_eq!(
            entries[0].content,
            "System initialized with model: claude-sonnet-4-20250514"
        );
    }

    #[test]
    fn test_assistant_message_parsing() {
        let assistant_json = r#"{"type":"assistant","message":{"role":"assistant","content":[{"type":"text","text":"Hello world"}]},"session_id":"abc123"}"#;
        let parsed: ClaudeJson = serde_json::from_str(assistant_json).unwrap();

        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "");
        assert_eq!(entries.len(), 1);
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::AssistantMessage
        ));
        assert_eq!(entries[0].content, "Hello world");
    }

    #[test]
    fn test_result_message_ignored() {
        let result_json = r#"{"type":"result","subtype":"success","is_error":false,"duration_ms":6059,"result":"Final result"}"#;
        let parsed: ClaudeJson = serde_json::from_str(result_json).unwrap();

        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "");
        assert_eq!(entries.len(), 0); // Should be ignored like in old implementation
    }

    #[test]
    fn test_thinking_content() {
        let thinking_json = r#"{"type":"assistant","message":{"role":"assistant","content":[{"type":"thinking","thinking":"Let me think about this..."}]}}"#;
        let parsed: ClaudeJson = serde_json::from_str(thinking_json).unwrap();

        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "");
        assert_eq!(entries.len(), 1);
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::Thinking
        ));
        assert_eq!(entries[0].content, "Let me think about this...");
    }

    #[test]
    fn test_todo_tool_empty_list() {
        // Test TodoWrite with empty todo list
        let empty_data = ClaudeToolData::TodoWrite { todos: vec![] };

        let action_type =
            ClaudeLogProcessor::extract_action_type(&empty_data, "/tmp/test-worktree");
        let result = ClaudeLogProcessor::generate_concise_content(
            &empty_data,
            &action_type,
            "/tmp/test-worktree",
        );

        assert_eq!(result, "TODO list updated");
    }

    #[test]
    fn test_glob_tool_content_extraction() {
        // Test Glob with pattern and path
        let glob_data = ClaudeToolData::Glob {
            pattern: "**/*.ts".to_string(),
            path: Some("/tmp/test-worktree/src".to_string()),
            limit: None,
        };

        let action_type = ClaudeLogProcessor::extract_action_type(&glob_data, "/tmp/test-worktree");
        let result = ClaudeLogProcessor::generate_concise_content(
            &glob_data,
            &action_type,
            "/tmp/test-worktree",
        );

        assert_eq!(result, "`**/*.ts`");
    }

    #[test]
    fn test_glob_tool_pattern_only() {
        // Test Glob with pattern only
        let glob_data = ClaudeToolData::Glob {
            pattern: "*.js".to_string(),
            path: None,
            limit: None,
        };

        let action_type = ClaudeLogProcessor::extract_action_type(&glob_data, "/tmp/test-worktree");
        let result = ClaudeLogProcessor::generate_concise_content(
            &glob_data,
            &action_type,
            "/tmp/test-worktree",
        );

        assert_eq!(result, "`*.js`");
    }

    #[test]
    fn test_ls_tool_content_extraction() {
        // Test LS with path
        let ls_data = ClaudeToolData::LS {
            path: "/tmp/test-worktree/components".to_string(),
        };

        let action_type = ClaudeLogProcessor::extract_action_type(&ls_data, "/tmp/test-worktree");
        let result = ClaudeLogProcessor::generate_concise_content(
            &ls_data,
            &action_type,
            "/tmp/test-worktree",
        );

        assert_eq!(result, "List directory: `components`");
    }

    #[test]
    fn test_path_relative_conversion() {
        // Test with relative path (should remain unchanged)
        let relative_result = make_path_relative("src/main.rs", "/tmp/test-worktree");
        assert_eq!(relative_result, "src/main.rs");

        // Test with absolute path (should become relative if possible)
        let test_worktree = "/tmp/test-worktree";
        let absolute_path = format!("{test_worktree}/src/main.rs");
        let absolute_result = make_path_relative(&absolute_path, test_worktree);
        assert_eq!(absolute_result, "src/main.rs");
    }

    #[tokio::test]
    async fn test_streaming_patch_generation() {
        use std::sync::Arc;

        use utils::msg_store::MsgStore;

        let executor = ClaudeCode {
            claude_code_router: Some(false),
            plan: None,
            model: None,
            append_prompt: AppendPrompt::default(),
            dangerously_skip_permissions: None,
            cmd: crate::command::CmdOverrides {
                base_command_override: None,
                additional_params: None,
            },
        };
        let msg_store = Arc::new(MsgStore::new());
        let current_dir = std::path::PathBuf::from("/tmp/test-worktree");

        // Push some test messages
        msg_store.push_stdout(
            r#"{"type":"system","subtype":"init","session_id":"test123"}"#.to_string(),
        );
        msg_store.push_stdout(r#"{"type":"assistant","message":{"role":"assistant","content":[{"type":"text","text":"Hello"}]}}"#.to_string());
        msg_store.push_finished();

        // Start normalization (this spawns async task)
        executor.normalize_logs(msg_store.clone(), &current_dir);

        // Give some time for async processing
        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;

        // Check that the history now contains patch messages
        let history = msg_store.get_history();
        let patch_count = history
            .iter()
            .filter(|msg| matches!(msg, utils::log_msg::LogMsg::JsonPatch(_)))
            .count();
        assert!(
            patch_count > 0,
            "Expected JsonPatch messages to be generated from streaming processing"
        );
    }

    #[test]
    fn test_session_id_extraction() {
        let system_json = r#"{"type":"system","session_id":"test-session-123"}"#;
        let parsed: ClaudeJson = serde_json::from_str(system_json).unwrap();

        assert_eq!(
            ClaudeLogProcessor::extract_session_id(&parsed),
            Some("test-session-123".to_string())
        );

        let tool_use_json =
            r#"{"type":"tool_use","tool_name":"read","input":{},"session_id":"another-session"}"#;
        let parsed_tool: ClaudeJson = serde_json::from_str(tool_use_json).unwrap();

        assert_eq!(
            ClaudeLogProcessor::extract_session_id(&parsed_tool),
            Some("another-session".to_string())
        );
    }

    #[test]
    fn test_amp_tool_aliases_create_file_and_edit_file() {
        // Amp "create_file" should deserialize into Write with alias field "path"
        let assistant_with_create = r#"{
            "type":"assistant",
            "message":{
                "role":"assistant",
                "content":[
                    {"type":"tool_use","id":"t1","name":"create_file","input":{"path":"/tmp/work/src/new.txt","content":"hello"}}
                ]
            }
        }"#;
        let parsed: ClaudeJson = serde_json::from_str(assistant_with_create).unwrap();
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "/tmp/work");
        assert_eq!(entries.len(), 1);
        match &entries[0].entry_type {
            NormalizedEntryType::ToolUse { action_type, .. } => match action_type {
                ActionType::FileEdit { path, .. } => assert_eq!(path, "src/new.txt"),
                other => panic!("Expected FileEdit, got {other:?}"),
            },
            other => panic!("Expected ToolUse, got {other:?}"),
        }

        // Amp "edit_file" should deserialize into Edit with aliases for path/old_str/new_str
        let assistant_with_edit = r#"{
            "type":"assistant",
            "message":{
                "role":"assistant",
                "content":[
                    {"type":"tool_use","id":"t2","name":"edit_file","input":{"path":"/tmp/work/README.md","old_str":"foo","new_str":"bar"}}
                ]
            }
        }"#;
        let parsed_edit: ClaudeJson = serde_json::from_str(assistant_with_edit).unwrap();
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed_edit, "/tmp/work");
        assert_eq!(entries.len(), 1);
        match &entries[0].entry_type {
            NormalizedEntryType::ToolUse { action_type, .. } => match action_type {
                ActionType::FileEdit { path, .. } => assert_eq!(path, "README.md"),
                other => panic!("Expected FileEdit, got {other:?}"),
            },
            other => panic!("Expected ToolUse, got {other:?}"),
        }
    }

    #[test]
    fn test_amp_tool_aliases_oracle_mermaid_codebase_undo() {
        // Oracle with task
        let oracle_json = r#"{
            "type":"assistant",
            "message":{
                "role":"assistant",
                "content":[
                    {"type":"tool_use","id":"t1","name":"oracle","input":{"task":"Assess project status"}}
                ]
            }
        }"#;
        let parsed: ClaudeJson = serde_json::from_str(oracle_json).unwrap();
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "/tmp/work");
        assert_eq!(entries.len(), 1);
        assert_eq!(entries[0].content, "Oracle: `Assess project status`");

        // Mermaid with code
        let mermaid_json = r#"{
            "type":"assistant",
            "message":{
                "role":"assistant",
                "content":[
                    {"type":"tool_use","id":"t2","name":"mermaid","input":{"code":"graph TD; A-->B;"}}
                ]
            }
        }"#;
        let parsed: ClaudeJson = serde_json::from_str(mermaid_json).unwrap();
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "/tmp/work");
        assert_eq!(entries.len(), 1);
        assert_eq!(entries[0].content, "Mermaid diagram");

        // CodebaseSearchAgent with query
        let csa_json = r#"{
            "type":"assistant",
            "message":{
                "role":"assistant",
                "content":[
                    {"type":"tool_use","id":"t3","name":"codebase_search_agent","input":{"query":"TODO markers"}}
                ]
            }
        }"#;
        let parsed: ClaudeJson = serde_json::from_str(csa_json).unwrap();
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "/tmp/work");
        assert_eq!(entries.len(), 1);
        assert_eq!(entries[0].content, "Codebase search: `TODO markers`");

        // UndoEdit shows file path when available
        let undo_json = r#"{
            "type":"assistant",
            "message":{
                "role":"assistant",
                "content":[
                    {"type":"tool_use","id":"t4","name":"undo_edit","input":{"path":"README.md"}}
                ]
            }
        }"#;
        let parsed: ClaudeJson = serde_json::from_str(undo_json).unwrap();
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "/tmp/work");
        assert_eq!(entries.len(), 1);
        assert_eq!(entries[0].content, "Undo edit: `README.md`");
    }

    #[test]
    fn test_amp_bash_and_task_content() {
        // Bash with alias field cmd
        let bash_json = r#"{
            "type":"assistant",
            "message":{
                "role":"assistant",
                "content":[
                    {"type":"tool_use","id":"t1","name":"bash","input":{"cmd":"echo hello"}}
                ]
            }
        }"#;
        let parsed: ClaudeJson = serde_json::from_str(bash_json).unwrap();
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "/tmp/work");
        assert_eq!(entries.len(), 1);
        // Content should display the command in backticks
        assert_eq!(entries[0].content, "`echo hello`");

        // Task content should include description/prompt wrapped in backticks
        let task_json = r#"{
            "type":"assistant",
            "message":{
                "role":"assistant",
                "content":[
                    {"type":"tool_use","id":"t2","name":"task","input":{"subagent_type":"Task","prompt":"Add header to README"}}
                ]
            }
        }"#;
        let parsed: ClaudeJson = serde_json::from_str(task_json).unwrap();
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "/tmp/work");
        assert_eq!(entries.len(), 1);
        assert_eq!(entries[0].content, "Task: `Add header to README`");
    }

    #[test]
    fn test_task_description_or_prompt_backticks() {
        // When description present, use it
        let with_desc = r#"{
            "type":"assistant",
            "message":{
                "role":"assistant",
                "content":[
                    {"type":"tool_use","id":"t3","name":"Task","input":{
                        "subagent_type":"Task",
                        "prompt":"Fallback prompt",
                        "description":"Primary description"
                    }}
                ]
            }
        }"#;
        let parsed: ClaudeJson = serde_json::from_str(with_desc).unwrap();
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "/tmp/work");
        assert_eq!(entries.len(), 1);
        assert_eq!(entries[0].content, "Task: `Primary description`");

        // When description missing, fall back to prompt
        let no_desc = r#"{
            "type":"assistant",
            "message":{
                "role":"assistant",
                "content":[
                    {"type":"tool_use","id":"t4","name":"Task","input":{
                        "subagent_type":"Task",
                        "prompt":"Only prompt"
                    }}
                ]
            }
        }"#;
        let parsed: ClaudeJson = serde_json::from_str(no_desc).unwrap();
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "/tmp/work");
        assert_eq!(entries.len(), 1);
        assert_eq!(entries[0].content, "Task: `Only prompt`");
    }

    #[test]
    fn test_tool_result_parsing_ignored() {
        let tool_result_json = r#"{"type":"tool_result","result":"File content here","is_error":false,"session_id":"test123"}"#;
        let parsed: ClaudeJson = serde_json::from_str(tool_result_json).unwrap();

        // Test session ID extraction from ToolResult still works
        assert_eq!(
            ClaudeLogProcessor::extract_session_id(&parsed),
            Some("test123".to_string())
        );

        // ToolResult messages should be ignored (produce no entries) until proper support is added
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "");
        assert_eq!(entries.len(), 0);
    }

    #[test]
    fn test_content_item_tool_result_ignored() {
        let assistant_with_tool_result = r#"{"type":"assistant","message":{"role":"assistant","content":[{"type":"tool_result","tool_use_id":"tool_123","content":"Operation completed","is_error":false}]}}"#;
        let parsed: ClaudeJson = serde_json::from_str(assistant_with_tool_result).unwrap();

        // ToolResult content items should be ignored (produce no entries) until proper support is added
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "");
        assert_eq!(entries.len(), 0);
    }

    #[test]
    fn test_api_key_source_warning() {
        // Test with ANTHROPIC_API_KEY - should generate warning
        let system_with_env_key = r#"{"type":"system","subtype":"init","apiKeySource":"ANTHROPIC_API_KEY","session_id":"test123"}"#;
        let parsed: ClaudeJson = serde_json::from_str(system_with_env_key).unwrap();
        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "");

        assert_eq!(entries.len(), 1);
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::SystemMessage
        ));
        assert_eq!(
            entries[0].content,
            "⚠️ ANTHROPIC_API_KEY env variable detected, your Anthropic subscription is not being used"
        );

        // Test with managed API key source - should not generate warning
        let system_with_managed_key = r#"{"type":"system","subtype":"init","apiKeySource":"/login managed key","session_id":"test123"}"#;
        let parsed_managed: ClaudeJson = serde_json::from_str(system_with_managed_key).unwrap();
        let entries_managed = ClaudeLogProcessor::new().normalize_entries(&parsed_managed, "");

        assert_eq!(entries_managed.len(), 0); // No warning for managed key

        // Test with other apiKeySource values - should not generate warning
        let system_other_key = r#"{"type":"system","subtype":"init","apiKeySource":"OTHER_KEY","session_id":"test123"}"#;
        let parsed_other: ClaudeJson = serde_json::from_str(system_other_key).unwrap();
        let entries_other = ClaudeLogProcessor::new().normalize_entries(&parsed_other, "");

        assert_eq!(entries_other.len(), 0); // No warning for other keys

        // Test with missing apiKeySource - should not generate warning
        let system_no_key = r#"{"type":"system","subtype":"init","session_id":"test123"}"#;
        let parsed_no_key: ClaudeJson = serde_json::from_str(system_no_key).unwrap();
        let entries_no_key = ClaudeLogProcessor::new().normalize_entries(&parsed_no_key, "");

        assert_eq!(entries_no_key.len(), 0); // No warning when field is missing
    }

    #[test]
    fn test_mixed_content_with_thinking_ignores_tool_result() {
        let complex_assistant_json = r#"{"type":"assistant","message":{"role":"assistant","content":[{"type":"thinking","thinking":"I need to read the file first"},{"type":"text","text":"I'll help you with that"},{"type":"tool_result","tool_use_id":"tool_789","content":"Success","is_error":false}]}}"#;
        let parsed: ClaudeJson = serde_json::from_str(complex_assistant_json).unwrap();

        let entries = ClaudeLogProcessor::new().normalize_entries(&parsed, "");
        // Only thinking and text entries should be processed, tool_result ignored
        assert_eq!(entries.len(), 2);

        // Check thinking entry
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::Thinking
        ));
        assert_eq!(entries[0].content, "I need to read the file first");

        // Check assistant message
        assert!(matches!(
            entries[1].entry_type,
            NormalizedEntryType::AssistantMessage
        ));
        assert_eq!(entries[1].content, "I'll help you with that");

        // ToolResult entry is ignored - no third entry
    }
}
</file>

<file path="crates/executors/src/executors/codex.rs">
mod session;

use std::{
    path::{Path, PathBuf},
    process::Stdio,
    sync::Arc,
};

use async_trait::async_trait;
use command_group::{AsyncCommandGroup, AsyncGroupChild};
use futures::StreamExt;
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use strum_macros::AsRefStr;
use tokio::{io::AsyncWriteExt, process::Command};
use ts_rs::TS;
use utils::{
    diff::{concatenate_diff_hunks, extract_unified_diff_hunks},
    msg_store::MsgStore,
    path::make_path_relative,
    shell::get_shell_command,
};

use crate::{
    command::{CmdOverrides, CommandBuilder, apply_overrides},
    executors::{
        AppendPrompt, ExecutorError, StandardCodingAgentExecutor, codex::session::SessionHandler,
    },
    logs::{
        ActionType, FileChange, NormalizedEntry, NormalizedEntryType,
        utils::{EntryIndexProvider, patch::ConversationPatch},
    },
};

/// Sandbox policy modes for Codex
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema, AsRefStr)]
#[serde(rename_all = "kebab-case")]
#[strum(serialize_all = "kebab-case")]
pub enum SandboxMode {
    Auto,
    ReadOnly,
    WorkspaceWrite,
    DangerFullAccess,
}

/// Reasoning effort for the underlying model
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema, AsRefStr)]
#[serde(rename_all = "kebab-case")]
#[strum(serialize_all = "kebab-case")]
pub enum ReasoningEffort {
    Low,
    Medium,
    High,
}

/// Model reasoning summary style
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema, AsRefStr)]
#[serde(rename_all = "kebab-case")]
#[strum(serialize_all = "kebab-case")]
pub enum ReasoningSummary {
    Auto,
    Concise,
    Detailed,
    None,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema)]
pub struct Codex {
    #[serde(default)]
    pub append_prompt: AppendPrompt,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub sandbox: Option<SandboxMode>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub oss: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub model: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub model_reasoning_effort: Option<ReasoningEffort>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub model_reasoning_summary: Option<ReasoningSummary>,
    #[serde(flatten)]
    pub cmd: CmdOverrides,
}

impl Codex {
    fn build_command_builder(&self) -> CommandBuilder {
        let mut builder = CommandBuilder::new("npx -y @openai/codex@0.38.0 exec")
            .params(["--json", "--skip-git-repo-check"]);

        if let Some(sandbox) = &self.sandbox {
            if sandbox == &SandboxMode::Auto {
                builder = builder.extend_params(["--full-auto"]);
            } else {
                builder = builder.extend_params(["--sandbox", sandbox.as_ref()]);
                if sandbox == &SandboxMode::DangerFullAccess {
                    builder = builder.extend_params(["--dangerously-bypass-approvals-and-sandbox"]);
                }
            }
        }

        if self.oss.unwrap_or(false) {
            builder = builder.extend_params(["--oss"]);
        }

        if let Some(model) = &self.model {
            builder = builder.extend_params(["--model", model]);
        }

        if let Some(effort) = &self.model_reasoning_effort {
            builder = builder.extend_params([
                "--config",
                &format!("model_reasoning_effort={}", effort.as_ref()),
            ]);
        }

        if let Some(summary) = &self.model_reasoning_summary {
            builder = builder.extend_params([
                "--config",
                &format!("model_reasoning_summary={}", summary.as_ref()),
            ]);
        }

        apply_overrides(builder, &self.cmd)
    }
}

#[async_trait]
impl StandardCodingAgentExecutor for Codex {
    async fn spawn(
        &self,
        current_dir: &Path,
        prompt: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        let (shell_cmd, shell_arg) = get_shell_command();
        let codex_command = self.build_command_builder().build_initial();

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(&codex_command)
            .env("NODE_NO_WARNINGS", "1")
            .env("RUST_LOG", "info");

        let mut child = command.group_spawn()?;

        // Feed the prompt in, then close the pipe so codex sees EOF
        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        Ok(child)
    }

    async fn spawn_follow_up(
        &self,
        current_dir: &Path,
        prompt: &str,
        session_id: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        // Fork rollout: copy and assign a new session id so each execution has a unique session
        let (_rollout_file_path, new_session_id) = SessionHandler::fork_rollout_file(session_id)
            .map_err(|e| ExecutorError::SpawnError(std::io::Error::other(e)))?;

        let (shell_cmd, shell_arg) = get_shell_command();
        let codex_command = self
            .build_command_builder()
            .build_follow_up(&["resume".to_string(), new_session_id]);

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(&codex_command)
            .env("NODE_NO_WARNINGS", "1")
            .env("RUST_LOG", "info");

        let mut child = command.group_spawn()?;

        // Feed the prompt in, then close the pipe so codex sees EOF
        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        Ok(child)
    }

    fn normalize_logs(&self, msg_store: Arc<MsgStore>, current_dir: &Path) {
        let entry_index_provider = EntryIndexProvider::start_from(&msg_store);

        // Process stderr logs for session extraction only (errors come through JSONL)
        SessionHandler::start_session_id_extraction(msg_store.clone());

        // Process stdout logs (Codex's JSONL output)
        let current_dir = current_dir.to_path_buf();
        tokio::spawn(async move {
            let mut stream = msg_store.stdout_lines_stream();
            use std::collections::HashMap;
            // Track exec call ids to entry index, tool_name, content, and command
            let mut exec_info_map: HashMap<String, (usize, String, String, String)> =
                HashMap::new();
            // Track MCP calls to index, tool_name, args, and initial content
            let mut mcp_info_map: HashMap<
                String,
                (usize, String, Option<serde_json::Value>, String),
            > = HashMap::new();

            while let Some(Ok(line)) = stream.next().await {
                let trimmed = line.trim();
                if trimmed.is_empty() {
                    continue;
                }

                if let Ok(cj) = serde_json::from_str::<CodexJson>(trimmed) {
                    // Handle result-carrying events that require replacement
                    match &cj {
                        CodexJson::StructuredMessage { msg, .. } => match msg {
                            CodexMsgContent::ExecCommandBegin {
                                call_id, command, ..
                            } => {
                                let command_str = command.join(" ");
                                let entry = NormalizedEntry {
                                    timestamp: None,
                                    entry_type: NormalizedEntryType::ToolUse {
                                        tool_name: if command_str.contains("bash") {
                                            "bash".to_string()
                                        } else {
                                            "shell".to_string()
                                        },
                                        action_type: ActionType::CommandRun {
                                            command: command_str.clone(),
                                            result: None,
                                        },
                                    },
                                    content: format!("`{command_str}`"),
                                    metadata: None,
                                };
                                let id = entry_index_provider.next();
                                if let Some(cid) = call_id.as_ref() {
                                    let tool_name = if command_str.contains("bash") {
                                        "bash".to_string()
                                    } else {
                                        "shell".to_string()
                                    };
                                    exec_info_map.insert(
                                        cid.clone(),
                                        (id, tool_name, entry.content.clone(), command_str.clone()),
                                    );
                                }
                                msg_store
                                    .push_patch(ConversationPatch::add_normalized_entry(id, entry));
                            }
                            CodexMsgContent::ExecCommandEnd {
                                call_id,
                                stdout,
                                stderr,
                                success,
                                exit_code,
                            } => {
                                if let Some(cid) = call_id.as_ref()
                                    && let Some((idx, tool_name, prev_content, prev_command)) =
                                        exec_info_map.get(cid).cloned()
                                {
                                    // Merge stdout and stderr for richer context
                                    let output = match (stdout.as_ref(), stderr.as_ref()) {
                                        (Some(sout), Some(serr)) => {
                                            let sout_trim = sout.trim();
                                            let serr_trim = serr.trim();
                                            if sout_trim.is_empty() && serr_trim.is_empty() {
                                                None
                                            } else if sout_trim.is_empty() {
                                                Some(serr.clone())
                                            } else if serr_trim.is_empty() {
                                                Some(sout.clone())
                                            } else {
                                                Some(format!(
                                                    "STDOUT:\n{sout_trim}\n\nSTDERR:\n{serr_trim}"
                                                ))
                                            }
                                        }
                                        (Some(sout), None) => {
                                            if sout.trim().is_empty() {
                                                None
                                            } else {
                                                Some(sout.clone())
                                            }
                                        }
                                        (None, Some(serr)) => {
                                            if serr.trim().is_empty() {
                                                None
                                            } else {
                                                Some(serr.clone())
                                            }
                                        }
                                        (None, None) => None,
                                    };
                                    let exit_status = if let Some(s) = success {
                                        Some(crate::logs::CommandExitStatus::Success {
                                            success: *s,
                                        })
                                    } else {
                                        exit_code.as_ref().map(|code| {
                                            crate::logs::CommandExitStatus::ExitCode { code: *code }
                                        })
                                    };
                                    let entry = NormalizedEntry {
                                        timestamp: None,
                                        entry_type: NormalizedEntryType::ToolUse {
                                            tool_name,
                                            action_type: ActionType::CommandRun {
                                                command: prev_command,
                                                result: Some(crate::logs::CommandRunResult {
                                                    exit_status,
                                                    output,
                                                }),
                                            },
                                        },
                                        content: prev_content,
                                        metadata: None,
                                    };
                                    msg_store.push_patch(ConversationPatch::replace(idx, entry));
                                }
                            }
                            CodexMsgContent::McpToolCallBegin {
                                call_id,
                                invocation,
                            } => {
                                let tool_name =
                                    format!("mcp:{}:{}", invocation.server, invocation.tool);
                                let content_str = invocation.tool.clone();
                                let entry = NormalizedEntry {
                                    timestamp: None,
                                    entry_type: NormalizedEntryType::ToolUse {
                                        tool_name: tool_name.clone(),
                                        action_type: ActionType::Tool {
                                            tool_name: tool_name.clone(),
                                            arguments: invocation.arguments.clone(),
                                            result: None,
                                        },
                                    },
                                    content: content_str.clone(),
                                    metadata: None,
                                };
                                let id = entry_index_provider.next();
                                mcp_info_map.insert(
                                    call_id.clone(),
                                    (
                                        id,
                                        tool_name.clone(),
                                        invocation.arguments.clone(),
                                        content_str,
                                    ),
                                );
                                msg_store
                                    .push_patch(ConversationPatch::add_normalized_entry(id, entry));
                            }
                            CodexMsgContent::McpToolCallEnd {
                                call_id, result, ..
                            } => {
                                if let Some((idx, tool_name, args, prev_content)) =
                                    mcp_info_map.remove(call_id)
                                {
                                    let entry = NormalizedEntry {
                                        timestamp: None,
                                        entry_type: NormalizedEntryType::ToolUse {
                                            tool_name: tool_name.clone(),
                                            action_type: ActionType::Tool {
                                                tool_name,
                                                arguments: args,
                                                result: Some(crate::logs::ToolResult {
                                                    r#type: crate::logs::ToolResultValueType::Json,
                                                    value: result.clone(),
                                                }),
                                            },
                                        },
                                        content: prev_content,
                                        metadata: None,
                                    };
                                    msg_store.push_patch(ConversationPatch::replace(idx, entry));
                                }
                            }
                            _ => {
                                if let Some(entries) = cj.to_normalized_entries(&current_dir) {
                                    for entry in entries {
                                        let new_id = entry_index_provider.next();
                                        let patch =
                                            ConversationPatch::add_normalized_entry(new_id, entry);
                                        msg_store.push_patch(patch);
                                    }
                                }
                            }
                        },
                        _ => {
                            if let Some(entries) = cj.to_normalized_entries(&current_dir) {
                                for entry in entries {
                                    let new_id = entry_index_provider.next();
                                    let patch =
                                        ConversationPatch::add_normalized_entry(new_id, entry);
                                    msg_store.push_patch(patch);
                                }
                            }
                        }
                    }
                } else {
                    // Handle malformed JSON as raw output
                    let entry = NormalizedEntry {
                        timestamp: None,
                        entry_type: NormalizedEntryType::SystemMessage,
                        content: trimmed.to_string(),
                        metadata: None,
                    };

                    let new_id = entry_index_provider.next();
                    let patch = ConversationPatch::add_normalized_entry(new_id, entry);
                    msg_store.push_patch(patch);
                }
            }
        });
    }

    // MCP configuration methods
    fn default_mcp_config_path(&self) -> Option<std::path::PathBuf> {
        dirs::home_dir().map(|home| home.join(".codex").join("config.toml"))
    }
}

// Data structures for parsing Codex's JSON output format
#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
#[serde(untagged)]
pub enum CodexJson {
    /// Structured message with id and msg fields
    StructuredMessage { id: String, msg: CodexMsgContent },
    /// Prompt message (user input)
    Prompt { prompt: String },
    /// System configuration message (first message with config fields)
    SystemConfig {
        #[serde(default)]
        model: Option<String>,
        #[serde(rename = "reasoning effort", default)]
        reasoning_effort: Option<String>,
        #[serde(default)]
        provider: Option<String>,
        #[serde(default)]
        sandbox: Option<String>,
        #[serde(default)]
        approval: Option<String>,
        #[serde(default)]
        workdir: Option<String>,
        #[serde(rename = "reasoning summaries", default)]
        reasoning_summaries: Option<String>,
        #[serde(flatten)]
        other_fields: std::collections::HashMap<String, serde_json::Value>,
    },
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct McpInvocation {
    pub server: String,
    pub tool: String,
    #[serde(default)]
    pub arguments: Option<serde_json::Value>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
#[serde(tag = "type")]
pub enum CodexMsgContent {
    #[serde(rename = "agent_message")]
    AgentMessage { message: String },

    #[serde(rename = "agent_reasoning")]
    AgentReasoning { text: String },

    #[serde(rename = "agent_reasoning_raw_content")]
    AgentReasoningRawContent { text: String },

    #[serde(rename = "agent_reasoning_raw_content_delta")]
    AgentReasoningRawContentDelta { delta: String },

    #[serde(rename = "error")]
    Error { message: Option<String> },

    #[serde(rename = "mcp_tool_call_begin")]
    McpToolCallBegin {
        call_id: String,
        invocation: McpInvocation,
    },

    #[serde(rename = "mcp_tool_call_end")]
    McpToolCallEnd {
        call_id: String,
        invocation: McpInvocation,
        #[serde(default)]
        duration: serde_json::Value,
        result: serde_json::Value,
    },

    #[serde(rename = "exec_command_begin")]
    ExecCommandBegin {
        call_id: Option<String>,
        command: Vec<String>,
        cwd: Option<String>,
    },

    #[serde(rename = "exec_command_output_delta")]
    ExecCommandOutputDelta {
        call_id: Option<String>,
        // "stdout" | "stderr" typically
        stream: Option<String>,
        // Could be bytes or string; keep flexible
        chunk: Option<serde_json::Value>,
    },

    #[serde(rename = "exec_command_end")]
    ExecCommandEnd {
        call_id: Option<String>,
        stdout: Option<String>,
        stderr: Option<String>,
        // Codex protocol has exit_code + duration; CLI may provide success; keep optional
        success: Option<bool>,
        #[serde(default)]
        exit_code: Option<i32>,
    },

    #[serde(rename = "exec_approval_request")]
    ExecApprovalRequest {
        call_id: Option<String>,
        command: Vec<String>,
        cwd: Option<String>,
        reason: Option<String>,
    },

    #[serde(rename = "apply_patch_approval_request")]
    ApplyPatchApprovalRequest {
        call_id: Option<String>,
        changes: std::collections::HashMap<String, serde_json::Value>,
        reason: Option<String>,
        grant_root: Option<String>,
    },

    #[serde(rename = "background_event")]
    BackgroundEvent { message: String },

    #[serde(rename = "patch_apply_begin")]
    PatchApplyBegin {
        call_id: Option<String>,
        auto_approved: Option<bool>,
        changes: std::collections::HashMap<String, CodexFileChange>,
    },

    #[serde(rename = "patch_apply_end")]
    PatchApplyEnd {
        call_id: Option<String>,
        stdout: Option<String>,
        stderr: Option<String>,
        success: Option<bool>,
    },

    #[serde(rename = "turn_diff")]
    TurnDiff { unified_diff: String },

    #[serde(rename = "get_history_entry_response")]
    GetHistoryEntryResponse {
        offset: Option<usize>,
        log_id: Option<u64>,
        entry: Option<serde_json::Value>,
    },

    #[serde(rename = "plan_update")]
    PlanUpdate {
        #[serde(flatten)]
        value: serde_json::Value,
    },

    #[serde(rename = "task_started")]
    TaskStarted,
    #[serde(rename = "task_complete")]
    TaskComplete { last_agent_message: Option<String> },
    #[serde(rename = "token_count")]
    TokenCount {
        input_tokens: Option<u64>,
        cached_input_tokens: Option<u64>,
        output_tokens: Option<u64>,
        reasoning_output_tokens: Option<u64>,
        total_tokens: Option<u64>,
    },

    // Catch-all for unknown message types
    #[serde(other)]
    Unknown,
}

#[derive(Debug, Clone, Deserialize, Serialize, PartialEq)]
#[serde(rename_all = "snake_case")]
pub enum CodexFileChange {
    Add {
        content: String,
    },
    Delete,
    Update {
        unified_diff: String,
        move_path: Option<PathBuf>,
    },
}

impl CodexJson {
    /// Convert to normalized entries
    pub fn to_normalized_entries(&self, current_dir: &Path) -> Option<Vec<NormalizedEntry>> {
        match self {
            CodexJson::SystemConfig { .. } => self.format_config_message().map(|content| {
                vec![NormalizedEntry {
                    timestamp: None,
                    entry_type: NormalizedEntryType::SystemMessage,
                    content,
                    metadata: Some(serde_json::to_value(self).unwrap_or(serde_json::Value::Null)),
                }]
            }),
            CodexJson::Prompt { .. } => None, // Skip prompt messages
            CodexJson::StructuredMessage { msg, .. } => {
                let this = &msg;

                match this {
                    CodexMsgContent::AgentMessage { message } => Some(vec![NormalizedEntry {
                        timestamp: None,
                        entry_type: NormalizedEntryType::AssistantMessage,
                        content: message.clone(),
                        metadata: None,
                    }]),
                    CodexMsgContent::AgentReasoning { text } => Some(vec![NormalizedEntry {
                        timestamp: None,
                        entry_type: NormalizedEntryType::Thinking,
                        content: text.clone(),
                        metadata: None,
                    }]),
                    CodexMsgContent::Error { message } => {
                        let error_message = message
                            .clone()
                            .unwrap_or_else(|| "Unknown error occurred".to_string());
                        Some(vec![NormalizedEntry {
                            timestamp: None,
                            entry_type: NormalizedEntryType::ErrorMessage,
                            content: error_message,
                            metadata: None,
                        }])
                    }
                    CodexMsgContent::ExecCommandBegin { .. } => None,
                    CodexMsgContent::PatchApplyBegin { changes, .. } => {
                        let mut entries = Vec::new();

                        for (file_path, change_data) in changes {
                            // Make path relative to current directory
                            let relative_path =
                                make_path_relative(file_path, &current_dir.to_string_lossy());

                            // Try to extract unified diff from change data
                            let mut changes = vec![];

                            match change_data {
                                CodexFileChange::Update {
                                    unified_diff,
                                    move_path,
                                } => {
                                    let mut new_path = relative_path.clone();

                                    if let Some(move_path) = move_path {
                                        new_path = make_path_relative(
                                            &move_path.to_string_lossy(),
                                            &current_dir.to_string_lossy(),
                                        );
                                        changes.push(FileChange::Rename {
                                            new_path: new_path.clone(),
                                        });
                                    }
                                    if !unified_diff.is_empty() {
                                        let hunks = extract_unified_diff_hunks(unified_diff);
                                        changes.push(FileChange::Edit {
                                            unified_diff: concatenate_diff_hunks(&new_path, &hunks),
                                            has_line_numbers: true,
                                        });
                                    }
                                }
                                CodexFileChange::Add { content } => {
                                    changes.push(FileChange::Write {
                                        content: content.clone(),
                                    });
                                }
                                CodexFileChange::Delete => {
                                    changes.push(FileChange::Delete);
                                }
                            };

                            entries.push(NormalizedEntry {
                                timestamp: None,
                                entry_type: NormalizedEntryType::ToolUse {
                                    tool_name: "edit".to_string(),
                                    action_type: ActionType::FileEdit {
                                        path: relative_path.clone(),
                                        changes,
                                    },
                                },
                                content: relative_path,
                                metadata: None,
                            });
                        }

                        Some(entries)
                    }
                    CodexMsgContent::McpToolCallBegin { .. } => None,
                    CodexMsgContent::ExecApprovalRequest {
                        command,
                        cwd,
                        reason,
                        ..
                    } => {
                        let command_str = command.join(" ");
                        let mut parts = vec![format!("command: `{}`", command_str)];
                        if let Some(c) = cwd {
                            parts.push(format!("cwd: {c}"));
                        }
                        if let Some(r) = reason {
                            parts.push(format!("reason: {r}"));
                        }
                        let content =
                            format!("Execution approval requested — {}", parts.join("  "));
                        Some(vec![NormalizedEntry {
                            timestamp: None,
                            entry_type: NormalizedEntryType::SystemMessage,
                            content,
                            metadata: None,
                        }])
                    }
                    CodexMsgContent::ApplyPatchApprovalRequest {
                        changes,
                        reason,
                        grant_root,
                        ..
                    } => {
                        let mut parts = vec![format!("files: {}", changes.len())];
                        if let Some(root) = grant_root {
                            parts.push(format!("grant_root: {root}"));
                        }
                        if let Some(r) = reason {
                            parts.push(format!("reason: {r}"));
                        }
                        let content = format!("Patch approval requested — {}", parts.join("  "));
                        Some(vec![NormalizedEntry {
                            timestamp: None,
                            entry_type: NormalizedEntryType::SystemMessage,
                            content,
                            metadata: None,
                        }])
                    }
                    CodexMsgContent::PlanUpdate { value } => Some(vec![NormalizedEntry {
                        timestamp: None,
                        entry_type: NormalizedEntryType::SystemMessage,
                        content: "Plan update".to_string(),
                        metadata: Some(value.clone()),
                    }]),

                    // Ignored message types
                    CodexMsgContent::AgentReasoningRawContent { .. }
                    | CodexMsgContent::AgentReasoningRawContentDelta { .. }
                    | CodexMsgContent::ExecCommandOutputDelta { .. }
                    | CodexMsgContent::GetHistoryEntryResponse { .. }
                    | CodexMsgContent::ExecCommandEnd { .. }
                    | CodexMsgContent::PatchApplyEnd { .. }
                    | CodexMsgContent::McpToolCallEnd { .. }
                    | CodexMsgContent::TaskStarted
                    | CodexMsgContent::TaskComplete { .. }
                    | CodexMsgContent::TokenCount { .. }
                    | CodexMsgContent::TurnDiff { .. }
                    | CodexMsgContent::BackgroundEvent { .. }
                    | CodexMsgContent::Unknown => None,
                }
            }
        }
    }

    /// Format system configuration message for display
    fn format_config_message(&self) -> Option<String> {
        if let CodexJson::SystemConfig {
            model,
            reasoning_effort,
            provider,
            sandbox: _,
            approval: _,
            workdir: _,
            reasoning_summaries: _,
            other_fields: _,
        } = self
        {
            let mut params = vec![];

            if let Some(model) = model {
                params.push(format!("model: {model}"));
            }
            if let Some(provider) = provider {
                params.push(format!("provider: {provider}"));
            }
            if let Some(reasoning_effort) = reasoning_effort {
                params.push(format!("reasoning effort: {reasoning_effort}"));
            }

            if params.is_empty() {
                None
            } else {
                Some(params.join("  ").to_string())
            }
        } else {
            None
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::logs::{ActionType, NormalizedEntry, NormalizedEntryType};

    /// Test helper that directly tests the JSON parsing functions
    fn parse_test_json_lines(input: &str) -> Vec<NormalizedEntry> {
        let current_dir = PathBuf::from("/tmp");
        let mut entries = Vec::new();

        for line in input.lines() {
            let trimmed = line.trim();
            if trimmed.is_empty() {
                continue;
            }

            if let Ok(parsed_entries) =
                serde_json::from_str::<CodexJson>(trimmed).map(|codex_json| {
                    codex_json
                        .to_normalized_entries(&current_dir)
                        .unwrap_or_default()
                })
            {
                entries.extend(parsed_entries);
            } else {
                // Handle malformed JSON as raw output
                entries.push(NormalizedEntry {
                    timestamp: None,
                    entry_type: NormalizedEntryType::SystemMessage,
                    content: trimmed.to_string(),
                    metadata: None,
                });
            }
        }

        entries
    }

    /// Test helper for testing CodexJson deserialization
    fn test_codex_json_parsing(json_str: &str) -> Result<CodexJson, serde_json::Error> {
        serde_json::from_str(json_str)
    }

    #[test]
    fn test_extract_session_id_from_line() {
        let line = "2025-07-23T15:47:59.877058Z  INFO codex_exec: Codex initialized with event: Event { id: \"0\", msg: SessionConfigured(SessionConfiguredEvent { session_id: 3cdcc4df-c7c3-4cca-8902-48c3d4a0f96b, model: \"codex-mini-latest\", history_log_id: 9104228, history_entry_count: 1 }) }";

        let session_id = SessionHandler::extract_session_id_from_line(line);
        assert_eq!(
            session_id,
            Some("3cdcc4df-c7c3-4cca-8902-48c3d4a0f96b".to_string())
        );
    }

    #[test]
    fn test_extract_session_id_no_match() {
        let line = "Some random log line without session id";
        let session_id = SessionHandler::extract_session_id_from_line(line);
        assert_eq!(session_id, None);
    }

    #[test]
    fn test_extract_session_id_from_line_new_format() {
        // Newer Codex versions wrap the UUID in ConversationId(...)
        let line = "2025-09-12T14:36:32.515901Z  INFO codex_exec: Codex initialized with event: SessionConfiguredEvent { session_id: ConversationId(bd823d48-4bd8-4d9e-9d87-93a66afbf4d2), model: \"gpt-5\", history_log_id: 0, history_entry_count: 0, initial_messages: None, rollout_path: \"/home/user/.codex/sessions/2025/09/12/rollout-2025-09-12T14-36-32-bd823d48-4bd8-4d9e-9d87-93a66afbf4d2.jsonl\" }";

        let session_id = SessionHandler::extract_session_id_from_line(line);
        assert_eq!(
            session_id,
            Some("bd823d48-4bd8-4d9e-9d87-93a66afbf4d2".to_string())
        );
    }

    #[test]
    fn test_normalize_logs_basic() {
        let logs = r#"{"id":"1","msg":{"type":"task_started"}}
{"id":"1","msg":{"type":"agent_reasoning","text":"**Inspecting the directory tree**\n\nI want to check the root directory tree and I think using `ls -1` is acceptable since the guidelines don't explicitly forbid it, unlike `ls -R`, `find`, or `grep`. I could also consider using `rg --files`, but that might be too overwhelming if there are many files. Focusing on the top-level files and directories seems like a better approach. I'm particularly interested in `LICENSE`, `README.md`, and any relevant README files. So, let's start with `ls -1`."}}
{"id":"1","msg":{"type":"exec_command_begin","call_id":"call_I1o1QnQDtlLjGMg4Vd9HXJLd","command":["bash","-lc","ls -1"],"cwd":"/Users/user/dev/vk-wip"}}
{"id":"1","msg":{"type":"exec_command_end","call_id":"call_I1o1QnQDtlLjGMg4Vd9HXJLd","stdout":"AGENT.md\nCLAUDE.md\nCODE-OF-CONDUCT.md\nCargo.lock\nCargo.toml\nDockerfile\nLICENSE\nREADME.md\nbackend\nbuild-npm-package.sh\ndev_assets\ndev_assets_seed\nfrontend\nnode_modules\nnpx-cli\npackage-lock.json\npackage.json\npnpm-lock.yaml\npnpm-workspace.yaml\nrust-toolchain.toml\nrustfmt.toml\nscripts\nshared\ntest-npm-package.sh\n","stderr":"","exit_code":0}}
{"id":"1","msg":{"type":"task_complete","last_agent_message":"I can see the directory structure of your project. This appears to be a Rust project with a frontend/backend architecture, using pnpm for package management. The project includes various configuration files, documentation, and development assets."}}"#;

        let entries = parse_test_json_lines(logs);

        // Should have only agent_reasoning (task_started, exec_command_begin, task_complete are skipped in to_normalized_entries)
        assert_eq!(entries.len(), 1);

        // Check agent reasoning (thinking)
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::Thinking
        ));
        assert!(entries[0].content.contains("Inspecting the directory tree"));

        // Command entries are handled in the streaming path, not to_normalized_entries
    }

    #[test]
    fn test_normalize_logs_shell_vs_bash_mapping() {
        // Test shell command (not bash)
        let shell_logs = r#"{"id":"1","msg":{"type":"exec_command_begin","call_id":"call_test","command":["sh","-c","echo hello"],"cwd":"/tmp"}}"#;
        let entries = parse_test_json_lines(shell_logs);
        // to_normalized_entries skips exec_command_begin; mapping is tested in streaming path
        assert_eq!(entries.len(), 0);

        // Test bash command
        let bash_logs = r#"{"id":"1","msg":{"type":"exec_command_begin","call_id":"call_test","command":["bash","-c","echo hello"],"cwd":"/tmp"}}"#;
        let entries = parse_test_json_lines(bash_logs);
        assert_eq!(entries.len(), 0);

        // Mapping to bash is exercised in the streaming path
    }

    #[test]
    fn test_normalize_logs_token_count_skipped() {
        let logs = r#"{"id":"1","msg":{"type":"task_started"}}
{"id":"1","msg":{"type":"token_count","input_tokens":1674,"cached_input_tokens":1627,"output_tokens":384,"reasoning_output_tokens":384,"total_tokens":2058}}
{"id":"1","msg":{"type":"task_complete","last_agent_message":"Done!"}}"#;

        let entries = parse_test_json_lines(logs);

        // Should have: nothing (task_started, task_complete, and token_count all skipped)
        assert_eq!(entries.len(), 0);
    }

    #[test]
    fn test_normalize_logs_malformed_json() {
        let logs = r#"{"id":"1","msg":{"type":"task_started"}}
invalid json line here
{"id":"1","msg":{"type":"task_complete","last_agent_message":"Done!"}}"#;

        let entries = parse_test_json_lines(logs);

        // Should have: raw output only (task_started and task_complete skipped)
        assert_eq!(entries.len(), 1);

        // Check that malformed JSON becomes raw output
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::SystemMessage
        ));
        assert!(entries[0].content.contains("invalid json line here"));
    }

    #[test]
    fn test_normalize_logs_prompt_ignored() {
        let logs = r#"{"prompt":"project_id: f61fbd6a-9552-4b68-a1fe-10561f028dfc\n            \nTask title: describe this repo"}
{"id":"1","msg":{"type":"task_started"}}
{"id":"1","msg":{"type":"agent_message","message":"Hello, I'll help you with that."}}"#;

        let entries = parse_test_json_lines(logs);

        // Should have 1 entry (prompt and task_started ignored, only agent_message)
        assert_eq!(entries.len(), 1);

        // Check that we only have agent_message
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::AssistantMessage
        ));
        assert_eq!(entries[0].content, "Hello, I'll help you with that.");
    }

    #[test]
    fn test_normalize_logs_error_message() {
        let logs = r#"{"id":"1","msg":{"type":"error","message":"Missing environment variable: `OPENAI_API_KEY`. Create an API key (https://platform.openai.com) and export it as an environment variable."}}"#;

        let entries = parse_test_json_lines(logs);

        // Should have 1 entry for the error message
        assert_eq!(entries.len(), 1);

        // Check error message
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::ErrorMessage
        ));
        assert!(
            entries[0]
                .content
                .contains("Missing environment variable: `OPENAI_API_KEY`")
        );
    }

    #[test]
    fn test_normalize_logs_error_message_no_content() {
        let logs = r#"{"id":"1","msg":{"type":"error"}}"#;

        let entries = parse_test_json_lines(logs);

        // Should have 1 entry for the error message
        assert_eq!(entries.len(), 1);

        // Check error message fallback
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::ErrorMessage
        ));
        assert_eq!(entries[0].content, "Unknown error occurred");
    }

    #[test]
    fn test_normalize_logs_real_example() {
        let logs = r#"{"sandbox":"danger-full-access","reasoning summaries":"auto","approval":"Never","provider":"openai","reasoning effort":"medium","workdir":"/private/var/folders/4m/6cwx14sx59lc2k9km5ph76gh0000gn/T/vibe-kanban-dev/vk-ec8b-describe-t","model":"codex-mini-latest"}
{"prompt":"project_id: f61fbd6a-9552-4b68-a1fe-10561f028dfc\n            \nTask title: describe this repo"}
{"id":"1","msg":{"type":"task_started"}}
{"id":"1","msg":{"type":"error","message":"Missing environment variable: `OPENAI_API_KEY`. Create an API key (https://platform.openai.com) and export it as an environment variable."}}"#;

        let entries = parse_test_json_lines(logs);

        // Should have 2 entries: config, error (prompt and task_started ignored)
        assert_eq!(entries.len(), 2);

        // Check configuration message
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::SystemMessage
        ));
        assert!(entries[0].content.contains("model"));

        // Check error message
        assert!(matches!(
            entries[1].entry_type,
            NormalizedEntryType::ErrorMessage
        ));
        assert!(entries[1].content.contains("Missing environment variable"));
    }

    #[test]
    fn test_normalize_logs_partial_config() {
        // Test with just model and provider (should still work)
        let logs = r#"{"model":"codex-mini-latest","provider":"openai"}"#;

        let entries = parse_test_json_lines(logs);

        // Should have 1 entry for the configuration message
        assert_eq!(entries.len(), 1);

        // Check configuration message contains available params
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::SystemMessage
        ));
    }

    #[test]
    fn test_normalize_logs_agent_message() {
        let logs = r#"{"id":"1","msg":{"type":"agent_message","message":"I've made a small restructuring of the top‐level README:\n\n- **Inserted a \"Table of Contents\"** under the screenshot, linking to all major sections (Overview, Installation, Documentation, Support, Contributing, Development → Prerequisites/Running/Build, Environment Variables, Custom OAuth, and License).\n- **Appended a \"License\" section** at the bottom pointing to the Apache 2.0 LICENSE file.\n\nThese tweaks should make navigation and licensing info more discoverable. Let me know if you'd like any other adjustments!"}}"#;

        let entries = parse_test_json_lines(logs);

        // Should have 1 entry for the agent message
        assert_eq!(entries.len(), 1);

        // Check agent message
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::AssistantMessage
        ));
        assert!(
            entries[0]
                .content
                .contains("I've made a small restructuring")
        );
        assert!(entries[0].content.contains("Table of Contents"));
    }

    #[test]
    fn test_normalize_logs_patch_apply() {
        let logs = r#"{"id":"1","msg":{"type":"patch_apply_begin","call_id":"call_zr84aWQuwJR3aWgJLkfv56Gl","auto_approved":true,"changes":{"/private/var/folders/4m/6cwx14sx59lc2k9km5ph76gh0000gn/T/vibe-kanban-dev/vk-a712-minor-rest/README.md":{"update":{"unified_diff":"@@ -18,2 +18,17 @@\n \n+## Table of Contents\n+\n+- [Overview](#overview)\n+- [Installation](#installation)","move_path":null}}}}}
{"id":"1","msg":{"type":"patch_apply_end","call_id":"call_zr84aWQuwJR3aWgJLkfv56Gl","stdout":"Success. Updated the following files:\nM /private/var/folders/4m/6cwx14sx59lc2k9km5ph76gh0000gn/T/vibe-kanban-dev/vk-a712-minor-rest/README.md\n","stderr":"","success":true}}"#;

        let entries = parse_test_json_lines(logs);

        // Should have 1 entry (patch_apply_begin, patch_apply_end skipped)
        assert_eq!(entries.len(), 1);

        // Check edit tool use (follows claude.rs pattern)
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::ToolUse { .. }
        ));
        if let NormalizedEntryType::ToolUse {
            tool_name,
            action_type,
        } = &entries[0].entry_type
        {
            assert_eq!(tool_name, "edit");
            assert!(matches!(action_type, ActionType::FileEdit { .. }));
        }
        assert!(entries[0].content.contains("README.md"));
    }

    #[test]
    fn test_normalize_logs_skip_task_messages() {
        let logs = r#"{"id":"1","msg":{"type":"task_started"}}
{"id":"1","msg":{"type":"agent_message","message":"Hello world"}}
{"id":"1","msg":{"type":"task_complete","last_agent_message":"Done!"}}"#;

        let entries = parse_test_json_lines(logs);

        // Should have 1 entry (task_started and task_complete skipped)
        assert_eq!(entries.len(), 1);

        // Check that only agent_message remains
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::AssistantMessage
        ));
        assert_eq!(entries[0].content, "Hello world");
    }

    #[test]
    fn test_normalize_logs_mcp_tool_calls() {
        let logs = r#"{"id":"1","msg":{"type":"mcp_tool_call_begin","call_id":"call_KHwEJyaUuL5D8sO7lPfImx7I","invocation":{"server":"vibe_kanban","tool":"list_projects","arguments":{}}}}
{"id":"1","msg":{"type":"mcp_tool_call_end","call_id":"call_KHwEJyaUuL5D8sO7lPfImx7I","invocation":{"server":"vibe_kanban","tool":"list_projects","arguments":{}},"result":{"Ok":{"content":[{"text":"Projects listed successfully"}],"isError":false}}}}
{"id":"1","msg":{"type":"agent_message","message":"Here are your projects"}}"#;

        let entries = parse_test_json_lines(logs);

        // Should have only agent_message (mcp_tool_call_begin/end are skipped in to_normalized_entries)
        assert_eq!(entries.len(), 1);
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::AssistantMessage
        ));
        assert_eq!(entries[0].content, "Here are your projects");
    }

    #[test]
    fn test_normalize_logs_mcp_tool_call_multiple() {
        let logs = r#"{"id":"1","msg":{"type":"mcp_tool_call_begin","call_id":"call_1","invocation":{"server":"vibe_kanban","tool":"create_task","arguments":{"title":"Test task"}}}}
{"id":"1","msg":{"type":"mcp_tool_call_end","call_id":"call_1","invocation":{"server":"vibe_kanban","tool":"create_task","arguments":{"title":"Test task"}},"result":{"Ok":{"content":[{"text":"Task created"}],"isError":false}}}}
{"id":"1","msg":{"type":"mcp_tool_call_begin","call_id":"call_2","invocation":{"server":"vibe_kanban","tool":"list_tasks","arguments":{}}}}
{"id":"1","msg":{"type":"mcp_tool_call_end","call_id":"call_2","invocation":{"server":"vibe_kanban","tool":"list_tasks","arguments":{}},"result":{"Ok":{"content":[{"text":"Tasks listed"}],"isError":false}}}}"#;

        let entries = parse_test_json_lines(logs);

        // to_normalized_entries skips mcp_tool_call_begin/end; expect none
        assert_eq!(entries.len(), 0);
    }

    #[test]
    fn test_codex_json_system_config_parsing() {
        let config_json = r#"{"sandbox":"danger-full-access","reasoning summaries":"auto","approval":"Never","provider":"openai","reasoning effort":"medium","workdir":"/tmp","model":"codex-mini-latest"}"#;

        let parsed = test_codex_json_parsing(config_json).unwrap();
        assert!(matches!(parsed, CodexJson::SystemConfig { .. }));

        let current_dir = PathBuf::from("/tmp");
        let entries = parsed.to_normalized_entries(&current_dir).unwrap();
        assert_eq!(entries.len(), 1);
        assert!(matches!(
            entries[0].entry_type,
            NormalizedEntryType::SystemMessage
        ));
        assert!(entries[0].content.contains("model: codex-mini-latest"));
    }

    #[test]
    fn test_codex_json_prompt_parsing() {
        let prompt_json = r#"{"prompt":"project_id: f61fbd6a-9552-4b68-a1fe-10561f028dfc\n\nTask title: describe this repo"}"#;

        let parsed = test_codex_json_parsing(prompt_json).unwrap();
        assert!(matches!(parsed, CodexJson::Prompt { .. }));

        let current_dir = PathBuf::from("/tmp");
        let entries = parsed.to_normalized_entries(&current_dir);
        assert!(entries.is_none()); // Should return None
    }

    #[test]
    fn test_set_session_id_in_rollout_meta_old_format() {
        let mut meta = serde_json::json!({
            "id": "8724aa3f-efb7-4bbb-96a4-63fb3cb7ee90",
            "timestamp": "2025-09-09T16:46:39.250Z",
            "instructions": "# ...",
            "git": {
                "commit_hash": "70497c4cb9d64473e1e7602083badf338e59e75a",
                "branch": "vk/9986-retry-with",
                "repository_url": "https://github.com/bloopai/vibe-kanban"
            }
        });
        let new_id = "11111111-2222-3333-4444-555555555555";
        SessionHandler::set_session_id_in_rollout_meta(&mut meta, new_id).unwrap();
        // After migration, we should write new-format header
        assert_eq!(meta["type"].as_str(), Some("session_meta"));
        assert_eq!(meta["payload"]["id"].as_str(), Some(new_id));
        // Preserve instructions and git inside payload when present
        assert_eq!(meta["payload"]["instructions"].as_str(), Some("# ..."));
        assert!(meta["payload"]["git"].is_object());
        // Top-level id should be absent in new format
        assert_eq!(meta.get("id").and_then(|v| v.as_str()), None);
    }

    #[test]
    fn test_set_session_id_in_rollout_meta_new_format() {
        let mut meta = serde_json::json!({
            "timestamp": "2025-09-12T15:34:41.080Z",
            "type": "session_meta",
            "payload": {
                "id": "0c2061fc-1da8-4733-b33f-70159b4c57f2",
                "timestamp": "2025-09-12T15:34:41.068Z",
                "cwd": "/var/tmp/vibe-kanban-dev/worktrees/vk-f625-hi",
                "originator": "codex_cli_rs",
                "cli_version": "0.34.0",
                "instructions": "# ...",
                "git": {
                    "commit_hash": "07fad5465fcdca9b719cea965372a0ea39f42d15",
                    "branch": "vk/f625-hi",
                    "repository_url": "https://github.com/bloopai/vibe-kanban"
                }
            }
        });
        let new_id = "aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee";
        SessionHandler::set_session_id_in_rollout_meta(&mut meta, new_id).unwrap();
        // New format takes precedence: payload.id updated
        assert_eq!(meta["payload"]["id"].as_str(), Some(new_id));
        // Top-level id should remain absent (new format only uses payload.id)
        assert_eq!(meta["id"].as_str(), None);
    }
}
</file>

<file path="crates/executors/src/executors/cursor.rs">
use core::str;
use std::{path::Path, process::Stdio, sync::Arc, time::Duration};

use async_trait::async_trait;
use command_group::{AsyncCommandGroup, AsyncGroupChild};
use futures::StreamExt;
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use tokio::{io::AsyncWriteExt, process::Command};
use ts_rs::TS;
use utils::{
    diff::{
        concatenate_diff_hunks, create_unified_diff, create_unified_diff_hunk,
        extract_unified_diff_hunks,
    },
    msg_store::MsgStore,
    path::make_path_relative,
    shell::{get_shell_command, resolve_executable_path},
};

use crate::{
    command::{CmdOverrides, CommandBuilder, apply_overrides},
    executors::{AppendPrompt, ExecutorError, StandardCodingAgentExecutor},
    logs::{
        ActionType, FileChange, NormalizedEntry, NormalizedEntryType, TodoItem,
        plain_text_processor::PlainTextLogProcessor,
        utils::{ConversationPatch, EntryIndexProvider},
    },
};

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema)]
pub struct Cursor {
    #[serde(default)]
    pub append_prompt: AppendPrompt,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub force: Option<bool>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub model: Option<String>,
    #[serde(flatten)]
    pub cmd: CmdOverrides,
}

impl Cursor {
    fn build_command_builder(&self) -> CommandBuilder {
        let mut builder =
            CommandBuilder::new("cursor-agent").params(["-p", "--output-format=stream-json"]);

        if self.force.unwrap_or(false) {
            builder = builder.extend_params(["--force"]);
        }

        if let Some(model) = &self.model {
            builder = builder.extend_params(["--model", model]);
        }

        apply_overrides(builder, &self.cmd)
    }
}

#[async_trait]
impl StandardCodingAgentExecutor for Cursor {
    async fn spawn(
        &self,
        current_dir: &Path,
        prompt: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        let (shell_cmd, shell_arg) = get_shell_command();
        let agent_cmd = self.build_command_builder().build_initial();

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(&agent_cmd);

        let mut child = command.group_spawn()?;

        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        Ok(child)
    }

    async fn spawn_follow_up(
        &self,
        current_dir: &Path,
        prompt: &str,
        session_id: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        let (shell_cmd, shell_arg) = get_shell_command();
        let agent_cmd = self
            .build_command_builder()
            .build_follow_up(&["--resume".to_string(), session_id.to_string()]);

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(&agent_cmd);

        let mut child = command.group_spawn()?;

        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        Ok(child)
    }

    fn normalize_logs(&self, msg_store: Arc<MsgStore>, worktree_path: &Path) {
        let entry_index_provider = EntryIndexProvider::start_from(&msg_store);

        // Process Cursor stdout JSONL with typed serde models
        let current_dir = worktree_path.to_path_buf();
        tokio::spawn(async move {
            let mut lines = msg_store.stdout_lines_stream();

            // Cursor agent doesn't use STDERR. Everything comes through STDOUT, both JSONL and raw error output.
            let mut error_plaintext_processor = PlainTextLogProcessor::builder()
                .normalized_entry_producer(Box::new(|content: String| NormalizedEntry {
                    timestamp: None,
                    entry_type: NormalizedEntryType::ErrorMessage,
                    content,
                    metadata: None,
                }))
                .time_gap(Duration::from_secs(2)) // Break messages if they are 2 seconds apart
                .index_provider(entry_index_provider.clone())
                .build();

            // Assistant streaming coalescer state
            let mut model_reported = false;
            let mut session_id_reported = false;

            let mut current_assistant_message_buffer = String::new();
            let mut current_assistant_message_index: Option<usize> = None;

            let worktree_str = current_dir.to_string_lossy().to_string();

            use std::collections::HashMap;
            // Track tool call_id -> entry index
            let mut call_index_map: HashMap<String, usize> = HashMap::new();

            while let Some(Ok(line)) = lines.next().await {
                // Parse line as CursorJson
                let cursor_json: CursorJson = match serde_json::from_str(&line) {
                    Ok(cursor_json) => cursor_json,
                    Err(_) => {
                        // Not valid JSON, treat as raw error output
                        let line = strip_ansi_escapes::strip_str(line);
                        let line = strip_cursor_ascii_art_banner(line);
                        if line.trim().is_empty() {
                            continue; // Skip empty lines after stripping Noise
                        }

                        // Provide a useful sign-in message if needed
                        let line = if line == "Press any key to sign in..." {
                            "Please sign in to Cursor CLI using `cursor-agent login` or set the CURSOR_API_KEY environment variable.".to_string()
                        } else {
                            line
                        };

                        for patch in error_plaintext_processor.process(line + "\n") {
                            msg_store.push_patch(patch);
                        }
                        continue;
                    }
                };

                // Push session_id if present
                if !session_id_reported && let Some(session_id) = cursor_json.extract_session_id() {
                    msg_store.push_session_id(session_id);
                    session_id_reported = true;
                }

                let is_assistant_message = matches!(cursor_json, CursorJson::Assistant { .. });
                if !is_assistant_message && current_assistant_message_index.is_some() {
                    // flush
                    current_assistant_message_index = None;
                    current_assistant_message_buffer.clear();
                }

                match &cursor_json {
                    CursorJson::System { model, .. } => {
                        if !model_reported && let Some(model) = model.as_ref() {
                            let entry = NormalizedEntry {
                                timestamp: None,
                                entry_type: NormalizedEntryType::SystemMessage,
                                content: format!("System initialized with model: {model}"),
                                metadata: None,
                            };
                            let id = entry_index_provider.next();
                            msg_store
                                .push_patch(ConversationPatch::add_normalized_entry(id, entry));
                            model_reported = true;
                        }
                    }

                    CursorJson::User { .. } => {}

                    CursorJson::Assistant { message, .. } => {
                        if let Some(chunk) = message.concat_text() {
                            current_assistant_message_buffer.push_str(&chunk);
                            let replace_entry = NormalizedEntry {
                                timestamp: None,
                                entry_type: NormalizedEntryType::AssistantMessage,
                                content: current_assistant_message_buffer.clone(),
                                metadata: None,
                            };
                            if let Some(id) = current_assistant_message_index {
                                msg_store.push_patch(ConversationPatch::replace(id, replace_entry))
                            } else {
                                let id = entry_index_provider.next();
                                current_assistant_message_index = Some(id);
                                msg_store.push_patch(ConversationPatch::add_normalized_entry(
                                    id,
                                    replace_entry,
                                ));
                            };
                        }
                    }

                    CursorJson::ToolCall {
                        subtype,
                        call_id,
                        tool_call,
                        ..
                    } => {
                        // Only process "started" subtype (completed contains results we currently ignore)
                        if subtype
                            .as_deref()
                            .map(|s| s.eq_ignore_ascii_case("started"))
                            .unwrap_or(false)
                        {
                            let tool_name = tool_call.get_name().to_string();
                            let (action_type, content) =
                                tool_call.to_action_and_content(&worktree_str);

                            let entry = NormalizedEntry {
                                timestamp: None,
                                entry_type: NormalizedEntryType::ToolUse {
                                    tool_name,
                                    action_type,
                                },
                                content,
                                metadata: None,
                            };
                            let id = entry_index_provider.next();
                            if let Some(cid) = call_id.as_ref() {
                                call_index_map.insert(cid.clone(), id);
                            }
                            msg_store
                                .push_patch(ConversationPatch::add_normalized_entry(id, entry));
                        } else if subtype
                            .as_deref()
                            .map(|s| s.eq_ignore_ascii_case("completed"))
                            .unwrap_or(false)
                            && let Some(cid) = call_id.as_ref()
                            && let Some(&idx) = call_index_map.get(cid)
                        {
                            // Compute base content and action again
                            let (mut new_action, content_str) =
                                tool_call.to_action_and_content(&worktree_str);
                            if let CursorToolCall::Shell { args, result } = &tool_call {
                                // Merge stdout/stderr and derive exit status when available using typed deserialization
                                let (stdout_val, stderr_val, exit_code) = if let Some(res) = result
                                {
                                    match serde_json::from_value::<CursorShellResult>(res.clone()) {
                                        Ok(r) => {
                                            if let Some(out) = r.into_outcome() {
                                                (out.stdout, out.stderr, out.exit_code)
                                            } else {
                                                (None, None, None)
                                            }
                                        }
                                        Err(_) => (None, None, None),
                                    }
                                } else {
                                    (None, None, None)
                                };
                                let output = match (stdout_val, stderr_val) {
                                    (Some(sout), Some(serr)) => {
                                        let st = sout.trim();
                                        let se = serr.trim();
                                        if st.is_empty() && se.is_empty() {
                                            None
                                        } else if st.is_empty() {
                                            Some(serr)
                                        } else if se.is_empty() {
                                            Some(sout)
                                        } else {
                                            Some(format!("STDOUT:\n{st}\n\nSTDERR:\n{se}"))
                                        }
                                    }
                                    (Some(sout), None) => {
                                        if sout.trim().is_empty() {
                                            None
                                        } else {
                                            Some(sout)
                                        }
                                    }
                                    (None, Some(serr)) => {
                                        if serr.trim().is_empty() {
                                            None
                                        } else {
                                            Some(serr)
                                        }
                                    }
                                    (None, None) => None,
                                };
                                let exit_status = exit_code
                                    .map(|code| crate::logs::CommandExitStatus::ExitCode { code });
                                new_action = ActionType::CommandRun {
                                    command: args.command.clone(),
                                    result: Some(crate::logs::CommandRunResult {
                                        exit_status,
                                        output,
                                    }),
                                };
                            } else if let CursorToolCall::Mcp { args, result } = &tool_call {
                                // Extract a human-readable text from content array using typed deserialization
                                let md: Option<String> = if let Some(res) = result {
                                    match serde_json::from_value::<CursorMcpResult>(res.clone()) {
                                        Ok(r) => r.into_markdown(),
                                        Err(_) => None,
                                    }
                                } else {
                                    None
                                };
                                let provider = args.provider_identifier.as_deref().unwrap_or("mcp");
                                let tname = args.tool_name.as_deref().unwrap_or(&args.name);
                                let label = format!("mcp:{provider}:{tname}");
                                new_action = ActionType::Tool {
                                    tool_name: label.clone(),
                                    arguments: Some(serde_json::json!({
                                        "name": args.name,
                                        "args": args.args,
                                        "providerIdentifier": args.provider_identifier,
                                        "toolName": args.tool_name,
                                    })),
                                    result: md.map(|s| crate::logs::ToolResult {
                                        r#type: crate::logs::ToolResultValueType::Markdown,
                                        value: serde_json::Value::String(s),
                                    }),
                                };
                            }
                            let entry = NormalizedEntry {
                                timestamp: None,
                                entry_type: NormalizedEntryType::ToolUse {
                                    tool_name: match &tool_call {
                                        CursorToolCall::Mcp { args, .. } => {
                                            let provider = args
                                                .provider_identifier
                                                .as_deref()
                                                .unwrap_or("mcp");
                                            let tname =
                                                args.tool_name.as_deref().unwrap_or(&args.name);
                                            format!("mcp:{provider}:{tname}")
                                        }
                                        _ => tool_call.get_name().to_string(),
                                    },
                                    action_type: new_action,
                                },
                                content: content_str,
                                metadata: None,
                            };
                            msg_store.push_patch(ConversationPatch::replace(idx, entry));
                        }
                    }

                    CursorJson::Result { .. } => {
                        // no-op; metadata-only events not surfaced
                    }

                    CursorJson::Unknown => {
                        let entry = NormalizedEntry {
                            timestamp: None,
                            entry_type: NormalizedEntryType::SystemMessage,
                            content: line,
                            metadata: None,
                        };
                        let id = entry_index_provider.next();
                        msg_store.push_patch(ConversationPatch::add_normalized_entry(id, entry));
                    }
                }
            }
        });
    }

    // MCP configuration methods
    fn default_mcp_config_path(&self) -> Option<std::path::PathBuf> {
        dirs::home_dir().map(|home| home.join(".cursor").join("mcp.json"))
    }

    async fn check_availability(&self) -> bool {
        resolve_executable_path("cursor-agent").is_some()
    }
}

fn strip_cursor_ascii_art_banner(line: String) -> String {
    static BANNER_LINES: std::sync::OnceLock<Vec<String>> = std::sync::OnceLock::new();
    let banner_lines = BANNER_LINES.get_or_init(|| {
        r#"            +i":;;
        [?+<l,",::;;;I
      {[]_~iI"":::;;;;II
  )){↗↗↗↗↗↗↗↗↗↗↗↗↗↗↗↗↗↗↗↗↗ll          …  Cursor Agent
  11{[#M##M##M#########*ppll
  11}[]-+############oppqqIl
  1}[]_+<il;,####bpqqqqwIIII
  []?_~<illi_++qqwwwwww;IIII
  ]?-+~>i~{??--wwwwwww;;;III
  -_+]>{{{}}[[[mmmmmm_<_:;;I
  r\\|||(()))))mmmm)1)111{?_
   t/\\\\\|||(|ZZZ||\\\/tf^
        ttttt/tZZfff^>
            ^^^O>>
              >>"#
        .lines()
        .map(str::to_string)
        .collect()
    });

    for banner_line in banner_lines {
        if line.starts_with(banner_line) {
            return line.replacen(banner_line, "", 1).trim().to_string();
        }
    }
    line
}

/* ===========================
Typed Cursor JSON structures
=========================== */

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
#[serde(tag = "type")]
pub enum CursorJson {
    #[serde(rename = "system")]
    System {
        #[serde(default)]
        subtype: Option<String>,
        #[serde(default, rename = "apiKeySource")]
        api_key_source: Option<String>,
        #[serde(default)]
        cwd: Option<String>,
        #[serde(default)]
        session_id: Option<String>,
        #[serde(default)]
        model: Option<String>,
        #[serde(default, rename = "permissionMode")]
        permission_mode: Option<String>,
    },
    #[serde(rename = "user")]
    User {
        message: CursorMessage,
        #[serde(default)]
        session_id: Option<String>,
    },
    #[serde(rename = "assistant")]
    Assistant {
        message: CursorMessage,
        #[serde(default)]
        session_id: Option<String>,
    },
    #[serde(rename = "tool_call")]
    ToolCall {
        #[serde(default)]
        subtype: Option<String>, // "started" | "completed"
        #[serde(default)]
        call_id: Option<String>,
        tool_call: CursorToolCall,
        #[serde(default)]
        session_id: Option<String>,
    },
    #[serde(rename = "result")]
    Result {
        #[serde(default)]
        subtype: Option<String>,
        #[serde(default)]
        is_error: Option<bool>,
        #[serde(default)]
        duration_ms: Option<u64>,
        #[serde(default)]
        result: Option<serde_json::Value>,
    },
    #[serde(other)]
    Unknown,
}

impl CursorJson {
    pub fn extract_session_id(&self) -> Option<String> {
        match self {
            CursorJson::System { session_id, .. } => session_id.clone(),
            CursorJson::User { session_id, .. } => session_id.clone(),
            CursorJson::Assistant { session_id, .. } => session_id.clone(),
            CursorJson::ToolCall { session_id, .. } => session_id.clone(),
            CursorJson::Result { .. } => None,
            CursorJson::Unknown => None,
        }
    }
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorMessage {
    pub role: String,
    pub content: Vec<CursorContentItem>,
}

impl CursorMessage {
    pub fn concat_text(&self) -> Option<String> {
        let mut out = String::new();
        for CursorContentItem::Text { text } in &self.content {
            out.push_str(text);
        }
        if out.is_empty() { None } else { Some(out) }
    }
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
#[serde(tag = "type")]
pub enum CursorContentItem {
    #[serde(rename = "text")]
    Text { text: String },
}

/* ===========================
Tool call structure
=========================== */

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub enum CursorToolCall {
    #[serde(rename = "shellToolCall")]
    Shell {
        args: CursorShellArgs,
        #[serde(default)]
        result: Option<serde_json::Value>,
    },
    #[serde(rename = "lsToolCall")]
    LS {
        args: CursorLsArgs,
        #[serde(default)]
        result: Option<serde_json::Value>,
    },
    #[serde(rename = "globToolCall")]
    Glob {
        args: CursorGlobArgs,
        #[serde(default)]
        result: Option<serde_json::Value>,
    },
    #[serde(rename = "grepToolCall")]
    Grep {
        args: CursorGrepArgs,
        #[serde(default)]
        result: Option<serde_json::Value>,
    },
    #[serde(rename = "writeToolCall")]
    Write {
        args: CursorWriteArgs,
        #[serde(default)]
        result: Option<serde_json::Value>,
    },
    #[serde(rename = "readToolCall")]
    Read {
        args: CursorReadArgs,
        #[serde(default)]
        result: Option<serde_json::Value>,
    },
    #[serde(rename = "editToolCall")]
    Edit {
        args: CursorEditArgs,
        #[serde(default)]
        result: Option<serde_json::Value>,
    },
    #[serde(rename = "deleteToolCall")]
    Delete {
        args: CursorDeleteArgs,
        #[serde(default)]
        result: Option<serde_json::Value>,
    },
    #[serde(rename = "updateTodosToolCall")]
    Todo {
        args: CursorUpdateTodosArgs,
        #[serde(default)]
        result: Option<serde_json::Value>,
    },
    #[serde(rename = "mcpToolCall")]
    Mcp {
        args: CursorMcpArgs,
        #[serde(default)]
        result: Option<serde_json::Value>,
    },
    /// Generic fallback for unknown tools (amp.rs pattern)
    #[serde(untagged)]
    Unknown {
        #[serde(flatten)]
        data: std::collections::HashMap<String, serde_json::Value>,
    },
}

impl CursorToolCall {
    pub fn get_name(&self) -> &str {
        match self {
            CursorToolCall::Shell { .. } => "shell",
            CursorToolCall::LS { .. } => "ls",
            CursorToolCall::Glob { .. } => "glob",
            CursorToolCall::Grep { .. } => "grep",
            CursorToolCall::Write { .. } => "write",
            CursorToolCall::Read { .. } => "read",
            CursorToolCall::Edit { .. } => "edit",
            CursorToolCall::Delete { .. } => "delete",
            CursorToolCall::Todo { .. } => "todo",
            CursorToolCall::Mcp { .. } => "mcp",
            CursorToolCall::Unknown { data } => {
                data.keys().next().map(|s| s.as_str()).unwrap_or("unknown")
            }
        }
    }

    pub fn to_action_and_content(&self, worktree_path: &str) -> (ActionType, String) {
        match self {
            CursorToolCall::Read { args, .. } => {
                let path = make_path_relative(&args.path, worktree_path);
                (
                    ActionType::FileRead { path: path.clone() },
                    format!("`{path}`"),
                )
            }
            CursorToolCall::Write { args, .. } => {
                let path = make_path_relative(&args.path, worktree_path);
                (
                    ActionType::FileEdit {
                        path: path.clone(),
                        changes: vec![],
                    },
                    format!("`{path}`"),
                )
            }
            CursorToolCall::Edit { args, .. } => {
                let path = make_path_relative(&args.path, worktree_path);
                let mut changes = vec![];

                if let Some(apply_patch) = &args.apply_patch {
                    let hunks = extract_unified_diff_hunks(&apply_patch.patch_content);
                    changes.push(FileChange::Edit {
                        unified_diff: concatenate_diff_hunks(&path, &hunks),
                        has_line_numbers: false,
                    });
                }

                if let Some(str_replace) = &args.str_replace {
                    changes.push(FileChange::Edit {
                        unified_diff: create_unified_diff(
                            &path,
                            &str_replace.old_text,
                            &str_replace.new_text,
                        ),
                        has_line_numbers: false,
                    });
                }

                if let Some(multi_str_replace) = &args.multi_str_replace {
                    let hunks: Vec<String> = multi_str_replace
                        .edits
                        .iter()
                        .map(|edit| create_unified_diff_hunk(&edit.old_text, &edit.new_text))
                        .collect();
                    changes.push(FileChange::Edit {
                        unified_diff: concatenate_diff_hunks(&path, &hunks),
                        has_line_numbers: false,
                    });
                }

                (
                    ActionType::FileEdit {
                        path: path.clone(),
                        changes,
                    },
                    format!("`{path}`"),
                )
            }
            CursorToolCall::Delete { args, .. } => {
                let path = make_path_relative(&args.path, worktree_path);
                (
                    ActionType::FileEdit {
                        path: path.clone(),
                        changes: vec![],
                    },
                    format!("`{path}`"),
                )
            }
            CursorToolCall::Shell { args, .. } => {
                let cmd = &args.command;
                (
                    ActionType::CommandRun {
                        command: cmd.clone(),
                        result: None,
                    },
                    format!("`{cmd}`"),
                )
            }
            CursorToolCall::Grep { args, .. } => {
                let pattern = &args.pattern;
                (
                    ActionType::Search {
                        query: pattern.clone(),
                    },
                    format!("`{pattern}`"),
                )
            }
            CursorToolCall::Glob { args, .. } => {
                let pattern = args.glob_pattern.clone().unwrap_or_else(|| "*".to_string());
                if let Some(path) = args.path.as_ref().or(args.target_directory.as_ref()) {
                    let path = make_path_relative(path, worktree_path);
                    (
                        ActionType::Search {
                            query: pattern.clone(),
                        },
                        format!("Find files: `{pattern}` in `{path}`"),
                    )
                } else {
                    (
                        ActionType::Search {
                            query: pattern.clone(),
                        },
                        format!("Find files: `{pattern}`"),
                    )
                }
            }
            CursorToolCall::LS { args, .. } => {
                let path = make_path_relative(&args.path, worktree_path);
                let content = if path.is_empty() {
                    "List directory".to_string()
                } else {
                    format!("List directory: `{path}`")
                };
                (
                    ActionType::Other {
                        description: "List directory".to_string(),
                    },
                    content,
                )
            }
            CursorToolCall::Todo { args, .. } => {
                let todos = args
                    .todos
                    .as_ref()
                    .map(|todos| {
                        todos
                            .iter()
                            .map(|t| TodoItem {
                                content: t.content.clone(),
                                status: t.status.clone(),
                                priority: None, // CursorTodoItem doesn't have priority field
                            })
                            .collect()
                    })
                    .unwrap_or_default();

                (
                    ActionType::TodoManagement {
                        todos,
                        operation: "write".to_string(),
                    },
                    "TODO list updated".to_string(),
                )
            }
            CursorToolCall::Mcp { args, .. } => {
                let provider = args.provider_identifier.as_deref().unwrap_or("mcp");
                let tool_name = args.tool_name.as_deref().unwrap_or(&args.name);
                let label = format!("mcp:{provider}:{tool_name}");
                let summary = tool_name.to_string();
                let mut arguments = serde_json::json!({
                    "name": args.name,
                    "args": args.args,
                });
                if let Some(p) = &args.provider_identifier {
                    arguments["providerIdentifier"] = serde_json::Value::String(p.clone());
                }
                if let Some(tn) = &args.tool_name {
                    arguments["toolName"] = serde_json::Value::String(tn.clone());
                }
                (
                    ActionType::Tool {
                        tool_name: label,
                        arguments: Some(arguments),
                        result: None,
                    },
                    summary,
                )
            }
            CursorToolCall::Unknown { .. } => (
                ActionType::Other {
                    description: format!("Tool: {}", self.get_name()),
                },
                self.get_name().to_string(),
            ),
        }
    }
}

/* ===========================
Typed tool results for Cursor
=========================== */

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorShellOutcome {
    #[serde(default)]
    pub stdout: Option<String>,
    #[serde(default)]
    pub stderr: Option<String>,
    #[serde(default, rename = "exitCode")]
    pub exit_code: Option<i32>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorShellWrappedResult {
    #[serde(default)]
    pub success: Option<CursorShellOutcome>,
    #[serde(default)]
    pub failure: Option<CursorShellOutcome>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
#[serde(untagged)]
pub enum CursorShellResult {
    Wrapped(CursorShellWrappedResult),
    Flat(CursorShellOutcome),
    Unknown(serde_json::Value),
}

impl CursorShellResult {
    pub fn into_outcome(self) -> Option<CursorShellOutcome> {
        match self {
            CursorShellResult::Flat(o) => Some(o),
            CursorShellResult::Wrapped(w) => w.success.or(w.failure),
            CursorShellResult::Unknown(_) => None,
        }
    }
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorMcpTextInner {
    pub text: String,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorMcpContentItem {
    #[serde(default)]
    pub text: Option<CursorMcpTextInner>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorMcpOutcome {
    #[serde(default)]
    pub content: Option<Vec<CursorMcpContentItem>>,
    #[serde(default, rename = "isError")]
    pub is_error: Option<bool>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorMcpWrappedResult {
    #[serde(default)]
    pub success: Option<CursorMcpOutcome>,
    #[serde(default)]
    pub failure: Option<CursorMcpOutcome>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
#[serde(untagged)]
pub enum CursorMcpResult {
    Wrapped(CursorMcpWrappedResult),
    Flat(CursorMcpOutcome),
    Unknown(serde_json::Value),
}

impl CursorMcpResult {
    pub fn into_markdown(self) -> Option<String> {
        let outcome = match self {
            CursorMcpResult::Flat(o) => Some(o),
            CursorMcpResult::Wrapped(w) => w.success.or(w.failure),
            CursorMcpResult::Unknown(_) => None,
        }?;

        let items = outcome.content.unwrap_or_default();
        let mut parts: Vec<String> = Vec::new();
        for item in items {
            if let Some(t) = item.text {
                parts.push(t.text);
            }
        }
        if parts.is_empty() {
            None
        } else {
            Some(parts.join("\n\n"))
        }
    }
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorShellArgs {
    pub command: String,
    #[serde(default, alias = "working_directory", alias = "workingDirectory")]
    pub working_directory: Option<String>,
    #[serde(default)]
    pub timeout: Option<u64>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorLsArgs {
    pub path: String,
    #[serde(default)]
    pub ignore: Vec<String>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorGlobArgs {
    #[serde(default, alias = "globPattern", alias = "glob_pattern")]
    pub glob_pattern: Option<String>,
    #[serde(default)]
    pub path: Option<String>,
    #[serde(default, alias = "target_directory")]
    pub target_directory: Option<String>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorGrepArgs {
    pub pattern: String,
    #[serde(default)]
    pub path: Option<String>,
    #[serde(default, alias = "glob")]
    pub glob_filter: Option<String>,
    #[serde(default, alias = "outputMode", alias = "output_mode")]
    pub output_mode: Option<String>,
    #[serde(default, alias = "-i", alias = "caseInsensitive")]
    pub case_insensitive: Option<bool>,
    #[serde(default)]
    pub multiline: Option<bool>,
    #[serde(default, alias = "headLimit", alias = "head_limit")]
    pub head_limit: Option<u64>,
    #[serde(default)]
    pub r#type: Option<String>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorWriteArgs {
    pub path: String,
    #[serde(
        default,
        alias = "fileText",
        alias = "file_text",
        alias = "contents",
        alias = "content"
    )]
    pub contents: Option<String>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorReadArgs {
    pub path: String,
    #[serde(default)]
    pub offset: Option<u64>,
    #[serde(default)]
    pub limit: Option<u64>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorEditArgs {
    pub path: String,
    #[serde(default, rename = "applyPatch")]
    pub apply_patch: Option<CursorApplyPatch>,
    #[serde(default, rename = "strReplace")]
    pub str_replace: Option<CursorStrReplace>,
    #[serde(default, rename = "multiStrReplace")]
    pub multi_str_replace: Option<CursorMultiStrReplace>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorApplyPatch {
    #[serde(rename = "patchContent")]
    pub patch_content: String,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorStrReplace {
    #[serde(rename = "oldText")]
    pub old_text: String,
    #[serde(rename = "newText")]
    pub new_text: String,
    #[serde(default, rename = "replaceAll")]
    pub replace_all: Option<bool>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorMultiStrReplace {
    pub edits: Vec<CursorMultiEditItem>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorMultiEditItem {
    #[serde(rename = "oldText")]
    pub old_text: String,
    #[serde(rename = "newText")]
    pub new_text: String,
    #[serde(default, rename = "replaceAll")]
    pub replace_all: Option<bool>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorDeleteArgs {
    pub path: String,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorUpdateTodosArgs {
    #[serde(default)]
    pub todos: Option<Vec<CursorTodoItem>>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorMcpArgs {
    pub name: String,
    #[serde(default)]
    pub args: serde_json::Value,
    #[serde(default, alias = "providerIdentifier")]
    pub provider_identifier: Option<String>,
    #[serde(default, alias = "toolName")]
    pub tool_name: Option<String>,
}

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq)]
pub struct CursorTodoItem {
    #[serde(default)]
    pub id: Option<String>,
    pub content: String,
    pub status: String,
    #[serde(default, rename = "createdAt")]
    pub created_at: Option<String>,
    #[serde(default, rename = "updatedAt")]
    pub updated_at: Option<String>,
    #[serde(default)]
    pub dependencies: Option<Vec<String>>,
}

/* ===========================
Tests
=========================== */

#[cfg(test)]
mod tests {
    use std::sync::Arc;

    use utils::msg_store::MsgStore;

    use super::*;

    #[tokio::test]
    async fn test_cursor_streaming_patch_generation() {
        // Avoid relying on feature flag in tests; construct with a dummy command
        let executor = Cursor {
            // No command field needed anymore
            append_prompt: AppendPrompt::default(),
            force: None,
            model: None,
            cmd: Default::default(),
        };
        let msg_store = Arc::new(MsgStore::new());
        let current_dir = std::path::PathBuf::from("/tmp/test-worktree");

        // A minimal synthetic init + assistant micro-chunks (as Cursor would emit)
        msg_store.push_stdout(format!(
            "{}\n",
            r#"{"type":"system","subtype":"init","session_id":"sess-123","model":"OpenAI GPT-5"}"#
        ));
        msg_store.push_stdout(format!(
            "{}\n",
            r#"{"type":"assistant","message":{"role":"assistant","content":[{"type":"text","text":"Hello"}]}}"#
        ));
        msg_store.push_stdout(format!(
            "{}\n",
            r#"{"type":"assistant","message":{"role":"assistant","content":[{"type":"text","text":" world"}]}}"#
        ));
        msg_store.push_finished();

        executor.normalize_logs(msg_store.clone(), &current_dir);

        tokio::time::sleep(tokio::time::Duration::from_millis(150)).await;

        // Verify patches were emitted (system init + assistant add/replace)
        let history = msg_store.get_history();
        let patch_count = history
            .iter()
            .filter(|m| matches!(m, utils::log_msg::LogMsg::JsonPatch(_)))
            .count();
        assert!(
            patch_count >= 2,
            "Expected at least 2 patches, got {patch_count}"
        );
    }

    #[test]
    fn test_session_id_extraction_from_system_line() {
        // Ensure we can parse and find session_id from a system JSON line
        let system_line = r#"{"type":"system","subtype":"init","session_id":"abc-xyz","model":"Claude 4 Sonnet"}"#;
        let parsed: CursorJson = serde_json::from_str(system_line).unwrap();
        assert_eq!(parsed.extract_session_id().as_deref(), Some("abc-xyz"));
    }

    #[test]
    fn test_cursor_tool_call_parsing() {
        // Test known variant (from reference JSONL)
        let shell_tool_json = r#"{"shellToolCall":{"args":{"command":"wc -l drill.md","workingDirectory":"","timeout":0}}}"#;
        let parsed: CursorToolCall = serde_json::from_str(shell_tool_json).unwrap();

        match parsed {
            CursorToolCall::Shell { args, result } => {
                assert_eq!(args.command, "wc -l drill.md");
                assert_eq!(args.working_directory, Some("".to_string()));
                assert_eq!(args.timeout, Some(0));
                assert_eq!(result, None);
            }
            _ => panic!("Expected Shell variant"),
        }

        // Test unknown variant (captures raw data)
        let unknown_tool_json =
            r#"{"unknownTool":{"args":{"someData":"value"},"result":{"status":"success"}}}"#;
        let parsed: CursorToolCall = serde_json::from_str(unknown_tool_json).unwrap();

        match parsed {
            CursorToolCall::Unknown { data } => {
                assert!(data.contains_key("unknownTool"));
                let unknown_tool = &data["unknownTool"];
                assert_eq!(unknown_tool["args"]["someData"], "value");
                assert_eq!(unknown_tool["result"]["status"], "success");
            }
            _ => panic!("Expected Unknown variant"),
        }
    }
}
</file>

<file path="crates/executors/src/executors/gemini.rs">
use std::{
    path::{Path, PathBuf},
    process::Stdio,
    sync::Arc,
};

use async_trait::async_trait;
use command_group::{AsyncCommandGroup, AsyncGroupChild};
use futures::{StreamExt, stream::BoxStream};
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use tokio::{
    fs::{self, OpenOptions},
    io::AsyncWriteExt,
    process::Command,
};
use ts_rs::TS;
use utils::{msg_store::MsgStore, shell::get_shell_command};

use crate::{
    command::{CmdOverrides, CommandBuilder, apply_overrides},
    executors::{AppendPrompt, ExecutorError, StandardCodingAgentExecutor},
    logs::{
        NormalizedEntry, NormalizedEntryType, plain_text_processor::PlainTextLogProcessor,
        stderr_processor::normalize_stderr_logs, utils::EntryIndexProvider,
    },
    stdout_dup,
};

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema)]
#[serde(rename_all = "snake_case")]
pub enum GeminiModel {
    Default, // no --model flag
    Flash,   // --model gemini-2.5-flash
}

impl GeminiModel {
    fn base_command(&self) -> &'static str {
        "npx -y @google/gemini-cli@latest"
    }

    fn build_command_builder(&self) -> CommandBuilder {
        let mut builder = CommandBuilder::new(self.base_command());

        if let GeminiModel::Flash = self {
            builder = builder.extend_params(["--model", "gemini-2.5-flash"]);
        }

        builder
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema)]
pub struct Gemini {
    #[serde(default)]
    pub append_prompt: AppendPrompt,
    pub model: GeminiModel,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub yolo: Option<bool>,
    #[serde(flatten)]
    pub cmd: CmdOverrides,
}

impl Gemini {
    fn build_command_builder(&self) -> CommandBuilder {
        let mut builder = self.model.build_command_builder();

        if self.yolo.unwrap_or(false) {
            builder = builder.extend_params(["--yolo"]);
        }

        apply_overrides(builder, &self.cmd)
    }
}

#[async_trait]
impl StandardCodingAgentExecutor for Gemini {
    async fn spawn(
        &self,
        current_dir: &Path,
        prompt: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        let (shell_cmd, shell_arg) = get_shell_command();
        let gemini_command = self.build_command_builder().build_initial();

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(gemini_command)
            .env("NODE_NO_WARNINGS", "1");

        let mut child = command.group_spawn()?;

        // Write prompt to stdin
        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        // Duplicate stdout for session logging
        let duplicate_stdout = stdout_dup::duplicate_stdout(&mut child)?;
        tokio::spawn(Self::record_session(
            duplicate_stdout,
            current_dir.to_path_buf(),
            prompt.to_string(),
            false,
        ));

        Ok(child)
    }

    async fn spawn_follow_up(
        &self,
        current_dir: &Path,
        prompt: &str,
        _session_id: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        // Build comprehensive prompt with session context
        let followup_prompt = self.build_followup_prompt(current_dir, prompt).await?;

        let (shell_cmd, shell_arg) = get_shell_command();
        let gemini_command = self.build_command_builder().build_follow_up(&[]);

        let mut command = Command::new(shell_cmd);

        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(gemini_command)
            .env("NODE_NO_WARNINGS", "1");

        let mut child = command.group_spawn()?;

        // Write comprehensive prompt to stdin
        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(followup_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        // Duplicate stdout for session logging (resume existing session)
        let duplicate_stdout = stdout_dup::duplicate_stdout(&mut child)?;
        tokio::spawn(Self::record_session(
            duplicate_stdout,
            current_dir.to_path_buf(),
            prompt.to_string(),
            true,
        ));

        Ok(child)
    }

    /// Parses both stderr and stdout logs for Gemini executor using PlainTextLogProcessor.
    ///
    /// - Stderr: uses the standard stderr log processor, which formats stderr output as ErrorMessage entries.
    /// - Stdout: applies custom `format_chunk` to insert line breaks on period-to-capital transitions,
    ///   then create assitant messages from the output.
    ///
    /// Each entry is converted into an `AssistantMessage` or `ErrorMessage` and emitted as patches.
    ///
    /// # Example
    ///
    /// ```rust,ignore
    /// gemini.normalize_logs(msg_store.clone(), &worktree_path);
    /// ```
    ///
    /// Subsequent queries to `msg_store` will receive JSON patches representing parsed log entries.
    /// Sets up log normalization for the Gemini executor:
    /// - stderr via [`normalize_stderr_logs`]
    /// - stdout via [`PlainTextLogProcessor`] with Gemini-specific formatting and default heuristics
    fn normalize_logs(&self, msg_store: Arc<MsgStore>, worktree_path: &Path) {
        let entry_index_counter = EntryIndexProvider::start_from(&msg_store);
        normalize_stderr_logs(msg_store.clone(), entry_index_counter.clone());

        // Send session ID to msg_store to enable follow-ups
        msg_store.push_session_id(
            worktree_path
                .file_name()
                .unwrap_or_default()
                .to_string_lossy()
                .to_string(),
        );

        // Normalize Agent logs
        tokio::spawn(async move {
            let mut stdout = msg_store.stdout_chunked_stream();

            // Create a processor with Gemini-specific formatting
            let mut processor = Self::create_gemini_style_processor(entry_index_counter);

            while let Some(Ok(chunk)) = stdout.next().await {
                for patch in processor.process(chunk) {
                    msg_store.push_patch(patch);
                }
            }
        });
    }

    // MCP configuration methods
    fn default_mcp_config_path(&self) -> Option<std::path::PathBuf> {
        dirs::home_dir().map(|home| home.join(".gemini").join("settings.json"))
    }
}

impl Gemini {
    /// Creates a PlainTextLogProcessor that applies Gemini's sentence-break heuristics.
    ///
    /// This processor formats chunks by inserting line breaks at period-to-capital transitions
    /// and filters out Gemini CLI noise messages.
    pub(crate) fn create_gemini_style_processor(
        index_provider: EntryIndexProvider,
    ) -> PlainTextLogProcessor {
        PlainTextLogProcessor::builder()
            .normalized_entry_producer(Box::new(|content: String| NormalizedEntry {
                timestamp: None,
                entry_type: NormalizedEntryType::AssistantMessage,
                content,
                metadata: None,
            }))
            .format_chunk(Box::new(|partial, chunk| {
                Self::format_stdout_chunk(&chunk, partial.unwrap_or(""))
            }))
            .transform_lines(Box::new(|lines: &mut Vec<String>| {
                lines.retain(|l| l != "Data collection is disabled.\n");
            }))
            .index_provider(index_provider)
            .build()
    }

    /// Make Gemini output more readable by inserting line breaks where periods are directly
    /// followed by capital letters (common Gemini CLI formatting issue).
    /// Handles both intra-chunk and cross-chunk period-to-capital transitions.
    fn format_stdout_chunk(content: &str, accumulated_message: &str) -> String {
        let mut result = String::with_capacity(content.len() + 100);
        let chars: Vec<char> = content.chars().collect();

        // Check for cross-chunk boundary: previous chunk ended with period, current starts with capital
        if !accumulated_message.is_empty() && !content.is_empty() {
            let ends_with_period = accumulated_message.ends_with('.');
            let starts_with_capital = chars
                .first()
                .map(|&c| c.is_uppercase() && c.is_alphabetic())
                .unwrap_or(false);

            if ends_with_period && starts_with_capital {
                result.push('\n');
            }
        }

        // Handle intra-chunk period-to-capital transitions
        for i in 0..chars.len() {
            result.push(chars[i]);

            // Check if current char is '.' and next char is uppercase letter (no space between)
            if chars[i] == '.' && i + 1 < chars.len() {
                let next_char = chars[i + 1];
                if next_char.is_uppercase() && next_char.is_alphabetic() {
                    result.push('\n');
                }
            }
        }

        result
    }

    async fn record_session(
        mut stdout_stream: BoxStream<'static, std::io::Result<String>>,
        current_dir: PathBuf,
        prompt: String,
        resume_session: bool,
    ) {
        let file_path = Self::get_session_file_path(&current_dir).await;

        // Ensure the directory exists
        if let Some(parent) = file_path.parent() {
            let _ = fs::create_dir_all(parent).await;
        }

        // If not resuming session, delete the file first
        if !resume_session {
            let _ = fs::remove_file(&file_path).await;
        }

        // Always append from here on
        let mut file = match OpenOptions::new()
            .create(true)
            .append(true)
            .open(&file_path)
            .await
        {
            Ok(file) => file,
            Err(_) => {
                tracing::error!("Failed to open session file: {:?}", file_path);
                return;
            }
        };

        // Write user message as normalized entry
        let mut user_message_json = serde_json::to_string(&NormalizedEntry {
            timestamp: None,
            entry_type: NormalizedEntryType::UserMessage,
            content: prompt,
            metadata: None,
        })
        .unwrap_or_default();
        user_message_json.push('\n');
        let _ = file.write_all(user_message_json.as_bytes()).await;

        // Read stdout incrementally and append assistant message
        let mut stdout_content = String::new();

        // Read stdout until the process finishes
        while let Some(Ok(chunk)) = stdout_stream.next().await {
            stdout_content.push_str(&chunk);
        }

        let mut assistant_message_json = serde_json::to_string(&NormalizedEntry {
            timestamp: None,
            entry_type: NormalizedEntryType::AssistantMessage,
            content: stdout_content,
            metadata: None,
        })
        .unwrap_or_default();
        assistant_message_json.push('\n');
        let _ = file.write_all(assistant_message_json.as_bytes()).await;
    }

    /// Build comprehensive prompt with session context for follow-up execution
    async fn build_followup_prompt(
        &self,
        current_dir: &Path,
        prompt: &str,
    ) -> Result<String, ExecutorError> {
        let session_file_path = Self::get_session_file_path(current_dir).await;

        // Read existing session context
        let session_context = fs::read_to_string(&session_file_path).await.map_err(|e| {
            ExecutorError::FollowUpNotSupported(format!(
                "No existing Gemini session found for this worktree. Session file not found at {session_file_path:?}: {e}"
            ))
        })?;

        Ok(format!(
            r#"RESUME CONTEXT FOR CONTINUING TASK

=== EXECUTION HISTORY ===
The following is the conversation history from this session:
{session_context}

=== CURRENT REQUEST ===
{prompt}

=== INSTRUCTIONS ===
You are continuing work on the above task. The execution history shows the previous conversation in this session. Please continue from where the previous execution left off, taking into account all the context provided above.{}
"#,
            self.append_prompt.get().unwrap_or_default(),
        ))
    }

    fn get_sessions_base_dir() -> PathBuf {
        // Determine base directory under user's home
        let home = dirs::home_dir().unwrap_or_else(std::env::temp_dir);
        if cfg!(debug_assertions) {
            home.join(".vibe-kanban")
                .join("dev")
                .join("gemini_sessions")
        } else {
            home.join(".vibe-kanban").join("gemini_sessions")
        }
    }

    fn get_legacy_sessions_base_dir() -> PathBuf {
        // Previous location was under the temp-based vibe-kanban dir
        utils::path::get_vibe_kanban_temp_dir().join("gemini_sessions")
    }

    async fn get_session_file_path(current_dir: &Path) -> PathBuf {
        let file_name = current_dir.file_name().unwrap_or_default();
        let new_base = Self::get_sessions_base_dir();
        let new_path = new_base.join(file_name);

        // Ensure base directory exists
        if let Some(parent) = new_path.parent() {
            let _ = fs::create_dir_all(parent).await;
        }

        // If the new file doesn't exist yet, try to migrate from legacy location
        let new_exists = fs::metadata(&new_path).await.is_ok();
        if !new_exists {
            let legacy_path = Self::get_legacy_sessions_base_dir().join(file_name);
            if fs::metadata(&legacy_path).await.is_ok() {
                if let Err(e) = fs::rename(&legacy_path, &new_path).await {
                    tracing::warn!(
                        "Failed to migrate Gemini session from {:?} to {:?}: {}",
                        legacy_path,
                        new_path,
                        e
                    );
                } else {
                    tracing::info!(
                        "Migrated Gemini session file from legacy temp directory to persistent directory: {:?}",
                        new_path
                    );
                }
            }
        }

        new_path
    }
}
</file>

<file path="crates/executors/src/executors/mod.rs">
use std::{path::Path, sync::Arc};

use async_trait::async_trait;
use command_group::AsyncGroupChild;
use enum_dispatch::enum_dispatch;
use futures_io::Error as FuturesIoError;
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use sqlx::Type;
use strum_macros::{Display, EnumDiscriminants, EnumString, VariantNames};
use thiserror::Error;
use ts_rs::TS;
use utils::msg_store::MsgStore;

use crate::{
    executors::{
        amp::Amp, claude::ClaudeCode, codex::Codex, cursor::Cursor, gemini::Gemini,
        opencode::Opencode, qwen::QwenCode,
    },
    mcp_config::McpConfig,
};

pub mod amp;
pub mod claude;
pub mod codex;
pub mod cursor;
pub mod gemini;
pub mod opencode;
pub mod qwen;

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, TS)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
pub enum BaseAgentCapability {
    SessionFork,
}

#[derive(Debug, Error)]
pub enum ExecutorError {
    #[error("Follow-up is not supported: {0}")]
    FollowUpNotSupported(String),
    #[error(transparent)]
    SpawnError(#[from] FuturesIoError),
    #[error("Unknown executor type: {0}")]
    UnknownExecutorType(String),
    #[error("I/O error: {0}")]
    Io(std::io::Error),
    #[error(transparent)]
    Json(#[from] serde_json::Error),
    #[error(transparent)]
    TomlSerialize(#[from] toml::ser::Error),
    #[error(transparent)]
    TomlDeserialize(#[from] toml::de::Error),
}

#[enum_dispatch]
#[derive(
    Debug, Clone, Serialize, Deserialize, PartialEq, TS, Display, EnumDiscriminants, VariantNames,
)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
#[strum(serialize_all = "SCREAMING_SNAKE_CASE")]
#[strum_discriminants(
    name(BaseCodingAgent),
    // Only add Hash; Eq/PartialEq are already provided by EnumDiscriminants.
    derive(EnumString, Hash, strum_macros::Display, Serialize, Deserialize, TS, Type),
    strum(serialize_all = "SCREAMING_SNAKE_CASE"),
    ts(use_ts_enum),
    serde(rename_all = "SCREAMING_SNAKE_CASE"),
    sqlx(type_name = "TEXT", rename_all = "SCREAMING_SNAKE_CASE")
)]
pub enum CodingAgent {
    ClaudeCode,
    Amp,
    Gemini,
    Codex,
    Opencode,
    Cursor,
    QwenCode,
}

impl CodingAgent {
    pub fn get_mcp_config(&self) -> McpConfig {
        match self {
            Self::Codex(_) => McpConfig::new(
                vec!["mcp_servers".to_string()],
                serde_json::json!({
                    "mcp_servers": {}
                }),
                self.preconfigured_mcp(),
                true,
            ),
            Self::Amp(_) => McpConfig::new(
                vec!["amp.mcpServers".to_string()],
                serde_json::json!({
                    "amp.mcpServers": {}
                }),
                self.preconfigured_mcp(),
                false,
            ),
            Self::Opencode(_) => McpConfig::new(
                vec!["mcp".to_string()],
                serde_json::json!({
                    "mcp": {},
                    "$schema": "https://opencode.ai/config.json"
                }),
                self.preconfigured_mcp(),
                false,
            ),
            _ => McpConfig::new(
                vec!["mcpServers".to_string()],
                serde_json::json!({
                    "mcpServers": {}
                }),
                self.preconfigured_mcp(),
                false,
            ),
        }
    }

    pub fn supports_mcp(&self) -> bool {
        self.default_mcp_config_path().is_some()
    }

    pub fn capabilities(&self) -> Vec<BaseAgentCapability> {
        match self {
            Self::ClaudeCode(_) => vec![BaseAgentCapability::SessionFork],
            Self::Amp(_) => vec![BaseAgentCapability::SessionFork],
            Self::Codex(_) => vec![BaseAgentCapability::SessionFork],
            Self::Gemini(_) | Self::Opencode(_) | Self::Cursor(_) | Self::QwenCode(_) => vec![],
        }
    }
}

#[async_trait]
#[enum_dispatch(CodingAgent)]
pub trait StandardCodingAgentExecutor {
    async fn spawn(
        &self,
        current_dir: &Path,
        prompt: &str,
    ) -> Result<AsyncGroupChild, ExecutorError>;
    async fn spawn_follow_up(
        &self,
        current_dir: &Path,
        prompt: &str,
        session_id: &str,
    ) -> Result<AsyncGroupChild, ExecutorError>;
    fn normalize_logs(&self, _raw_logs_event_store: Arc<MsgStore>, _worktree_path: &Path);

    // MCP configuration methods
    fn default_mcp_config_path(&self) -> Option<std::path::PathBuf>;

    async fn check_availability(&self) -> bool {
        self.default_mcp_config_path()
            .map(|path| path.exists())
            .unwrap_or(false)
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema)]
#[serde(transparent)]
#[schemars(
    title = "Append Prompt",
    description = "Extra text appended to the prompt",
    extend("format" = "textarea")
)]
#[derive(Default)]
pub struct AppendPrompt(pub Option<String>);

impl AppendPrompt {
    pub fn get(&self) -> Option<String> {
        self.0.clone()
    }

    pub fn combine_prompt(&self, prompt: &str) -> String {
        match self {
            AppendPrompt(Some(value)) => format!("{prompt}{value}"),
            AppendPrompt(None) => prompt.to_string(),
        }
    }
}
</file>

<file path="crates/executors/src/executors/opencode.rs">
mod share_bridge;

use std::{
    path::{Path, PathBuf},
    process::Stdio,
    sync::Arc,
};

use async_trait::async_trait;
use command_group::{AsyncCommandGroup, AsyncGroupChild};
use fork_stream::StreamExt as _;
use futures::{StreamExt, future::ready, stream::BoxStream};
use lazy_static::lazy_static;
use regex::Regex;
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use tokio::{io::AsyncWriteExt, process::Command};
use ts_rs::TS;
use utils::{msg_store::MsgStore, path::make_path_relative, shell::get_shell_command};

use crate::{
    command::{CmdOverrides, CommandBuilder, apply_overrides},
    executors::{
        AppendPrompt, ExecutorError, StandardCodingAgentExecutor,
        opencode::share_bridge::Bridge as ShareBridge,
    },
    logs::{
        ActionType, FileChange, NormalizedEntry, NormalizedEntryType, TodoItem,
        utils::EntryIndexProvider,
    },
    stdout_dup,
};

// Typed structures for oc-share tool state
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
struct OcToolInput {
    #[serde(rename = "filePath", default)]
    file_path: Option<String>,
    #[serde(default)]
    path: Option<String>,
    #[serde(default)]
    include: Option<String>,
    #[serde(default)]
    pattern: Option<String>,
    #[serde(default)]
    command: Option<String>,
    #[serde(default)]
    description: Option<String>,
    #[serde(default)]
    url: Option<String>,
    #[serde(default)]
    format: Option<String>,
    #[serde(default)]
    timeout: Option<u64>,
    #[serde(rename = "oldString", default)]
    old_string: Option<String>,
    #[serde(rename = "newString", default)]
    new_string: Option<String>,
    #[serde(rename = "replaceAll", default)]
    replace_all: Option<bool>,
    #[serde(default)]
    content: Option<String>,
    #[serde(default)]
    todos: Option<Vec<TodoInfo>>,
}

#[derive(Debug, Clone, Serialize, Deserialize, Default)]
struct OcToolMetadata {
    #[serde(default)]
    description: Option<String>,
    #[serde(default)]
    exit: Option<i32>,
    #[serde(default)]
    diff: Option<String>,
    #[serde(default)]
    count: Option<u64>,
    #[serde(default)]
    truncated: Option<bool>,
    #[serde(default)]
    preview: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, Default)]
struct OcToolState {
    #[serde(default)]
    input: Option<OcToolInput>,
    #[serde(default)]
    metadata: Option<OcToolMetadata>,
    #[serde(default)]
    output: Option<String>,
    #[serde(default)]
    status: Option<String>,
    #[serde(default)]
    title: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema)]
pub struct Opencode {
    #[serde(default)]
    pub append_prompt: AppendPrompt,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub model: Option<String>,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub agent: Option<String>,
    #[serde(flatten)]
    pub cmd: CmdOverrides,
}

impl Opencode {
    fn build_command_builder(&self) -> CommandBuilder {
        let mut builder = CommandBuilder::new("npx -y opencode-ai@latest run").params([
            "--print-logs",
            "--log-level",
            "ERROR",
        ]);

        if let Some(model) = &self.model {
            builder = builder.extend_params(["--model", model]);
        }

        if let Some(agent) = &self.agent {
            builder = builder.extend_params(["--agent", agent]);
        }

        apply_overrides(builder, &self.cmd)
    }
}

#[async_trait]
impl StandardCodingAgentExecutor for Opencode {
    async fn spawn(
        &self,
        current_dir: &Path,
        prompt: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        // Start a dedicated local share bridge bound to this opencode process
        let bridge = ShareBridge::start().await.map_err(ExecutorError::Io)?;
        let (shell_cmd, shell_arg) = get_shell_command();
        let opencode_command = self.build_command_builder().build_initial();

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped()) // Keep stdout but we won't use it
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(opencode_command)
            .env("NODE_NO_WARNINGS", "1")
            .env("OPENCODE_AUTO_SHARE", "1")
            .env("OPENCODE_API", bridge.base_url.clone());

        let mut child = match command.group_spawn() {
            Ok(c) => c,
            Err(e) => {
                // If opencode fails to start, shut down the bridge to free the port
                bridge.shutdown().await;
                return Err(ExecutorError::SpawnError(e));
            }
        };

        // Write prompt to stdin
        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }
        // Transfer share events as lines for normalization through stdout
        let (mut dup_stream, appender) = stdout_dup::tee_stdout_with_appender(&mut child)?;
        let mut rx = bridge.subscribe();
        tokio::spawn(async move {
            while let Ok(crate::executors::opencode::share_bridge::ShareEvent::Sync(mut req)) =
                rx.recv().await
            {
                req.secret.clear();
                if let Ok(json) = serde_json::to_string(&req) {
                    appender.append_line(format!("{}{}", Opencode::SHARE_PREFIX, json));
                }
            }
        });

        // Monitor child's stdout end; when it closes, shut down the bridge to release the port
        let bridge_for_shutdown = bridge.clone();
        tokio::spawn(async move {
            use futures::StreamExt;
            while let Some(_chunk) = dup_stream.next().await {}
            tracing::debug!("Opencode process stdout closed");
            bridge_for_shutdown.shutdown().await;
        });
        Ok(child)
    }

    async fn spawn_follow_up(
        &self,
        current_dir: &Path,
        prompt: &str,
        session_id: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        // Start a dedicated local share bridge bound to this opencode process
        let bridge = ShareBridge::start().await.map_err(ExecutorError::Io)?;
        let (shell_cmd, shell_arg) = get_shell_command();
        let opencode_command = self
            .build_command_builder()
            .build_follow_up(&["--session".to_string(), session_id.to_string()]);

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped()) // Keep stdout but we won't use it
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(&opencode_command)
            .env("NODE_NO_WARNINGS", "1")
            .env("OPENCODE_AUTO_SHARE", "1")
            .env("OPENCODE_API", bridge.base_url.clone());

        let mut child = match command.group_spawn() {
            Ok(c) => c,
            Err(e) => {
                bridge.shutdown().await;
                return Err(ExecutorError::SpawnError(e));
            }
        };

        // Write prompt to stdin
        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }
        // Transfer share events as lines for normalization through stdout
        let (mut dup_stream, appender) = stdout_dup::tee_stdout_with_appender(&mut child)?;
        let mut rx = bridge.subscribe();
        tokio::spawn(async move {
            while let Ok(crate::executors::opencode::share_bridge::ShareEvent::Sync(mut req)) =
                rx.recv().await
            {
                req.secret.clear();
                if let Ok(json) = serde_json::to_string(&req) {
                    appender.append_line(format!("{}{}", Opencode::SHARE_PREFIX, json));
                }
            }
        });

        let bridge_for_shutdown = bridge.clone();
        tokio::spawn(async move {
            use futures::StreamExt;
            while let Some(_chunk) = dup_stream.next().await {}
            bridge_for_shutdown.shutdown().await;
        });
        Ok(child)
    }

    /// Normalize logs for OpenCode executor
    ///
    /// This implementation uses three separate threads:
    /// 1. Session ID thread: read by line, search for session ID format, store it.
    /// 2. Error log recognition thread: read by line, identify error log lines, store them as error messages.
    /// 3. Main normalizer thread: read stderr by line, filter out log lines, send lines (with '\n' appended) to plain text normalizer,
    ///    then define predicate for split and create appropriate normalized entry (either assistant or tool call).
    fn normalize_logs(&self, msg_store: Arc<MsgStore>, worktree_path: &Path) {
        let entry_index_counter = EntryIndexProvider::start_from(&msg_store);

        let stderr_lines = msg_store
            .stderr_lines_stream()
            .filter_map(|res| ready(res.ok()))
            .map(|line| strip_ansi_escapes::strip_str(&line))
            .fork();

        // Log line: INFO  2025-08-05T10:17:26 +1ms service=session id=ses_786439b6dffe4bLqNBS4fGd7mJ
        // error line: !  some error message
        let log_lines = stderr_lines
            .clone()
            .filter(|line| {
                ready(OPENCODE_LOG_REGEX.is_match(line) || LogUtils::is_error_line(line))
            })
            .boxed();

        // Process log lines, which contain error messages. We now source session ID
        // from the oc-share stream instead of stderr.
        tokio::spawn(Self::process_opencode_log_lines(
            log_lines,
            msg_store.clone(),
            entry_index_counter.clone(),
            worktree_path.to_path_buf(),
        ));

        // Also parse share events from stdout
        let share_events = msg_store
            .stdout_lines_stream()
            .filter_map(|res| ready(res.ok()))
            .filter(|line| ready(line.starts_with(Opencode::SHARE_PREFIX)))
            .map(|line| line[Opencode::SHARE_PREFIX.len()..].to_string())
            .boxed();
        tokio::spawn(Self::process_share_events(
            share_events,
            worktree_path.to_path_buf(),
            entry_index_counter.clone(),
            msg_store.clone(),
        ));
    }

    // MCP configuration methods
    fn default_mcp_config_path(&self) -> Option<std::path::PathBuf> {
        #[cfg(unix)]
        {
            xdg::BaseDirectories::with_prefix("opencode").get_config_file("opencode.json")
        }
        #[cfg(not(unix))]
        {
            dirs::config_dir().map(|config| config.join("opencode").join("opencode.json"))
        }
    }
}
impl Opencode {
    const SHARE_PREFIX: &'static str = "[oc-share] ";
    async fn process_opencode_log_lines(
        mut log_lines: BoxStream<'_, String>,
        msg_store: Arc<MsgStore>,
        entry_index_counter: EntryIndexProvider,
        _worktree_path: PathBuf,
    ) {
        while let Some(line) = log_lines.next().await {
            if line.starts_with("ERROR") || LogUtils::is_error_line(&line) {
                let entry = NormalizedEntry {
                    timestamp: None,
                    entry_type: NormalizedEntryType::ErrorMessage,
                    content: line.clone(),
                    metadata: None,
                };

                // Create a patch for this single entry
                let patch = crate::logs::utils::ConversationPatch::add_normalized_entry(
                    entry_index_counter.next(),
                    entry,
                );
                msg_store.push_patch(patch);
            }
        }
    }
}

impl Opencode {
    /// Parse share events and emit normalized patches
    async fn process_share_events(
        mut lines: BoxStream<'_, String>,
        worktree_path: PathBuf,
        entry_index_counter: EntryIndexProvider,
        msg_store: Arc<MsgStore>,
    ) {
        use std::collections::HashMap;

        use serde::Deserialize;

        use crate::logs::utils::ConversationPatch;

        #[derive(Debug, Clone, Deserialize)]
        #[allow(dead_code)]
        struct TimeObj {
            #[serde(default)]
            start: Option<i64>,
            #[serde(default)]
            end: Option<i64>,
        }

        // Tool input/state structures are defined at module level (OcToolInput/OcToolState)

        #[derive(Debug, Clone, Deserialize)]
        #[serde(tag = "type", rename_all = "lowercase")]
        #[allow(clippy::large_enum_variant)]
        #[allow(dead_code)]
        enum ShareContent {
            Text {
                id: String,
                #[serde(rename = "messageID")]
                message_id: String,
                #[serde(rename = "sessionID")]
                session_id: String,
                #[serde(default)]
                text: Option<String>,
                #[serde(default)]
                time: Option<TimeObj>,
            },
            Tool {
                id: String,
                #[serde(rename = "messageID")]
                message_id: String,
                #[serde(rename = "sessionID")]
                session_id: String,
                #[serde(rename = "callID", default)]
                call_id: Option<String>,
                tool: String,
                #[serde(default)]
                state: Box<Option<OcToolState>>,
            },
        }

        #[derive(Debug, Clone, Deserialize)]
        #[allow(dead_code)]
        struct ShareSyncEnvelope {
            #[serde(rename = "sessionID")]
            session_id: String,
            secret: String,
            key: String,
            content: serde_json::Value,
        }

        let mut index_by_part: HashMap<String, usize> = HashMap::new();
        // For text aggregation across parts under the same message (scoped by session)
        // Key format: "{sessionID}:{messageID}"
        let mut index_by_message: HashMap<String, usize> = HashMap::new();
        let mut message_parts_order: HashMap<String, Vec<String>> = HashMap::new();
        let mut message_part_texts: HashMap<String, HashMap<String, String>> = HashMap::new();
        let mut message_aggregated: HashMap<String, String> = HashMap::new();
        // Segment tracking per message to force splits after tool events
        // base_key = "{sessionID}:{messageID}", seg_key = "{base_key}#<n>"
        let mut message_segment: HashMap<String, usize> = HashMap::new();
        let mut message_pending_break: HashMap<String, bool> = HashMap::new();
        let mut message_roles: HashMap<String, String> = HashMap::new();
        let mut session_id_set = false;

        use std::collections::hash_map::Entry;
        let mut upsert_by_part = |entry: NormalizedEntry, part_id: String| {
            let (idx, is_new) = match index_by_part.entry(part_id) {
                Entry::Occupied(o) => (*o.get(), false),
                Entry::Vacant(v) => {
                    let i = entry_index_counter.next();
                    v.insert(i);
                    (i, true)
                }
            };
            if is_new {
                ConversationPatch::add_normalized_entry(idx, entry)
            } else {
                ConversationPatch::replace(idx, entry)
            }
        };

        while let Some(line) = lines.next().await {
            let Ok(env) = serde_json::from_str::<ShareSyncEnvelope>(&line) else {
                continue;
            };
            // Record session id once from stream
            if !session_id_set {
                msg_store.push_session_id(env.session_id.clone());
                session_id_set = true;
            }

            // Capture message role metadata from session/message events
            if env.key.starts_with("session/message/") {
                #[derive(Deserialize)]
                struct MessageMeta {
                    id: String,
                    #[serde(default)]
                    role: Option<String>,
                }
                if let Ok(meta) = serde_json::from_value::<MessageMeta>(env.content.clone())
                    && let Some(role) = meta.role
                {
                    message_roles.insert(meta.id.clone(), role.clone());

                    // If we have aggregated text already for this message, create or update the entry now
                    let base_key = format!("{}:{}", env.session_id, meta.id);
                    let seg = *message_segment.get(&base_key).unwrap_or(&0);
                    let seg_key = format!("{base_key}#{seg}");
                    if let Some(content) = message_aggregated.get(&seg_key).cloned() {
                        // Skip emitting user role messages entirely
                        if role == "user" {
                            // Do not emit user text messages
                        } else {
                            let entry_type = match role.as_str() {
                                "system" => NormalizedEntryType::SystemMessage,
                                _ => NormalizedEntryType::AssistantMessage,
                            };
                            use std::collections::hash_map::Entry as HmEntry;
                            match index_by_message.entry(seg_key) {
                                HmEntry::Occupied(o) => {
                                    let idx = *o.get();
                                    let entry = NormalizedEntry {
                                        timestamp: None,
                                        entry_type,
                                        content,
                                        metadata: None,
                                    };
                                    msg_store.push_patch(ConversationPatch::replace(idx, entry));
                                }
                                HmEntry::Vacant(v) => {
                                    let idx = entry_index_counter.next();
                                    v.insert(idx);
                                    let entry = NormalizedEntry {
                                        timestamp: None,
                                        entry_type,
                                        content,
                                        metadata: None,
                                    };
                                    msg_store.push_patch(ConversationPatch::add_normalized_entry(
                                        idx, entry,
                                    ));
                                }
                            }
                        }
                    }
                }
                continue;
            }

            if !env.key.starts_with("session/part/") {
                continue;
            }

            match serde_json::from_value::<ShareContent>(env.content.clone()) {
                Ok(ShareContent::Text {
                    id,
                    message_id,
                    text,
                    ..
                }) => {
                    let text = text.unwrap_or_default();
                    // Scope aggregation by sessionID and segment to avoid cross-session and enforce breaks
                    let base_key = format!("{}:{}", env.session_id, message_id);
                    if message_pending_break.remove(&base_key).unwrap_or(false) {
                        let e = message_segment.entry(base_key.clone()).or_insert(0);
                        *e += 1;
                    }
                    let seg = *message_segment.get(&base_key).unwrap_or(&0);
                    let msg_key = format!("{base_key}#{seg}");

                    // Track parts order for this message
                    let parts_order = message_parts_order.entry(msg_key.clone()).or_default();
                    if !parts_order.iter().any(|p| p == &id) {
                        parts_order.push(id.clone());
                    }

                    // Update latest text for this part under the message
                    let part_texts = message_part_texts.entry(msg_key.clone()).or_default();
                    part_texts.insert(id.clone(), text);

                    // Rebuild aggregated message text by concatenating parts in stable order
                    let aggregated = parts_order
                        .iter()
                        .filter_map(|pid| part_texts.get(pid))
                        .cloned()
                        .collect::<Vec<_>>()
                        .join("");
                    message_aggregated.insert(msg_key.clone(), aggregated.clone());

                    // Determine role; if unknown yet, wait until message metadata arrives.
                    match message_roles.get(&message_id).map(|s| s.as_str()) {
                        Some("user") => {
                            // Do not emit user text messages
                        }
                        Some(role) => {
                            // Upsert by message id to keep a single entry per message
                            let (idx, is_new) = match index_by_message.entry(msg_key) {
                                Entry::Occupied(o) => (*o.get(), false),
                                Entry::Vacant(v) => {
                                    let i = entry_index_counter.next();
                                    v.insert(i);
                                    (i, true)
                                }
                            };
                            let entry_type = match role {
                                "system" => NormalizedEntryType::SystemMessage,
                                _ => NormalizedEntryType::AssistantMessage,
                            };
                            let entry = NormalizedEntry {
                                timestamp: None,
                                entry_type,
                                content: aggregated,
                                metadata: None,
                            };
                            let patch = if is_new {
                                ConversationPatch::add_normalized_entry(idx, entry)
                            } else {
                                ConversationPatch::replace(idx, entry)
                            };
                            msg_store.push_patch(patch);
                        }
                        None => {
                            // Role unknown; accumulate but don't emit yet
                        }
                    }
                }
                Ok(ShareContent::Tool {
                    id,
                    tool,
                    state,
                    message_id,
                    ..
                }) => {
                    // If there is pending text in the current segment, mark to break before next text
                    let base_key = format!("{}:{}", env.session_id, message_id);
                    let seg = *message_segment.get(&base_key).unwrap_or(&0);
                    let seg_key = format!("{base_key}#{seg}");
                    if message_aggregated
                        .get(&seg_key)
                        .map(|s| !s.is_empty())
                        .unwrap_or(false)
                    {
                        message_pending_break.insert(base_key.clone(), true);
                    }
                    let state = (*state).unwrap_or_default();
                    let status = state.status.as_deref().unwrap_or("");

                    let exit_status = state
                        .metadata
                        .as_ref()
                        .and_then(|m| m.exit)
                        .map(|code| crate::logs::CommandExitStatus::ExitCode { code });

                    let (result, mut content_text) = match status {
                        "completed" => {
                            let output = state.output.as_deref().unwrap_or("");
                            let title = state.title.as_deref().unwrap_or("");
                            let header = if title.is_empty() {
                                format!("{tool} completed")
                            } else {
                                format!("{tool}: {title}")
                            };
                            (
                                Some(crate::logs::ToolResult {
                                    r#type: crate::logs::ToolResultValueType::Markdown,
                                    value: serde_json::Value::String(output.to_string()),
                                }),
                                format!("{header}\n"),
                            )
                        }
                        "error" => {
                            let err = state
                                .metadata
                                .as_ref()
                                .and_then(|m| m.description.as_deref())
                                .unwrap_or("");
                            (
                                Some(crate::logs::ToolResult {
                                    r#type: crate::logs::ToolResultValueType::Markdown,
                                    value: serde_json::Value::String(format!("Error: {err}")),
                                }),
                                format!("{tool} error: {err}\n"),
                            )
                        }
                        "running" => (None, format!("{tool} started\n")),
                        _ => (None, String::new()),
                    };

                    // Compute concise normalized summary for known tools using a typed mapping
                    let worktree = worktree_path.to_string_lossy();
                    #[derive(Deserialize)]
                    #[serde(tag = "tool", rename_all = "lowercase")]
                    #[allow(dead_code)]
                    enum TypedTool {
                        #[serde(rename = "read")]
                        Read {
                            #[serde(default)]
                            input: OcToolInput,
                        },
                        #[serde(rename = "list")]
                        List {
                            #[serde(default)]
                            input: OcToolInput,
                        },
                        #[serde(rename = "grep")]
                        Grep {
                            #[serde(default)]
                            input: OcToolInput,
                        },
                        #[serde(rename = "glob")]
                        Glob {
                            #[serde(default)]
                            input: OcToolInput,
                        },
                        #[serde(rename = "webfetch")]
                        Webfetch {
                            #[serde(default)]
                            input: OcToolInput,
                        },
                        #[serde(other)]
                        Other,
                    }
                    if let Ok(v) = serde_json::to_value(&state.input).and_then(|input| {
                        serde_json::from_value::<TypedTool>(serde_json::json!({
                            "tool": tool,
                            "input": input
                        }))
                    }) {
                        match v {
                            TypedTool::Read { input } => {
                                let p = input.file_path.as_deref().unwrap_or("");
                                content_text = format!("`{}`", make_path_relative(p, &worktree));
                            }
                            TypedTool::List { input } => {
                                let p = input.path.as_deref().unwrap_or(".");
                                content_text = format!(
                                    "List directory: `{}`",
                                    make_path_relative(p, &worktree)
                                );
                            }
                            TypedTool::Grep { input } => {
                                let pat = input.pattern.as_deref().unwrap_or("");
                                let p = input.path.as_deref().unwrap_or(".");
                                let rel = make_path_relative(p, &worktree);
                                if let Some(inc) = input.include.as_deref() {
                                    content_text = format!("`{pat}` in `{rel}` ({inc})");
                                } else {
                                    content_text = format!("`{pat}` in `{rel}`");
                                }
                            }
                            TypedTool::Glob { input } => {
                                let pat = input.pattern.as_deref().unwrap_or("");
                                let p = input.path.as_deref().unwrap_or(".");
                                let rel = make_path_relative(p, &worktree);
                                content_text = format!("glob `{pat}` in `{rel}`");
                            }
                            TypedTool::Webfetch { input } => {
                                let url = input.url.as_deref().unwrap_or("");
                                content_text = format!("fetch `{url}`");
                            }
                            TypedTool::Other => {}
                        }
                    }

                    // Prepare normalized arguments for potential Tool action (fallback only)
                    let args_json = if let Some(input) = state.input.as_ref() {
                        let mut map = serde_json::Map::new();
                        if let Some(p) = input.file_path.as_deref() {
                            map.insert(
                                "filePath".into(),
                                serde_json::Value::String(make_path_relative(
                                    p,
                                    &worktree_path.to_string_lossy(),
                                )),
                            );
                        }
                        if let Some(p) = input.path.as_deref() {
                            map.insert(
                                "path".into(),
                                serde_json::Value::String(make_path_relative(
                                    p,
                                    &worktree_path.to_string_lossy(),
                                )),
                            );
                        }
                        if let Some(v) = input.include.as_ref() {
                            map.insert("include".into(), serde_json::Value::String(v.clone()));
                        }
                        if let Some(v) = input.pattern.as_ref() {
                            map.insert("pattern".into(), serde_json::Value::String(v.clone()));
                        }
                        if let Some(v) = input.command.as_ref() {
                            map.insert("command".into(), serde_json::Value::String(v.clone()));
                        }
                        if let Some(v) = input.description.as_ref() {
                            map.insert("description".into(), serde_json::Value::String(v.clone()));
                        }
                        if let Some(v) = input.url.as_ref() {
                            map.insert("url".into(), serde_json::Value::String(v.clone()));
                        }
                        if let Some(v) = input.format.as_ref() {
                            map.insert("format".into(), serde_json::Value::String(v.clone()));
                        }
                        if let Some(v) = input.timeout.as_ref() {
                            map.insert("timeout".into(), serde_json::Value::from(*v));
                        }
                        serde_json::Value::Object(map)
                    } else {
                        serde_json::Value::Null
                    };

                    // Derive ActionType and attach command results if applicable
                    let action_type = Self::derive_action_type(&tool, &state, &worktree_path);
                    let resolved_action_type = match action_type {
                        Some(mut at) => match (&mut at, &result) {
                            (ActionType::CommandRun { result: r, .. }, Some(res)) => {
                                *r = Some(crate::logs::CommandRunResult {
                                    exit_status: exit_status.clone(),
                                    output: res.value.as_str().map(|s| s.to_owned()),
                                });
                                at
                            }
                            _ => at,
                        },
                        None => ActionType::Tool {
                            tool_name: tool.clone(),
                            arguments: Some(args_json.clone()),
                            result: result.clone(),
                        },
                    };

                    let entry = NormalizedEntry {
                        timestamp: None,
                        entry_type: NormalizedEntryType::ToolUse {
                            tool_name: tool.clone(),
                            action_type: resolved_action_type,
                        },
                        content: content_text,
                        metadata: None,
                    };

                    let patch = upsert_by_part(entry, id);
                    msg_store.push_patch(patch);
                }
                Err(_) => {}
            }
        }
    }

    /// Map tool name and state to a rich ActionType used by frontend renderers.
    fn derive_action_type(
        tool_name: &str,
        state: &OcToolState,
        worktree_path: &Path,
    ) -> Option<ActionType> {
        // Deserialize "tool" + typed input into a tagged enum to avoid stringly logic
        #[derive(Deserialize)]
        #[serde(tag = "tool", rename_all = "lowercase")]
        #[allow(dead_code)]
        enum ActionTool {
            Read {
                input: OcToolInput,
            },
            Write {
                input: OcToolInput,
            },
            Edit {
                input: OcToolInput,
            },
            Bash {
                input: OcToolInput,
            },
            Grep {
                input: OcToolInput,
            },
            Glob {
                input: OcToolInput,
            },
            Webfetch {
                input: OcToolInput,
            },
            Task {
                input: OcToolInput,
            },
            Todowrite {
                input: OcToolInput,
            },
            Todoread,
            List {
                input: OcToolInput,
            },
            #[serde(other)]
            Other,
        }

        let input_json = serde_json::to_value(state.input.clone().unwrap_or_default())
            .unwrap_or(serde_json::Value::Null);
        let v = serde_json::json!({ "tool": tool_name, "input": input_json });
        let parsed: ActionTool = serde_json::from_value(v).unwrap_or(ActionTool::Other);
        match parsed {
            ActionTool::Read { input } => {
                let path = input.file_path.as_deref().unwrap_or("");
                Some(ActionType::FileRead {
                    path: make_path_relative(path, &worktree_path.to_string_lossy()),
                })
            }
            ActionTool::Write { input } => {
                let path = input.file_path.as_deref().unwrap_or("");
                let content = input.content.unwrap_or_default();
                Some(ActionType::FileEdit {
                    path: make_path_relative(path, &worktree_path.to_string_lossy()),
                    changes: vec![FileChange::Write { content }],
                })
            }
            ActionTool::Edit { input } => {
                let path = input.file_path.as_deref().unwrap_or("");
                let diff = state
                    .metadata
                    .as_ref()
                    .and_then(|m| m.diff.as_deref())
                    .unwrap_or("");
                if diff.is_empty() {
                    return None;
                }
                Some(ActionType::FileEdit {
                    path: make_path_relative(path, &worktree_path.to_string_lossy()),
                    changes: vec![FileChange::Edit {
                        unified_diff: diff.to_string(),
                        has_line_numbers: false,
                    }],
                })
            }
            ActionTool::Bash { input } => {
                let command = input.command.unwrap_or_default();
                Some(ActionType::CommandRun {
                    command,
                    result: None,
                })
            }
            ActionTool::Grep { input } => {
                let query = input.pattern.unwrap_or_default();
                Some(ActionType::Search { query })
            }
            ActionTool::Glob { input } => {
                let query = input.pattern.unwrap_or_default();
                Some(ActionType::Search { query })
            }
            ActionTool::Webfetch { input } => {
                let url = input.url.unwrap_or_default();
                Some(ActionType::WebFetch { url })
            }
            ActionTool::Todowrite { input } => {
                let todos = input
                    .todos
                    .unwrap_or_default()
                    .into_iter()
                    .map(|t| TodoItem {
                        content: t.content,
                        status: t.status,
                        priority: t.priority,
                    })
                    .collect::<Vec<_>>();
                Some(ActionType::TodoManagement {
                    todos,
                    operation: "write".into(),
                })
            }
            ActionTool::Todoread => Some(ActionType::TodoManagement {
                todos: vec![],
                operation: "read".into(),
            }),
            ActionTool::List { .. } | ActionTool::Task { .. } | ActionTool::Other => None,
        }
    }
}

// =============================================================================
// TOOL DEFINITIONS
// =============================================================================

/// TODO information structure
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema)]
pub struct TodoInfo {
    pub content: String,
    pub status: String,
    #[serde(default)]
    pub priority: Option<String>,
}

// =============================================================================
// Log interpretation UTILITIES
// =============================================================================

lazy_static! {
    // Accurate regex for OpenCode log lines: LEVEL timestamp +ms ...
    static ref OPENCODE_LOG_REGEX: Regex = Regex::new(r"^(INFO|DEBUG|WARN|ERROR)\s+\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\s+\+\d+\s*ms.*").unwrap();
}

/// Log utilities for OpenCode processing
pub struct LogUtils;

impl LogUtils {
    pub fn is_error_line(line: &str) -> bool {
        line.starts_with("!  ")
    }
}
</file>

<file path="crates/executors/src/executors/qwen.rs">
use std::{path::Path, process::Stdio, sync::Arc};

use async_trait::async_trait;
use command_group::{AsyncCommandGroup, AsyncGroupChild};
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use tokio::{io::AsyncWriteExt, process::Command};
use ts_rs::TS;
use utils::{msg_store::MsgStore, shell::get_shell_command};

use crate::{
    command::{CmdOverrides, CommandBuilder, apply_overrides},
    executors::{AppendPrompt, ExecutorError, StandardCodingAgentExecutor, gemini::Gemini},
    logs::{stderr_processor::normalize_stderr_logs, utils::EntryIndexProvider},
};

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema)]
pub struct QwenCode {
    #[serde(default)]
    pub append_prompt: AppendPrompt,
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub yolo: Option<bool>,
    #[serde(flatten)]
    pub cmd: CmdOverrides,
}

impl QwenCode {
    fn build_command_builder(&self) -> CommandBuilder {
        let mut builder = CommandBuilder::new("npx -y @qwen-code/qwen-code@latest");

        if self.yolo.unwrap_or(false) {
            builder = builder.extend_params(["--yolo"]);
        }

        apply_overrides(builder, &self.cmd)
    }
}

#[async_trait]
impl StandardCodingAgentExecutor for QwenCode {
    async fn spawn(
        &self,
        current_dir: &Path,
        prompt: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        let (shell_cmd, shell_arg) = get_shell_command();
        let qwen_command = self.build_command_builder().build_initial();

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(&qwen_command);

        let mut child = command.group_spawn()?;

        // Feed the prompt in, then close the pipe
        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        Ok(child)
    }

    async fn spawn_follow_up(
        &self,
        current_dir: &Path,
        prompt: &str,
        session_id: &str,
    ) -> Result<AsyncGroupChild, ExecutorError> {
        let (shell_cmd, shell_arg) = get_shell_command();
        let qwen_command = self
            .build_command_builder()
            .build_follow_up(&["--resume".to_string(), session_id.to_string()]);

        let combined_prompt = self.append_prompt.combine_prompt(prompt);

        let mut command = Command::new(shell_cmd);
        command
            .kill_on_drop(true)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::piped())
            .current_dir(current_dir)
            .arg(shell_arg)
            .arg(&qwen_command);

        let mut child = command.group_spawn()?;

        // Feed the followup prompt in, then close the pipe
        if let Some(mut stdin) = child.inner().stdin.take() {
            stdin.write_all(combined_prompt.as_bytes()).await?;
            stdin.shutdown().await?;
        }

        Ok(child)
    }

    fn normalize_logs(&self, msg_store: Arc<MsgStore>, current_dir: &Path) {
        // QwenCode has similar output format to Gemini CLI
        // Use Gemini's proven sentence-break formatting instead of simple replace
        let entry_index_counter = EntryIndexProvider::start_from(&msg_store);
        normalize_stderr_logs(msg_store.clone(), entry_index_counter.clone());

        // Send session ID to msg_store to enable follow-ups
        msg_store.push_session_id(
            current_dir
                .file_name()
                .unwrap_or_default()
                .to_string_lossy()
                .to_string(),
        );

        // Use Gemini's log processor for consistent formatting
        tokio::spawn(async move {
            use futures::StreamExt;
            let mut stdout = msg_store.stdout_chunked_stream();

            // Use Gemini's proven sentence-break heuristics
            let mut processor = Gemini::create_gemini_style_processor(entry_index_counter);

            while let Some(Ok(chunk)) = stdout.next().await {
                for patch in processor.process(chunk) {
                    msg_store.push_patch(patch);
                }
            }
        });
    }

    // MCP configuration methods
    fn default_mcp_config_path(&self) -> Option<std::path::PathBuf> {
        dirs::home_dir().map(|home| home.join(".qwen").join("settings.json"))
    }
}
</file>

<file path="crates/executors/src/logs/utils/entry_index.rs">
//! Entry Index Provider for thread-safe monotonic indexing

use std::sync::{
    Arc,
    atomic::{AtomicUsize, Ordering},
};

use json_patch::PatchOperation;
use utils::{log_msg::LogMsg, msg_store::MsgStore};

/// Thread-safe provider for monotonically increasing entry indexes
#[derive(Debug, Clone)]
pub struct EntryIndexProvider(Arc<AtomicUsize>);

impl EntryIndexProvider {
    /// Create a new index provider starting from 0 (private; prefer seeding)
    fn new() -> Self {
        Self(Arc::new(AtomicUsize::new(0)))
    }

    /// Get the next available index
    pub fn next(&self) -> usize {
        self.0.fetch_add(1, Ordering::Relaxed)
    }

    /// Get the current index without incrementing
    pub fn current(&self) -> usize {
        self.0.load(Ordering::Relaxed)
    }

    pub fn reset(&self) {
        self.0.store(0, Ordering::Relaxed);
    }

    /// Create a provider starting from the maximum existing normalized-entry index
    /// observed in prior JSON patches in `MsgStore`.
    pub fn start_from(msg_store: &MsgStore) -> Self {
        let provider = EntryIndexProvider::new();

        let max_index: Option<usize> = msg_store
            .get_history()
            .iter()
            .filter_map(|msg| {
                if let LogMsg::JsonPatch(patch) = msg {
                    patch.iter().find_map(|op| {
                        if let PatchOperation::Add(add) = op {
                            add.path
                                .strip_prefix("/entries/")
                                .and_then(|n_str| n_str.parse::<usize>().ok())
                        } else {
                            None
                        }
                    })
                } else {
                    None
                }
            })
            .max();

        let start_at = max_index.map_or(0, |n| n.saturating_add(1));
        provider.0.store(start_at, Ordering::Relaxed);
        provider
    }
}

impl Default for EntryIndexProvider {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
impl EntryIndexProvider {
    /// Test-only constructor for a fresh provider starting at 0
    pub fn test_new() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_entry_index_provider() {
        let provider = EntryIndexProvider::test_new();
        assert_eq!(provider.next(), 0);
        assert_eq!(provider.next(), 1);
        assert_eq!(provider.next(), 2);
    }

    #[test]
    fn test_entry_index_provider_clone() {
        let provider1 = EntryIndexProvider::test_new();
        let provider2 = provider1.clone();

        assert_eq!(provider1.next(), 0);
        assert_eq!(provider2.next(), 1);
        assert_eq!(provider1.next(), 2);
    }

    #[test]
    fn test_current_index() {
        let provider = EntryIndexProvider::test_new();
        assert_eq!(provider.current(), 0);

        provider.next();
        assert_eq!(provider.current(), 1);

        provider.next();
        assert_eq!(provider.current(), 2);
    }
}
</file>

<file path="crates/executors/src/logs/utils/mod.rs">
//! Utility modules for executor framework

pub mod entry_index;
pub mod patch;

pub use entry_index::EntryIndexProvider;
pub use patch::ConversationPatch;
</file>

<file path="crates/executors/src/logs/utils/patch.rs">
use json_patch::Patch;
use serde::{Deserialize, Serialize};
use serde_json::{from_value, json};
use ts_rs::TS;
use utils::diff::Diff;

use crate::logs::NormalizedEntry;

#[derive(Deserialize, Serialize, Debug, Clone, PartialEq, Eq, TS)]
#[serde(rename_all = "lowercase")]
enum PatchOperation {
    Add,
    Replace,
    Remove,
}

#[allow(clippy::large_enum_variant)]
#[derive(Serialize, TS)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE", tag = "type", content = "content")]
pub enum PatchType {
    NormalizedEntry(NormalizedEntry),
    Stdout(String),
    Stderr(String),
    Diff(Diff),
}

#[derive(Serialize)]
struct PatchEntry {
    op: PatchOperation,
    path: String,
    value: PatchType,
}

pub fn escape_json_pointer_segment(s: &str) -> String {
    s.replace('~', "~0").replace('/', "~1")
}

/// Helper functions to create JSON patches for conversation entries
pub struct ConversationPatch;

impl ConversationPatch {
    /// Create an ADD patch for a new conversation entry at the given index
    pub fn add_normalized_entry(entry_index: usize, entry: NormalizedEntry) -> Patch {
        let patch_entry = PatchEntry {
            op: PatchOperation::Add,
            path: format!("/entries/{entry_index}"),
            value: PatchType::NormalizedEntry(entry),
        };

        from_value(json!([patch_entry])).unwrap()
    }

    /// Create an ADD patch for a new string at the given index
    pub fn add_stdout(entry_index: usize, entry: String) -> Patch {
        let patch_entry = PatchEntry {
            op: PatchOperation::Add,
            path: format!("/entries/{entry_index}"),
            value: PatchType::Stdout(entry),
        };

        from_value(json!([patch_entry])).unwrap()
    }

    /// Create an ADD patch for a new string at the given index
    pub fn add_stderr(entry_index: usize, entry: String) -> Patch {
        let patch_entry = PatchEntry {
            op: PatchOperation::Add,
            path: format!("/entries/{entry_index}"),
            value: PatchType::Stderr(entry),
        };

        from_value(json!([patch_entry])).unwrap()
    }

    /// Create an ADD patch for a new diff at the given index
    pub fn add_diff(entry_index: String, diff: Diff) -> Patch {
        let patch_entry = PatchEntry {
            op: PatchOperation::Add,
            path: format!("/entries/{entry_index}"),
            value: PatchType::Diff(diff),
        };

        from_value(json!([patch_entry])).unwrap()
    }

    /// Create an ADD patch for a new diff at the given index
    pub fn replace_diff(entry_index: String, diff: Diff) -> Patch {
        let patch_entry = PatchEntry {
            op: PatchOperation::Replace,
            path: format!("/entries/{entry_index}"),
            value: PatchType::Diff(diff),
        };

        from_value(json!([patch_entry])).unwrap()
    }

    /// Create a REMOVE patch for removing a diff
    pub fn remove_diff(entry_index: String) -> Patch {
        from_value(json!([{
            "op": PatchOperation::Remove,
            "path": format!("/entries/{entry_index}"),
        }]))
        .unwrap()
    }

    /// Create a REPLACE patch for updating an existing conversation entry at the given index
    pub fn replace(entry_index: usize, entry: NormalizedEntry) -> Patch {
        let patch_entry = PatchEntry {
            op: PatchOperation::Replace,
            path: format!("/entries/{entry_index}"),
            value: PatchType::NormalizedEntry(entry),
        };

        from_value(json!([patch_entry])).unwrap()
    }
}
</file>

<file path="crates/executors/src/logs/mod.rs">
use serde::{Deserialize, Serialize};
use ts_rs::TS;

pub mod plain_text_processor;
pub mod stderr_processor;
pub mod utils;

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[serde(tag = "type", rename_all = "snake_case")]
#[ts(export)]
pub enum ToolResultValueType {
    Markdown,
    Json,
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub struct ToolResult {
    pub r#type: ToolResultValueType,
    /// For Markdown, this will be a JSON string; for JSON, a structured value
    pub value: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[serde(tag = "type", rename_all = "snake_case")]
#[ts(export)]
pub enum CommandExitStatus {
    ExitCode { code: i32 },
    Success { success: bool },
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub struct CommandRunResult {
    pub exit_status: Option<CommandExitStatus>,
    pub output: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct NormalizedConversation {
    pub entries: Vec<NormalizedEntry>,
    pub session_id: Option<String>,
    pub executor_type: String,
    pub prompt: Option<String>,
    pub summary: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[serde(tag = "type", rename_all = "snake_case")]
pub enum NormalizedEntryType {
    UserMessage,
    AssistantMessage,
    ToolUse {
        tool_name: String,
        action_type: ActionType,
    },
    SystemMessage,
    ErrorMessage,
    Thinking,
    Loading,
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct NormalizedEntry {
    pub timestamp: Option<String>,
    pub entry_type: NormalizedEntryType,
    pub content: String,
    #[ts(skip)]
    pub metadata: Option<serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
pub struct TodoItem {
    pub content: String,
    pub status: String,
    #[serde(default)]
    pub priority: Option<String>,
}

/// Types of tool actions that can be performed
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
#[serde(tag = "action", rename_all = "snake_case")]
pub enum ActionType {
    FileRead {
        path: String,
    },
    FileEdit {
        path: String,
        changes: Vec<FileChange>,
    },
    CommandRun {
        command: String,
        #[serde(default)]
        result: Option<CommandRunResult>,
    },
    Search {
        query: String,
    },
    WebFetch {
        url: String,
    },
    /// Generic tool with optional arguments and result for rich rendering
    Tool {
        tool_name: String,
        #[serde(default)]
        arguments: Option<serde_json::Value>,
        #[serde(default)]
        result: Option<ToolResult>,
    },
    TaskCreate {
        description: String,
    },
    PlanPresentation {
        plan: String,
    },
    TodoManagement {
        todos: Vec<TodoItem>,
        operation: String,
    },
    Other {
        description: String,
    },
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[serde(tag = "action", rename_all = "snake_case")]
pub enum FileChange {
    /// Create a file if it doesn't exist, and overwrite its content.
    Write { content: String },
    /// Delete a file.
    Delete,
    /// Rename a file.
    Rename { new_path: String },
    /// Edit a file with a unified diff.
    Edit {
        /// Unified diff containing file header and hunks.
        unified_diff: String,
        /// Whether line number in the hunks are reliable.
        has_line_numbers: bool,
    },
}
</file>

<file path="crates/executors/src/logs/plain_text_processor.rs">
//! Reusable log processor for plain-text streams with flexible clustering and formatting.
//!
//! Clusters messages into entries based on configurable size and time-gap heuristics, and supports
//! pluggable formatters for transforming or annotating chunks (e.g., inserting line breaks or parsing tool calls).
//!
//! Capable of handling mixed-format streams, including interleaved tool calls and assistant messages,
//! with custom split predicates to detect embedded markers and emit separate entries.
//!
//! ## Use cases
//! - **stderr_processor**: Cluster stderr lines by time gap and format as `ErrorMessage` log entries.
//!   See [`stderr_processor::normalize_stderr_logs`].
//! - **Gemini executor**: Post-process Gemini CLI output to make it prettier, then format it as assistant messages clustered by size.
//!   See [`crate::executors::gemini::Gemini::format_stdout_chunk`].
//! - **Tool call support**: detect lines starting with a distinct marker via `message_boundary_predicate` to separate tool invocations.
use std::{
    time::{Duration, Instant},
    vec,
};

use bon::bon;
use json_patch::Patch;

use super::{
    NormalizedEntry,
    utils::{ConversationPatch, EntryIndexProvider},
};

/// Controls message boundary for advanced executors.
/// The main use-case is to support mixed-content log streams where tool calls and assistant messages are interleaved.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum MessageBoundary {
    /// Conclude the current message entry at the given line.
    /// Useful when we detect a message of a different kind than the current one, e.g., when a tool call starts we need to close the current assistant message.
    Split(usize),
    /// Request more content. Signals that the current entry is incomplete and should not be emitted yet.
    /// This should only be the case in tool calls, as assistant messages can be partially emitted.
    IncompleteContent,
}

/// Internal buffer for collecting streaming text into individual lines.
/// Maintains line and size information for heuristics and processing.
#[derive(Debug)]
struct PlainTextBuffer {
    /// All lines including last partial line. Complete lines have trailing \n, partial line doesn't
    lines: Vec<String>,
    /// Current buffered length
    total_len: usize,
}

impl PlainTextBuffer {
    /// Create a new empty buffer
    pub fn new() -> Self {
        Self {
            lines: Vec::new(),
            total_len: 0,
        }
    }

    /// Ingest a new text chunk into the buffer.
    pub fn ingest(&mut self, text_chunk: String) {
        debug_assert!(!text_chunk.is_empty());

        // Add a new lines or grow the current partial line
        let current_partial = if self.lines.last().is_some_and(|l| !l.ends_with('\n')) {
            let partial = self.lines.pop().unwrap();
            self.total_len = self.total_len.saturating_sub(partial.len());
            partial
        } else {
            String::new()
        };

        // Process chunk
        let combined_text = current_partial + &text_chunk;
        let size = combined_text.len();

        // Append new lines
        let parts: Vec<String> = combined_text
            .split_inclusive('\n')
            .map(ToString::to_string)
            .collect();
        self.lines.extend(parts);
        self.total_len += size;
    }

    /// Remove and return the first `n` buffered lines,
    pub fn drain_lines(&mut self, n: usize) -> Vec<String> {
        let n = n.min(self.lines.len());
        let drained: Vec<String> = self.lines.drain(..n).collect();

        // Update total_bytes
        for line in &drained {
            self.total_len = self.total_len.saturating_sub(line.len());
        }

        drained
    }

    /// Remove and return lines until the content length is at least `len`.
    /// Useful for size-based splitting of content.
    pub fn drain_size(&mut self, len: usize) -> Vec<String> {
        let mut drained_len = 0;
        let mut lines_to_drain = 0;

        for line in &self.lines {
            if drained_len >= len && lines_to_drain > 0 {
                break;
            }
            drained_len += line.len();
            lines_to_drain += 1;
        }

        self.drain_lines(lines_to_drain)
    }

    /// Empty the buffer, removing and returning all content,
    pub fn flush(&mut self) -> Vec<String> {
        let result = self.lines.drain(..).collect();
        self.total_len = 0;
        result
    }

    /// Return the total length of content.
    pub fn total_len(&self) -> usize {
        self.total_len
    }

    /// View lines.
    pub fn lines(&self) -> &[String] {
        &self.lines
    }

    /// Mutably view lines for in-place transformations.
    pub fn lines_mut(&mut self) -> &mut Vec<String> {
        &mut self.lines
    }

    /// Recompute cached total length from current lines.
    pub fn recompute_len(&mut self) {
        self.total_len = self.lines.iter().map(|s| s.len()).sum();
    }

    /// Get the current parial line.
    pub fn partial_line(&self) -> Option<&str> {
        if let Some(last) = self.lines.last()
            && !last.ends_with('\n')
        {
            return Some(last);
        }
        None
    }

    /// Check if the buffer is empty.
    pub fn is_empty(&self) -> bool {
        debug_assert!(self.lines.len() == 0 || self.total_len > 0);
        self.total_len == 0
    }
}

impl Default for PlainTextBuffer {
    fn default() -> Self {
        Self::new()
    }
}

/// Optional content formatting function. Can be used post-process raw output before creating normalized entries.
pub type FormatChunkFn = Box<dyn Fn(Option<&str>, String) -> String + Send + 'static>;

/// Optional predicate function to determine message boundaries. This enables detecting tool calls interleaved with assistant messages.
pub type MessageBoundaryPredicateFn =
    Box<dyn Fn(&[String]) -> Option<MessageBoundary> + Send + 'static>;

/// Function to create a `NormalizedEntry` from content.
pub type NormalizedEntryProducerFn = Box<dyn Fn(String) -> NormalizedEntry + Send + 'static>;

/// Optional function to transform buffered lines in-place before boundary checks.
pub type LinesTransformFn = Box<dyn FnMut(&mut Vec<String>) + Send + 'static>;

/// High-level plain text log processor with configurable formatting and splitting
pub struct PlainTextLogProcessor {
    buffer: PlainTextBuffer,
    index_provider: EntryIndexProvider,
    entry_size_threshold: Option<usize>,
    time_gap: Option<Duration>,
    format_chunk: Option<FormatChunkFn>,
    transform_lines: Option<LinesTransformFn>,
    message_boundary_predicate: Option<MessageBoundaryPredicateFn>,
    normalized_entry_producer: NormalizedEntryProducerFn,
    last_chunk_arrival_time: Instant, // time since last chunk arrived
    current_entry_index: Option<usize>,
}

impl PlainTextLogProcessor {
    /// Process incoming text and return JSON patches for any complete entries
    pub fn process(&mut self, text_chunk: String) -> Vec<Patch> {
        if text_chunk.is_empty() {
            return vec![];
        }

        if !self.buffer.is_empty() {
            // If the new content arrived after the (**Optional**) time threshold between messages, we consider it a new entry.
            // Useful for stderr streams where we want to group related lines into a single entry.
            if self
                .time_gap
                .is_some_and(|time_gap| self.last_chunk_arrival_time.elapsed() >= time_gap)
            {
                let lines = self.buffer.flush();
                if !lines.is_empty() {
                    return vec![self.create_patch(lines)];
                }
                self.current_entry_index = None;
            }
        }

        self.last_chunk_arrival_time = Instant::now();

        let formatted_chunk = if let Some(format_chunk) = self.format_chunk.as_ref() {
            format_chunk(self.buffer.partial_line(), text_chunk)
        } else {
            text_chunk
        };

        if formatted_chunk.is_empty() {
            return vec![];
        }

        // Let the buffer handle text buffering
        self.buffer.ingest(formatted_chunk);

        if let Some(transform_lines) = self.transform_lines.as_mut() {
            transform_lines(self.buffer.lines_mut());
            self.buffer.recompute_len();
            if self.buffer.is_empty() {
                // Nothing left to process after transformation
                return vec![];
            }
        }

        let mut patches = Vec::new();

        // Check if we have a custom message boundary predicate
        loop {
            let message_boundary_predicate = self
                .message_boundary_predicate
                .as_ref()
                .and_then(|predicate| predicate(self.buffer.lines()));

            match message_boundary_predicate {
                // Predicate decided to conclude the current entry at `line_idx`
                Some(MessageBoundary::Split(line_idx)) => {
                    let lines = self.buffer.drain_lines(line_idx);
                    if !lines.is_empty() {
                        patches.push(self.create_patch(lines));
                        // Move to next entry after split
                        self.current_entry_index = None;
                    }
                }
                // Predicate decided that current content cannot be sent yet.
                Some(MessageBoundary::IncompleteContent) => {
                    // Stop processing, wait for more content.
                    // Partial updates will be disabled.
                    return patches;
                }
                None => {
                    // No more splits, break and continue to size/latency heuristics
                    break;
                }
            }
        }

        // Check message size. If entry is large enough, break it into smaller entries.
        if let Some(size_threshold) = self.entry_size_threshold {
            // Check message size. If entry is large enough, create a new entry.
            while self.buffer.total_len() >= size_threshold {
                let lines = self.buffer.drain_size(size_threshold);
                if lines.is_empty() {
                    break;
                }
                patches.push(self.create_patch(lines));
                // Move to next entry after size split
                self.current_entry_index = None;
            }
        }

        // Send partial udpdates
        if !self.buffer.is_empty() {
            // Stream updates without consuming buffer
            patches.push(self.create_patch(self.buffer.lines().to_vec()));
        }
        patches
    }

    /// Create patch
    fn create_patch(&mut self, lines: Vec<String>) -> Patch {
        let content = lines.concat();
        let entry = (self.normalized_entry_producer)(content);

        let added = self.current_entry_index.is_some();
        let index = if let Some(idx) = self.current_entry_index {
            idx
        } else {
            // If no current index, get next from provider
            let idx = self.index_provider.next();
            self.current_entry_index = Some(idx);
            idx
        };

        if !added {
            ConversationPatch::add_normalized_entry(index, entry)
        } else {
            ConversationPatch::replace(index, entry)
        }
    }
}

#[bon]
impl PlainTextLogProcessor {
    /// Create a builder for configuring PlainTextLogProcessor.
    ///
    /// # Parameters
    /// * `normalized_entry_producer` - Required function to convert text content into a `NormalizedEntry`.
    /// * `size_threshold` - Optional size threshold for individual entries. Once an entry content exceeds this size, a new entry is created.
    /// * `time_gap` - Optional time gap between individual entries. When new content arrives after this duration, it is considered a new entry.
    /// * `format_chunk` - Optional function to fix raw output before creating normalized entries.
    /// * `message_boundary_predicate` - Optional function to determine custom message boundaries. Useful when content is heterogeneous (e.g., tool calls interleaved with assistant messages).
    /// * `index_provider` - Required sharable atomic counter for tracking entry indices.
    ///
    /// When both `size_threshold` and `time_gap` are `None`, a default size threshold of 8 KiB is used.
    #[builder]
    pub fn new(
        normalized_entry_producer: impl Fn(String) -> NormalizedEntry + 'static + Send,
        size_threshold: Option<usize>,
        time_gap: Option<Duration>,
        format_chunk: Option<FormatChunkFn>,
        transform_lines: Option<LinesTransformFn>,
        message_boundary_predicate: Option<MessageBoundaryPredicateFn>,
        index_provider: EntryIndexProvider,
    ) -> Self {
        Self {
            buffer: PlainTextBuffer::new(),
            index_provider,
            entry_size_threshold: if size_threshold.is_none() && time_gap.is_none() {
                Some(8 * 1024) // Default 8KiB when neither is set
            } else {
                size_threshold
            },
            time_gap,
            format_chunk: format_chunk.map(|f| {
                Box::new(f) as Box<dyn Fn(Option<&str>, String) -> String + Send + 'static>
            }),
            transform_lines: transform_lines
                .map(|f| Box::new(f) as Box<dyn FnMut(&mut Vec<String>) + Send + 'static>),
            message_boundary_predicate: message_boundary_predicate.map(|p| {
                Box::new(p) as Box<dyn Fn(&[String]) -> Option<MessageBoundary> + Send + 'static>
            }),
            normalized_entry_producer: Box::new(normalized_entry_producer),
            last_chunk_arrival_time: Instant::now(),
            current_entry_index: None,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::logs::NormalizedEntryType;

    #[test]
    fn test_plain_buffer_flush() {
        let mut buffer = PlainTextBuffer::new();

        buffer.ingest("line1\npartial".to_string());
        assert_eq!(buffer.lines().len(), 2);

        let lines = buffer.flush();
        assert_eq!(lines, vec!["line1\n", "partial"]);
        assert_eq!(buffer.lines().len(), 0);
    }

    #[test]
    fn test_plain_buffer_len() {
        let mut buffer = PlainTextBuffer::new();

        buffer.ingest("abc\ndef\n".to_string());
        assert_eq!(buffer.total_len(), 8); // "abc\n" + "def\n"

        buffer.drain_lines(1);
        assert_eq!(buffer.total_len(), 4); // "def\n"
    }

    #[test]
    fn test_drain_until_size() {
        let mut buffer = PlainTextBuffer::new();

        buffer.ingest("short\nlonger line\nvery long line here\n".to_string());

        // Drain until we have at least 10 bytes
        let drained = buffer.drain_size(10);
        assert_eq!(drained.len(), 2); // "short\n" (6) + "longer line\n" (12) = 18 bytes total
        assert_eq!(drained, vec!["short\n", "longer line\n"]);
    }

    #[test]
    fn test_processor_simple() {
        let producer = |content: String| -> NormalizedEntry {
            NormalizedEntry {
                timestamp: None, // Avoid creating artificial timestamps during normalization
                entry_type: NormalizedEntryType::SystemMessage,
                content: content.to_string(),
                metadata: None,
            }
        };

        let mut processor = PlainTextLogProcessor::builder()
            .normalized_entry_producer(producer)
            .index_provider(EntryIndexProvider::test_new())
            .build();

        let patches = processor.process("hello world\n".to_string());
        assert_eq!(patches.len(), 1);
    }

    #[test]
    fn test_processor_custom_log_formatter() {
        // Example Level 1 producer that parses tool calls
        let tool_producer = |content: String| -> NormalizedEntry {
            if content.starts_with("TOOL:") {
                let tool_name = content.strip_prefix("TOOL:").unwrap_or("unknown").trim();
                NormalizedEntry {
                    timestamp: None,
                    entry_type: NormalizedEntryType::ToolUse {
                        tool_name: tool_name.to_string(),
                        action_type: super::super::ActionType::Other {
                            description: tool_name.to_string(),
                        },
                    },
                    content,
                    metadata: None,
                }
            } else {
                NormalizedEntry {
                    timestamp: None,
                    entry_type: NormalizedEntryType::SystemMessage,
                    content: content.to_string(),
                    metadata: None,
                }
            }
        };

        let mut processor = PlainTextLogProcessor::builder()
            .normalized_entry_producer(tool_producer)
            .index_provider(EntryIndexProvider::test_new())
            .build();

        let patches = processor.process("TOOL: file_read\n".to_string());
        assert_eq!(patches.len(), 1);
    }

    #[test]
    fn test_processor_transform_lines_clears_first_line() {
        let producer = |content: String| -> NormalizedEntry {
            NormalizedEntry {
                timestamp: None,
                entry_type: NormalizedEntryType::SystemMessage,
                content,
                metadata: None,
            }
        };

        let mut processor = PlainTextLogProcessor::builder()
            .normalized_entry_producer(producer)
            .transform_lines(Box::new(|lines: &mut Vec<String>| {
                // Drop a specific leading banner line if present
                if !lines.is_empty()
                    && lines.first().map(|s| s.as_str()) == Some("BANNER LINE TO DROP\n")
                {
                    lines.remove(0);
                }
            }))
            .index_provider(EntryIndexProvider::test_new())
            .build();

        // Provide a single-line chunk. The transform removes it, leaving nothing to emit.
        let patches = processor.process("BANNER LINE TO DROP\n".to_string());
        assert_eq!(patches.len(), 0);

        // Next, add actual content; should emit one patch with the content
        let patches = processor.process("real content\n".to_string());
        assert_eq!(patches.len(), 1);
    }
}
</file>

<file path="crates/executors/src/logs/stderr_processor.rs">
//! Standard stderr log processor for executors
//!
//! Uses `PlainTextLogProcessor` with a 2-second `latency_threshold` to split stderr streams into entries.
//! Each entry is normalized as `ErrorMessage` and emitted as JSON patches to the message store.
//!
//! Example:
//! ```rust,ignore
//! normalize_stderr_logs(msg_store.clone(), EntryIndexProvider::new());
//! ```
//!
use std::{sync::Arc, time::Duration};

use futures::StreamExt;
use utils::msg_store::MsgStore;

use super::{NormalizedEntry, NormalizedEntryType, plain_text_processor::PlainTextLogProcessor};
use crate::logs::utils::EntryIndexProvider;

/// Standard stderr log normalizer that uses PlainTextLogProcessor to stream error logs.
///
/// Splits stderr output into discrete entries based on a latency threshold (2s) to group
/// related lines into a single error entry. Each entry is normalized as an `ErrorMessage`
/// and emitted as JSON patches for downstream consumption (e.g., UI or log aggregation).
///
/// # Options
/// - `latency_threshold`: 2 seconds to separate error messages based on time gaps.
/// - `normalized_entry_producer`: maps each chunk into an `ErrorMessage` entry.
///
/// # Use case
/// Intended for executor stderr streams, grouping multi-line errors into cohesive entries
/// instead of emitting each line separately.
///
/// # Arguments
/// * `msg_store` - the message store providing a stream of stderr chunks and accepting patches.
/// * `entry_index_provider` - provider of incremental entry indices for patch ordering.
pub fn normalize_stderr_logs(msg_store: Arc<MsgStore>, entry_index_provider: EntryIndexProvider) {
    tokio::spawn(async move {
        let mut stderr = msg_store.stderr_chunked_stream();

        // Create a processor with time-based emission for stderr
        let mut processor = PlainTextLogProcessor::builder()
            .normalized_entry_producer(Box::new(|content: String| NormalizedEntry {
                timestamp: None,
                entry_type: NormalizedEntryType::ErrorMessage,
                content,
                metadata: None,
            }))
            .time_gap(Duration::from_secs(2)) // Break messages if they are 2 seconds apart
            .index_provider(entry_index_provider)
            .build();

        while let Some(Ok(chunk)) = stderr.next().await {
            for patch in processor.process(chunk) {
                msg_store.push_patch(patch);
            }
        }
    });
}
</file>

<file path="crates/executors/src/command.rs">
use schemars::JsonSchema;
use serde::{Deserialize, Serialize};
use ts_rs::TS;

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema, Default)]
pub struct CmdOverrides {
    #[schemars(
        title = "Base Command Override",
        description = "Override the base command with a custom command"
    )]
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub base_command_override: Option<String>,
    #[schemars(
        title = "Additional Parameters",
        description = "Additional parameters to append to the base command"
    )]
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub additional_params: Option<Vec<String>>,
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, JsonSchema)]
pub struct CommandBuilder {
    /// Base executable command (e.g., "npx -y @anthropic-ai/claude-code@latest")
    pub base: String,
    /// Optional parameters to append to the base command
    pub params: Option<Vec<String>>,
}

impl CommandBuilder {
    pub fn new<S: Into<String>>(base: S) -> Self {
        Self {
            base: base.into(),
            params: None,
        }
    }

    pub fn params<I>(mut self, params: I) -> Self
    where
        I: IntoIterator,
        I::Item: Into<String>,
    {
        self.params = Some(params.into_iter().map(|p| p.into()).collect());
        self
    }

    pub fn override_base<S: Into<String>>(mut self, base: S) -> Self {
        self.base = base.into();
        self
    }

    pub fn extend_params<I>(mut self, more: I) -> Self
    where
        I: IntoIterator,
        I::Item: Into<String>,
    {
        let extra: Vec<String> = more.into_iter().map(|p| p.into()).collect();
        match &mut self.params {
            Some(p) => p.extend(extra),
            None => self.params = Some(extra),
        }
        self
    }
    pub fn build_initial(&self) -> String {
        let mut parts = vec![self.base.clone()];
        if let Some(ref params) = self.params {
            parts.extend(params.clone());
        }
        parts.join(" ")
    }

    pub fn build_follow_up(&self, additional_args: &[String]) -> String {
        let mut parts = vec![self.base.clone()];
        if let Some(ref params) = self.params {
            parts.extend(params.clone());
        }
        parts.extend(additional_args.iter().cloned());
        parts.join(" ")
    }
}

pub fn apply_overrides(builder: CommandBuilder, overrides: &CmdOverrides) -> CommandBuilder {
    let builder = if let Some(ref base) = overrides.base_command_override {
        builder.override_base(base.clone())
    } else {
        builder
    };
    if let Some(ref extra) = overrides.additional_params {
        builder.extend_params(extra.clone())
    } else {
        builder
    }
}
</file>

<file path="crates/executors/src/lib.rs">
pub mod actions;
pub mod command;
pub mod executors;
pub mod logs;
pub mod mcp_config;
pub mod profile;
pub mod stdout_dup;
</file>

<file path="crates/executors/src/mcp_config.rs">
//! Utilities for reading and writing external agent config files (not the server's own config).
//!
//! These helpers abstract over JSON vs TOML formats used by different agents.

use std::{collections::HashMap, sync::LazyLock};

use serde::{Deserialize, Serialize};
use serde_json::{Map, Value};
use tokio::fs;
use ts_rs::TS;

use crate::executors::{CodingAgent, ExecutorError};

static DEFAULT_MCP_JSON: &str = include_str!("../default_mcp.json");
pub static PRECONFIGURED_MCP_SERVERS: LazyLock<Value> = LazyLock::new(|| {
    serde_json::from_str::<Value>(DEFAULT_MCP_JSON).expect("Failed to parse default MCP JSON")
});

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct McpConfig {
    servers: HashMap<String, serde_json::Value>,
    pub servers_path: Vec<String>,
    pub template: serde_json::Value,
    pub preconfigured: serde_json::Value,
    pub is_toml_config: bool,
}

impl McpConfig {
    pub fn new(
        servers_path: Vec<String>,
        template: serde_json::Value,
        preconfigured: serde_json::Value,
        is_toml_config: bool,
    ) -> Self {
        Self {
            servers: HashMap::new(),
            servers_path,
            template,
            preconfigured,
            is_toml_config,
        }
    }
    pub fn set_servers(&mut self, servers: HashMap<String, serde_json::Value>) {
        self.servers = servers;
    }
}

/// Read an agent's external config file (JSON or TOML) and normalize it to serde_json::Value.
pub async fn read_agent_config(
    config_path: &std::path::Path,
    mcp_config: &McpConfig,
) -> Result<Value, ExecutorError> {
    if let Ok(file_content) = fs::read_to_string(config_path).await {
        if mcp_config.is_toml_config {
            // Parse TOML then convert to JSON Value
            if file_content.trim().is_empty() {
                return Ok(serde_json::json!({}));
            }
            let toml_val: toml::Value = toml::from_str(&file_content)?;
            let json_string = serde_json::to_string(&toml_val)?;
            Ok(serde_json::from_str(&json_string)?)
        } else {
            Ok(serde_json::from_str(&file_content)?)
        }
    } else {
        Ok(mcp_config.template.clone())
    }
}

/// Write an agent's external config (as serde_json::Value) back to disk in the agent's format (JSON or TOML).
pub async fn write_agent_config(
    config_path: &std::path::Path,
    mcp_config: &McpConfig,
    config: &Value,
) -> Result<(), ExecutorError> {
    if mcp_config.is_toml_config {
        // Convert JSON Value back to TOML
        let toml_value: toml::Value = serde_json::from_str(&serde_json::to_string(config)?)?;
        let toml_content = toml::to_string_pretty(&toml_value)?;
        fs::write(config_path, toml_content).await?;
    } else {
        let json_content = serde_json::to_string_pretty(config)?;
        fs::write(config_path, json_content).await?;
    }
    Ok(())
}

type ServerMap = Map<String, Value>;

fn is_http_server(s: &Map<String, Value>) -> bool {
    matches!(s.get("type").and_then(Value::as_str), Some("http"))
}

fn is_stdio(s: &Map<String, Value>) -> bool {
    !is_http_server(s) && s.get("command").is_some()
}

fn extract_meta(mut obj: ServerMap) -> (ServerMap, Option<Value>) {
    let meta = obj.remove("meta");
    (obj, meta)
}

fn attach_meta(mut obj: ServerMap, meta: Option<Value>) -> Value {
    if let Some(m) = meta {
        obj.insert("meta".to_string(), m);
    }
    Value::Object(obj)
}

fn ensure_header(headers: &mut Map<String, Value>, key: &str, val: &str) {
    match headers.get_mut(key) {
        Some(Value::String(_)) => {}
        _ => {
            headers.insert(key.to_string(), Value::String(val.to_string()));
        }
    }
}

fn transform_http_servers<F>(mut servers: ServerMap, mut f: F) -> ServerMap
where
    F: FnMut(Map<String, Value>) -> Map<String, Value>,
{
    for (_k, v) in servers.iter_mut() {
        if let Value::Object(s) = v
            && is_http_server(s)
        {
            let taken = std::mem::take(s);
            *s = f(taken);
        }
    }
    servers
}

// --- Adapters ---------------------------------------------------------------

fn adapt_passthrough(servers: ServerMap, meta: Option<Value>) -> Value {
    attach_meta(servers, meta)
}

fn adapt_gemini(servers: ServerMap, meta: Option<Value>) -> Value {
    let servers = transform_http_servers(servers, |mut s| {
        let url = s
            .remove("url")
            .unwrap_or_else(|| Value::String(String::new()));
        let mut headers = s
            .remove("headers")
            .and_then(|v| v.as_object().cloned())
            .unwrap_or_default();

        ensure_header(
            &mut headers,
            "Accept",
            "application/json, text/event-stream",
        );
        Map::from_iter([
            ("httpUrl".to_string(), url),
            ("headers".to_string(), Value::Object(headers)),
        ])
    });
    attach_meta(servers, meta)
}

fn adapt_cursor(servers: ServerMap, meta: Option<Value>) -> Value {
    let servers = transform_http_servers(servers, |mut s| {
        let url = s
            .remove("url")
            .unwrap_or_else(|| Value::String(String::new()));
        let headers = s
            .remove("headers")
            .unwrap_or_else(|| Value::Object(Default::default()));
        Map::from_iter([("url".to_string(), url), ("headers".to_string(), headers)])
    });
    attach_meta(servers, meta)
}

fn adapt_codex(mut servers: ServerMap, mut meta: Option<Value>) -> Value {
    servers.retain(|_, v| v.as_object().map(is_stdio).unwrap_or(false));

    if let Some(Value::Object(ref mut m)) = meta {
        m.retain(|k, _| servers.contains_key(k));
        servers.insert("meta".to_string(), Value::Object(std::mem::take(m)));
        meta = None; // already attached above
    }
    attach_meta(servers, meta)
}

fn adapt_opencode(servers: ServerMap, meta: Option<Value>) -> Value {
    let mut servers = transform_http_servers(servers, |mut s| {
        let url = s
            .remove("url")
            .unwrap_or_else(|| Value::String(String::new()));

        let mut headers = s
            .remove("headers")
            .and_then(|v| v.as_object().cloned())
            .unwrap_or_default();

        ensure_header(
            &mut headers,
            "Accept",
            "application/json, text/event-stream",
        );

        Map::from_iter([
            ("type".to_string(), Value::String("remote".to_string())),
            ("url".to_string(), url),
            ("headers".to_string(), Value::Object(headers)),
            ("enabled".to_string(), Value::Bool(true)),
        ])
    });

    for (_k, v) in servers.iter_mut() {
        if let Value::Object(s) = v
            && is_stdio(s)
        {
            let command_str = s
                .remove("command")
                .and_then(|v| match v {
                    Value::String(s) => Some(s),
                    _ => None,
                })
                .unwrap_or_default();

            let mut cmd_vec: Vec<Value> = Vec::new();
            if !command_str.is_empty() {
                cmd_vec.push(Value::String(command_str));
            }

            if let Some(arr) = s.remove("args").and_then(|v| match v {
                Value::Array(arr) => Some(arr),
                _ => None,
            }) {
                for a in arr {
                    match a {
                        Value::String(s) => cmd_vec.push(Value::String(s)),
                        other => cmd_vec.push(other), // fall back to raw value if not string
                    }
                }
            }

            let mut new_map = Map::new();
            new_map.insert("type".to_string(), Value::String("local".to_string()));
            new_map.insert("command".to_string(), Value::Array(cmd_vec));
            new_map.insert("enabled".to_string(), Value::Bool(true));
            *s = new_map;
        }
    }

    attach_meta(servers, meta)
}

enum Adapter {
    Passthrough,
    Gemini,
    Cursor,
    Codex,
    Opencode,
}

fn apply_adapter(adapter: Adapter, canonical: Value) -> Value {
    let (servers_only, meta) = match canonical.as_object() {
        Some(map) => extract_meta(map.clone()),
        None => (ServerMap::new(), None),
    };

    match adapter {
        Adapter::Passthrough => adapt_passthrough(servers_only, meta),
        Adapter::Gemini => adapt_gemini(servers_only, meta),
        Adapter::Cursor => adapt_cursor(servers_only, meta),
        Adapter::Codex => adapt_codex(servers_only, meta),
        Adapter::Opencode => adapt_opencode(servers_only, meta),
    }
}

impl CodingAgent {
    pub fn preconfigured_mcp(&self) -> Value {
        use Adapter::*;

        let adapter = match self {
            CodingAgent::ClaudeCode(_) | CodingAgent::Amp(_) => Passthrough,
            CodingAgent::QwenCode(_) | CodingAgent::Gemini(_) => Gemini,
            CodingAgent::Cursor(_) => Cursor,
            CodingAgent::Codex(_) => Codex,
            CodingAgent::Opencode(_) => Opencode,
        };

        let canonical = PRECONFIGURED_MCP_SERVERS.clone();
        apply_adapter(adapter, canonical)
    }
}
</file>

<file path="crates/executors/src/profile.rs">
use std::{collections::HashMap, fs, str::FromStr, sync::RwLock};

use convert_case::{Case, Casing};
use lazy_static::lazy_static;
use serde::{Deserialize, Deserializer, Serialize, de::Error as DeError};
use thiserror::Error;
use ts_rs::TS;

use crate::executors::{BaseCodingAgent, CodingAgent, StandardCodingAgentExecutor};

/// Return the canonical form for variant keys.
/// – "DEFAULT" is kept as-is  
/// – everything else is converted to SCREAMING_SNAKE_CASE
pub fn canonical_variant_key<S: AsRef<str>>(raw: S) -> String {
    let key = raw.as_ref();
    if key.eq_ignore_ascii_case("DEFAULT") {
        "DEFAULT".to_string()
    } else {
        // Convert to SCREAMING_SNAKE_CASE by first going to snake_case then uppercase
        key.to_case(Case::Snake).to_case(Case::ScreamingSnake)
    }
}

#[derive(Error, Debug)]
pub enum ProfileError {
    #[error("Built-in executor '{executor}' cannot be deleted")]
    CannotDeleteExecutor { executor: BaseCodingAgent },

    #[error("Built-in configuration '{executor}:{variant}' cannot be deleted")]
    CannotDeleteBuiltInConfig {
        executor: BaseCodingAgent,
        variant: String,
    },

    #[error("Validation error: {0}")]
    Validation(String),

    #[error(transparent)]
    Io(#[from] std::io::Error),

    #[error(transparent)]
    Serde(#[from] serde_json::Error),

    #[error("No available executor profile")]
    NoAvailableExecutorProfile,
}

lazy_static! {
    static ref EXECUTOR_PROFILES_CACHE: RwLock<ExecutorConfigs> =
        RwLock::new(ExecutorConfigs::load());
}

// New format default profiles (v3 - flattened)
const DEFAULT_PROFILES_JSON: &str = include_str!("../default_profiles.json");

// Executor-centric profile identifier
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS, Hash, Eq)]
pub struct ExecutorProfileId {
    /// The executor type (e.g., "CLAUDE_CODE", "AMP")
    #[serde(alias = "profile", deserialize_with = "de_base_coding_agent_kebab")]
    // Backwards compatability with ProfileVariantIds, esp stored in DB under ExecutorAction
    pub executor: BaseCodingAgent,
    /// Optional variant name (e.g., "PLAN", "ROUTER")
    #[serde(skip_serializing_if = "Option::is_none")]
    pub variant: Option<String>,
}

// Convert legacy profile/executor names from kebab-case to SCREAMING_SNAKE_CASE, can be deleted 14 days from 3/9/25
fn de_base_coding_agent_kebab<'de, D>(de: D) -> Result<BaseCodingAgent, D::Error>
where
    D: Deserializer<'de>,
{
    let raw = String::deserialize(de)?;
    // kebab-case -> SCREAMING_SNAKE_CASE
    let norm = raw.replace('-', "_").to_ascii_uppercase();
    BaseCodingAgent::from_str(&norm)
        .map_err(|_| D::Error::custom(format!("unknown executor '{raw}' (normalized to '{norm}')")))
}

impl ExecutorProfileId {
    /// Create a new executor profile ID with default variant
    pub fn new(executor: BaseCodingAgent) -> Self {
        Self {
            executor,
            variant: None,
        }
    }

    /// Create a new executor profile ID with specific variant
    pub fn with_variant(executor: BaseCodingAgent, variant: String) -> Self {
        Self {
            executor,
            variant: Some(variant),
        }
    }

    /// Get cache key for this executor profile
    pub fn cache_key(&self) -> String {
        match &self.variant {
            Some(variant) => format!("{}:{}", self.executor, variant),
            None => self.executor.clone().to_string(),
        }
    }
}

impl std::fmt::Display for ExecutorProfileId {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match &self.variant {
            Some(variant) => write!(f, "{}:{}", self.executor, variant),
            None => write!(f, "{}", self.executor),
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS)]
pub struct ExecutorConfig {
    #[serde(flatten)]
    pub configurations: HashMap<String, CodingAgent>,
}

impl ExecutorConfig {
    /// Get variant configuration by name, or None if not found
    pub fn get_variant(&self, variant: &str) -> Option<&CodingAgent> {
        self.configurations.get(variant)
    }

    /// Get the default configuration for this executor
    pub fn get_default(&self) -> Option<&CodingAgent> {
        self.configurations.get("DEFAULT")
    }

    /// Create a new executor profile with just a default configuration
    pub fn new_with_default(default_config: CodingAgent) -> Self {
        let mut configurations = HashMap::new();
        configurations.insert("DEFAULT".to_string(), default_config);
        Self { configurations }
    }

    /// Add or update a variant configuration
    pub fn set_variant(
        &mut self,
        variant_name: String,
        config: CodingAgent,
    ) -> Result<(), &'static str> {
        let key = canonical_variant_key(&variant_name);
        if key == "DEFAULT" {
            return Err(
                "Cannot override 'DEFAULT' variant using set_variant, use set_default instead",
            );
        }
        self.configurations.insert(key, config);
        Ok(())
    }

    /// Set the default configuration
    pub fn set_default(&mut self, config: CodingAgent) {
        self.configurations.insert("DEFAULT".to_string(), config);
    }

    /// Get all variant names (excluding "DEFAULT")
    pub fn variant_names(&self) -> Vec<&String> {
        self.configurations
            .keys()
            .filter(|k| *k != "DEFAULT")
            .collect()
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS)]
pub struct ExecutorConfigs {
    pub executors: HashMap<BaseCodingAgent, ExecutorConfig>,
}

impl ExecutorConfigs {
    /// Normalise all variant keys in-place
    fn canonicalise(&mut self) {
        for profile in self.executors.values_mut() {
            let mut replacements = Vec::new();
            for key in profile.configurations.keys().cloned().collect::<Vec<_>>() {
                let canon = canonical_variant_key(&key);
                if canon != key {
                    replacements.push((key, canon));
                }
            }
            for (old, new) in replacements {
                if let Some(cfg) = profile.configurations.remove(&old) {
                    // If both lowercase and canonical forms existed, keep canonical one
                    profile.configurations.entry(new).or_insert(cfg);
                }
            }
        }
    }

    /// Get cached executor profiles
    pub fn get_cached() -> ExecutorConfigs {
        EXECUTOR_PROFILES_CACHE.read().unwrap().clone()
    }

    /// Reload executor profiles cache
    pub fn reload() {
        let mut cache = EXECUTOR_PROFILES_CACHE.write().unwrap();
        *cache = Self::load();
    }

    /// Load executor profiles from file or defaults
    pub fn load() -> Self {
        let profiles_path = utils::assets::profiles_path();

        // Load defaults first
        let mut defaults = Self::from_defaults();
        defaults.canonicalise();

        // Try to load user overrides
        let content = match fs::read_to_string(&profiles_path) {
            Ok(content) => content,
            Err(_) => {
                tracing::info!("No user profiles.json found, using defaults only");
                return defaults;
            }
        };

        // Parse user overrides
        match serde_json::from_str::<Self>(&content) {
            Ok(mut user_overrides) => {
                tracing::info!("Loaded user profile overrides from profiles.json");
                user_overrides.canonicalise();
                Self::merge_with_defaults(defaults, user_overrides)
            }
            Err(e) => {
                tracing::warn!(
                    "Failed to parse user profiles.json: {}, using defaults only",
                    e
                );
                defaults
            }
        }
    }

    /// Save user profile overrides to file (only saves what differs from defaults)
    pub fn save_overrides(&self) -> Result<(), ProfileError> {
        let profiles_path = utils::assets::profiles_path();
        let mut defaults = Self::from_defaults();
        defaults.canonicalise();

        // Canonicalise current config before computing overrides
        let mut self_clone = self.clone();
        self_clone.canonicalise();

        // Compute differences from defaults
        let overrides = Self::compute_overrides(&defaults, &self_clone)?;

        // Validate the merged result would be valid
        let merged = Self::merge_with_defaults(defaults, overrides.clone());
        Self::validate_merged(&merged)?;

        // Write overrides directly to file
        let content = serde_json::to_string_pretty(&overrides)?;
        fs::write(&profiles_path, content)?;

        tracing::info!("Saved profile overrides to {:?}", profiles_path);
        Ok(())
    }

    /// Deep merge defaults with user overrides
    fn merge_with_defaults(mut defaults: Self, overrides: Self) -> Self {
        for (executor_key, override_profile) in overrides.executors {
            match defaults.executors.get_mut(&executor_key) {
                Some(default_profile) => {
                    // Merge configurations (user configs override defaults, new ones are added)
                    for (config_name, config) in override_profile.configurations {
                        default_profile.configurations.insert(config_name, config);
                    }
                }
                None => {
                    // New executor, add completely
                    defaults.executors.insert(executor_key, override_profile);
                }
            }
        }
        defaults
    }

    /// Compute what overrides are needed to transform defaults into current config
    fn compute_overrides(defaults: &Self, current: &Self) -> Result<Self, ProfileError> {
        let mut overrides = Self {
            executors: HashMap::new(),
        };

        // Fast scan for any illegal deletions BEFORE allocating/cloning
        for (executor_key, default_profile) in &defaults.executors {
            // Check if executor was removed entirely
            if !current.executors.contains_key(executor_key) {
                return Err(ProfileError::CannotDeleteExecutor {
                    executor: *executor_key,
                });
            }

            let current_profile = &current.executors[executor_key];

            // Check if ANY built-in configuration was removed
            for config_name in default_profile.configurations.keys() {
                if !current_profile.configurations.contains_key(config_name) {
                    return Err(ProfileError::CannotDeleteBuiltInConfig {
                        executor: *executor_key,
                        variant: config_name.clone(),
                    });
                }
            }
        }

        for (executor_key, current_profile) in &current.executors {
            if let Some(default_profile) = defaults.executors.get(executor_key) {
                let mut override_configurations = HashMap::new();

                // Check each configuration in current profile
                for (config_name, current_config) in &current_profile.configurations {
                    if let Some(default_config) = default_profile.configurations.get(config_name) {
                        // Only include if different from default
                        if current_config != default_config {
                            override_configurations
                                .insert(config_name.clone(), current_config.clone());
                        }
                    } else {
                        // New configuration, always include
                        override_configurations.insert(config_name.clone(), current_config.clone());
                    }
                }

                // Only include executor if there are actual differences
                if !override_configurations.is_empty() {
                    overrides.executors.insert(
                        *executor_key,
                        ExecutorConfig {
                            configurations: override_configurations,
                        },
                    );
                }
            } else {
                // New executor, include completely
                overrides
                    .executors
                    .insert(*executor_key, current_profile.clone());
            }
        }

        Ok(overrides)
    }

    /// Validate that merged profiles are consistent and valid
    fn validate_merged(merged: &Self) -> Result<(), ProfileError> {
        for (executor_key, profile) in &merged.executors {
            // Ensure default configuration exists
            let default_config = profile.configurations.get("DEFAULT").ok_or_else(|| {
                ProfileError::Validation(format!(
                    "Executor '{executor_key}' is missing required 'default' configuration"
                ))
            })?;

            // Validate that the default agent type matches the executor key
            if BaseCodingAgent::from(default_config) != *executor_key {
                return Err(ProfileError::Validation(format!(
                    "Executor key '{executor_key}' does not match the agent variant '{default_config}'"
                )));
            }

            // Ensure configuration names don't conflict with reserved words
            for config_name in profile.configurations.keys() {
                if config_name.starts_with("__") {
                    return Err(ProfileError::Validation(format!(
                        "Configuration name '{config_name}' is reserved (starts with '__')"
                    )));
                }
            }
        }
        Ok(())
    }

    /// Load from the new v3 defaults
    pub fn from_defaults() -> Self {
        serde_json::from_str(DEFAULT_PROFILES_JSON).unwrap_or_else(|e| {
            tracing::error!("Failed to parse embedded default_profiles.json: {}", e);
            panic!("Default profiles v3 JSON is invalid")
        })
    }

    pub fn get_coding_agent(&self, executor_profile_id: &ExecutorProfileId) -> Option<CodingAgent> {
        self.executors
            .get(&executor_profile_id.executor)
            .and_then(|executor| {
                executor.get_variant(
                    &executor_profile_id
                        .variant
                        .clone()
                        .unwrap_or("DEFAULT".to_string()),
                )
            })
            .cloned()
    }

    pub fn get_coding_agent_or_default(
        &self,
        executor_profile_id: &ExecutorProfileId,
    ) -> CodingAgent {
        self.get_coding_agent(executor_profile_id)
            .unwrap_or_else(|| {
                let mut default_executor_profile_id = executor_profile_id.clone();
                default_executor_profile_id.variant = Some("DEFAULT".to_string());
                self.get_coding_agent(&default_executor_profile_id)
                    .expect("No default variant found")
            })
    }
    /// Get the first available executor profile for new users
    pub async fn get_recommended_executor_profile(
        &self,
    ) -> Result<ExecutorProfileId, ProfileError> {
        for &base_agent in self.executors.keys() {
            let profile_id = ExecutorProfileId::new(base_agent);
            if let Some(coding_agent) = self.get_coding_agent(&profile_id)
                && coding_agent.check_availability().await
            {
                tracing::info!("Detected available executor: {}", base_agent);
                return Ok(profile_id);
            }
        }
        Err(ProfileError::NoAvailableExecutorProfile)
    }
}
</file>

<file path="crates/executors/src/stdout_dup.rs">
//! Cross-platform stdout duplication utility for child processes
//!
//! Provides a single function to duplicate a child process's stdout stream.
//! Supports Unix and Windows platforms.

#[cfg(unix)]
use std::os::unix::io::{FromRawFd, IntoRawFd, OwnedFd};
#[cfg(windows)]
use std::os::windows::io::{FromRawHandle, IntoRawHandle, OwnedHandle};

use command_group::AsyncGroupChild;
use futures::{StreamExt, stream::BoxStream};
use tokio::io::{AsyncWrite, AsyncWriteExt};
use tokio_stream::wrappers::UnboundedReceiverStream;
use tokio_util::io::ReaderStream;

use crate::executors::ExecutorError;

/// Duplicate stdout from AsyncGroupChild.
///
/// Creates a stream that mirrors stdout of child process without consuming it.
///
/// # Returns
/// A stream of `io::Result<String>` that receives a copy of all stdout data.
pub fn duplicate_stdout(
    child: &mut AsyncGroupChild,
) -> Result<BoxStream<'static, std::io::Result<String>>, ExecutorError> {
    // The implementation strategy is:
    // 1. create a new file descriptor.
    // 2. read the original stdout file descriptor.
    // 3. write the data to both the new file descriptor and a duplicate stream.

    // Take the original stdout
    let original_stdout = child.inner().stdout.take().ok_or_else(|| {
        ExecutorError::Io(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Child process has no stdout",
        ))
    })?;

    // Create a new file descriptor in a cross-platform way (using os_pipe crate)
    let (pipe_reader, pipe_writer) = os_pipe::pipe().map_err(|e| {
        ExecutorError::Io(std::io::Error::other(format!("Failed to create pipe: {e}")))
    })?;
    // Use fd as new child stdout
    child.inner().stdout = Some(wrap_fd_as_child_stdout(pipe_reader)?);

    // Obtain writer from fd
    let mut fd_writer = wrap_fd_as_tokio_writer(pipe_writer)?;

    // Create the duplicate stdout stream
    let (dup_writer, dup_reader) =
        tokio::sync::mpsc::unbounded_channel::<std::io::Result<String>>();

    // Read original stdout and write to both new ChildStdout and duplicate stream
    tokio::spawn(async move {
        let mut stdout_stream = ReaderStream::new(original_stdout);

        while let Some(res) = stdout_stream.next().await {
            match res {
                Ok(data) => {
                    let _ = fd_writer.write_all(&data).await;

                    let string_chunk = String::from_utf8_lossy(&data).into_owned();
                    let _ = dup_writer.send(Ok(string_chunk));
                }
                Err(err) => {
                    tracing::error!("Error reading from child stdout: {}", err);
                    let _ = dup_writer.send(Err(err));
                }
            }
        }
    });

    // Return the channel receiver as a boxed stream
    Ok(Box::pin(UnboundedReceiverStream::new(dup_reader)))
}

/// Handle to append additional lines into the child's stdout stream.
pub struct StdoutAppender {
    tx: tokio::sync::mpsc::UnboundedSender<String>,
}

impl StdoutAppender {
    pub fn append_line<S: Into<String>>(&self, line: S) {
        // Best-effort; ignore send errors if writer task ended
        let _ = self.tx.send(line.into());
    }
}

/// Tee the child's stdout and provide both a duplicate stream and an appender to write additional
/// lines into the child's stdout. This keeps the original stdout functional and mirrors output to
/// the returned duplicate stream.
pub fn tee_stdout_with_appender(
    child: &mut AsyncGroupChild,
) -> Result<(BoxStream<'static, std::io::Result<String>>, StdoutAppender), ExecutorError> {
    // Take original stdout
    let original_stdout = child.inner().stdout.take().ok_or_else(|| {
        ExecutorError::Io(std::io::Error::new(
            std::io::ErrorKind::NotFound,
            "Child process has no stdout",
        ))
    })?;

    // Create replacement pipe and set as new child stdout
    let (pipe_reader, pipe_writer) = os_pipe::pipe().map_err(|e| {
        ExecutorError::Io(std::io::Error::other(format!("Failed to create pipe: {e}")))
    })?;
    child.inner().stdout = Some(wrap_fd_as_child_stdout(pipe_reader)?);

    // Single shared writer for both original stdout forwarding and injected lines
    let writer = wrap_fd_as_tokio_writer(pipe_writer)?;
    let shared_writer = std::sync::Arc::new(tokio::sync::Mutex::new(writer));

    // Create duplicate stream publisher
    let (dup_tx, dup_rx) = tokio::sync::mpsc::unbounded_channel::<std::io::Result<String>>();
    // Create injector channel
    let (inj_tx, mut inj_rx) = tokio::sync::mpsc::unbounded_channel::<String>();

    // Task 1: forward original stdout to child stdout and duplicate stream
    {
        let shared_writer = shared_writer.clone();
        tokio::spawn(async move {
            let mut stdout_stream = ReaderStream::new(original_stdout);
            while let Some(res) = stdout_stream.next().await {
                match res {
                    Ok(data) => {
                        // forward to child stdout
                        let mut w = shared_writer.lock().await;
                        let _ = w.write_all(&data).await;
                        // publish duplicate
                        let string_chunk = String::from_utf8_lossy(&data).into_owned();
                        let _ = dup_tx.send(Ok(string_chunk));
                    }
                    Err(err) => {
                        let _ = dup_tx.send(Err(err));
                    }
                }
            }
        });
    }

    // Task 2: write injected lines to child stdout
    {
        let shared_writer = shared_writer.clone();
        tokio::spawn(async move {
            while let Some(line) = inj_rx.recv().await {
                let mut data = line.into_bytes();
                data.push(b'\n');
                let mut w = shared_writer.lock().await;
                let _ = w.write_all(&data).await;
            }
        });
    }

    Ok((
        Box::pin(UnboundedReceiverStream::new(dup_rx)),
        StdoutAppender { tx: inj_tx },
    ))
}

// =========================================
// OS file descriptor helper functions
// =========================================

/// Convert os_pipe::PipeReader to tokio::process::ChildStdout
fn wrap_fd_as_child_stdout(
    pipe_reader: os_pipe::PipeReader,
) -> Result<tokio::process::ChildStdout, ExecutorError> {
    #[cfg(unix)]
    {
        // On Unix: PipeReader -> raw fd -> OwnedFd -> std::process::ChildStdout -> tokio::process::ChildStdout
        let raw_fd = pipe_reader.into_raw_fd();
        let owned_fd = unsafe { OwnedFd::from_raw_fd(raw_fd) };
        let std_stdout = std::process::ChildStdout::from(owned_fd);
        tokio::process::ChildStdout::from_std(std_stdout).map_err(ExecutorError::Io)
    }

    #[cfg(windows)]
    {
        // On Windows: PipeReader -> raw handle -> OwnedHandle -> std::process::ChildStdout -> tokio::process::ChildStdout
        let raw_handle = pipe_reader.into_raw_handle();
        let owned_handle = unsafe { OwnedHandle::from_raw_handle(raw_handle) };
        let std_stdout = std::process::ChildStdout::from(owned_handle);
        tokio::process::ChildStdout::from_std(std_stdout).map_err(ExecutorError::Io)
    }
}

/// Convert os_pipe::PipeWriter to a tokio file for async writing
fn wrap_fd_as_tokio_writer(
    pipe_writer: os_pipe::PipeWriter,
) -> Result<impl AsyncWrite, ExecutorError> {
    #[cfg(unix)]
    {
        // On Unix: PipeWriter -> raw fd -> OwnedFd -> std::fs::File -> tokio::fs::File
        let raw_fd = pipe_writer.into_raw_fd();
        let owned_fd = unsafe { OwnedFd::from_raw_fd(raw_fd) };
        let std_file = std::fs::File::from(owned_fd);
        Ok(tokio::fs::File::from_std(std_file))
    }

    #[cfg(windows)]
    {
        // On Windows: PipeWriter -> raw handle -> OwnedHandle -> std::fs::File -> tokio::fs::File
        let raw_handle = pipe_writer.into_raw_handle();
        let owned_handle = unsafe { OwnedHandle::from_raw_handle(raw_handle) };
        let std_file = std::fs::File::from(owned_handle);
        Ok(tokio::fs::File::from_std(std_file))
    }
}
</file>

<file path="crates/executors/Cargo.toml">
[package]
name = "executors"
version = "0.0.94"
edition = "2024"

[dependencies]
utils = { path = "../utils" }
tokio = { workspace = true }
tokio-util = { version = "0.7", features = ["io"] }
bytes = "1.0"
serde = { workspace = true, features = ["derive"] }
serde_json = { workspace = true }
tracing = { workspace = true }
toml = "0.8"
tracing-subscriber = { workspace = true }
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1.0", features = ["v4", "serde"] }
ts-rs = { workspace = true, features = ["serde-json-impl"]}
schemars = { workspace = true }
dirs = "5.0"
xdg = "3.0"
async-trait = "0.1"
rust-embed = "8.2"
directories = "6.0.0"
command-group = { version = "5.0", features = ["with-tokio"] }
regex = "1.11.1"
sentry-tracing = { version = "0.41.0", features = ["backtrace"] }
lazy_static = "1.4"
json-patch = "2.0"
thiserror = { workspace = true }
enum_dispatch = "0.3.13"
futures-io = "0.3.31"
tokio-stream = { version = "0.1.17", features = ["io-util"] }
futures = "0.3.31"
bon = "3.6"
fork_stream = "0.1.0"
os_pipe = "1.2"
strip-ansi-escapes = "0.2.1"
strum = "0.27.2"
strum_macros = "0.27.2"
convert_case = "0.6"
sqlx = "0.8.6"
axum = { workspace = true }
</file>

<file path="crates/executors/default_mcp.json">
{
  "vibe_kanban": {
    "command": "npx",
    "args": [
      "-y",
      "vibe-kanban",
      "--mcp"
    ]
  },
  "context7": {
    "type": "http",
    "url": "https://mcp.context7.com/mcp",
    "headers": {
      "CONTEXT7_API_KEY": "YOUR_API_KEY"
    }
  },
  "playwright": {
    "command": "npx",
    "args": [
      "@playwright/mcp@latest"
    ]
  },
  "meta": {
    "vibe_kanban": {
      "name": "Vibe Kanban",
      "description": "Create, update and delete Vibe Kanban tasks",
      "url": "https://www.vibekanban.com/docs/integrations/vibe-kanban-mcp-server",
      "icon": "viba-kanban-favicon.png"
    },
    "context7": {
      "name": "Context7",
      "description": "Fetch up-to-date documentation and code examples",
      "url": "https://github.com/upstash/context7",
      "icon": "mcp/context7logo.png"
    },
    "playwright": {
      "name": "Playwright",
      "description": "Browser automation with Playwright",
      "url": "https://github.com/microsoft/playwright-mcp",
      "icon": "mcp/playwright_logo_icon.svg"
    }
  }
}
</file>

<file path="crates/executors/default_profiles.json">
{
  "executors": {
    "CLAUDE_CODE": {
      "DEFAULT": {
        "CLAUDE_CODE": {
          "dangerously_skip_permissions": true
        }
      },
      "PLAN": {
        "CLAUDE_CODE": {
          "plan": true
        }
      }
    },
    "AMP": {
      "DEFAULT": {
        "AMP": {
          "dangerously_allow_all": true
        }
      }
    },
    "GEMINI": {
      "DEFAULT": {
        "GEMINI": {
          "model": "default",
          "yolo": true
        }
      },
      "FLASH": {
        "GEMINI": {
          "model": "flash",
          "yolo": true
        }
      }
    },
    "CODEX": {
      "DEFAULT": {
        "CODEX": {
          "sandbox": "auto",
          "model": "gpt-5-codex"
        }
      },
      "HIGH": {
        "CODEX": {
          "sandbox": "auto",
          "model_reasoning_effort": "high",
          "model": "gpt-5-codex"
        }
      },
      "GPT_5": {
        "CODEX": {
          "sandbox": "auto",
          "model": "gpt-5"
        }
      },
      "GPT_5_HIGH": {
        "CODEX": {
          "sandbox": "auto",
          "model_reasoning_effort": "high",
          "model": "gpt-5"
        }
      }
    },
    "OPENCODE": {
      "DEFAULT": {
        "OPENCODE": {}
      }
    },
    "QWEN_CODE": {
      "DEFAULT": {
        "QWEN_CODE": {
          "yolo": true
        }
      }
    },
    "CURSOR": {
      "DEFAULT": {
        "CURSOR": {
          "force": true
        }
      }
    }
  }
}
</file>

<file path="crates/local-deployment/src/command.rs">
use command_group::AsyncGroupChild;
#[cfg(unix)]
use nix::{
    sys::signal::{Signal, killpg},
    unistd::{Pid, getpgid},
};
use services::services::container::ContainerError;
use tokio::time::Duration;

pub async fn kill_process_group(child: &mut AsyncGroupChild) -> Result<(), ContainerError> {
    // hit the whole process group, not just the leader
    #[cfg(unix)]
    {
        if let Some(pid) = child.inner().id() {
            let pgid = getpgid(Some(Pid::from_raw(pid as i32)))
                .map_err(|e| ContainerError::KillFailed(std::io::Error::other(e)))?;

            for sig in [Signal::SIGINT, Signal::SIGTERM, Signal::SIGKILL] {
                if let Err(e) = killpg(pgid, sig) {
                    tracing::warn!(
                        "Failed to send signal {:?} to process group {}: {}",
                        sig,
                        pgid,
                        e
                    );
                }
                tokio::time::sleep(Duration::from_secs(2)).await;
                if child
                    .inner()
                    .try_wait()
                    .map_err(ContainerError::Io)?
                    .is_some()
                {
                    break;
                }
            }
        }
    }

    let _ = child.kill().await;
    let _ = child.wait().await;
    Ok(())
}
</file>

<file path="crates/local-deployment/src/container.rs">
use std::{
    collections::{HashMap, HashSet},
    io,
    path::{Path, PathBuf},
    sync::{
        Arc,
        atomic::{AtomicUsize, Ordering},
    },
    time::Duration,
};

use anyhow::anyhow;
use async_stream::try_stream;
use async_trait::async_trait;
use axum::response::sse::Event;
use command_group::AsyncGroupChild;
use db::{
    DBService,
    models::{
        execution_process::{
            ExecutionContext, ExecutionProcess, ExecutionProcessRunReason, ExecutionProcessStatus,
        },
        executor_session::ExecutorSession,
        follow_up_draft::FollowUpDraft,
        image::TaskImage,
        merge::Merge,
        project::Project,
        task::{Task, TaskStatus},
        task_attempt::TaskAttempt,
    },
};
use deployment::DeploymentError;
use executors::{
    actions::{Executable, ExecutorAction},
    logs::{
        NormalizedEntry, NormalizedEntryType,
        utils::{ConversationPatch, patch::escape_json_pointer_segment},
    },
};
use futures::{StreamExt, TryStreamExt, stream::select};
use notify_debouncer_full::DebouncedEvent;
use serde_json::json;
use services::services::{
    analytics::AnalyticsContext,
    config::Config,
    container::{ContainerError, ContainerRef, ContainerService},
    filesystem_watcher,
    git::{DiffTarget, GitService},
    image::ImageService,
    notification::NotificationService,
    worktree_manager::WorktreeManager,
};
use tokio::{sync::RwLock, task::JoinHandle};
use tokio_util::io::ReaderStream;
use utils::{
    diff::create_unified_diff_hunk,
    log_msg::LogMsg,
    msg_store::MsgStore,
    text::{git_branch_id, short_uuid},
};
use uuid::Uuid;

use crate::command;

#[derive(Clone)]
pub struct LocalContainerService {
    db: DBService,
    child_store: Arc<RwLock<HashMap<Uuid, Arc<RwLock<AsyncGroupChild>>>>>,
    msg_stores: Arc<RwLock<HashMap<Uuid, Arc<MsgStore>>>>,
    config: Arc<RwLock<Config>>,
    git: GitService,
    image_service: ImageService,
    analytics: Option<AnalyticsContext>,
}

impl LocalContainerService {
    // Max cumulative content bytes allowed per diff stream
    const MAX_CUMULATIVE_DIFF_BYTES: usize = 50 * 1024; // 50KB

    // Apply stream-level omit policy based on cumulative bytes.
    // If adding this diff's contents exceeds the cap, strip contents and set stats.
    fn apply_stream_omit_policy(
        &self,
        diff: &mut utils::diff::Diff,
        sent_bytes: &Arc<AtomicUsize>,
    ) {
        // Compute size of current diff payload
        let mut size = 0usize;
        if let Some(ref s) = diff.old_content {
            size += s.len();
        }
        if let Some(ref s) = diff.new_content {
            size += s.len();
        }

        if size == 0 {
            return; // nothing to account
        }

        let current = sent_bytes.load(Ordering::Relaxed);
        if current.saturating_add(size) > Self::MAX_CUMULATIVE_DIFF_BYTES {
            // We will omit content for this diff. If we still have both sides loaded
            // (i.e., not already omitted by file-size guards), compute stats for UI.
            if diff.additions.is_none() && diff.deletions.is_none() {
                let old = diff.old_content.as_deref().unwrap_or("");
                let new = diff.new_content.as_deref().unwrap_or("");
                let hunk = create_unified_diff_hunk(old, new);
                let mut add = 0usize;
                let mut del = 0usize;
                for line in hunk.lines() {
                    if let Some(first) = line.chars().next() {
                        if first == '+' {
                            add += 1;
                        } else if first == '-' {
                            del += 1;
                        }
                    }
                }
                diff.additions = Some(add);
                diff.deletions = Some(del);
            }

            diff.old_content = None;
            diff.new_content = None;
            diff.content_omitted = true;
        } else {
            // safe to include; account for it
            let _ = sent_bytes.fetch_add(size, Ordering::Relaxed);
        }
    }
    pub fn new(
        db: DBService,
        msg_stores: Arc<RwLock<HashMap<Uuid, Arc<MsgStore>>>>,
        config: Arc<RwLock<Config>>,
        git: GitService,
        image_service: ImageService,
        analytics: Option<AnalyticsContext>,
    ) -> Self {
        let child_store = Arc::new(RwLock::new(HashMap::new()));

        LocalContainerService {
            db,
            child_store,
            msg_stores,
            config,
            git,
            image_service,
            analytics,
        }
    }

    pub async fn get_child_from_store(&self, id: &Uuid) -> Option<Arc<RwLock<AsyncGroupChild>>> {
        let map = self.child_store.read().await;
        map.get(id).cloned()
    }

    pub async fn add_child_to_store(&self, id: Uuid, exec: AsyncGroupChild) {
        let mut map = self.child_store.write().await;
        map.insert(id, Arc::new(RwLock::new(exec)));
    }

    pub async fn remove_child_from_store(&self, id: &Uuid) {
        let mut map = self.child_store.write().await;
        map.remove(id);
    }

    /// A context is finalized when
    /// - The next action is None (no follow-up actions)
    /// - The run reason is not DevServer
    fn should_finalize(ctx: &ExecutionContext) -> bool {
        ctx.execution_process
            .executor_action()
            .unwrap()
            .next_action
            .is_none()
            && (!matches!(
                ctx.execution_process.run_reason,
                ExecutionProcessRunReason::DevServer
            ))
    }

    /// Finalize task execution by updating status to InReview and sending notifications
    async fn finalize_task(db: &DBService, config: &Arc<RwLock<Config>>, ctx: &ExecutionContext) {
        if let Err(e) = Task::update_status(&db.pool, ctx.task.id, TaskStatus::InReview).await {
            tracing::error!("Failed to update task status to InReview: {e}");
        }
        let notify_cfg = config.read().await.notifications.clone();
        NotificationService::notify_execution_halted(notify_cfg, ctx).await;
    }

    /// Defensively check for externally deleted worktrees and mark them as deleted in the database
    async fn check_externally_deleted_worktrees(db: &DBService) -> Result<(), DeploymentError> {
        let active_attempts = TaskAttempt::find_by_worktree_deleted(&db.pool).await?;
        tracing::debug!(
            "Checking {} active worktrees for external deletion...",
            active_attempts.len()
        );
        for (attempt_id, worktree_path) in active_attempts {
            // Check if worktree directory exists
            if !std::path::Path::new(&worktree_path).exists() {
                // Worktree was deleted externally, mark as deleted in database
                if let Err(e) = TaskAttempt::mark_worktree_deleted(&db.pool, attempt_id).await {
                    tracing::error!(
                        "Failed to mark externally deleted worktree as deleted for attempt {}: {}",
                        attempt_id,
                        e
                    );
                } else {
                    tracing::info!(
                        "Marked externally deleted worktree as deleted for attempt {} (path: {})",
                        attempt_id,
                        worktree_path
                    );
                }
            }
        }
        Ok(())
    }

    /// Find and delete orphaned worktrees that don't correspond to any task attempts
    async fn cleanup_orphaned_worktrees(&self) {
        // Check if orphan cleanup is disabled via environment variable
        if std::env::var("DISABLE_WORKTREE_ORPHAN_CLEANUP").is_ok() {
            tracing::debug!(
                "Orphan worktree cleanup is disabled via DISABLE_WORKTREE_ORPHAN_CLEANUP environment variable"
            );
            return;
        }
        let worktree_base_dir = WorktreeManager::get_worktree_base_dir();
        if !worktree_base_dir.exists() {
            tracing::debug!(
                "Worktree base directory {} does not exist, skipping orphan cleanup",
                worktree_base_dir.display()
            );
            return;
        }
        let entries = match std::fs::read_dir(&worktree_base_dir) {
            Ok(entries) => entries,
            Err(e) => {
                tracing::error!(
                    "Failed to read worktree base directory {}: {}",
                    worktree_base_dir.display(),
                    e
                );
                return;
            }
        };
        for entry in entries {
            let entry = match entry {
                Ok(entry) => entry,
                Err(e) => {
                    tracing::warn!("Failed to read directory entry: {}", e);
                    continue;
                }
            };
            let path = entry.path();
            // Only process directories
            if !path.is_dir() {
                continue;
            }

            let worktree_path_str = path.to_string_lossy().to_string();
            if let Ok(false) =
                TaskAttempt::container_ref_exists(&self.db().pool, &worktree_path_str).await
            {
                // This is an orphaned worktree - delete it
                tracing::info!("Found orphaned worktree: {}", worktree_path_str);
                if let Err(e) = WorktreeManager::cleanup_worktree(&path, None).await {
                    tracing::error!(
                        "Failed to remove orphaned worktree {}: {}",
                        worktree_path_str,
                        e
                    );
                } else {
                    tracing::info!(
                        "Successfully removed orphaned worktree: {}",
                        worktree_path_str
                    );
                }
            }
        }
    }

    pub async fn cleanup_expired_attempt(
        db: &DBService,
        attempt_id: Uuid,
        worktree_path: PathBuf,
        git_repo_path: PathBuf,
    ) -> Result<(), DeploymentError> {
        WorktreeManager::cleanup_worktree(&worktree_path, Some(&git_repo_path)).await?;
        // Mark worktree as deleted in database after successful cleanup
        TaskAttempt::mark_worktree_deleted(&db.pool, attempt_id).await?;
        tracing::info!("Successfully marked worktree as deleted for attempt {attempt_id}",);
        Ok(())
    }

    pub async fn cleanup_expired_attempts(db: &DBService) -> Result<(), DeploymentError> {
        let expired_attempts = TaskAttempt::find_expired_for_cleanup(&db.pool).await?;
        if expired_attempts.is_empty() {
            tracing::debug!("No expired worktrees found");
            return Ok(());
        }
        tracing::info!(
            "Found {} expired worktrees to clean up",
            expired_attempts.len()
        );
        for (attempt_id, worktree_path, git_repo_path) in expired_attempts {
            Self::cleanup_expired_attempt(
                db,
                attempt_id,
                PathBuf::from(worktree_path),
                PathBuf::from(git_repo_path),
            )
            .await
            .unwrap_or_else(|e| {
                tracing::error!("Failed to clean up expired attempt {attempt_id}: {e}",);
            });
        }
        Ok(())
    }

    pub async fn spawn_worktree_cleanup(&self) {
        let db = self.db.clone();
        let mut cleanup_interval = tokio::time::interval(tokio::time::Duration::from_secs(1800)); // 30 minutes
        self.cleanup_orphaned_worktrees().await;
        tokio::spawn(async move {
            loop {
                cleanup_interval.tick().await;
                tracing::info!("Starting periodic worktree cleanup...");
                Self::check_externally_deleted_worktrees(&db)
                    .await
                    .unwrap_or_else(|e| {
                        tracing::error!("Failed to check externally deleted worktrees: {}", e);
                    });
                Self::cleanup_expired_attempts(&db)
                    .await
                    .unwrap_or_else(|e| {
                        tracing::error!("Failed to clean up expired worktree attempts: {}", e)
                    });
            }
        });
    }

    /// Spawn a background task that polls the child process for completion and
    /// cleans up the execution entry when it exits.
    pub fn spawn_exit_monitor(&self, exec_id: &Uuid) -> JoinHandle<()> {
        let exec_id = *exec_id;
        let child_store = self.child_store.clone();
        let msg_stores = self.msg_stores.clone();
        let db = self.db.clone();
        let config = self.config.clone();
        let container = self.clone();
        let analytics = self.analytics.clone();

        tokio::spawn(async move {
            loop {
                let status_opt = {
                    let child_lock = {
                        let map = child_store.read().await;
                        map.get(&exec_id)
                            .cloned()
                            .unwrap_or_else(|| panic!("Child handle missing for {exec_id}"))
                    };

                    let mut child_handler = child_lock.write().await;
                    match child_handler.try_wait() {
                        Ok(Some(status)) => Some(Ok(status)),
                        Ok(None) => None,
                        Err(e) => Some(Err(e)),
                    }
                };

                // Update execution process and cleanup if exit
                if let Some(status_result) = status_opt {
                    // Update execution process record with completion info
                    let (exit_code, status) = match status_result {
                        Ok(exit_status) => {
                            let code = exit_status.code().unwrap_or(-1) as i64;
                            let status = if exit_status.success() {
                                ExecutionProcessStatus::Completed
                            } else {
                                ExecutionProcessStatus::Failed
                            };
                            (Some(code), status)
                        }
                        Err(_) => (None, ExecutionProcessStatus::Failed),
                    };

                    if !ExecutionProcess::was_killed(&db.pool, exec_id).await
                        && let Err(e) = ExecutionProcess::update_completion(
                            &db.pool,
                            exec_id,
                            status.clone(),
                            exit_code,
                        )
                        .await
                    {
                        tracing::error!("Failed to update execution process completion: {}", e);
                    }

                    if let Ok(ctx) = ExecutionProcess::load_context(&db.pool, exec_id).await {
                        // Update executor session summary if available
                        if let Err(e) = container.update_executor_session_summary(&exec_id).await {
                            tracing::warn!("Failed to update executor session summary: {}", e);
                        }

                        // (moved) capture after-head commit occurs later, after commit/next-action handling

                        if matches!(
                            ctx.execution_process.status,
                            ExecutionProcessStatus::Completed
                        ) && exit_code == Some(0)
                        {
                            // Commit changes (if any) and get feedback about whether changes were made
                            let changes_committed = match container.try_commit_changes(&ctx).await {
                                Ok(committed) => committed,
                                Err(e) => {
                                    tracing::error!(
                                        "Failed to commit changes after execution: {}",
                                        e
                                    );
                                    // Treat commit failures as if changes were made to be safe
                                    true
                                }
                            };

                            // Determine whether to start the next action based on execution context
                            let should_start_next = if matches!(
                                ctx.execution_process.run_reason,
                                ExecutionProcessRunReason::CodingAgent
                            ) {
                                // Skip CleanupScript when CodingAgent produced no changes
                                changes_committed
                            } else {
                                // SetupScript always proceeds to CodingAgent
                                true
                            };

                            if should_start_next {
                                // If the process exited successfully, start the next action
                                if let Err(e) = container.try_start_next_action(&ctx).await {
                                    tracing::error!(
                                        "Failed to start next action after completion: {}",
                                        e
                                    );
                                }
                            } else {
                                tracing::info!(
                                    "Skipping cleanup script for task attempt {} - no changes made by coding agent",
                                    ctx.task_attempt.id
                                );

                                // Manually finalize task since we're bypassing normal execution flow
                                Self::finalize_task(&db, &config, &ctx).await;
                            }
                        }

                        if Self::should_finalize(&ctx) {
                            Self::finalize_task(&db, &config, &ctx).await;
                            // After finalization, check if a queued follow-up exists and start it
                            if let Err(e) = container.try_consume_queued_followup(&ctx).await {
                                tracing::error!(
                                    "Failed to start queued follow-up for attempt {}: {}",
                                    ctx.task_attempt.id,
                                    e
                                );
                            }
                        }

                        // Fire event when CodingAgent execution has finished
                        if config.read().await.analytics_enabled == Some(true)
                            && matches!(
                                &ctx.execution_process.run_reason,
                                ExecutionProcessRunReason::CodingAgent
                            )
                            && let Some(analytics) = &analytics
                        {
                            analytics.analytics_service.track_event(&analytics.user_id, "task_attempt_finished", Some(json!({
                                    "task_id": ctx.task.id.to_string(),
                                    "project_id": ctx.task.project_id.to_string(),
                                    "attempt_id": ctx.task_attempt.id.to_string(),
                                    "execution_success": matches!(ctx.execution_process.status, ExecutionProcessStatus::Completed),
                                    "exit_code": ctx.execution_process.exit_code,
                                })));
                        }
                    }

                    // Now that commit/next-action/finalization steps for this process are complete,
                    // capture the HEAD OID as the definitive "after" state (best-effort).
                    if let Ok(ctx) = ExecutionProcess::load_context(&db.pool, exec_id).await {
                        let worktree_dir = container.task_attempt_to_current_dir(&ctx.task_attempt);
                        if let Ok(head) = container.git().get_head_info(&worktree_dir)
                            && let Err(e) = ExecutionProcess::update_after_head_commit(
                                &db.pool, exec_id, &head.oid,
                            )
                            .await
                        {
                            tracing::warn!(
                                "Failed to update after_head_commit for {}: {}",
                                exec_id,
                                e
                            );
                        }
                    }

                    // Cleanup msg store
                    if let Some(msg_arc) = msg_stores.write().await.remove(&exec_id) {
                        msg_arc.push_finished();
                        tokio::time::sleep(Duration::from_millis(50)).await; // Wait for the finish message to propogate
                        match Arc::try_unwrap(msg_arc) {
                            Ok(inner) => drop(inner),
                            Err(arc) => tracing::error!(
                                "There are still {} strong Arcs to MsgStore for {}",
                                Arc::strong_count(&arc),
                                exec_id
                            ),
                        }
                    }

                    // Cleanup child handle
                    child_store.write().await.remove(&exec_id);
                    break;
                }

                // still running, sleep and try again
                tokio::time::sleep(Duration::from_millis(250)).await;
            }
        })
    }

    pub fn dir_name_from_task_attempt(attempt_id: &Uuid, task_title: &str) -> String {
        let task_title_id = git_branch_id(task_title);
        format!("vk-{}-{}", short_uuid(attempt_id), task_title_id)
    }

    pub fn git_branch_from_task_attempt(attempt_id: &Uuid, task_title: &str) -> String {
        let task_title_id = git_branch_id(task_title);
        format!("vk/{}-{}", short_uuid(attempt_id), task_title_id)
    }

    async fn track_child_msgs_in_store(&self, id: Uuid, child: &mut AsyncGroupChild) {
        let store = Arc::new(MsgStore::new());

        let out = child.inner().stdout.take().expect("no stdout");
        let err = child.inner().stderr.take().expect("no stderr");

        // Map stdout bytes -> LogMsg::Stdout
        let out = ReaderStream::new(out)
            .map_ok(|chunk| LogMsg::Stdout(String::from_utf8_lossy(&chunk).into_owned()));

        // Map stderr bytes -> LogMsg::Stderr
        let err = ReaderStream::new(err)
            .map_ok(|chunk| LogMsg::Stderr(String::from_utf8_lossy(&chunk).into_owned()));

        // If you have a JSON Patch source, map it to LogMsg::JsonPatch too, then select all three.

        // Merge and forward into the store
        let merged = select(out, err); // Stream<Item = Result<LogMsg, io::Error>>
        let debounced = utils::stream_ext::debounce_logs(merged);
        store.clone().spawn_forwarder(debounced);

        let mut map = self.msg_stores().write().await;
        map.insert(id, store);
    }

    /// Get the worktree path for a task attempt
    #[allow(dead_code)]
    async fn get_worktree_path(
        &self,
        task_attempt: &TaskAttempt,
    ) -> Result<PathBuf, ContainerError> {
        let container_ref = self.ensure_container_exists(task_attempt).await?;
        let worktree_dir = PathBuf::from(&container_ref);

        if !worktree_dir.exists() {
            return Err(ContainerError::Other(anyhow!(
                "Worktree directory not found"
            )));
        }

        Ok(worktree_dir)
    }

    /// Get the project repository path for a task attempt
    async fn get_project_repo_path(
        &self,
        task_attempt: &TaskAttempt,
    ) -> Result<PathBuf, ContainerError> {
        let project_repo_path = task_attempt
            .parent_task(&self.db().pool)
            .await?
            .ok_or(ContainerError::Other(anyhow!("Parent task not found")))?
            .parent_project(&self.db().pool)
            .await?
            .ok_or(ContainerError::Other(anyhow!("Parent project not found")))?
            .git_repo_path;

        Ok(project_repo_path)
    }

    /// Create a diff stream for merged attempts (never changes)
    fn create_merged_diff_stream(
        &self,
        project_repo_path: &Path,
        merge_commit_id: &str,
    ) -> Result<futures::stream::BoxStream<'static, Result<Event, std::io::Error>>, ContainerError>
    {
        let diffs = self.git().get_diffs(
            DiffTarget::Commit {
                repo_path: project_repo_path,
                commit_sha: merge_commit_id,
            },
            None,
        )?;

        let cum = Arc::new(AtomicUsize::new(0));
        let diffs: Vec<_> = diffs
            .into_iter()
            .map(|mut d| {
                self.apply_stream_omit_policy(&mut d, &cum);
                d
            })
            .collect();

        let stream = futures::stream::iter(diffs.into_iter().map(|diff| {
            let entry_index = GitService::diff_path(&diff);
            let patch =
                ConversationPatch::add_diff(escape_json_pointer_segment(&entry_index), diff);
            let event = LogMsg::JsonPatch(patch).to_sse_event();
            Ok::<_, std::io::Error>(event)
        }))
        .chain(futures::stream::once(async {
            Ok::<_, std::io::Error>(LogMsg::Finished.to_sse_event())
        }))
        .boxed();

        Ok(stream)
    }

    /// Create a live diff stream for ongoing attempts
    async fn create_live_diff_stream(
        &self,
        worktree_path: &Path,
        task_branch: &str,
        base_branch: &str,
    ) -> Result<futures::stream::BoxStream<'static, Result<Event, std::io::Error>>, ContainerError>
    {
        // Get initial snapshot
        let git_service = self.git().clone();
        let initial_diffs = git_service.get_diffs(
            DiffTarget::Worktree {
                worktree_path,
                branch_name: task_branch,
                base_branch,
            },
            None,
        )?;

        // cumulative counter for entire stream
        let cumulative = Arc::new(AtomicUsize::new(0));
        // track which file paths have been emitted with full content already
        let full_sent = Arc::new(std::sync::RwLock::new(HashSet::<String>::new()));
        let initial_diffs: Vec<_> = initial_diffs
            .into_iter()
            .map(|mut d| {
                self.apply_stream_omit_policy(&mut d, &cumulative);
                d
            })
            .collect();

        // Record which paths were sent with full content
        {
            let mut guard = full_sent.write().unwrap();
            for d in &initial_diffs {
                if !d.content_omitted {
                    let p = GitService::diff_path(d);
                    guard.insert(p);
                }
            }
        }

        let initial_stream = futures::stream::iter(initial_diffs.into_iter().map(|diff| {
            let entry_index = GitService::diff_path(&diff);
            let patch =
                ConversationPatch::add_diff(escape_json_pointer_segment(&entry_index), diff);
            let event = LogMsg::JsonPatch(patch).to_sse_event();
            Ok::<_, std::io::Error>(event)
        }))
        .boxed();

        // Create live update stream
        let worktree_path = worktree_path.to_path_buf();
        let task_branch = task_branch.to_string();
        let base_branch = base_branch.to_string();

        let live_stream = {
            let git_service = git_service.clone();
            let worktree_path_for_spawn = worktree_path.clone();
            let cumulative = Arc::clone(&cumulative);
            let full_sent = Arc::clone(&full_sent);
            try_stream! {
                // Move the expensive watcher setup to blocking thread to avoid blocking the async runtime
                let watcher_result = tokio::task::spawn_blocking(move || {
                    filesystem_watcher::async_watcher(worktree_path_for_spawn)
                })
                .await
                .map_err(|e| io::Error::other(format!("Failed to spawn watcher setup: {e}")))?;

                let (_debouncer, mut rx, canonical_worktree_path) = watcher_result
                    .map_err(|e| io::Error::other(e.to_string()))?;

                while let Some(result) = rx.next().await {
                    match result {
                        Ok(events) => {
                            let changed_paths = Self::extract_changed_paths(&events, &canonical_worktree_path, &worktree_path);

                            if !changed_paths.is_empty() {
                                for event in Self::process_file_changes(
                                    &git_service,
                                    &worktree_path,
                                    &task_branch,
                                    &base_branch,
                                    &changed_paths,
                                    &cumulative,
                                    &full_sent,
                                ).map_err(|e| {
                                    tracing::error!("Error processing file changes: {}", e);
                                    io::Error::other(e.to_string())
                                })? {
                                    yield event;
                                }
                            }
                        }
                        Err(errors) => {
                            let error_msg = errors.iter()
                                .map(|e| e.to_string())
                                .collect::<Vec<_>>()
                                .join("; ");
                            tracing::error!("Filesystem watcher error: {}", error_msg);
                            Err(io::Error::other(error_msg))?;
                        }
                    }
                }
            }
        }.boxed();

        // Ensure all initial diffs are emitted before live updates, to avoid
        // earlier files being abbreviated due to interleaving ordering.
        let combined_stream = initial_stream.chain(live_stream);
        Ok(combined_stream.boxed())
    }

    /// Extract changed file paths from filesystem events
    fn extract_changed_paths(
        events: &[DebouncedEvent],
        canonical_worktree_path: &Path,
        worktree_path: &Path,
    ) -> Vec<String> {
        events
            .iter()
            .flat_map(|event| &event.paths)
            .filter_map(|path| {
                path.strip_prefix(canonical_worktree_path)
                    .or_else(|_| path.strip_prefix(worktree_path))
                    .ok()
                    .map(|p| p.to_string_lossy().replace('\\', "/"))
            })
            .filter(|s| !s.is_empty())
            .collect()
    }

    /// Process file changes and generate diff events
    fn process_file_changes(
        git_service: &GitService,
        worktree_path: &Path,
        task_branch: &str,
        base_branch: &str,
        changed_paths: &[String],
        cumulative_bytes: &Arc<AtomicUsize>,
        full_sent_paths: &Arc<std::sync::RwLock<HashSet<String>>>,
    ) -> Result<Vec<Event>, ContainerError> {
        let path_filter: Vec<&str> = changed_paths.iter().map(|s| s.as_str()).collect();

        let current_diffs = git_service.get_diffs(
            DiffTarget::Worktree {
                worktree_path,
                branch_name: task_branch,
                base_branch,
            },
            Some(&path_filter),
        )?;

        let mut events = Vec::new();
        let mut files_with_diffs = HashSet::new();

        // Add/update files that have diffs
        for mut diff in current_diffs {
            let file_path = GitService::diff_path(&diff);
            files_with_diffs.insert(file_path.clone());
            // Apply stream-level omit policy (affects contents and stats)
            // Note: we can't call self methods from static fn; implement inline
            {
                // Compute size
                let mut size = 0usize;
                if let Some(ref s) = diff.old_content {
                    size += s.len();
                }
                if let Some(ref s) = diff.new_content {
                    size += s.len();
                }
                if size > 0 {
                    let current = cumulative_bytes.load(Ordering::Relaxed);
                    if current.saturating_add(size)
                        > LocalContainerService::MAX_CUMULATIVE_DIFF_BYTES
                    {
                        if diff.additions.is_none() && diff.deletions.is_none() {
                            let old = diff.old_content.as_deref().unwrap_or("");
                            let new = diff.new_content.as_deref().unwrap_or("");
                            let hunk = create_unified_diff_hunk(old, new);
                            let mut add = 0usize;
                            let mut del = 0usize;
                            for line in hunk.lines() {
                                if let Some(first) = line.chars().next() {
                                    if first == '+' {
                                        add += 1;
                                    } else if first == '-' {
                                        del += 1;
                                    }
                                }
                            }
                            diff.additions = Some(add);
                            diff.deletions = Some(del);
                        }
                        diff.old_content = None;
                        diff.new_content = None;
                        diff.content_omitted = true;
                    } else {
                        let _ = cumulative_bytes.fetch_add(size, Ordering::Relaxed);
                    }
                }
            }

            // If this diff would be omitted and we already sent a full-content
            // version of this path earlier in the stream, skip sending a
            // degrading replacement.
            if diff.content_omitted {
                if full_sent_paths.read().unwrap().contains(&file_path) {
                    continue;
                }
            } else {
                // Track that we have sent a full-content version
                {
                    let mut guard = full_sent_paths.write().unwrap();
                    guard.insert(file_path.clone());
                }
            }

            let patch = ConversationPatch::add_diff(escape_json_pointer_segment(&file_path), diff);
            let event = LogMsg::JsonPatch(patch).to_sse_event();
            events.push(event);
        }

        // Remove files that changed but no longer have diffs
        for changed_path in changed_paths {
            if !files_with_diffs.contains(changed_path) {
                let patch =
                    ConversationPatch::remove_diff(escape_json_pointer_segment(changed_path));
                let event = LogMsg::JsonPatch(patch).to_sse_event();
                events.push(event);
            }
        }

        Ok(events)
    }
}

#[async_trait]
impl ContainerService for LocalContainerService {
    fn msg_stores(&self) -> &Arc<RwLock<HashMap<Uuid, Arc<MsgStore>>>> {
        &self.msg_stores
    }

    fn db(&self) -> &DBService {
        &self.db
    }

    fn git(&self) -> &GitService {
        &self.git
    }

    fn task_attempt_to_current_dir(&self, task_attempt: &TaskAttempt) -> PathBuf {
        PathBuf::from(task_attempt.container_ref.clone().unwrap_or_default())
    }
    /// Create a container
    async fn create(&self, task_attempt: &TaskAttempt) -> Result<ContainerRef, ContainerError> {
        let task = task_attempt
            .parent_task(&self.db.pool)
            .await?
            .ok_or(sqlx::Error::RowNotFound)?;

        let worktree_dir_name =
            LocalContainerService::dir_name_from_task_attempt(&task_attempt.id, &task.title);
        let worktree_path = WorktreeManager::get_worktree_base_dir().join(&worktree_dir_name);

        let git_branch_name =
            LocalContainerService::git_branch_from_task_attempt(&task_attempt.id, &task.title);

        let project = task
            .parent_project(&self.db.pool)
            .await?
            .ok_or(sqlx::Error::RowNotFound)?;

        WorktreeManager::create_worktree(
            &project.git_repo_path,
            &git_branch_name,
            &worktree_path,
            &task_attempt.base_branch,
            true, // create new branch
        )
        .await?;

        // Copy files specified in the project's copy_files field
        if let Some(copy_files) = &project.copy_files
            && !copy_files.trim().is_empty()
        {
            self.copy_project_files(&project.git_repo_path, &worktree_path, copy_files)
                .await
                .unwrap_or_else(|e| {
                    tracing::warn!("Failed to copy project files: {}", e);
                });
        }

        // Copy task images from cache to worktree
        if let Err(e) = self
            .image_service
            .copy_images_by_task_to_worktree(&worktree_path, task.id)
            .await
        {
            tracing::warn!("Failed to copy task images to worktree: {}", e);
        }

        // Update both container_ref and branch in the database
        TaskAttempt::update_container_ref(
            &self.db.pool,
            task_attempt.id,
            &worktree_path.to_string_lossy(),
        )
        .await?;

        TaskAttempt::update_branch(&self.db.pool, task_attempt.id, &git_branch_name).await?;

        Ok(worktree_path.to_string_lossy().to_string())
    }

    async fn delete_inner(&self, task_attempt: &TaskAttempt) -> Result<(), ContainerError> {
        // cleanup the container, here that means deleting the worktree
        let task = task_attempt
            .parent_task(&self.db.pool)
            .await?
            .ok_or(sqlx::Error::RowNotFound)?;
        let git_repo_path = match Project::find_by_id(&self.db.pool, task.project_id).await {
            Ok(Some(project)) => Some(project.git_repo_path.clone()),
            Ok(None) => None,
            Err(e) => {
                tracing::error!("Failed to fetch project {}: {}", task.project_id, e);
                None
            }
        };
        WorktreeManager::cleanup_worktree(
            &PathBuf::from(task_attempt.container_ref.clone().unwrap_or_default()),
            git_repo_path.as_deref(),
        )
        .await
        .unwrap_or_else(|e| {
            tracing::warn!(
                "Failed to clean up worktree for task attempt {}: {}",
                task_attempt.id,
                e
            );
        });
        Ok(())
    }

    async fn ensure_container_exists(
        &self,
        task_attempt: &TaskAttempt,
    ) -> Result<ContainerRef, ContainerError> {
        // Get required context
        let task = task_attempt
            .parent_task(&self.db.pool)
            .await?
            .ok_or(sqlx::Error::RowNotFound)?;

        let project = task
            .parent_project(&self.db.pool)
            .await?
            .ok_or(sqlx::Error::RowNotFound)?;

        let container_ref = task_attempt.container_ref.as_ref().ok_or_else(|| {
            ContainerError::Other(anyhow!("Container ref not found for task attempt"))
        })?;
        let worktree_path = PathBuf::from(container_ref);

        let branch_name = task_attempt
            .branch
            .as_ref()
            .ok_or_else(|| ContainerError::Other(anyhow!("Branch not found for task attempt")))?;

        WorktreeManager::ensure_worktree_exists(
            &project.git_repo_path,
            branch_name,
            &worktree_path,
        )
        .await?;

        Ok(container_ref.to_string())
    }

    async fn is_container_clean(&self, task_attempt: &TaskAttempt) -> Result<bool, ContainerError> {
        if let Some(container_ref) = &task_attempt.container_ref {
            // If container_ref is set, check if the worktree exists
            let path = PathBuf::from(container_ref);
            if path.exists() {
                self.git().is_worktree_clean(&path).map_err(|e| e.into())
            } else {
                return Ok(true); // No worktree means it's clean
            }
        } else {
            return Ok(true); // No container_ref means no worktree, so it's clean
        }
    }

    async fn start_execution_inner(
        &self,
        task_attempt: &TaskAttempt,
        execution_process: &ExecutionProcess,
        executor_action: &ExecutorAction,
    ) -> Result<(), ContainerError> {
        // Get the worktree path
        let container_ref = task_attempt
            .container_ref
            .as_ref()
            .ok_or(ContainerError::Other(anyhow!(
                "Container ref not found for task attempt"
            )))?;
        let current_dir = PathBuf::from(container_ref);

        // Create the child and stream, add to execution tracker
        let mut child = executor_action.spawn(&current_dir).await?;

        self.track_child_msgs_in_store(execution_process.id, &mut child)
            .await;

        self.add_child_to_store(execution_process.id, child).await;

        // Spawn exit monitor
        let _hn = self.spawn_exit_monitor(&execution_process.id);

        Ok(())
    }

    async fn stop_execution(
        &self,
        execution_process: &ExecutionProcess,
    ) -> Result<(), ContainerError> {
        let child = self
            .get_child_from_store(&execution_process.id)
            .await
            .ok_or_else(|| {
                ContainerError::Other(anyhow!("Child process not found for execution"))
            })?;
        ExecutionProcess::update_completion(
            &self.db.pool,
            execution_process.id,
            ExecutionProcessStatus::Killed,
            None,
        )
        .await?;

        // Kill the child process and remove from the store
        {
            let mut child_guard = child.write().await;
            if let Err(e) = command::kill_process_group(&mut child_guard).await {
                tracing::error!(
                    "Failed to stop execution process {}: {}",
                    execution_process.id,
                    e
                );
                return Err(e);
            }
        }
        self.remove_child_from_store(&execution_process.id).await;

        // Mark the process finished in the MsgStore
        if let Some(msg) = self.msg_stores.write().await.remove(&execution_process.id) {
            msg.push_finished();
        }

        // Update task status to InReview when execution is stopped
        if let Ok(ctx) = ExecutionProcess::load_context(&self.db.pool, execution_process.id).await
            && !matches!(
                ctx.execution_process.run_reason,
                ExecutionProcessRunReason::DevServer
            )
            && let Err(e) =
                Task::update_status(&self.db.pool, ctx.task.id, TaskStatus::InReview).await
        {
            tracing::error!("Failed to update task status to InReview: {e}");
        }

        tracing::debug!(
            "Execution process {} stopped successfully",
            execution_process.id
        );

        // Record after-head commit OID (best-effort)
        if let Ok(ctx) = ExecutionProcess::load_context(&self.db.pool, execution_process.id).await {
            let worktree = self.task_attempt_to_current_dir(&ctx.task_attempt);
            if let Ok(head) = self.git().get_head_info(&worktree) {
                let _ = ExecutionProcess::update_after_head_commit(
                    &self.db.pool,
                    execution_process.id,
                    &head.oid,
                )
                .await;
            }
        }

        Ok(())
    }

    async fn get_diff(
        &self,
        task_attempt: &TaskAttempt,
    ) -> Result<futures::stream::BoxStream<'static, Result<Event, std::io::Error>>, ContainerError>
    {
        let project_repo_path = self.get_project_repo_path(task_attempt).await?;
        let latest_merge =
            Merge::find_latest_by_task_attempt_id(&self.db.pool, task_attempt.id).await?;
        let task_branch = task_attempt
            .branch
            .clone()
            .ok_or(ContainerError::Other(anyhow!(
                "Task attempt {} does not have a branch",
                task_attempt.id
            )))?;

        let is_ahead = if let Ok((ahead, _)) = self.git().get_branch_status(
            &project_repo_path,
            &task_branch,
            &task_attempt.base_branch,
        ) {
            ahead > 0
        } else {
            false
        };

        // Show merged diff when no new work is on the branch or container
        if let Some(merge) = &latest_merge
            && let Some(commit) = merge.merge_commit()
            && self.is_container_clean(task_attempt).await?
            && !is_ahead
        {
            return self.create_merged_diff_stream(&project_repo_path, &commit);
        }

        // worktree is needed for non-merged diffs
        let container_ref = self.ensure_container_exists(task_attempt).await?;
        let worktree_path = PathBuf::from(container_ref);

        // Handle ongoing attempts (live streaming diff)
        self.create_live_diff_stream(&worktree_path, &task_branch, &task_attempt.base_branch)
            .await
    }

    async fn try_commit_changes(&self, ctx: &ExecutionContext) -> Result<bool, ContainerError> {
        if !matches!(
            ctx.execution_process.run_reason,
            ExecutionProcessRunReason::CodingAgent | ExecutionProcessRunReason::CleanupScript,
        ) {
            return Ok(false);
        }

        let message = match ctx.execution_process.run_reason {
            ExecutionProcessRunReason::CodingAgent => {
                // Try to retrieve the task summary from the executor session
                // otherwise fallback to default message
                match ExecutorSession::find_by_execution_process_id(
                    &self.db().pool,
                    ctx.execution_process.id,
                )
                .await
                {
                    Ok(Some(session)) if session.summary.is_some() => session.summary.unwrap(),
                    Ok(_) => {
                        tracing::debug!(
                            "No summary found for execution process {}, using default message",
                            ctx.execution_process.id
                        );
                        format!(
                            "Commit changes from coding agent for task attempt {}",
                            ctx.task_attempt.id
                        )
                    }
                    Err(e) => {
                        tracing::debug!(
                            "Failed to retrieve summary for execution process {}: {}",
                            ctx.execution_process.id,
                            e
                        );
                        format!(
                            "Commit changes from coding agent for task attempt {}",
                            ctx.task_attempt.id
                        )
                    }
                }
            }
            ExecutionProcessRunReason::CleanupScript => {
                format!(
                    "Cleanup script changes for task attempt {}",
                    ctx.task_attempt.id
                )
            }
            _ => Err(ContainerError::Other(anyhow::anyhow!(
                "Invalid run reason for commit"
            )))?,
        };

        let container_ref = ctx.task_attempt.container_ref.as_ref().ok_or_else(|| {
            ContainerError::Other(anyhow::anyhow!("Container reference not found"))
        })?;

        tracing::debug!(
            "Committing changes for task attempt {} at path {:?}: '{}'",
            ctx.task_attempt.id,
            &container_ref,
            message
        );

        let changes_committed = self.git().commit(Path::new(container_ref), &message)?;
        Ok(changes_committed)
    }

    /// Copy files from the original project directory to the worktree
    async fn copy_project_files(
        &self,
        source_dir: &Path,
        target_dir: &Path,
        copy_files: &str,
    ) -> Result<(), ContainerError> {
        let files: Vec<&str> = copy_files
            .split(',')
            .map(|s| s.trim())
            .filter(|s| !s.is_empty())
            .collect();

        for file_path in files {
            let source_file = source_dir.join(file_path);
            let target_file = target_dir.join(file_path);

            // Create parent directories if needed
            if let Some(parent) = target_file.parent()
                && !parent.exists()
            {
                std::fs::create_dir_all(parent).map_err(|e| {
                    ContainerError::Other(anyhow!("Failed to create directory {parent:?}: {e}"))
                })?;
            }

            // Copy the file
            if source_file.exists() {
                std::fs::copy(&source_file, &target_file).map_err(|e| {
                    ContainerError::Other(anyhow!(
                        "Failed to copy file {source_file:?} to {target_file:?}: {e}"
                    ))
                })?;
                tracing::info!("Copied file {:?} to worktree", file_path);
            } else {
                return Err(ContainerError::Other(anyhow!(
                    "File {source_file:?} does not exist in the project directory"
                )));
            }
        }
        Ok(())
    }
}

impl LocalContainerService {
    /// Extract the last assistant message from the MsgStore history
    fn extract_last_assistant_message(&self, exec_id: &Uuid) -> Option<String> {
        // Get the MsgStore for this execution
        let msg_stores = self.msg_stores.try_read().ok()?;
        let msg_store = msg_stores.get(exec_id)?;

        // Get the history and scan in reverse for the last assistant message
        let history = msg_store.get_history();

        for msg in history.iter().rev() {
            if let LogMsg::JsonPatch(patch) = msg {
                // Try to extract a NormalizedEntry from the patch
                if let Some(entry) = self.extract_normalized_entry_from_patch(patch)
                    && matches!(entry.entry_type, NormalizedEntryType::AssistantMessage)
                {
                    let content = entry.content.trim();
                    if !content.is_empty() {
                        // Truncate to reasonable size (4KB as Oracle suggested)
                        const MAX_SUMMARY_LENGTH: usize = 4096;
                        if content.len() > MAX_SUMMARY_LENGTH {
                            return Some(format!("{}...", &content[..MAX_SUMMARY_LENGTH]));
                        }
                        return Some(content.to_string());
                    }
                }
            }
        }

        None
    }

    /// Extract a NormalizedEntry from a JsonPatch if it contains one
    fn extract_normalized_entry_from_patch(
        &self,
        patch: &json_patch::Patch,
    ) -> Option<NormalizedEntry> {
        // Convert the patch to JSON to examine its structure
        if let Ok(patch_json) = serde_json::to_value(patch)
            && let Some(operations) = patch_json.as_array()
        {
            for operation in operations {
                if let Some(value) = operation.get("value") {
                    // Try to extract a NormalizedEntry from the value
                    if let Some(patch_type) = value.get("type").and_then(|t| t.as_str())
                        && patch_type == "NORMALIZED_ENTRY"
                        && let Some(content) = value.get("content")
                        && let Ok(entry) =
                            serde_json::from_value::<NormalizedEntry>(content.clone())
                    {
                        return Some(entry);
                    }
                }
            }
        }
        None
    }

    /// Update the executor session summary with the final assistant message
    async fn update_executor_session_summary(&self, exec_id: &Uuid) -> Result<(), anyhow::Error> {
        // Check if there's an executor session for this execution process
        let session =
            ExecutorSession::find_by_execution_process_id(&self.db.pool, *exec_id).await?;

        if let Some(session) = session {
            // Only update if summary is not already set
            if session.summary.is_none() {
                if let Some(summary) = self.extract_last_assistant_message(exec_id) {
                    ExecutorSession::update_summary(&self.db.pool, *exec_id, &summary).await?;
                } else {
                    tracing::debug!("No assistant message found for execution {}", exec_id);
                }
            }
        }

        Ok(())
    }

    /// If a queued follow-up draft exists for this attempt and nothing is running,
    /// start it immediately and clear the draft.
    async fn try_consume_queued_followup(
        &self,
        ctx: &ExecutionContext,
    ) -> Result<(), ContainerError> {
        // Only consider CodingAgent/cleanup chains; skip DevServer completions
        if matches!(
            ctx.execution_process.run_reason,
            ExecutionProcessRunReason::DevServer
        ) {
            return Ok(());
        }

        // If anything is running for this attempt, bail
        let procs =
            ExecutionProcess::find_by_task_attempt_id(&self.db.pool, ctx.task_attempt.id, false)
                .await?;
        if procs
            .iter()
            .any(|p| matches!(p.status, ExecutionProcessStatus::Running))
        {
            return Ok(());
        }

        // Load draft and ensure it's eligible
        let Some(draft) =
            FollowUpDraft::find_by_task_attempt_id(&self.db.pool, ctx.task_attempt.id).await?
        else {
            return Ok(());
        };

        if !draft.queued || draft.prompt.trim().is_empty() {
            return Ok(());
        }

        // Atomically acquire sending lock; if not acquired, someone else is sending.
        if !FollowUpDraft::try_mark_sending(&self.db.pool, ctx.task_attempt.id)
            .await
            .unwrap_or(false)
        {
            return Ok(());
        }

        // Ensure worktree exists
        let container_ref = self.ensure_container_exists(&ctx.task_attempt).await?;

        // Get session id
        let Some(session_id) = ExecutionProcess::find_latest_session_id_by_task_attempt(
            &self.db.pool,
            ctx.task_attempt.id,
        )
        .await?
        else {
            tracing::warn!(
                "No session id found for attempt {}. Cannot start queued follow-up.",
                ctx.task_attempt.id
            );
            return Ok(());
        };

        // Get last coding agent process to inherit executor profile
        let Some(latest) = ExecutionProcess::find_latest_by_task_attempt_and_run_reason(
            &self.db.pool,
            ctx.task_attempt.id,
            &ExecutionProcessRunReason::CodingAgent,
        )
        .await?
        else {
            tracing::warn!(
                "No prior CodingAgent process for attempt {}. Cannot start queued follow-up.",
                ctx.task_attempt.id
            );
            return Ok(());
        };

        use executors::actions::ExecutorActionType;
        let initial_executor_profile_id = match &latest.executor_action()?.typ {
            ExecutorActionType::CodingAgentInitialRequest(req) => req.executor_profile_id.clone(),
            ExecutorActionType::CodingAgentFollowUpRequest(req) => req.executor_profile_id.clone(),
            _ => {
                tracing::warn!(
                    "Latest process for attempt {} is not a coding agent; skipping queued follow-up",
                    ctx.task_attempt.id
                );
                return Ok(());
            }
        };

        let executor_profile_id = executors::profile::ExecutorProfileId {
            executor: initial_executor_profile_id.executor,
            variant: draft.variant.clone(),
        };

        // Prepare cleanup action
        let cleanup_action = ctx
            .task
            .parent_project(&self.db.pool)
            .await?
            .and_then(|p| p.cleanup_script)
            .map(|script| {
                Box::new(executors::actions::ExecutorAction::new(
                    executors::actions::ExecutorActionType::ScriptRequest(
                        executors::actions::script::ScriptRequest {
                            script,
                            language: executors::actions::script::ScriptRequestLanguage::Bash,
                            context: executors::actions::script::ScriptContext::CleanupScript,
                        },
                    ),
                    None,
                ))
            });

        // Handle images: associate, copy to worktree, canonicalize prompt
        let mut prompt = draft.prompt.clone();
        if let Some(image_ids) = &draft.image_ids {
            // Associate to task
            let _ = TaskImage::associate_many_dedup(&self.db.pool, ctx.task.id, image_ids).await;

            // Copy to worktree and canonicalize
            let worktree_path = std::path::PathBuf::from(&container_ref);
            if let Err(e) = self
                .image_service
                .copy_images_by_ids_to_worktree(&worktree_path, image_ids)
                .await
            {
                tracing::warn!("Failed to copy images to worktree: {}", e);
            } else {
                prompt = ImageService::canonicalise_image_paths(&prompt, &worktree_path);
            }
        }

        let follow_up_request =
            executors::actions::coding_agent_follow_up::CodingAgentFollowUpRequest {
                prompt,
                session_id,
                executor_profile_id,
            };

        let follow_up_action = executors::actions::ExecutorAction::new(
            executors::actions::ExecutorActionType::CodingAgentFollowUpRequest(follow_up_request),
            cleanup_action,
        );

        // Start the execution
        let _ = self
            .start_execution(
                &ctx.task_attempt,
                &follow_up_action,
                &ExecutionProcessRunReason::CodingAgent,
            )
            .await?;

        // Clear the draft to reflect that it has been consumed
        let _ = FollowUpDraft::clear_after_send(&self.db.pool, ctx.task_attempt.id).await;

        Ok(())
    }
}
</file>

<file path="crates/local-deployment/src/lib.rs">
use std::{collections::HashMap, sync::Arc};

use async_trait::async_trait;
use db::DBService;
use deployment::{Deployment, DeploymentError};
use executors::profile::ExecutorConfigs;
use services::services::{
    analytics::{AnalyticsConfig, AnalyticsContext, AnalyticsService, generate_user_id},
    auth::AuthService,
    config::{Config, load_config_from_file, save_config_to_file},
    container::ContainerService,
    events::EventService,
    file_search_cache::FileSearchCache,
    filesystem::FilesystemService,
    git::GitService,
    image::ImageService,
    sentry::SentryService,
};
use tokio::sync::RwLock;
use utils::{assets::config_path, msg_store::MsgStore};
use uuid::Uuid;

use crate::container::LocalContainerService;

mod command;
pub mod container;

#[derive(Clone)]
pub struct LocalDeployment {
    config: Arc<RwLock<Config>>,
    sentry: SentryService,
    user_id: String,
    db: DBService,
    analytics: Option<AnalyticsService>,
    msg_stores: Arc<RwLock<HashMap<Uuid, Arc<MsgStore>>>>,
    container: LocalContainerService,
    git: GitService,
    auth: AuthService,
    image: ImageService,
    filesystem: FilesystemService,
    events: EventService,
    file_search_cache: Arc<FileSearchCache>,
}

#[async_trait]
impl Deployment for LocalDeployment {
    async fn new() -> Result<Self, DeploymentError> {
        let mut raw_config = load_config_from_file(&config_path()).await;

        let profiles = ExecutorConfigs::get_cached();
        if !raw_config.onboarding_acknowledged
            && let Ok(recommended_executor) = profiles.get_recommended_executor_profile().await
        {
            raw_config.executor_profile = recommended_executor;
        }

        // Check if app version has changed and set release notes flag
        {
            let current_version = utils::version::APP_VERSION;
            let stored_version = raw_config.last_app_version.as_deref();

            if stored_version != Some(current_version) {
                // Show release notes only if this is an upgrade (not first install)
                raw_config.show_release_notes = stored_version.is_some();
                raw_config.last_app_version = Some(current_version.to_string());
            }
        }

        // Always save config (may have been migrated or version updated)
        save_config_to_file(&raw_config, &config_path()).await?;

        let config = Arc::new(RwLock::new(raw_config));
        let sentry = SentryService::new();
        let user_id = generate_user_id();
        let analytics = AnalyticsConfig::new().map(AnalyticsService::new);
        let git = GitService::new();
        let msg_stores = Arc::new(RwLock::new(HashMap::new()));
        let auth = AuthService::new();
        let filesystem = FilesystemService::new();

        // Create shared components for EventService
        let events_msg_store = Arc::new(MsgStore::new());
        let events_entry_count = Arc::new(RwLock::new(0));

        // Create DB with event hooks
        let db = {
            let hook = EventService::create_hook(
                events_msg_store.clone(),
                events_entry_count.clone(),
                DBService::new().await?, // Temporary DB service for the hook
            );
            DBService::new_with_after_connect(hook).await?
        };

        let image = ImageService::new(db.clone().pool)?;
        {
            let image_service = image.clone();
            tokio::spawn(async move {
                tracing::info!("Starting orphaned image cleanup...");
                if let Err(e) = image_service.delete_orphaned_images().await {
                    tracing::error!("Failed to clean up orphaned images: {}", e);
                }
            });
        }

        // We need to make analytics accessible to the ContainerService
        // TODO: Handle this more gracefully
        let analytics_ctx = analytics.as_ref().map(|s| AnalyticsContext {
            user_id: user_id.clone(),
            analytics_service: s.clone(),
        });
        let container = LocalContainerService::new(
            db.clone(),
            msg_stores.clone(),
            config.clone(),
            git.clone(),
            image.clone(),
            analytics_ctx,
        );
        container.spawn_worktree_cleanup().await;

        let events = EventService::new(db.clone(), events_msg_store, events_entry_count);
        let file_search_cache = Arc::new(FileSearchCache::new());

        Ok(Self {
            config,
            sentry,
            user_id,
            db,
            analytics,
            msg_stores,
            container,
            git,
            auth,
            image,
            filesystem,
            events,
            file_search_cache,
        })
    }

    fn user_id(&self) -> &str {
        &self.user_id
    }

    fn shared_types() -> Vec<String> {
        vec![]
    }

    fn config(&self) -> &Arc<RwLock<Config>> {
        &self.config
    }

    fn sentry(&self) -> &SentryService {
        &self.sentry
    }

    fn db(&self) -> &DBService {
        &self.db
    }

    fn analytics(&self) -> &Option<AnalyticsService> {
        &self.analytics
    }

    fn container(&self) -> &impl ContainerService {
        &self.container
    }
    fn auth(&self) -> &AuthService {
        &self.auth
    }

    fn git(&self) -> &GitService {
        &self.git
    }

    fn image(&self) -> &ImageService {
        &self.image
    }

    fn filesystem(&self) -> &FilesystemService {
        &self.filesystem
    }

    fn msg_stores(&self) -> &Arc<RwLock<HashMap<Uuid, Arc<MsgStore>>>> {
        &self.msg_stores
    }

    fn events(&self) -> &EventService {
        &self.events
    }

    fn file_search_cache(&self) -> &Arc<FileSearchCache> {
        &self.file_search_cache
    }
}
</file>

<file path="crates/local-deployment/Cargo.toml">
[package]
name = "local-deployment"
version = "0.0.94"
edition = "2024"

[dependencies]
db = { path = "../db" }
executors = { path="../executors" }
deployment = { path = "../deployment" }
services = { path = "../services" }
utils = { path = "../utils" }
tokio-util = { version = "0.7", features = ["io"] }
bytes = "1.0"
axum = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
anyhow = { workspace = true }
tracing = { workspace = true }
tracing-subscriber = { workspace = true }
sqlx = { version = "0.8.6", features = ["runtime-tokio-rustls", "sqlite", "chrono", "uuid"] }
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1.0", features = ["v4", "serde"] }
ts-rs = { workspace = true }
async-trait = "0.1"
rust-embed = "8.2"
ignore = "0.4"
command-group = { version = "5.0", features = ["with-tokio"] }
nix = { version = "0.29", features = ["signal", "process"] }
openssl-sys = { workspace = true }
regex = "1.11.1"
notify-rust = "4.11"
notify = "8.2.0"
notify-debouncer-full = "0.5.0"
sentry = { version = "0.41.0", features = ["anyhow", "backtrace", "panic", "debug-images"] }
sentry-tracing = { version = "0.41.0", features = ["backtrace"] }
reqwest = { version = "0.12", features = ["json"] }
futures = "0.3"
async-stream = "0.3"
json-patch = "2.0"
tokio = { workspace = true }
tokio-stream = { version = "0.1.17", features = ["sync"] }
</file>

<file path="crates/server/src/bin/generate_types.rs">
use std::{env, fs, path::Path};

use schemars::{JsonSchema, Schema, SchemaGenerator, generate::SchemaSettings};
use ts_rs::TS;

fn generate_types_content() -> String {
    // 4. Friendly banner
    const HEADER: &str = "// This file was generated by `crates/core/src/bin/generate_types.rs`.\n
// Do not edit this file manually.\n
// If you are an AI, and you absolutely have to edit this file, please confirm with the user first.";

    let decls: Vec<String> = vec![
        services::services::filesystem::DirectoryEntry::decl(),
        services::services::filesystem::DirectoryListResponse::decl(),
        db::models::project::Project::decl(),
        db::models::project::CreateProject::decl(),
        db::models::project::UpdateProject::decl(),
        db::models::project::SearchResult::decl(),
        db::models::project::SearchMatchType::decl(),
        executors::actions::ExecutorAction::decl(),
        executors::mcp_config::McpConfig::decl(),
        executors::actions::ExecutorActionType::decl(),
        executors::actions::script::ScriptContext::decl(),
        executors::actions::script::ScriptRequest::decl(),
        executors::actions::script::ScriptRequestLanguage::decl(),
        executors::executors::BaseCodingAgent::decl(),
        executors::executors::CodingAgent::decl(),
        db::models::task_template::TaskTemplate::decl(),
        db::models::task_template::CreateTaskTemplate::decl(),
        db::models::task_template::UpdateTaskTemplate::decl(),
        db::models::task::TaskStatus::decl(),
        db::models::task::Task::decl(),
        db::models::task::TaskWithAttemptStatus::decl(),
        db::models::task::TaskRelationships::decl(),
        db::models::task::CreateTask::decl(),
        db::models::task::UpdateTask::decl(),
        db::models::image::Image::decl(),
        db::models::image::CreateImage::decl(),
        utils::response::ApiResponse::<()>::decl(),
        server::routes::config::UserSystemInfo::decl(),
        server::routes::config::Environment::decl(),
        server::routes::config::McpServerQuery::decl(),
        server::routes::config::UpdateMcpServersBody::decl(),
        server::routes::config::GetMcpServerResponse::decl(),
        server::routes::task_attempts::CreateFollowUpAttempt::decl(),
        server::routes::task_attempts::FollowUpDraftResponse::decl(),
        server::routes::task_attempts::UpdateFollowUpDraftRequest::decl(),
        server::routes::tasks::CreateAndStartTaskRequest::decl(),
        server::routes::task_attempts::CreateGitHubPrRequest::decl(),
        server::routes::images::ImageResponse::decl(),
        services::services::github_service::GitHubServiceError::decl(),
        services::services::config::Config::decl(),
        services::services::config::NotificationConfig::decl(),
        services::services::config::ThemeMode::decl(),
        services::services::config::EditorConfig::decl(),
        services::services::config::EditorType::decl(),
        services::services::config::GitHubConfig::decl(),
        services::services::config::SoundFile::decl(),
        services::services::config::UiLanguage::decl(),
        services::services::auth::DeviceFlowStartResponse::decl(),
        server::routes::auth::DevicePollStatus::decl(),
        server::routes::auth::CheckTokenResponse::decl(),
        services::services::git::GitBranch::decl(),
        utils::diff::Diff::decl(),
        utils::diff::DiffChangeKind::decl(),
        services::services::github_service::RepositoryInfo::decl(),
        executors::command::CommandBuilder::decl(),
        executors::profile::ExecutorProfileId::decl(),
        executors::profile::ExecutorConfig::decl(),
        executors::executors::BaseAgentCapability::decl(),
        executors::executors::claude::ClaudeCode::decl(),
        executors::executors::gemini::Gemini::decl(),
        executors::executors::gemini::GeminiModel::decl(),
        executors::executors::amp::Amp::decl(),
        executors::executors::codex::Codex::decl(),
        executors::executors::codex::SandboxMode::decl(),
        executors::executors::codex::ReasoningEffort::decl(),
        executors::executors::codex::ReasoningSummary::decl(),
        executors::executors::cursor::Cursor::decl(),
        executors::executors::opencode::Opencode::decl(),
        executors::executors::qwen::QwenCode::decl(),
        executors::executors::AppendPrompt::decl(),
        executors::actions::coding_agent_initial::CodingAgentInitialRequest::decl(),
        executors::actions::coding_agent_follow_up::CodingAgentFollowUpRequest::decl(),
        server::routes::task_attempts::CreateTaskAttemptBody::decl(),
        server::routes::task_attempts::RebaseTaskAttemptRequest::decl(),
        server::routes::task_attempts::GitOperationError::decl(),
        server::routes::task_attempts::ReplaceProcessRequest::decl(),
        server::routes::task_attempts::CommitInfo::decl(),
        server::routes::task_attempts::BranchStatus::decl(),
        services::services::git::ConflictOp::decl(),
        db::models::task_attempt::TaskAttempt::decl(),
        db::models::execution_process::ExecutionProcess::decl(),
        db::models::execution_process::ExecutionProcessStatus::decl(),
        db::models::execution_process::ExecutionProcessRunReason::decl(),
        db::models::merge::Merge::decl(),
        db::models::merge::DirectMerge::decl(),
        db::models::merge::PrMerge::decl(),
        db::models::merge::MergeStatus::decl(),
        db::models::merge::PullRequestInfo::decl(),
        db::models::follow_up_draft::FollowUpDraft::decl(),
        executors::logs::CommandExitStatus::decl(),
        executors::logs::CommandRunResult::decl(),
        executors::logs::NormalizedEntry::decl(),
        executors::logs::NormalizedEntryType::decl(),
        executors::logs::FileChange::decl(),
        executors::logs::ActionType::decl(),
        executors::logs::TodoItem::decl(),
        executors::logs::ToolResult::decl(),
        executors::logs::ToolResultValueType::decl(),
        executors::logs::utils::patch::PatchType::decl(),
        serde_json::Value::decl(),
    ];

    let body = decls
        .into_iter()
        .map(|d| {
            let trimmed = d.trim_start();
            if trimmed.starts_with("export") {
                d
            } else {
                format!("export {trimmed}")
            }
        })
        .collect::<Vec<_>>()
        .join("\n\n");

    format!("{HEADER}\n\n{body}")
}

fn write_schema<T: JsonSchema>(
    name: &str,
    schemas_dir: &std::path::Path,
) -> Result<(), Box<dyn std::error::Error>> {
    // Draft-07, inline everything (no $defs)
    let mut settings = SchemaSettings::draft07();
    settings.inline_subschemas = true;

    let generator: SchemaGenerator = settings.into_generator();
    let schema: Schema = generator.into_root_schema_for::<T>();

    // Convert to JSON value to manipulate it
    let mut schema_value: serde_json::Value = serde_json::to_value(&schema)?;

    // Remove the title from root schema to prevent RJSF from creating an outer field container
    if let Some(obj) = schema_value.as_object_mut() {
        obj.remove("title");
    }

    let schema_json = serde_json::to_string_pretty(&schema_value)?;
    std::fs::write(schemas_dir.join(format!("{name}.json")), schema_json)?;
    Ok(())
}

fn generate_schemas() -> Result<(), Box<dyn std::error::Error>> {
    // Create schemas directory
    let schemas_dir = Path::new("shared/schemas");
    fs::create_dir_all(schemas_dir)?;

    println!("Generating JSON schemas…");

    // Generate schemas for all executor types
    write_schema::<executors::executors::amp::Amp>("amp", schemas_dir)?;
    write_schema::<executors::executors::claude::ClaudeCode>("claude_code", schemas_dir)?;
    write_schema::<executors::executors::gemini::Gemini>("gemini", schemas_dir)?;
    write_schema::<executors::executors::codex::Codex>("codex", schemas_dir)?;
    write_schema::<executors::executors::cursor::Cursor>("cursor", schemas_dir)?;
    write_schema::<executors::executors::opencode::Opencode>("opencode", schemas_dir)?;
    write_schema::<executors::executors::qwen::QwenCode>("qwen_code", schemas_dir)?;

    Ok(())
}

fn main() {
    let args: Vec<String> = env::args().collect();
    let check_mode = args.iter().any(|arg| arg == "--check");

    let shared_path = Path::new("shared");

    println!("Generating TypeScript types…");

    let generated = generate_types_content();
    let types_path = shared_path.join("types.ts");

    if check_mode {
        // Read the current file
        let current = fs::read_to_string(&types_path).unwrap_or_default();
        if current == generated {
            println!("✅ shared/types.ts is up to date.");
            std::process::exit(0);
        } else {
            eprintln!(
                "❌ shared/types.ts is not up to date. Please run 'npm run generate-types' and commit the changes."
            );
            std::process::exit(1);
        }
    } else {
        // Wipe existing shared
        fs::remove_dir_all(shared_path).ok();

        // Recreate folder
        fs::create_dir_all(shared_path).expect("cannot create shared");

        // Write the file as before
        fs::write(&types_path, generated).expect("unable to write types.ts");
        println!("✅ TypeScript types generated in shared/");

        // Generate JSON schemas
        if let Err(e) = generate_schemas() {
            eprintln!("❌ Failed to generate schemas: {}", e);
            std::process::exit(1);
        }

        println!("✅ JSON schemas generated in shared/schemas/");
    }
}
</file>

<file path="crates/server/src/bin/mcp_task_server.rs">
use std::str::FromStr;

use rmcp::{ServiceExt, transport::stdio};
use server::mcp::task_server::TaskServer;
use sqlx::{SqlitePool, sqlite::SqliteConnectOptions};
use tracing_subscriber::{EnvFilter, prelude::*};
use utils::{assets::asset_dir, sentry::sentry_layer};

fn main() -> anyhow::Result<()> {
    let environment = if cfg!(debug_assertions) {
        "dev"
    } else {
        "production"
    };
    let _guard = sentry::init((
        "https://1065a1d276a581316999a07d5dffee26@o4509603705192449.ingest.de.sentry.io/4509605576441937",
        sentry::ClientOptions {
            release: sentry::release_name!(),
            environment: Some(environment.into()),
            ..Default::default()
        },
    ));
    sentry::configure_scope(|scope| {
        scope.set_tag("source", "mcp");
    });
    tokio::runtime::Builder::new_multi_thread()
        .enable_all()
        .build()
        .unwrap()
        .block_on(async {
            tracing_subscriber::registry()
                .with(
                    tracing_subscriber::fmt::layer()
                        .with_writer(std::io::stderr)
                        .with_filter(EnvFilter::new("debug")),
                )
                .with(sentry_layer())
                .init();

            let version = env!("CARGO_PKG_VERSION");
            tracing::debug!("[MCP] Starting MCP task server version {version}...");

            // Database connection
            let database_url = format!(
                "sqlite://{}",
                asset_dir().join("db.sqlite").to_string_lossy()
            );

            let options = SqliteConnectOptions::from_str(&database_url)?.create_if_missing(false);
            let pool = SqlitePool::connect_with(options).await?;

            let service = TaskServer::new(pool)
                .serve(stdio())
                .await
                .inspect_err(|e| {
                    tracing::error!("serving error: {:?}", e);
                    sentry::capture_error(e);
                })?;

            service.waiting().await?;
            Ok(())
        })
}
</file>

<file path="crates/server/src/mcp/mod.rs">
pub mod task_server;
</file>

<file path="crates/server/src/mcp/task_server.rs">
use std::{future::Future, path::PathBuf};

use db::models::{
    project::Project,
    task::{CreateTask, Task, TaskStatus},
};
use rmcp::{
    ErrorData, ServerHandler,
    handler::server::tool::{Parameters, ToolRouter},
    model::{
        CallToolResult, Content, Implementation, ProtocolVersion, ServerCapabilities, ServerInfo,
    },
    schemars, tool, tool_handler, tool_router,
};
use serde::{Deserialize, Serialize};
use serde_json;
use sqlx::SqlitePool;
use uuid::Uuid;

#[derive(Debug, Deserialize, schemars::JsonSchema)]
pub struct CreateTaskRequest {
    #[schemars(description = "The ID of the project to create the task in. This is required!")]
    pub project_id: String,
    #[schemars(description = "The title of the task")]
    pub title: String,
    #[schemars(description = "Optional description of the task")]
    pub description: Option<String>,
}

#[derive(Debug, Serialize, schemars::JsonSchema)]
pub struct CreateTaskResponse {
    pub success: bool,
    pub task_id: String,
    pub message: String,
}

#[derive(Debug, Serialize, schemars::JsonSchema)]
pub struct ProjectSummary {
    #[schemars(description = "The unique identifier of the project")]
    pub id: String,
    #[schemars(description = "The name of the project")]
    pub name: String,
    #[schemars(description = "The path to the git repository")]
    pub git_repo_path: PathBuf,
    #[schemars(description = "Optional setup script for the project")]
    pub setup_script: Option<String>,
    #[schemars(description = "Optional cleanup script for the project")]
    pub cleanup_script: Option<String>,
    #[schemars(description = "Optional development script for the project")]
    pub dev_script: Option<String>,
    #[schemars(description = "When the project was created")]
    pub created_at: String,
    #[schemars(description = "When the project was last updated")]
    pub updated_at: String,
}

#[derive(Debug, Serialize, schemars::JsonSchema)]
pub struct ListProjectsResponse {
    pub success: bool,
    pub projects: Vec<ProjectSummary>,
    pub count: usize,
}

#[derive(Debug, Deserialize, schemars::JsonSchema)]
pub struct ListTasksRequest {
    #[schemars(description = "The ID of the project to list tasks from")]
    pub project_id: String,
    #[schemars(
        description = "Optional status filter: 'todo', 'inprogress', 'inreview', 'done', 'cancelled'"
    )]
    pub status: Option<String>,
    #[schemars(description = "Maximum number of tasks to return (default: 50)")]
    pub limit: Option<i32>,
}

#[derive(Debug, Serialize, schemars::JsonSchema)]
pub struct TaskSummary {
    #[schemars(description = "The unique identifier of the task")]
    pub id: String,
    #[schemars(description = "The title of the task")]
    pub title: String,
    #[schemars(description = "Optional description of the task")]
    pub description: Option<String>,
    #[schemars(description = "Current status of the task")]
    pub status: String,
    #[schemars(description = "When the task was created")]
    pub created_at: String,
    #[schemars(description = "When the task was last updated")]
    pub updated_at: String,
    #[schemars(description = "Whether the task has an in-progress execution attempt")]
    pub has_in_progress_attempt: Option<bool>,
    #[schemars(description = "Whether the task has a merged execution attempt")]
    pub has_merged_attempt: Option<bool>,
    #[schemars(description = "Whether the last execution attempt failed")]
    pub last_attempt_failed: Option<bool>,
}

#[derive(Debug, Serialize, schemars::JsonSchema)]
pub struct ListTasksResponse {
    pub success: bool,
    pub tasks: Vec<TaskSummary>,
    pub count: usize,
    pub project_id: String,
    pub project_name: Option<String>,
    pub applied_filters: ListTasksFilters,
}

#[derive(Debug, Serialize, schemars::JsonSchema)]
pub struct ListTasksFilters {
    pub status: Option<String>,
    pub limit: i32,
}

fn parse_task_status(status_str: &str) -> Option<TaskStatus> {
    match status_str.to_lowercase().as_str() {
        "todo" => Some(TaskStatus::Todo),
        "inprogress" | "in-progress" | "in_progress" => Some(TaskStatus::InProgress),
        "inreview" | "in-review" | "in_review" => Some(TaskStatus::InReview),
        "done" | "completed" => Some(TaskStatus::Done),
        "cancelled" | "canceled" => Some(TaskStatus::Cancelled),
        _ => None,
    }
}

fn task_status_to_string(status: &TaskStatus) -> String {
    match status {
        TaskStatus::Todo => "todo".to_string(),
        TaskStatus::InProgress => "in-progress".to_string(),
        TaskStatus::InReview => "in-review".to_string(),
        TaskStatus::Done => "done".to_string(),
        TaskStatus::Cancelled => "cancelled".to_string(),
    }
}

#[derive(Debug, Deserialize, schemars::JsonSchema)]
pub struct UpdateTaskRequest {
    #[schemars(description = "The ID of the project containing the task")]
    pub project_id: String,
    #[schemars(description = "The ID of the task to update")]
    pub task_id: String,
    #[schemars(description = "New title for the task")]
    pub title: Option<String>,
    #[schemars(description = "New description for the task")]
    pub description: Option<String>,
    #[schemars(description = "New status: 'todo', 'inprogress', 'inreview', 'done', 'cancelled'")]
    pub status: Option<String>,
}

#[derive(Debug, Serialize, schemars::JsonSchema)]
pub struct UpdateTaskResponse {
    pub success: bool,
    pub message: String,
    pub task: Option<TaskSummary>,
}

#[derive(Debug, Deserialize, schemars::JsonSchema)]
pub struct DeleteTaskRequest {
    #[schemars(description = "The ID of the project containing the task")]
    pub project_id: String,
    #[schemars(description = "The ID of the task to delete")]
    pub task_id: String,
}

#[derive(Debug, Serialize, schemars::JsonSchema)]
pub struct DeleteTaskResponse {
    pub success: bool,
    pub message: String,
    pub deleted_task_id: Option<String>,
}

#[derive(Debug, Serialize, schemars::JsonSchema)]
pub struct SimpleTaskResponse {
    pub success: bool,
    pub message: String,
    pub task_title: String,
    pub new_status: Option<String>,
}

#[derive(Debug, Deserialize, schemars::JsonSchema)]
pub struct GetTaskRequest {
    #[schemars(description = "The ID of the project containing the task")]
    pub project_id: String,
    #[schemars(description = "The ID of the task to retrieve")]
    pub task_id: String,
}

#[derive(Debug, Serialize, schemars::JsonSchema)]
pub struct GetTaskResponse {
    pub success: bool,
    pub task: Option<TaskSummary>,
    pub project_name: Option<String>,
}

#[derive(Debug, Clone)]
pub struct TaskServer {
    pub pool: SqlitePool,
    tool_router: ToolRouter<TaskServer>,
}

impl TaskServer {
    #[allow(dead_code)]
    pub fn new(pool: SqlitePool) -> Self {
        Self {
            pool,
            tool_router: Self::tool_router(),
        }
    }
}

#[tool_router]
impl TaskServer {
    #[tool(
        description = "Create a new task/ticket in a project. Always pass the `project_id` of the project you want to create the task in - it is required!"
    )]
    async fn create_task(
        &self,
        Parameters(CreateTaskRequest {
            project_id,
            title,
            description,
        }): Parameters<CreateTaskRequest>,
    ) -> Result<CallToolResult, ErrorData> {
        // Parse project_id from string to UUID
        let project_uuid = match Uuid::parse_str(&project_id) {
            Ok(uuid) => uuid,
            Err(_) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Invalid project ID format. Must be a valid UUID.",
                    "project_id": project_id
                });
                return Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response)
                        .unwrap_or_else(|_| "Invalid project ID format".to_string()),
                )]));
            }
        };

        // Check if project exists
        match Project::exists(&self.pool, project_uuid).await {
            Ok(false) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Project not found",
                    "project_id": project_id
                });
                return Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response)
                        .unwrap_or_else(|_| "Project not found".to_string()),
                )]));
            }
            Err(e) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Failed to check project existence",
                    "details": e.to_string(),
                    "project_id": project_id
                });
                return Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response)
                        .unwrap_or_else(|_| "Database error".to_string()),
                )]));
            }
            Ok(true) => {}
        }

        let task_id = Uuid::new_v4();
        let create_task_data = CreateTask {
            project_id: project_uuid,
            title: title.clone(),
            description: description.clone(),
            parent_task_attempt: None,
            image_ids: None,
        };

        match Task::create(&self.pool, &create_task_data, task_id).await {
            Ok(_task) => {
                let success_response = CreateTaskResponse {
                    success: true,
                    task_id: task_id.to_string(),
                    message: "Task created successfully".to_string(),
                };
                Ok(CallToolResult::success(vec![Content::text(
                    serde_json::to_string_pretty(&success_response)
                        .unwrap_or_else(|_| "Task created successfully".to_string()),
                )]))
            }
            Err(e) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Failed to create task",
                    "details": e.to_string(),
                    "project_id": project_id,
                    "title": title
                });
                Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response)
                        .unwrap_or_else(|_| "Failed to create task".to_string()),
                )]))
            }
        }
    }

    #[tool(description = "List all the available projects")]
    async fn list_projects(&self) -> Result<CallToolResult, ErrorData> {
        match Project::find_all(&self.pool).await {
            Ok(projects) => {
                let count = projects.len();
                let project_summaries: Vec<ProjectSummary> = projects
                    .into_iter()
                    .map(|project| ProjectSummary {
                        id: project.id.to_string(),
                        name: project.name,
                        git_repo_path: project.git_repo_path,
                        setup_script: project.setup_script,
                        cleanup_script: project.cleanup_script,
                        dev_script: project.dev_script,
                        created_at: project.created_at.to_rfc3339(),
                        updated_at: project.updated_at.to_rfc3339(),
                    })
                    .collect();

                let response = ListProjectsResponse {
                    success: true,
                    projects: project_summaries,
                    count,
                };

                Ok(CallToolResult::success(vec![Content::text(
                    serde_json::to_string_pretty(&response)
                        .unwrap_or_else(|_| "Failed to serialize projects".to_string()),
                )]))
            }
            Err(e) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Failed to retrieve projects",
                    "details": e.to_string()
                });
                Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response)
                        .unwrap_or_else(|_| "Database error".to_string()),
                )]))
            }
        }
    }

    #[tool(
        description = "List all the task/tickets in a project with optional filtering and execution status. `project_id` is required!"
    )]
    async fn list_tasks(
        &self,
        Parameters(ListTasksRequest {
            project_id,
            status,
            limit,
        }): Parameters<ListTasksRequest>,
    ) -> Result<CallToolResult, ErrorData> {
        let project_uuid = match Uuid::parse_str(&project_id) {
            Ok(uuid) => uuid,
            Err(_) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Invalid project ID format. Must be a valid UUID.",
                    "project_id": project_id
                });
                return Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response)
                        .unwrap_or_else(|_| "Invalid project ID format".to_string()),
                )]));
            }
        };

        let status_filter = if let Some(ref status_str) = status {
            match parse_task_status(status_str) {
                Some(status) => Some(status),
                None => {
                    let error_response = serde_json::json!({
                        "success": false,
                        "error": "Invalid status filter. Valid values: 'todo', 'inprogress', 'inreview', 'done', 'cancelled'",
                        "provided_status": status_str
                    });
                    return Ok(CallToolResult::error(vec![Content::text(
                        serde_json::to_string_pretty(&error_response)
                            .unwrap_or_else(|_| "Invalid status filter".to_string()),
                    )]));
                }
            }
        } else {
            None
        };

        let project = match Project::find_by_id(&self.pool, project_uuid).await {
            Ok(Some(project)) => project,
            Ok(None) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Project not found",
                    "project_id": project_id
                });
                return Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response)
                        .unwrap_or_else(|_| "Project not found".to_string()),
                )]));
            }
            Err(e) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Failed to check project existence",
                    "details": e.to_string(),
                    "project_id": project_id
                });
                return Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response)
                        .unwrap_or_else(|_| "Database error".to_string()),
                )]));
            }
        };

        let task_limit = limit.unwrap_or(50).clamp(1, 200); // Reasonable limits

        let tasks_result =
            Task::find_by_project_id_with_attempt_status(&self.pool, project_uuid).await;

        match tasks_result {
            Ok(tasks) => {
                let filtered_tasks: Vec<_> = tasks
                    .into_iter()
                    .filter(|task| {
                        if let Some(ref filter_status) = status_filter {
                            &task.status == filter_status
                        } else {
                            true
                        }
                    })
                    .take(task_limit as usize)
                    .collect();

                let task_summaries: Vec<TaskSummary> = filtered_tasks
                    .into_iter()
                    .map(|task| TaskSummary {
                        id: task.id.to_string(),
                        title: task.title.clone(),
                        description: task.description.clone(),
                        status: task_status_to_string(&task.status),
                        created_at: task.created_at.to_rfc3339(),
                        updated_at: task.updated_at.to_rfc3339(),
                        has_in_progress_attempt: Some(task.has_in_progress_attempt),
                        has_merged_attempt: Some(task.has_merged_attempt),
                        last_attempt_failed: Some(task.last_attempt_failed),
                    })
                    .collect();

                let count = task_summaries.len();
                let response = ListTasksResponse {
                    success: true,
                    tasks: task_summaries,
                    count,
                    project_id: project_id.clone(),
                    project_name: Some(project.name),
                    applied_filters: ListTasksFilters {
                        status: status.clone(),
                        limit: task_limit,
                    },
                };

                Ok(CallToolResult::success(vec![Content::text(
                    serde_json::to_string_pretty(&response)
                        .unwrap_or_else(|_| "Failed to serialize tasks".to_string()),
                )]))
            }
            Err(e) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Failed to retrieve tasks",
                    "details": e.to_string(),
                    "project_id": project_id
                });
                Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response)
                        .unwrap_or_else(|_| "Database error".to_string()),
                )]))
            }
        }
    }

    #[tool(
        description = "Update an existing task/ticket's title, description, or status. `project_id` and `task_id` are required! `title`, `description`, and `status` are optional."
    )]
    async fn update_task(
        &self,
        Parameters(UpdateTaskRequest {
            project_id,
            task_id,
            title,
            description,
            status,
        }): Parameters<UpdateTaskRequest>,
    ) -> Result<CallToolResult, ErrorData> {
        let project_uuid = match Uuid::parse_str(&project_id) {
            Ok(uuid) => uuid,
            Err(_) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Invalid project ID format. Must be a valid UUID.",
                    "project_id": project_id
                });
                return Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response).unwrap(),
                )]));
            }
        };

        let task_uuid = match Uuid::parse_str(&task_id) {
            Ok(uuid) => uuid,
            Err(_) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Invalid task ID format. Must be a valid UUID.",
                    "task_id": task_id
                });
                return Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response).unwrap(),
                )]));
            }
        };

        let status_enum = if let Some(ref status_str) = status {
            match parse_task_status(status_str) {
                Some(status) => Some(status),
                None => {
                    let error_response = serde_json::json!({
                        "success": false,
                        "error": "Invalid status. Valid values: 'todo', 'inprogress', 'inreview', 'done', 'cancelled'",
                        "provided_status": status_str
                    });
                    return Ok(CallToolResult::error(vec![Content::text(
                        serde_json::to_string_pretty(&error_response).unwrap(),
                    )]));
                }
            }
        } else {
            None
        };

        let current_task =
            match Task::find_by_id_and_project_id(&self.pool, task_uuid, project_uuid).await {
                Ok(Some(task)) => task,
                Ok(None) => {
                    let error_response = serde_json::json!({
                        "success": false,
                        "error": "Task not found in the specified project",
                        "task_id": task_id,
                        "project_id": project_id
                    });
                    return Ok(CallToolResult::error(vec![Content::text(
                        serde_json::to_string_pretty(&error_response).unwrap(),
                    )]));
                }
                Err(e) => {
                    let error_response = serde_json::json!({
                        "success": false,
                        "error": "Failed to retrieve task",
                        "details": e.to_string()
                    });
                    return Ok(CallToolResult::error(vec![Content::text(
                        serde_json::to_string_pretty(&error_response).unwrap(),
                    )]));
                }
            };

        let new_title = title.unwrap_or(current_task.title);
        let new_description = description.or(current_task.description);
        let new_status = status_enum.unwrap_or(current_task.status);
        let new_parent_task_attempt = current_task.parent_task_attempt;

        match Task::update(
            &self.pool,
            task_uuid,
            project_uuid,
            new_title,
            new_description,
            new_status,
            new_parent_task_attempt,
        )
        .await
        {
            Ok(updated_task) => {
                let task_summary = TaskSummary {
                    id: updated_task.id.to_string(),
                    title: updated_task.title,
                    description: updated_task.description,
                    status: task_status_to_string(&updated_task.status),
                    created_at: updated_task.created_at.to_rfc3339(),
                    updated_at: updated_task.updated_at.to_rfc3339(),
                    has_in_progress_attempt: None,
                    has_merged_attempt: None,
                    last_attempt_failed: None,
                };

                let response = UpdateTaskResponse {
                    success: true,
                    message: "Task updated successfully".to_string(),
                    task: Some(task_summary),
                };

                Ok(CallToolResult::success(vec![Content::text(
                    serde_json::to_string_pretty(&response).unwrap(),
                )]))
            }
            Err(e) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Failed to update task",
                    "details": e.to_string()
                });
                Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response).unwrap(),
                )]))
            }
        }
    }

    #[tool(
        description = "Delete a task/ticket from a project. `project_id` and `task_id` are required!"
    )]
    async fn delete_task(
        &self,
        Parameters(DeleteTaskRequest {
            project_id,
            task_id,
        }): Parameters<DeleteTaskRequest>,
    ) -> Result<CallToolResult, ErrorData> {
        let project_uuid = match Uuid::parse_str(&project_id) {
            Ok(uuid) => uuid,
            Err(_) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Invalid project ID format"
                });
                return Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response).unwrap(),
                )]));
            }
        };

        let task_uuid = match Uuid::parse_str(&task_id) {
            Ok(uuid) => uuid,
            Err(_) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Invalid task ID format"
                });
                return Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response).unwrap(),
                )]));
            }
        };

        match Task::exists(&self.pool, task_uuid, project_uuid).await {
            Ok(true) => {
                // Delete the task
                match Task::delete(&self.pool, task_uuid).await {
                    Ok(rows_affected) => {
                        if rows_affected > 0 {
                            let response = DeleteTaskResponse {
                                success: true,
                                message: "Task deleted successfully".to_string(),
                                deleted_task_id: Some(task_id),
                            };
                            Ok(CallToolResult::success(vec![Content::text(
                                serde_json::to_string_pretty(&response).unwrap(),
                            )]))
                        } else {
                            let error_response = serde_json::json!({
                                "success": false,
                                "error": "Task not found or already deleted"
                            });
                            Ok(CallToolResult::error(vec![Content::text(
                                serde_json::to_string_pretty(&error_response).unwrap(),
                            )]))
                        }
                    }
                    Err(e) => {
                        let error_response = serde_json::json!({
                            "success": false,
                            "error": "Failed to delete task",
                            "details": e.to_string()
                        });
                        Ok(CallToolResult::error(vec![Content::text(
                            serde_json::to_string_pretty(&error_response).unwrap(),
                        )]))
                    }
                }
            }
            Ok(false) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Task not found in the specified project"
                });
                Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response).unwrap(),
                )]))
            }
            Err(e) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Failed to check task existence",
                    "details": e.to_string()
                });
                Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response).unwrap(),
                )]))
            }
        }
    }

    #[tool(
        description = "Get detailed information about a specific task/ticket. `project_id` and `task_id` are required!"
    )]
    async fn get_task(
        &self,
        Parameters(GetTaskRequest {
            project_id,
            task_id,
        }): Parameters<GetTaskRequest>,
    ) -> Result<CallToolResult, ErrorData> {
        let project_uuid = match Uuid::parse_str(&project_id) {
            Ok(uuid) => uuid,
            Err(_) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Invalid project ID format"
                });
                return Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response).unwrap(),
                )]));
            }
        };

        let task_uuid = match Uuid::parse_str(&task_id) {
            Ok(uuid) => uuid,
            Err(_) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Invalid task ID format"
                });
                return Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response).unwrap(),
                )]));
            }
        };

        let task_result =
            Task::find_by_id_and_project_id(&self.pool, task_uuid, project_uuid).await;
        let project_result = Project::find_by_id(&self.pool, project_uuid).await;

        match (task_result, project_result) {
            (Ok(Some(task)), Ok(Some(project))) => {
                let task_summary = TaskSummary {
                    id: task.id.to_string(),
                    title: task.title,
                    description: task.description,
                    status: task_status_to_string(&task.status),
                    created_at: task.created_at.to_rfc3339(),
                    updated_at: task.updated_at.to_rfc3339(),
                    has_in_progress_attempt: None,
                    has_merged_attempt: None,
                    last_attempt_failed: None,
                };

                let response = GetTaskResponse {
                    success: true,
                    task: Some(task_summary),
                    project_name: Some(project.name),
                };

                Ok(CallToolResult::success(vec![Content::text(
                    serde_json::to_string_pretty(&response).unwrap(),
                )]))
            }
            (Ok(None), _) | (_, Ok(None)) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Task or project not found"
                });
                Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response).unwrap(),
                )]))
            }
            (Err(e), _) | (_, Err(e)) => {
                let error_response = serde_json::json!({
                    "success": false,
                    "error": "Failed to retrieve task or project",
                    "details": e.to_string()
                });
                Ok(CallToolResult::error(vec![Content::text(
                    serde_json::to_string_pretty(&error_response).unwrap(),
                )]))
            }
        }
    }
}

#[tool_handler]
impl ServerHandler for TaskServer {
    fn get_info(&self) -> ServerInfo {
        ServerInfo {
            protocol_version: ProtocolVersion::V_2025_03_26,
            capabilities: ServerCapabilities::builder()
                .enable_tools()
                .build(),
            server_info: Implementation {
                name: "vibe-kanban".to_string(),
                version: "1.0.0".to_string(),
            },
            instructions: Some("A task and project management server. If you need to create or update tickets or tasks then use these tools. Most of them absolutely require that you pass the `project_id` of the project that you are currently working on. This should be provided to you. Call `list_tasks` to fetch the `task_ids` of all the tasks in a project`. TOOLS: 'list_projects', 'list_tasks', 'create_task', 'get_task', 'update_task', 'delete_task'. Make sure to pass `project_id` or `task_id` where required. You can use list tools to get the available ids.".to_string()),
        }
    }
}
</file>

<file path="crates/server/src/middleware/mod.rs">
pub mod model_loaders;

pub use model_loaders::*;
</file>

<file path="crates/server/src/middleware/model_loaders.rs">
use axum::{
    extract::{Path, Request, State},
    http::StatusCode,
    middleware::Next,
    response::Response,
};
use db::models::{
    execution_process::ExecutionProcess, project::Project, task::Task, task_attempt::TaskAttempt,
    task_template::TaskTemplate,
};
use deployment::Deployment;
use uuid::Uuid;

use crate::DeploymentImpl;

pub async fn load_project_middleware(
    State(deployment): State<DeploymentImpl>,
    Path(project_id): Path<Uuid>,
    request: Request,
    next: Next,
) -> Result<Response, StatusCode> {
    // Load the project from the database
    let project = match Project::find_by_id(&deployment.db().pool, project_id).await {
        Ok(Some(project)) => project,
        Ok(None) => {
            tracing::warn!("Project {} not found", project_id);
            return Err(StatusCode::NOT_FOUND);
        }
        Err(e) => {
            tracing::error!("Failed to fetch project {}: {}", project_id, e);
            return Err(StatusCode::INTERNAL_SERVER_ERROR);
        }
    };

    // Insert the project as an extension
    let mut request = request;
    request.extensions_mut().insert(project);

    // Continue with the next middleware/handler
    Ok(next.run(request).await)
}

pub async fn load_task_middleware(
    State(deployment): State<DeploymentImpl>,
    Path(task_id): Path<Uuid>,
    request: Request,
    next: Next,
) -> Result<Response, StatusCode> {
    // Load the task and validate it belongs to the project
    let task = match Task::find_by_id(&deployment.db().pool, task_id).await {
        Ok(Some(task)) => task,
        Ok(None) => {
            tracing::warn!("Task {} not found", task_id);
            return Err(StatusCode::NOT_FOUND);
        }
        Err(e) => {
            tracing::error!("Failed to fetch task {}: {}", task_id, e);
            return Err(StatusCode::INTERNAL_SERVER_ERROR);
        }
    };

    // Insert both models as extensions
    let mut request = request;
    request.extensions_mut().insert(task);

    // Continue with the next middleware/handler
    Ok(next.run(request).await)
}

pub async fn load_task_attempt_middleware(
    State(deployment): State<DeploymentImpl>,
    Path(task_attempt_id): Path<Uuid>,
    mut request: Request,
    next: Next,
) -> Result<Response, StatusCode> {
    // Load the TaskAttempt from the database
    let attempt = match TaskAttempt::find_by_id(&deployment.db().pool, task_attempt_id).await {
        Ok(Some(a)) => a,
        Ok(None) => {
            tracing::warn!("TaskAttempt {} not found", task_attempt_id);
            return Err(StatusCode::NOT_FOUND);
        }
        Err(e) => {
            tracing::error!("Failed to fetch TaskAttempt {}: {}", task_attempt_id, e);
            return Err(StatusCode::INTERNAL_SERVER_ERROR);
        }
    };

    // Insert the attempt into extensions
    request.extensions_mut().insert(attempt);

    // Continue on
    Ok(next.run(request).await)
}

pub async fn load_execution_process_middleware(
    State(deployment): State<DeploymentImpl>,
    Path(process_id): Path<Uuid>,
    mut request: Request,
    next: Next,
) -> Result<Response, StatusCode> {
    // Load the execution process from the database
    let execution_process =
        match ExecutionProcess::find_by_id(&deployment.db().pool, process_id).await {
            Ok(Some(process)) => process,
            Ok(None) => {
                tracing::warn!("ExecutionProcess {} not found", process_id);
                return Err(StatusCode::NOT_FOUND);
            }
            Err(e) => {
                tracing::error!("Failed to fetch execution process {}: {}", process_id, e);
                return Err(StatusCode::INTERNAL_SERVER_ERROR);
            }
        };

    // Inject the execution process into the request
    request.extensions_mut().insert(execution_process);

    // Continue to the next middleware/handler
    Ok(next.run(request).await)
}

// TODO: fix
// Middleware that loads and injects Project, Task, TaskAttempt, and ExecutionProcess
// based on the path parameters: project_id, task_id, attempt_id, process_id
// pub async fn load_execution_process_with_context_middleware(
//     State(deployment): State<DeploymentImpl>,
//     Path((project_id, task_id, attempt_id, process_id)): Path<(Uuid, Uuid, Uuid, Uuid)>,
//     request: axum::extract::Request,
//     next: Next,
// ) -> Result<Response, StatusCode> {
//     // Load the task attempt context first
//     let context = match TaskAttempt::load_context(
//         &deployment.db().pool,
//         attempt_id,
//         task_id,
//         project_id,
//     )
//     .await
//     {
//         Ok(context) => context,
//         Err(e) => {
//             tracing::error!(
//                 "Failed to load context for attempt {} in task {} in project {}: {}",
//                 attempt_id,
//                 task_id,
//                 project_id,
//                 e
//             );
//             return Err(StatusCode::NOT_FOUND);
//         }
//     };

//     // Load the execution process
//     let execution_process = match ExecutionProcess::find_by_id(&deployment.db().pool, process_id).await
//     {
//         Ok(Some(process)) => process,
//         Ok(None) => {
//             tracing::warn!("ExecutionProcess {} not found", process_id);
//             return Err(StatusCode::NOT_FOUND);
//         }
//         Err(e) => {
//             tracing::error!("Failed to fetch execution process {}: {}", process_id, e);
//             return Err(StatusCode::INTERNAL_SERVER_ERROR);
//         }
//     };

//     // Insert all models as extensions
//     let mut request = request;
//     request.extensions_mut().insert(context.project);
//     request.extensions_mut().insert(context.task);
//     request.extensions_mut().insert(context.task_attempt);
//     request.extensions_mut().insert(execution_process);

//     // Continue with the next middleware/handler
//     Ok(next.run(request).await)
// }

// Middleware that loads and injects TaskTemplate based on the template_id path parameter
pub async fn load_task_template_middleware(
    State(deployment): State<DeploymentImpl>,
    Path(template_id): Path<Uuid>,
    request: axum::extract::Request,
    next: Next,
) -> Result<Response, StatusCode> {
    // Load the task template from the database
    let task_template = match TaskTemplate::find_by_id(&deployment.db().pool, template_id).await {
        Ok(Some(template)) => template,
        Ok(None) => {
            tracing::warn!("TaskTemplate {} not found", template_id);
            return Err(StatusCode::NOT_FOUND);
        }
        Err(e) => {
            tracing::error!("Failed to fetch task template {}: {}", template_id, e);
            return Err(StatusCode::INTERNAL_SERVER_ERROR);
        }
    };

    // Insert the task template as an extension
    let mut request = request;
    request.extensions_mut().insert(task_template);

    // Continue with the next middleware/handler
    Ok(next.run(request).await)
}
</file>

<file path="crates/server/src/routes/auth.rs">
use axum::{
    Router,
    extract::{Request, State},
    http::StatusCode,
    middleware::{Next, from_fn_with_state},
    response::{Json as ResponseJson, Response},
    routing::{get, post},
};
use deployment::Deployment;
use octocrab::auth::Continue;
use serde::{Deserialize, Serialize};
use services::services::{
    auth::{AuthError, DeviceFlowStartResponse},
    config::save_config_to_file,
    github_service::{GitHubService, GitHubServiceError},
};
use utils::response::ApiResponse;

use crate::{DeploymentImpl, error::ApiError};

pub fn router(deployment: &DeploymentImpl) -> Router<DeploymentImpl> {
    Router::new()
        .route("/auth/github/device/start", post(device_start))
        .route("/auth/github/device/poll", post(device_poll))
        .route("/auth/github/check", get(github_check_token))
        .layer(from_fn_with_state(
            deployment.clone(),
            sentry_user_context_middleware,
        ))
}

/// POST /auth/github/device/start
async fn device_start(
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<DeviceFlowStartResponse>>, ApiError> {
    let device_start_response = deployment.auth().device_start().await?;
    Ok(ResponseJson(ApiResponse::success(device_start_response)))
}

#[derive(Serialize, Deserialize, ts_rs::TS)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
#[ts(use_ts_enum)]
pub enum DevicePollStatus {
    SlowDown,
    AuthorizationPending,
    Success,
}

#[derive(Serialize, Deserialize, ts_rs::TS)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
#[ts(use_ts_enum)]
pub enum CheckTokenResponse {
    Valid,
    Invalid,
}

/// POST /auth/github/device/poll
async fn device_poll(
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<DevicePollStatus>>, ApiError> {
    let user_info = match deployment.auth().device_poll().await {
        Ok(info) => info,
        Err(AuthError::Pending(Continue::SlowDown)) => {
            return Ok(ResponseJson(ApiResponse::success(
                DevicePollStatus::SlowDown,
            )));
        }
        Err(AuthError::Pending(Continue::AuthorizationPending)) => {
            return Ok(ResponseJson(ApiResponse::success(
                DevicePollStatus::AuthorizationPending,
            )));
        }
        Err(e) => return Err(e.into()),
    };
    // Save to config
    {
        let config_path = utils::assets::config_path();
        let mut config = deployment.config().write().await;
        config.github.username = Some(user_info.username.clone());
        config.github.primary_email = user_info.primary_email.clone();
        config.github.oauth_token = Some(user_info.token.to_string());
        config.github_login_acknowledged = true; // Also acknowledge the GitHub login step
        save_config_to_file(&config.clone(), &config_path).await?;
    }
    let _ = deployment.update_sentry_scope().await;
    let props = serde_json::json!({
        "username": user_info.username,
        "email": user_info.primary_email,
    });
    deployment
        .track_if_analytics_allowed("$identify", props)
        .await;
    Ok(ResponseJson(ApiResponse::success(
        DevicePollStatus::Success,
    )))
}

/// GET /auth/github/check
async fn github_check_token(
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<CheckTokenResponse>>, ApiError> {
    let gh_config = deployment.config().read().await.github.clone();
    let Some(token) = gh_config.token() else {
        return Ok(ResponseJson(ApiResponse::success(
            CheckTokenResponse::Invalid,
        )));
    };
    let gh = GitHubService::new(&token)?;
    match gh.check_token().await {
        Ok(()) => Ok(ResponseJson(ApiResponse::success(
            CheckTokenResponse::Valid,
        ))),
        Err(GitHubServiceError::TokenInvalid) => Ok(ResponseJson(ApiResponse::success(
            CheckTokenResponse::Invalid,
        ))),
        Err(e) => Err(e.into()),
    }
}

/// Middleware to set Sentry user context for every request
pub async fn sentry_user_context_middleware(
    State(deployment): State<DeploymentImpl>,
    req: Request,
    next: Next,
) -> Result<Response, StatusCode> {
    let _ = deployment.update_sentry_scope().await;
    Ok(next.run(req).await)
}
</file>

<file path="crates/server/src/routes/config.rs">
use std::collections::HashMap;

use axum::{
    Json, Router,
    body::Body,
    extract::{Path, Query, State},
    http,
    response::{Json as ResponseJson, Response},
    routing::{get, put},
};
use deployment::{Deployment, DeploymentError};
use executors::{
    executors::{BaseAgentCapability, BaseCodingAgent, StandardCodingAgentExecutor},
    mcp_config::{McpConfig, read_agent_config, write_agent_config},
    profile::{ExecutorConfigs, ExecutorProfileId},
};
use serde::{Deserialize, Serialize};
use serde_json::Value;
use services::services::config::{Config, ConfigError, SoundFile, save_config_to_file};
use tokio::fs;
use ts_rs::TS;
use utils::{assets::config_path, response::ApiResponse};

use crate::{DeploymentImpl, error::ApiError};

pub fn router() -> Router<DeploymentImpl> {
    Router::new()
        .route("/info", get(get_user_system_info))
        .route("/config", put(update_config))
        .route("/sounds/{sound}", get(get_sound))
        .route("/mcp-config", get(get_mcp_servers).post(update_mcp_servers))
        .route("/profiles", get(get_profiles).put(update_profiles))
}

#[derive(Debug, Serialize, Deserialize, TS)]
pub struct Environment {
    pub os_type: String,
    pub os_version: String,
    pub os_architecture: String,
    pub bitness: String,
}

impl Default for Environment {
    fn default() -> Self {
        Self::new()
    }
}

impl Environment {
    pub fn new() -> Self {
        let info = os_info::get();
        Environment {
            os_type: info.os_type().to_string(),
            os_version: info.version().to_string(),
            os_architecture: info.architecture().unwrap_or("unknown").to_string(),
            bitness: info.bitness().to_string(),
        }
    }
}

#[derive(Debug, Serialize, Deserialize, TS)]
pub struct UserSystemInfo {
    pub config: Config,
    #[serde(flatten)]
    pub profiles: ExecutorConfigs,
    pub environment: Environment,
    /// Capabilities supported per executor (e.g., { "CLAUDE_CODE": ["SESSION_FORK"] })
    pub capabilities: HashMap<String, Vec<BaseAgentCapability>>,
}

// TODO: update frontend, BE schema has changed, this replaces GET /config and /config/constants
#[axum::debug_handler]
async fn get_user_system_info(
    State(deployment): State<DeploymentImpl>,
) -> ResponseJson<ApiResponse<UserSystemInfo>> {
    let config = deployment.config().read().await;

    let user_system_info = UserSystemInfo {
        config: config.clone(),
        profiles: ExecutorConfigs::get_cached(),
        environment: Environment::new(),
        capabilities: {
            let mut caps: HashMap<String, Vec<BaseAgentCapability>> = HashMap::new();
            let profs = ExecutorConfigs::get_cached();
            for key in profs.executors.keys() {
                if let Some(agent) = profs.get_coding_agent(&ExecutorProfileId::new(*key)) {
                    caps.insert(key.to_string(), agent.capabilities());
                }
            }
            caps
        },
    };

    ResponseJson(ApiResponse::success(user_system_info))
}

async fn update_config(
    State(deployment): State<DeploymentImpl>,
    Json(new_config): Json<Config>,
) -> ResponseJson<ApiResponse<Config>> {
    let config_path = config_path();

    // Get old config state before updating
    let old_config = deployment.config().read().await.clone();

    match save_config_to_file(&new_config, &config_path).await {
        Ok(_) => {
            let mut config = deployment.config().write().await;
            *config = new_config.clone();
            drop(config);

            // Track config events when fields transition from false → true and run side effects
            handle_config_events(&deployment, &old_config, &new_config).await;

            ResponseJson(ApiResponse::success(new_config))
        }
        Err(e) => ResponseJson(ApiResponse::error(&format!("Failed to save config: {}", e))),
    }
}

/// Track config events when fields transition from false → true
async fn track_config_events(deployment: &DeploymentImpl, old: &Config, new: &Config) {
    let events = [
        (
            !old.disclaimer_acknowledged && new.disclaimer_acknowledged,
            "onboarding_disclaimer_accepted",
            serde_json::json!({}),
        ),
        (
            !old.onboarding_acknowledged && new.onboarding_acknowledged,
            "onboarding_completed",
            serde_json::json!({
                "profile": new.executor_profile,
                "editor": new.editor
            }),
        ),
        (
            !old.github_login_acknowledged && new.github_login_acknowledged,
            "onboarding_github_login_completed",
            serde_json::json!({
                "username": new.github.username,
                "email": new.github.primary_email,
                "auth_method": if new.github.oauth_token.is_some() { "oauth" }
                              else if new.github.pat.is_some() { "pat" }
                              else { "none" },
                "has_default_pr_base": new.github.default_pr_base.is_some(),
                "skipped": new.github.username.is_none()
            }),
        ),
        (
            !old.telemetry_acknowledged && new.telemetry_acknowledged,
            "onboarding_telemetry_choice",
            serde_json::json!({}),
        ),
        (
            !old.analytics_enabled.unwrap_or(false) && new.analytics_enabled.unwrap_or(false),
            "analytics_session_start",
            serde_json::json!({}),
        ),
    ];

    for (should_track, event_name, properties) in events {
        if should_track {
            deployment
                .track_if_analytics_allowed(event_name, properties)
                .await;
        }
    }
}

async fn handle_config_events(deployment: &DeploymentImpl, old: &Config, new: &Config) {
    track_config_events(deployment, old, new).await;

    if !old.disclaimer_acknowledged && new.disclaimer_acknowledged {
        deployment.trigger_auto_project_setup().await;
    }
}

async fn get_sound(Path(sound): Path<SoundFile>) -> Result<Response, ApiError> {
    let sound = sound.serve().await.map_err(DeploymentError::Other)?;
    let response = Response::builder()
        .status(http::StatusCode::OK)
        .header(
            http::header::CONTENT_TYPE,
            http::HeaderValue::from_static("audio/wav"),
        )
        .body(Body::from(sound.data.into_owned()))
        .unwrap();
    Ok(response)
}

#[derive(TS, Debug, Deserialize)]
pub struct McpServerQuery {
    executor: BaseCodingAgent,
}

#[derive(TS, Debug, Serialize, Deserialize)]
pub struct GetMcpServerResponse {
    // servers: HashMap<String, Value>,
    mcp_config: McpConfig,
    config_path: String,
}

#[derive(TS, Debug, Serialize, Deserialize)]
pub struct UpdateMcpServersBody {
    servers: HashMap<String, Value>,
}

async fn get_mcp_servers(
    State(_deployment): State<DeploymentImpl>,
    Query(query): Query<McpServerQuery>,
) -> Result<ResponseJson<ApiResponse<GetMcpServerResponse>>, ApiError> {
    let coding_agent = ExecutorConfigs::get_cached()
        .get_coding_agent(&ExecutorProfileId::new(query.executor))
        .ok_or(ConfigError::ValidationError(
            "Executor not found".to_string(),
        ))?;

    if !coding_agent.supports_mcp() {
        return Ok(ResponseJson(ApiResponse::error(
            "MCP not supported by this executor",
        )));
    }

    // Resolve supplied config path or agent default
    let config_path = match coding_agent.default_mcp_config_path() {
        Some(path) => path,
        None => {
            return Ok(ResponseJson(ApiResponse::error(
                "Could not determine config file path",
            )));
        }
    };

    let mut mcpc = coding_agent.get_mcp_config();
    let raw_config = read_agent_config(&config_path, &mcpc).await?;
    let servers = get_mcp_servers_from_config_path(&raw_config, &mcpc.servers_path);
    mcpc.set_servers(servers);
    Ok(ResponseJson(ApiResponse::success(GetMcpServerResponse {
        mcp_config: mcpc,
        config_path: config_path.to_string_lossy().to_string(),
    })))
}

async fn update_mcp_servers(
    State(_deployment): State<DeploymentImpl>,
    Query(query): Query<McpServerQuery>,
    Json(payload): Json<UpdateMcpServersBody>,
) -> Result<ResponseJson<ApiResponse<String>>, ApiError> {
    let profiles = ExecutorConfigs::get_cached();
    let agent = profiles
        .get_coding_agent(&ExecutorProfileId::new(query.executor))
        .ok_or(ConfigError::ValidationError(
            "Executor not found".to_string(),
        ))?;

    if !agent.supports_mcp() {
        return Ok(ResponseJson(ApiResponse::error(
            "This executor does not support MCP servers",
        )));
    }

    // Resolve supplied config path or agent default
    let config_path = match agent.default_mcp_config_path() {
        Some(path) => path.to_path_buf(),
        None => {
            return Ok(ResponseJson(ApiResponse::error(
                "Could not determine config file path",
            )));
        }
    };

    let mcpc = agent.get_mcp_config();
    match update_mcp_servers_in_config(&config_path, &mcpc, payload.servers).await {
        Ok(message) => Ok(ResponseJson(ApiResponse::success(message))),
        Err(e) => Ok(ResponseJson(ApiResponse::error(&format!(
            "Failed to update MCP servers: {}",
            e
        )))),
    }
}

async fn update_mcp_servers_in_config(
    config_path: &std::path::Path,
    mcpc: &McpConfig,
    new_servers: HashMap<String, Value>,
) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
    // Ensure parent directory exists
    if let Some(parent) = config_path.parent() {
        fs::create_dir_all(parent).await?;
    }
    // Read existing config (JSON or TOML depending on agent)
    let mut config = read_agent_config(config_path, mcpc).await?;

    // Get the current server count for comparison
    let old_servers = get_mcp_servers_from_config_path(&config, &mcpc.servers_path).len();

    // Set the MCP servers using the correct attribute path
    set_mcp_servers_in_config_path(&mut config, &mcpc.servers_path, &new_servers)?;

    // Write the updated config back to file (JSON or TOML depending on agent)
    write_agent_config(config_path, mcpc, &config).await?;

    let new_count = new_servers.len();
    let message = match (old_servers, new_count) {
        (0, 0) => "No MCP servers configured".to_string(),
        (0, n) => format!("Added {} MCP server(s)", n),
        (old, new) if old == new => format!("Updated MCP server configuration ({} server(s))", new),
        (old, new) => format!(
            "Updated MCP server configuration (was {}, now {})",
            old, new
        ),
    };

    Ok(message)
}

/// Helper function to get MCP servers from config using a path
fn get_mcp_servers_from_config_path(raw_config: &Value, path: &[String]) -> HashMap<String, Value> {
    let mut current = raw_config;
    for part in path {
        current = match current.get(part) {
            Some(val) => val,
            None => return HashMap::new(),
        };
    }
    // Extract the servers object
    match current.as_object() {
        Some(servers) => servers
            .iter()
            .map(|(k, v)| (k.clone(), v.clone()))
            .collect(),
        None => HashMap::new(),
    }
}

/// Helper function to set MCP servers in config using a path
fn set_mcp_servers_in_config_path(
    raw_config: &mut Value,
    path: &[String],
    servers: &HashMap<String, Value>,
) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    // Ensure config is an object
    if !raw_config.is_object() {
        *raw_config = serde_json::json!({});
    }

    let mut current = raw_config;
    // Navigate/create the nested structure (all parts except the last)
    for part in &path[..path.len() - 1] {
        if current.get(part).is_none() {
            current
                .as_object_mut()
                .unwrap()
                .insert(part.to_string(), serde_json::json!({}));
        }
        current = current.get_mut(part).unwrap();
        if !current.is_object() {
            *current = serde_json::json!({});
        }
    }

    // Set the final attribute
    let final_attr = path.last().unwrap();
    current
        .as_object_mut()
        .unwrap()
        .insert(final_attr.to_string(), serde_json::to_value(servers)?);

    Ok(())
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ProfilesContent {
    pub content: String,
    pub path: String,
}

async fn get_profiles(
    State(_deployment): State<DeploymentImpl>,
) -> ResponseJson<ApiResponse<ProfilesContent>> {
    let profiles_path = utils::assets::profiles_path();

    // Use cached data to ensure consistency with runtime and PUT updates
    let profiles = ExecutorConfigs::get_cached();

    let content = serde_json::to_string_pretty(&profiles).unwrap_or_else(|e| {
        tracing::error!("Failed to serialize profiles to JSON: {}", e);
        serde_json::to_string_pretty(&ExecutorConfigs::from_defaults())
            .unwrap_or_else(|_| "{}".to_string())
    });

    ResponseJson(ApiResponse::success(ProfilesContent {
        content,
        path: profiles_path.display().to_string(),
    }))
}

async fn update_profiles(
    State(_deployment): State<DeploymentImpl>,
    body: String,
) -> ResponseJson<ApiResponse<String>> {
    // Try to parse as ExecutorProfileConfigs format
    match serde_json::from_str::<ExecutorConfigs>(&body) {
        Ok(executor_profiles) => {
            // Save the profiles to file
            match executor_profiles.save_overrides() {
                Ok(_) => {
                    tracing::info!("Executor profiles saved successfully");
                    // Reload the cached profiles
                    ExecutorConfigs::reload();
                    ResponseJson(ApiResponse::success(
                        "Executor profiles updated successfully".to_string(),
                    ))
                }
                Err(e) => {
                    tracing::error!("Failed to save executor profiles: {}", e);
                    ResponseJson(ApiResponse::error(&format!(
                        "Failed to save executor profiles: {}",
                        e
                    )))
                }
            }
        }
        Err(e) => ResponseJson(ApiResponse::error(&format!(
            "Invalid executor profiles format: {}",
            e
        ))),
    }
}
</file>

<file path="crates/server/src/routes/containers.rs">
use axum::{
    Router,
    extract::{Query, State},
    response::Json as ResponseJson,
    routing::get,
};
use db::models::task_attempt::TaskAttempt;
use deployment::Deployment;
use serde::{Deserialize, Serialize};
use ts_rs::TS;
use utils::response::ApiResponse;
use uuid::Uuid;

use crate::{DeploymentImpl, error::ApiError};

#[derive(Debug, Serialize, TS)]
pub struct ContainerInfo {
    pub attempt_id: Uuid,
    pub task_id: Uuid,
    pub project_id: Uuid,
}

#[derive(Debug, Deserialize)]
pub struct ContainerQuery {
    #[serde(rename = "ref")]
    pub container_ref: String,
}

pub async fn get_container_info(
    Query(query): Query<ContainerQuery>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<ContainerInfo>>, ApiError> {
    let pool = &deployment.db().pool;

    let (attempt_id, task_id, project_id) =
        TaskAttempt::resolve_container_ref(pool, &query.container_ref)
            .await
            .map_err(|e| match e {
                sqlx::Error::RowNotFound => ApiError::Database(e),
                _ => ApiError::Database(e),
            })?;

    let container_info = ContainerInfo {
        attempt_id,
        task_id,
        project_id,
    };

    Ok(ResponseJson(ApiResponse::success(container_info)))
}

pub fn router(_deployment: &DeploymentImpl) -> Router<DeploymentImpl> {
    Router::new().route("/containers/info", get(get_container_info))
}
</file>

<file path="crates/server/src/routes/events.rs">
use axum::{
    BoxError, Router,
    extract::State,
    response::{
        Sse,
        sse::{Event, KeepAlive},
    },
    routing::get,
};
use deployment::Deployment;
use futures_util::TryStreamExt;

use crate::DeploymentImpl;

pub async fn events(
    State(deployment): State<DeploymentImpl>,
) -> Result<Sse<impl futures_util::Stream<Item = Result<Event, BoxError>>>, axum::http::StatusCode>
{
    // Ask the container service for a combined "history + live" stream
    let stream = deployment.stream_events().await;
    Ok(Sse::new(stream.map_err(|e| -> BoxError { e.into() })).keep_alive(KeepAlive::default()))
}

pub fn router(_: &DeploymentImpl) -> Router<DeploymentImpl> {
    let events_router = Router::new().route("/", get(events));

    Router::new().nest("/events", events_router)
}
</file>

<file path="crates/server/src/routes/execution_processes.rs">
use anyhow;
use axum::{
    Extension, Router,
    extract::{
        Path, Query, State,
        ws::{WebSocket, WebSocketUpgrade},
    },
    middleware::from_fn_with_state,
    response::{IntoResponse, Json as ResponseJson},
    routing::{get, post},
};
use db::models::execution_process::{ExecutionProcess, ExecutionProcessError};
use deployment::Deployment;
use futures_util::{SinkExt, StreamExt, TryStreamExt};
use serde::Deserialize;
use services::services::container::ContainerService;
use utils::{log_msg::LogMsg, response::ApiResponse};
use uuid::Uuid;

use crate::{DeploymentImpl, error::ApiError, middleware::load_execution_process_middleware};

#[derive(Debug, Deserialize)]
pub struct ExecutionProcessQuery {
    pub task_attempt_id: Uuid,
    /// If true, include soft-deleted (dropped) processes in results/stream
    #[serde(default)]
    pub show_soft_deleted: Option<bool>,
}

pub async fn get_execution_processes(
    State(deployment): State<DeploymentImpl>,
    Query(query): Query<ExecutionProcessQuery>,
) -> Result<ResponseJson<ApiResponse<Vec<ExecutionProcess>>>, ApiError> {
    let pool = &deployment.db().pool;
    let execution_processes = ExecutionProcess::find_by_task_attempt_id(
        pool,
        query.task_attempt_id,
        query.show_soft_deleted.unwrap_or(false),
    )
    .await?;

    Ok(ResponseJson(ApiResponse::success(execution_processes)))
}

pub async fn get_execution_process_by_id(
    Extension(execution_process): Extension<ExecutionProcess>,
    State(_deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<ExecutionProcess>>, ApiError> {
    Ok(ResponseJson(ApiResponse::success(execution_process)))
}

pub async fn stream_raw_logs_ws(
    ws: WebSocketUpgrade,
    State(deployment): State<DeploymentImpl>,
    Path(exec_id): Path<Uuid>,
) -> Result<impl IntoResponse, ApiError> {
    // Check if the stream exists before upgrading the WebSocket
    let _stream = deployment
        .container()
        .stream_raw_logs(&exec_id)
        .await
        .ok_or_else(|| {
            ApiError::ExecutionProcess(ExecutionProcessError::ExecutionProcessNotFound)
        })?;

    Ok(ws.on_upgrade(move |socket| async move {
        if let Err(e) = handle_raw_logs_ws(socket, deployment, exec_id).await {
            tracing::warn!("raw logs WS closed: {}", e);
        }
    }))
}

async fn handle_raw_logs_ws(
    socket: WebSocket,
    deployment: DeploymentImpl,
    exec_id: Uuid,
) -> anyhow::Result<()> {
    use std::sync::{
        Arc,
        atomic::{AtomicUsize, Ordering},
    };

    use executors::logs::utils::patch::ConversationPatch;
    use utils::log_msg::LogMsg;

    // Get the raw stream and convert to JSON patches on-the-fly
    let raw_stream = deployment
        .container()
        .stream_raw_logs(&exec_id)
        .await
        .ok_or_else(|| anyhow::anyhow!("Execution process not found"))?;

    let counter = Arc::new(AtomicUsize::new(0));
    let mut stream = raw_stream.map_ok({
        let counter = counter.clone();
        move |m| match m {
            LogMsg::Stdout(content) => {
                let index = counter.fetch_add(1, Ordering::SeqCst);
                let patch = ConversationPatch::add_stdout(index, content);
                LogMsg::JsonPatch(patch).to_ws_message_unchecked()
            }
            LogMsg::Stderr(content) => {
                let index = counter.fetch_add(1, Ordering::SeqCst);
                let patch = ConversationPatch::add_stderr(index, content);
                LogMsg::JsonPatch(patch).to_ws_message_unchecked()
            }
            LogMsg::Finished => LogMsg::Finished.to_ws_message_unchecked(),
            _ => unreachable!("Raw stream should only have Stdout/Stderr/Finished"),
        }
    });

    // Split socket into sender and receiver
    let (mut sender, mut receiver) = socket.split();

    // Drain (and ignore) any client->server messages so pings/pongs work
    tokio::spawn(async move { while let Some(Ok(_)) = receiver.next().await {} });

    // Forward server messages
    while let Some(item) = stream.next().await {
        match item {
            Ok(msg) => {
                if sender.send(msg).await.is_err() {
                    break; // client disconnected
                }
            }
            Err(e) => {
                tracing::error!("stream error: {}", e);
                break;
            }
        }
    }
    Ok(())
}

pub async fn stream_normalized_logs_ws(
    ws: WebSocketUpgrade,
    State(deployment): State<DeploymentImpl>,
    Path(exec_id): Path<Uuid>,
) -> Result<impl IntoResponse, ApiError> {
    let stream = deployment
        .container()
        .stream_normalized_logs(&exec_id)
        .await
        .ok_or_else(|| {
            ApiError::ExecutionProcess(ExecutionProcessError::ExecutionProcessNotFound)
        })?;

    // Convert the error type to anyhow::Error and turn TryStream -> Stream<Result<_, _>>
    let stream = stream.err_into::<anyhow::Error>().into_stream();

    Ok(ws.on_upgrade(move |socket| async move {
        if let Err(e) = handle_normalized_logs_ws(socket, stream).await {
            tracing::warn!("normalized logs WS closed: {}", e);
        }
    }))
}

async fn handle_normalized_logs_ws(
    socket: WebSocket,
    stream: impl futures_util::Stream<Item = anyhow::Result<LogMsg>> + Unpin + Send + 'static,
) -> anyhow::Result<()> {
    let mut stream = stream.map_ok(|msg| msg.to_ws_message_unchecked());
    let (mut sender, mut receiver) = socket.split();
    tokio::spawn(async move { while let Some(Ok(_)) = receiver.next().await {} });
    while let Some(item) = stream.next().await {
        match item {
            Ok(msg) => {
                if sender.send(msg).await.is_err() {
                    break;
                }
            }
            Err(e) => {
                tracing::error!("stream error: {}", e);
                break;
            }
        }
    }
    Ok(())
}

pub async fn stop_execution_process(
    Extension(execution_process): Extension<ExecutionProcess>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    deployment
        .container()
        .stop_execution(&execution_process)
        .await?;

    Ok(ResponseJson(ApiResponse::success(())))
}

pub async fn stream_execution_processes_ws(
    ws: WebSocketUpgrade,
    State(deployment): State<DeploymentImpl>,
    Query(query): Query<ExecutionProcessQuery>,
) -> impl IntoResponse {
    ws.on_upgrade(move |socket| async move {
        if let Err(e) = handle_execution_processes_ws(
            socket,
            deployment,
            query.task_attempt_id,
            query.show_soft_deleted.unwrap_or(false),
        )
        .await
        {
            tracing::warn!("execution processes WS closed: {}", e);
        }
    })
}

async fn handle_execution_processes_ws(
    socket: WebSocket,
    deployment: DeploymentImpl,
    task_attempt_id: uuid::Uuid,
    show_soft_deleted: bool,
) -> anyhow::Result<()> {
    // Get the raw stream and convert LogMsg to WebSocket messages
    let mut stream = deployment
        .events()
        .stream_execution_processes_for_attempt_raw(task_attempt_id, show_soft_deleted)
        .await?
        .map_ok(|msg| msg.to_ws_message_unchecked());

    // Split socket into sender and receiver
    let (mut sender, mut receiver) = socket.split();

    // Drain (and ignore) any client->server messages so pings/pongs work
    tokio::spawn(async move { while let Some(Ok(_)) = receiver.next().await {} });

    // Forward server messages
    while let Some(item) = stream.next().await {
        match item {
            Ok(msg) => {
                if sender.send(msg).await.is_err() {
                    break; // client disconnected
                }
            }
            Err(e) => {
                tracing::error!("stream error: {}", e);
                break;
            }
        }
    }
    Ok(())
}

pub fn router(deployment: &DeploymentImpl) -> Router<DeploymentImpl> {
    let task_attempt_id_router = Router::new()
        .route("/", get(get_execution_process_by_id))
        .route("/stop", post(stop_execution_process))
        .route("/raw-logs/ws", get(stream_raw_logs_ws))
        .route("/normalized-logs/ws", get(stream_normalized_logs_ws))
        .layer(from_fn_with_state(
            deployment.clone(),
            load_execution_process_middleware,
        ));

    let task_attempts_router = Router::new()
        .route("/", get(get_execution_processes))
        .route("/stream/ws", get(stream_execution_processes_ws))
        .nest("/{id}", task_attempt_id_router);

    Router::new().nest("/execution-processes", task_attempts_router)
}
</file>

<file path="crates/server/src/routes/filesystem.rs">
use axum::{
    Router,
    extract::{Query, State},
    response::Json as ResponseJson,
    routing::get,
};
use deployment::Deployment;
use serde::Deserialize;
use services::services::filesystem::{DirectoryEntry, DirectoryListResponse, FilesystemError};
use utils::response::ApiResponse;

use crate::{DeploymentImpl, error::ApiError};

#[derive(Debug, Deserialize)]
pub struct ListDirectoryQuery {
    path: Option<String>,
}

pub async fn list_directory(
    State(deployment): State<DeploymentImpl>,
    Query(query): Query<ListDirectoryQuery>,
) -> Result<ResponseJson<ApiResponse<DirectoryListResponse>>, ApiError> {
    match deployment.filesystem().list_directory(query.path).await {
        Ok(response) => Ok(ResponseJson(ApiResponse::success(response))),
        Err(FilesystemError::DirectoryDoesNotExist) => {
            Ok(ResponseJson(ApiResponse::error("Directory does not exist")))
        }
        Err(FilesystemError::PathIsNotDirectory) => {
            Ok(ResponseJson(ApiResponse::error("Path is not a directory")))
        }
        Err(FilesystemError::Io(e)) => {
            tracing::error!("Failed to read directory: {}", e);
            Ok(ResponseJson(ApiResponse::error(&format!(
                "Failed to read directory: {}",
                e
            ))))
        }
    }
}

pub async fn list_git_repos(
    State(deployment): State<DeploymentImpl>,
    Query(query): Query<ListDirectoryQuery>,
) -> Result<ResponseJson<ApiResponse<Vec<DirectoryEntry>>>, ApiError> {
    let res = if let Some(ref path) = query.path {
        deployment
            .filesystem()
            .list_git_repos(Some(path.clone()), 800, 1200, Some(3))
            .await
    } else {
        deployment
            .filesystem()
            .list_common_git_repos(800, 1200, Some(4))
            .await
    };
    match res {
        Ok(response) => Ok(ResponseJson(ApiResponse::success(response))),
        Err(FilesystemError::DirectoryDoesNotExist) => {
            Ok(ResponseJson(ApiResponse::error("Directory does not exist")))
        }
        Err(FilesystemError::PathIsNotDirectory) => {
            Ok(ResponseJson(ApiResponse::error("Path is not a directory")))
        }
        Err(FilesystemError::Io(e)) => {
            tracing::error!("Failed to read directory: {}", e);
            Ok(ResponseJson(ApiResponse::error(&format!(
                "Failed to read directory: {}",
                e
            ))))
        }
    }
}

pub fn router() -> Router<DeploymentImpl> {
    Router::new()
        .route("/filesystem/directory", get(list_directory))
        .route("/filesystem/git-repos", get(list_git_repos))
}
</file>

<file path="crates/server/src/routes/frontend.rs">
use axum::{
    body::Body,
    http::HeaderValue,
    response::{IntoResponse, Response},
};
use reqwest::{StatusCode, header};
use rust_embed::RustEmbed;

#[derive(RustEmbed)]
#[folder = "../../frontend/dist"]
pub struct Assets;

pub async fn serve_frontend(uri: axum::extract::Path<String>) -> impl IntoResponse {
    let path = uri.trim_start_matches('/');
    serve_file(path).await
}

pub async fn serve_frontend_root() -> impl IntoResponse {
    serve_file("index.html").await
}

async fn serve_file(path: &str) -> impl IntoResponse + use<> {
    let file = Assets::get(path);

    match file {
        Some(content) => {
            let mime = mime_guess::from_path(path).first_or_octet_stream();

            Response::builder()
                .status(StatusCode::OK)
                .header(
                    header::CONTENT_TYPE,
                    HeaderValue::from_str(mime.as_ref()).unwrap(),
                )
                .body(Body::from(content.data.into_owned()))
                .unwrap()
        }
        None => {
            // For SPA routing, serve index.html for unknown routes
            if let Some(index) = Assets::get("index.html") {
                Response::builder()
                    .status(StatusCode::OK)
                    .header(header::CONTENT_TYPE, HeaderValue::from_static("text/html"))
                    .body(Body::from(index.data.into_owned()))
                    .unwrap()
            } else {
                Response::builder()
                    .status(StatusCode::NOT_FOUND)
                    .body(Body::from("404 Not Found"))
                    .unwrap()
            }
        }
    }
}
</file>

<file path="crates/server/src/routes/github.rs">
#![cfg(feature = "cloud")]

use axum::{
    extract::{Query, State},
    http::StatusCode,
    response::Json as ResponseJson,
    routing::{get, post},
    Json, Router,
};
use serde::Deserialize;
use ts_rs::TS;
use uuid::Uuid;

use crate::{
    app_state::AppState,
    models::{
        project::{CreateProject, Project},
        ApiResponse,
    },
    services::{
        git_service::GitService,
        github_service::{GitHubService, RepositoryInfo},
        GitHubServiceError,
    },
};

#[derive(Debug, Deserialize, TS)]
pub struct CreateProjectFromGitHub {
    pub repository_id: i64,
    pub name: String,
    pub clone_url: String,
    pub setup_script: Option<String>,
    pub dev_script: Option<String>,
    pub cleanup_script: Option<String>,
}

#[derive(serde::Deserialize)]
pub struct RepositoryQuery {
    pub page: Option<u8>,
}

/// List GitHub repositories for the authenticated user
pub async fn list_repositories(
    State(app_state): State<AppState>,
    Query(params): Query<RepositoryQuery>,
) -> Result<ResponseJson<ApiResponse<Vec<RepositoryInfo>>>, StatusCode> {
    let page = params.page.unwrap_or(1);

    // Get GitHub configuration
    let github_config = {
        let config = app_state.get_config().read().await;
        config.github.clone()
    };

    // Check if GitHub is configured
    if github_config.token.is_none() {
        return Ok(ResponseJson(ApiResponse::error(
            "GitHub token not configured. Please authenticate with GitHub first.",
        )));
    }

    // Create GitHub service with token
    let github_token = github_config.token.as_deref().unwrap();
    let github_service = match GitHubService::new(github_token) {
        Ok(service) => service,
        Err(e) => {
            tracing::error!("Failed to create GitHub service: {}", e);
            return Err(StatusCode::INTERNAL_SERVER_ERROR);
        }
    };

    // List repositories
    match github_service.list_repositories(page).await {
        Ok(repositories) => {
            tracing::info!(
                "Retrieved {} repositories from GitHub (page {})",
                repositories.len(),
                page
            );
            Ok(ResponseJson(ApiResponse::success(repositories)))
        }
        Err(GitHubServiceError::TokenInvalid) => Ok(ResponseJson(ApiResponse::error(
            "GitHub token is invalid or expired. Please re-authenticate with GitHub.",
        ))),
        Err(e) => {
            tracing::error!("Failed to list GitHub repositories: {}", e);
            Ok(ResponseJson(ApiResponse::error(&format!(
                "Failed to retrieve repositories: {}",
                e
            ))))
        }
    }
}

/// Create a project from a GitHub repository
pub async fn create_project_from_github(
    State(app_state): State<AppState>,
    Json(payload): Json<CreateProjectFromGitHub>,
) -> Result<ResponseJson<ApiResponse<Project>>, StatusCode> {
    tracing::debug!("Creating project '{}' from GitHub repository", payload.name);

    // Get workspace path
    let workspace_path = match app_state.get_workspace_path().await {
        Ok(path) => path,
        Err(e) => {
            tracing::error!("Failed to get workspace path: {}", e);
            return Err(StatusCode::INTERNAL_SERVER_ERROR);
        }
    };

    let target_path = workspace_path.join(&payload.name);

    // Check if project directory already exists
    if target_path.exists() {
        return Ok(ResponseJson(ApiResponse::error(
            "A project with this name already exists in the workspace",
        )));
    }

    // Check if git repo path is already used by another project
    match Project::find_by_git_repo_path(&app_state.db_pool, &target_path.to_string_lossy()).await {
        Ok(Some(_)) => {
            return Ok(ResponseJson(ApiResponse::error(
                "A project with this git repository path already exists",
            )));
        }
        Ok(None) => {
            // Path is available, continue
        }
        Err(e) => {
            tracing::error!("Failed to check for existing git repo path: {}", e);
            return Err(StatusCode::INTERNAL_SERVER_ERROR);
        }
    }

    // Get GitHub token
    let github_token = {
        let config = app_state.get_config().read().await;
        config.github.token.clone()
    };

    // Clone the repository
    match GitService::clone_repository(&payload.clone_url, &target_path, github_token.as_deref()) {
        Ok(_) => {
            tracing::info!(
                "Successfully cloned repository {} to {}",
                payload.clone_url,
                target_path.display()
            );
        }
        Err(e) => {
            tracing::error!("Failed to clone repository: {}", e);
            return Ok(ResponseJson(ApiResponse::error(&format!(
                "Failed to clone repository: {}",
                e
            ))));
        }
    }

    // Create project record in database
    let has_setup_script = payload.setup_script.is_some();
    let has_dev_script = payload.dev_script.is_some();
    let project_data = CreateProject {
        name: payload.name.clone(),
        git_repo_path: target_path.to_string_lossy().to_string(),
        use_existing_repo: true, // Since we just cloned it
        setup_script: payload.setup_script,
        dev_script: payload.dev_script,
        cleanup_script: payload.cleanup_script,
    };

    let project_id = Uuid::new_v4();
    match Project::create(&app_state.db_pool, &project_data, project_id).await {
        Ok(project) => {
            // Track project creation event
            app_state
                .track_analytics_event(
                    "project_created_from_github",
                    Some(serde_json::json!({
                        "project_id": project.id.to_string(),
                        "repository_id": payload.repository_id,
                        "clone_url": payload.clone_url,
                        "has_setup_script": has_setup_script,
                        "has_dev_script": has_dev_script,
                    })),
                )
                .await;

            Ok(ResponseJson(ApiResponse::success(project)))
        }
        Err(e) => {
            tracing::error!("Failed to create project: {}", e);

            // Clean up cloned repository if project creation failed
            if target_path.exists() {
                if let Err(cleanup_err) = std::fs::remove_dir_all(&target_path) {
                    tracing::error!("Failed to cleanup cloned repository: {}", cleanup_err);
                }
            }

            Err(StatusCode::INTERNAL_SERVER_ERROR)
        }
    }
}

/// Create router for GitHub-related endpoints (only registered in cloud mode)
pub fn github_router() -> Router<AppState> {
    Router::new()
        .route("/github/repositories", get(list_repositories))
        .route("/projects/from-github", post(create_project_from_github))
}
</file>

<file path="crates/server/src/routes/health.rs">
use axum::response::Json;
use utils::response::ApiResponse;

pub async fn health_check() -> Json<ApiResponse<String>> {
    Json(ApiResponse::success("OK".to_string()))
}
</file>

<file path="crates/server/src/routes/images.rs">
use axum::{
    Router,
    body::Body,
    extract::{DefaultBodyLimit, Multipart, Path, State},
    http::{StatusCode, header},
    response::{Json as ResponseJson, Response},
    routing::{delete, get, post},
};
use chrono::{DateTime, Utc};
use db::models::image::Image;
use deployment::Deployment;
use serde::{Deserialize, Serialize};
use services::services::image::ImageError;
use tokio::fs::File;
use tokio_util::io::ReaderStream;
use ts_rs::TS;
use utils::response::ApiResponse;
use uuid::Uuid;

use crate::{DeploymentImpl, error::ApiError};

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct ImageResponse {
    pub id: Uuid,
    pub file_path: String, // relative path to display in markdown
    pub original_name: String,
    pub mime_type: Option<String>,
    pub size_bytes: i64,
    pub hash: String,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

impl ImageResponse {
    pub fn from_image(image: Image) -> Self {
        // special relative path for images
        let markdown_path = format!("{}/{}", utils::path::VIBE_IMAGES_DIR, image.file_path);
        Self {
            id: image.id,
            file_path: markdown_path,
            original_name: image.original_name,
            mime_type: image.mime_type,
            size_bytes: image.size_bytes,
            hash: image.hash,
            created_at: image.created_at,
            updated_at: image.updated_at,
        }
    }
}

pub async fn upload_image(
    State(deployment): State<DeploymentImpl>,
    mut multipart: Multipart,
) -> Result<ResponseJson<ApiResponse<ImageResponse>>, ApiError> {
    let image_service = deployment.image();
    while let Some(field) = multipart.next_field().await? {
        if field.name() == Some("image") {
            let filename = field
                .file_name()
                .map(|s| s.to_string())
                .unwrap_or_else(|| "image.png".to_string());

            let data = field.bytes().await?;
            let image = image_service.store_image(&data, &filename).await?;

            deployment
                .track_if_analytics_allowed(
                    "image_uploaded",
                    serde_json::json!({
                        "image_id": image.id.to_string(),
                        "size_bytes": image.size_bytes,
                        "mime_type": image.mime_type,
                    }),
                )
                .await;

            let image_response = ImageResponse::from_image(image);
            return Ok(ResponseJson(ApiResponse::success(image_response)));
        }
    }

    Err(ApiError::Image(ImageError::NotFound))
}

/// Serve an image file by ID
pub async fn serve_image(
    Path(image_id): Path<Uuid>,
    State(deployment): State<DeploymentImpl>,
) -> Result<Response, ApiError> {
    let image_service = deployment.image();
    let image = image_service
        .get_image(image_id)
        .await?
        .ok_or_else(|| ApiError::Image(ImageError::NotFound))?;
    let file_path = image_service.get_absolute_path(&image);

    let file = File::open(&file_path).await?;
    let metadata = file.metadata().await?;

    let stream = ReaderStream::new(file);
    let body = Body::from_stream(stream);

    let content_type = image
        .mime_type
        .as_deref()
        .unwrap_or("application/octet-stream");

    let response = Response::builder()
        .status(StatusCode::OK)
        .header(header::CONTENT_TYPE, content_type)
        .header(header::CONTENT_LENGTH, metadata.len())
        .header(header::CACHE_CONTROL, "public, max-age=31536000") // Cache for 1 year
        .body(body)
        .map_err(|e| ApiError::Image(ImageError::ResponseBuildError(e.to_string())))?;

    Ok(response)
}

pub async fn delete_image(
    Path(image_id): Path<Uuid>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    let image_service = deployment.image();
    image_service.delete_image(image_id).await?;
    Ok(ResponseJson(ApiResponse::success(())))
}

pub async fn get_task_images(
    Path(task_id): Path<Uuid>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<Vec<ImageResponse>>>, ApiError> {
    let images = Image::find_by_task_id(&deployment.db().pool, task_id).await?;
    let image_responses = images.into_iter().map(ImageResponse::from_image).collect();
    Ok(ResponseJson(ApiResponse::success(image_responses)))
}

pub fn routes() -> Router<DeploymentImpl> {
    Router::new()
        .route(
            "/upload",
            post(upload_image).layer(DefaultBodyLimit::max(20 * 1024 * 1024)), // 20MB limit
        )
        .route("/{id}/file", get(serve_image))
        .route("/{id}", delete(delete_image))
        .route("/task/{task_id}", get(get_task_images))
}
</file>

<file path="crates/server/src/routes/mod.rs">
use axum::{
    Router,
    routing::{IntoMakeService, get},
};

use crate::DeploymentImpl;

pub mod auth;
pub mod config;
pub mod containers;
pub mod filesystem;
// pub mod github;
pub mod events;
pub mod execution_processes;
pub mod frontend;
pub mod health;
pub mod images;
pub mod projects;
pub mod task_attempts;
pub mod task_templates;
pub mod tasks;

pub fn router(deployment: DeploymentImpl) -> IntoMakeService<Router> {
    // Create routers with different middleware layers
    let base_routes = Router::new()
        .route("/health", get(health::health_check))
        .merge(config::router())
        .merge(containers::router(&deployment))
        .merge(projects::router(&deployment))
        .merge(tasks::router(&deployment))
        .merge(task_attempts::router(&deployment))
        .merge(execution_processes::router(&deployment))
        .merge(task_templates::router(&deployment))
        .merge(auth::router(&deployment))
        .merge(filesystem::router())
        .merge(events::router(&deployment))
        .nest("/images", images::routes())
        .with_state(deployment);

    Router::new()
        .route("/", get(frontend::serve_frontend_root))
        .route("/{*path}", get(frontend::serve_frontend))
        .nest("/api", base_routes)
        .into_make_service()
}
</file>

<file path="crates/server/src/routes/projects.rs">
use std::path::Path;

use axum::{
    Extension, Json, Router,
    extract::{Query, State},
    http::StatusCode,
    middleware::from_fn_with_state,
    response::Json as ResponseJson,
    routing::{get, post},
};
use db::models::project::{
    CreateProject, Project, ProjectError, SearchMatchType, SearchResult, UpdateProject,
};
use deployment::Deployment;
use ignore::WalkBuilder;
use services::services::{
    file_ranker::FileRanker,
    file_search_cache::{CacheError, SearchMode, SearchQuery},
    git::GitBranch,
};
use utils::{path::expand_tilde, response::ApiResponse};
use uuid::Uuid;

use crate::{DeploymentImpl, error::ApiError, middleware::load_project_middleware};

pub async fn get_projects(
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<Vec<Project>>>, ApiError> {
    let projects = Project::find_all(&deployment.db().pool).await?;
    Ok(ResponseJson(ApiResponse::success(projects)))
}

pub async fn get_project(
    Extension(project): Extension<Project>,
) -> Result<ResponseJson<ApiResponse<Project>>, ApiError> {
    Ok(ResponseJson(ApiResponse::success(project)))
}

pub async fn get_project_branches(
    Extension(project): Extension<Project>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<Vec<GitBranch>>>, ApiError> {
    let branches = deployment.git().get_all_branches(&project.git_repo_path)?;
    Ok(ResponseJson(ApiResponse::success(branches)))
}

pub async fn create_project(
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<CreateProject>,
) -> Result<ResponseJson<ApiResponse<Project>>, ApiError> {
    let id = Uuid::new_v4();
    let CreateProject {
        name,
        git_repo_path,
        setup_script,
        dev_script,
        cleanup_script,
        copy_files,
        use_existing_repo,
    } = payload;
    tracing::debug!("Creating project '{}'", name);

    // Validate and setup git repository
    // Expand tilde in git repo path if present
    let path = expand_tilde(&git_repo_path);
    // Check if git repo path is already used by another project
    match Project::find_by_git_repo_path(&deployment.db().pool, path.to_string_lossy().as_ref())
        .await
    {
        Ok(Some(_)) => {
            return Ok(ResponseJson(ApiResponse::error(
                "A project with this git repository path already exists",
            )));
        }
        Ok(None) => {
            // Path is available, continue
        }
        Err(e) => {
            return Err(ProjectError::GitRepoCheckFailed(e.to_string()).into());
        }
    }

    if use_existing_repo {
        // For existing repos, validate that the path exists and is a git repository
        if !path.exists() {
            return Ok(ResponseJson(ApiResponse::error(
                "The specified path does not exist",
            )));
        }

        if !path.is_dir() {
            return Ok(ResponseJson(ApiResponse::error(
                "The specified path is not a directory",
            )));
        }

        if !path.join(".git").exists() {
            return Ok(ResponseJson(ApiResponse::error(
                "The specified directory is not a git repository",
            )));
        }

        // Ensure existing repo has a main branch if it's empty
        if let Err(e) = deployment.git().ensure_main_branch_exists(&path) {
            tracing::error!("Failed to ensure main branch exists: {}", e);
            return Ok(ResponseJson(ApiResponse::error(&format!(
                "Failed to ensure main branch exists: {}",
                e
            ))));
        }
    } else {
        // For new repos, create directory and initialize git

        // Create directory if it doesn't exist
        if !path.exists()
            && let Err(e) = std::fs::create_dir_all(&path)
        {
            tracing::error!("Failed to create directory: {}", e);
            return Ok(ResponseJson(ApiResponse::error(&format!(
                "Failed to create directory: {}",
                e
            ))));
        }

        // Check if it's already a git repo, if not initialize it
        if !path.join(".git").exists()
            && let Err(e) = deployment.git().initialize_repo_with_main_branch(&path)
        {
            tracing::error!("Failed to initialize git repository: {}", e);
            return Ok(ResponseJson(ApiResponse::error(&format!(
                "Failed to initialize git repository: {}",
                e
            ))));
        }
    }

    match Project::create(
        &deployment.db().pool,
        &CreateProject {
            name,
            git_repo_path: path.to_string_lossy().to_string(),
            use_existing_repo,
            setup_script,
            dev_script,
            cleanup_script,
            copy_files,
        },
        id,
    )
    .await
    {
        Ok(project) => {
            // Track project creation event
            deployment
                .track_if_analytics_allowed(
                    "project_created",
                    serde_json::json!({
                        "project_id": project.id.to_string(),
                        "use_existing_repo": use_existing_repo,
                        "has_setup_script": project.setup_script.is_some(),
                        "has_dev_script": project.dev_script.is_some(),
                    }),
                )
                .await;

            Ok(ResponseJson(ApiResponse::success(project)))
        }
        Err(e) => Err(ProjectError::CreateFailed(e.to_string()).into()),
    }
}

pub async fn update_project(
    Extension(existing_project): Extension<Project>,
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<UpdateProject>,
) -> Result<ResponseJson<ApiResponse<Project>>, StatusCode> {
    // Destructure payload to handle field updates.
    // This allows us to treat `None` from the payload as an explicit `null` to clear a field,
    // as the frontend currently sends all fields on update.
    let UpdateProject {
        name,
        git_repo_path,
        setup_script,
        dev_script,
        cleanup_script,
        copy_files,
    } = payload;
    // If git_repo_path is being changed, check if the new path is already used by another project
    let git_repo_path = if let Some(new_git_repo_path) = git_repo_path.map(|s| expand_tilde(&s))
        && new_git_repo_path != existing_project.git_repo_path
    {
        match Project::find_by_git_repo_path_excluding_id(
            &deployment.db().pool,
            new_git_repo_path.to_string_lossy().as_ref(),
            existing_project.id,
        )
        .await
        {
            Ok(Some(_)) => {
                return Ok(ResponseJson(ApiResponse::error(
                    "A project with this git repository path already exists",
                )));
            }
            Ok(None) => new_git_repo_path,
            Err(e) => {
                tracing::error!("Failed to check for existing git repo path: {}", e);
                return Err(StatusCode::INTERNAL_SERVER_ERROR);
            }
        }
    } else {
        existing_project.git_repo_path
    };

    match Project::update(
        &deployment.db().pool,
        existing_project.id,
        name.unwrap_or(existing_project.name),
        git_repo_path.to_string_lossy().to_string(),
        setup_script,
        dev_script,
        cleanup_script,
        copy_files,
    )
    .await
    {
        Ok(project) => Ok(ResponseJson(ApiResponse::success(project))),
        Err(e) => {
            tracing::error!("Failed to update project: {}", e);
            Err(StatusCode::INTERNAL_SERVER_ERROR)
        }
    }
}

pub async fn delete_project(
    Extension(project): Extension<Project>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<()>>, StatusCode> {
    match Project::delete(&deployment.db().pool, project.id).await {
        Ok(rows_affected) => {
            if rows_affected == 0 {
                Err(StatusCode::NOT_FOUND)
            } else {
                Ok(ResponseJson(ApiResponse::success(())))
            }
        }
        Err(e) => {
            tracing::error!("Failed to delete project: {}", e);
            Err(StatusCode::INTERNAL_SERVER_ERROR)
        }
    }
}

#[derive(serde::Deserialize)]
pub struct OpenEditorRequest {
    editor_type: Option<String>,
}

pub async fn open_project_in_editor(
    Extension(project): Extension<Project>,
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<Option<OpenEditorRequest>>,
) -> Result<ResponseJson<ApiResponse<()>>, StatusCode> {
    let path = project.git_repo_path.to_string_lossy();

    let editor_config = {
        let config = deployment.config().read().await;
        let editor_type_str = payload.as_ref().and_then(|req| req.editor_type.as_deref());
        config.editor.with_override(editor_type_str)
    };

    match editor_config.open_file(&path) {
        Ok(_) => {
            tracing::info!("Opened editor for project {} at path: {}", project.id, path);
            Ok(ResponseJson(ApiResponse::success(())))
        }
        Err(e) => {
            tracing::error!("Failed to open editor for project {}: {}", project.id, e);
            Err(StatusCode::INTERNAL_SERVER_ERROR)
        }
    }
}

pub async fn search_project_files(
    State(deployment): State<DeploymentImpl>,
    Extension(project): Extension<Project>,
    Query(search_query): Query<SearchQuery>,
) -> Result<ResponseJson<ApiResponse<Vec<SearchResult>>>, StatusCode> {
    let query = search_query.q.trim();
    let mode = search_query.mode;

    if query.is_empty() {
        return Ok(ResponseJson(ApiResponse::error(
            "Query parameter 'q' is required and cannot be empty",
        )));
    }

    let repo_path = &project.git_repo_path;
    let file_search_cache = deployment.file_search_cache();

    // Try cache first
    match file_search_cache
        .search(repo_path, query, mode.clone())
        .await
    {
        Ok(results) => {
            tracing::debug!(
                "Cache hit for repo {:?}, query: {}, mode: {:?}",
                repo_path,
                query,
                mode
            );
            Ok(ResponseJson(ApiResponse::success(results)))
        }
        Err(CacheError::Miss) => {
            // Cache miss - fall back to filesystem search
            tracing::debug!(
                "Cache miss for repo {:?}, query: {}, mode: {:?}",
                repo_path,
                query,
                mode
            );
            match search_files_in_repo(&project.git_repo_path.to_string_lossy(), query, mode).await
            {
                Ok(results) => Ok(ResponseJson(ApiResponse::success(results))),
                Err(e) => {
                    tracing::error!("Failed to search files: {}", e);
                    Err(StatusCode::INTERNAL_SERVER_ERROR)
                }
            }
        }
        Err(CacheError::BuildError(e)) => {
            tracing::error!("Cache build error for repo {:?}: {}", repo_path, e);
            // Fall back to filesystem search
            match search_files_in_repo(&project.git_repo_path.to_string_lossy(), query, mode).await
            {
                Ok(results) => Ok(ResponseJson(ApiResponse::success(results))),
                Err(e) => {
                    tracing::error!("Failed to search files: {}", e);
                    Err(StatusCode::INTERNAL_SERVER_ERROR)
                }
            }
        }
    }
}

async fn search_files_in_repo(
    repo_path: &str,
    query: &str,
    mode: SearchMode,
) -> Result<Vec<SearchResult>, Box<dyn std::error::Error + Send + Sync>> {
    let repo_path = Path::new(repo_path);

    if !repo_path.exists() {
        return Err("Repository path does not exist".into());
    }

    let mut results = Vec::new();
    let query_lower = query.to_lowercase();

    // Configure walker based on mode
    let walker = match mode {
        SearchMode::Settings => {
            // Settings mode: Include ignored files but exclude performance killers
            WalkBuilder::new(repo_path)
                .git_ignore(false) // Include ignored files like .env
                .git_global(false)
                .git_exclude(false)
                .hidden(false)
                .filter_entry(|entry| {
                    let name = entry.file_name().to_string_lossy();
                    // Always exclude .git directories and performance killers
                    name != ".git"
                        && name != "node_modules"
                        && name != "target"
                        && name != "dist"
                        && name != "build"
                })
                .build()
        }
        SearchMode::TaskForm => {
            // Task form mode: Respect gitignore (cleaner results)
            WalkBuilder::new(repo_path)
                .git_ignore(true) // Respect .gitignore
                .git_global(true) // Respect global .gitignore
                .git_exclude(true) // Respect .git/info/exclude
                .hidden(false) // Still show hidden files like .env (if not gitignored)
                .filter_entry(|entry| {
                    let name = entry.file_name().to_string_lossy();
                    name != ".git"
                })
                .build()
        }
    };

    for result in walker {
        let entry = result?;
        let path = entry.path();

        // Skip the root directory itself
        if path == repo_path {
            continue;
        }

        let relative_path = path.strip_prefix(repo_path)?;
        let relative_path_str = relative_path.to_string_lossy().to_lowercase();

        let file_name = path
            .file_name()
            .map(|name| name.to_string_lossy().to_lowercase())
            .unwrap_or_default();

        // Check for matches
        if file_name.contains(&query_lower) {
            results.push(SearchResult {
                path: relative_path.to_string_lossy().to_string(),
                is_file: path.is_file(),
                match_type: SearchMatchType::FileName,
            });
        } else if relative_path_str.contains(&query_lower) {
            // Check if it's a directory name match or full path match
            let match_type = if path
                .parent()
                .and_then(|p| p.file_name())
                .map(|name| name.to_string_lossy().to_lowercase())
                .unwrap_or_default()
                .contains(&query_lower)
            {
                SearchMatchType::DirectoryName
            } else {
                SearchMatchType::FullPath
            };

            results.push(SearchResult {
                path: relative_path.to_string_lossy().to_string(),
                is_file: path.is_file(),
                match_type,
            });
        }
    }

    // Apply git history-based ranking
    let file_ranker = FileRanker::new();
    match file_ranker.get_stats(repo_path).await {
        Ok(stats) => {
            // Re-rank results using git history
            file_ranker.rerank(&mut results, &stats);
        }
        Err(e) => {
            tracing::warn!(
                "Failed to get git stats for ranking, using basic sort: {}",
                e
            );
            // Fallback to basic priority sorting
            results.sort_by(|a, b| {
                let priority = |match_type: &SearchMatchType| match match_type {
                    SearchMatchType::FileName => 0,
                    SearchMatchType::DirectoryName => 1,
                    SearchMatchType::FullPath => 2,
                };

                priority(&a.match_type)
                    .cmp(&priority(&b.match_type))
                    .then_with(|| a.path.cmp(&b.path))
            });
        }
    }

    // Limit to top 10 results
    results.truncate(10);

    Ok(results)
}

pub fn router(deployment: &DeploymentImpl) -> Router<DeploymentImpl> {
    let project_id_router = Router::new()
        .route(
            "/",
            get(get_project).put(update_project).delete(delete_project),
        )
        .route("/branches", get(get_project_branches))
        .route("/search", get(search_project_files))
        .route("/open-editor", post(open_project_in_editor))
        .layer(from_fn_with_state(
            deployment.clone(),
            load_project_middleware,
        ));

    let projects_router = Router::new()
        .route("/", get(get_projects).post(create_project))
        .nest("/{id}", project_id_router);

    Router::new().nest("/projects", projects_router)
}
</file>

<file path="crates/server/src/routes/task_attempts.rs">
use std::path::PathBuf;

use axum::{
    BoxError, Extension, Json, Router,
    extract::{
        Query, State,
        ws::{WebSocket, WebSocketUpgrade},
    },
    http::StatusCode,
    middleware::from_fn_with_state,
    response::{
        IntoResponse, Json as ResponseJson, Sse,
        sse::{Event, KeepAlive},
    },
    routing::{get, post},
};
use db::models::{
    execution_process::{ExecutionProcess, ExecutionProcessRunReason},
    follow_up_draft::FollowUpDraft,
    image::TaskImage,
    merge::{Merge, MergeStatus, PrMerge, PullRequestInfo},
    project::{Project, ProjectError},
    task::{Task, TaskRelationships, TaskStatus},
    task_attempt::{CreateTaskAttempt, TaskAttempt, TaskAttemptError},
};
use deployment::Deployment;
use executors::{
    actions::{
        ExecutorAction, ExecutorActionType,
        coding_agent_follow_up::CodingAgentFollowUpRequest,
        script::{ScriptContext, ScriptRequest, ScriptRequestLanguage},
    },
    profile::ExecutorProfileId,
};
use futures_util::TryStreamExt;
use git2::BranchType;
use serde::{Deserialize, Serialize};
use services::services::{
    container::ContainerService,
    git::ConflictOp,
    github_service::{CreatePrRequest, GitHubService, GitHubServiceError},
    image::ImageService,
};
use sqlx::Error as SqlxError;
use ts_rs::TS;
use utils::response::ApiResponse;
use uuid::Uuid;

use crate::{DeploymentImpl, error::ApiError, middleware::load_task_attempt_middleware};

#[derive(Debug, Deserialize, Serialize, TS)]
pub struct RebaseTaskAttemptRequest {
    pub new_base_branch: Option<String>,
}

#[derive(Debug, Serialize, Deserialize, TS)]
#[serde(tag = "type", rename_all = "snake_case")]
#[ts(tag = "type", rename_all = "snake_case")]
pub enum GitOperationError {
    MergeConflicts { message: String, op: ConflictOp },
    RebaseInProgress,
}

#[derive(Debug, Deserialize, Serialize, TS)]
pub struct ReplaceProcessRequest {
    /// Process to replace (delete this and later ones)
    pub process_id: Uuid,
    /// New prompt to use for the replacement follow-up
    pub prompt: String,
    /// Optional variant override
    pub variant: Option<String>,
    /// If true, allow resetting Git even when uncommitted changes exist
    pub force_when_dirty: Option<bool>,
    /// If false, skip performing the Git reset step (history drop still applies)
    pub perform_git_reset: Option<bool>,
}

#[derive(Debug, Serialize, TS)]
pub struct ReplaceProcessResult {
    pub deleted_count: i64,
    pub git_reset_needed: bool,
    pub git_reset_applied: bool,
    pub target_before_oid: Option<String>,
    pub new_execution_id: Option<Uuid>,
}

#[derive(Debug, Deserialize, Serialize, TS)]
pub struct CreateGitHubPrRequest {
    pub title: String,
    pub body: Option<String>,
    pub base_branch: Option<String>,
}

#[derive(Debug, Serialize)]
pub struct FollowUpResponse {
    pub message: String,
    pub actual_attempt_id: Uuid,
    pub created_new_attempt: bool,
}

#[derive(Debug, Deserialize)]
pub struct TaskAttemptQuery {
    pub task_id: Option<Uuid>,
}

pub async fn get_task_attempts(
    State(deployment): State<DeploymentImpl>,
    Query(query): Query<TaskAttemptQuery>,
) -> Result<ResponseJson<ApiResponse<Vec<TaskAttempt>>>, ApiError> {
    let pool = &deployment.db().pool;
    let attempts = TaskAttempt::fetch_all(pool, query.task_id).await?;
    Ok(ResponseJson(ApiResponse::success(attempts)))
}

pub async fn get_task_attempt(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(_deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<TaskAttempt>>, ApiError> {
    Ok(ResponseJson(ApiResponse::success(task_attempt)))
}

#[derive(Debug, Deserialize, ts_rs::TS)]
pub struct CreateTaskAttemptBody {
    pub task_id: Uuid,
    /// Executor profile specification
    pub executor_profile_id: ExecutorProfileId,
    pub base_branch: String,
}

impl CreateTaskAttemptBody {
    /// Get the executor profile ID
    pub fn get_executor_profile_id(&self) -> ExecutorProfileId {
        self.executor_profile_id.clone()
    }
}

#[axum::debug_handler]
pub async fn create_task_attempt(
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<CreateTaskAttemptBody>,
) -> Result<ResponseJson<ApiResponse<TaskAttempt>>, ApiError> {
    let executor_profile_id = payload.get_executor_profile_id();

    let task_attempt = TaskAttempt::create(
        &deployment.db().pool,
        &CreateTaskAttempt {
            executor: executor_profile_id.executor,
            base_branch: payload.base_branch.clone(),
        },
        payload.task_id,
    )
    .await?;

    let execution_process = deployment
        .container()
        .start_attempt(&task_attempt, executor_profile_id.clone())
        .await?;

    deployment
        .track_if_analytics_allowed(
            "task_attempt_started",
            serde_json::json!({
                "task_id": task_attempt.task_id.to_string(),
                "variant": &executor_profile_id.variant,
                "executor": &executor_profile_id.executor,
                "attempt_id": task_attempt.id.to_string(),
            }),
        )
        .await;

    tracing::info!("Started execution process {}", execution_process.id);

    Ok(ResponseJson(ApiResponse::success(task_attempt)))
}

#[derive(Debug, Deserialize, TS)]
pub struct CreateFollowUpAttempt {
    pub prompt: String,
    pub variant: Option<String>,
    pub image_ids: Option<Vec<Uuid>>,
}

pub async fn follow_up(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<CreateFollowUpAttempt>,
) -> Result<ResponseJson<ApiResponse<ExecutionProcess>>, ApiError> {
    tracing::info!("{:?}", task_attempt);

    // Ensure worktree exists (recreate if needed for cold task support)
    deployment
        .container()
        .ensure_container_exists(&task_attempt)
        .await?;

    // Get latest session id (ignoring dropped)
    let session_id = ExecutionProcess::find_latest_session_id_by_task_attempt(
        &deployment.db().pool,
        task_attempt.id,
    )
    .await?
    .ok_or(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
        "Couldn't find a prior session_id, please create a new task attempt".to_string(),
    )))?;

    // Get ExecutionProcess for profile data
    let latest_execution_process = ExecutionProcess::find_latest_by_task_attempt_and_run_reason(
        &deployment.db().pool,
        task_attempt.id,
        &ExecutionProcessRunReason::CodingAgent,
    )
    .await?
    .ok_or(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
        "Couldn't find initial coding agent process, has it run yet?".to_string(),
    )))?;
    let initial_executor_profile_id = match &latest_execution_process
        .executor_action()
        .map_err(|e| ApiError::TaskAttempt(TaskAttemptError::ValidationError(e.to_string())))?
        .typ
    {
        ExecutorActionType::CodingAgentInitialRequest(request) => {
            Ok(request.executor_profile_id.clone())
        }
        ExecutorActionType::CodingAgentFollowUpRequest(request) => {
            Ok(request.executor_profile_id.clone())
        }
        _ => Err(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
            "Couldn't find profile from initial request".to_string(),
        ))),
    }?;

    let executor_profile_id = ExecutorProfileId {
        executor: initial_executor_profile_id.executor,
        variant: payload.variant,
    };

    // Get parent task
    let task = task_attempt
        .parent_task(&deployment.db().pool)
        .await?
        .ok_or(SqlxError::RowNotFound)?;

    // Get parent project
    let project = task
        .parent_project(&deployment.db().pool)
        .await?
        .ok_or(SqlxError::RowNotFound)?;

    let mut prompt = payload.prompt;
    if let Some(image_ids) = &payload.image_ids {
        TaskImage::associate_many_dedup(&deployment.db().pool, task.id, image_ids).await?;

        // Copy new images from the image cache to the worktree
        if let Some(container_ref) = &task_attempt.container_ref {
            let worktree_path = std::path::PathBuf::from(container_ref);
            deployment
                .image()
                .copy_images_by_ids_to_worktree(&worktree_path, image_ids)
                .await?;

            // Update image paths in prompt with full worktree path
            prompt = ImageService::canonicalise_image_paths(&prompt, &worktree_path);
        }
    }

    let cleanup_action = project.cleanup_script.map(|script| {
        Box::new(ExecutorAction::new(
            ExecutorActionType::ScriptRequest(ScriptRequest {
                script,
                language: ScriptRequestLanguage::Bash,
                context: ScriptContext::CleanupScript,
            }),
            None,
        ))
    });

    let follow_up_request = CodingAgentFollowUpRequest {
        prompt,
        session_id,
        executor_profile_id,
    };

    let follow_up_action = ExecutorAction::new(
        ExecutorActionType::CodingAgentFollowUpRequest(follow_up_request),
        cleanup_action,
    );

    let execution_process = deployment
        .container()
        .start_execution(
            &task_attempt,
            &follow_up_action,
            &ExecutionProcessRunReason::CodingAgent,
        )
        .await?;

    // Clear any persisted follow-up draft for this attempt to avoid stale UI after manual send
    let _ = FollowUpDraft::clear_after_send(&deployment.db().pool, task_attempt.id).await;

    Ok(ResponseJson(ApiResponse::success(execution_process)))
}

// Follow-up draft APIs and queueing
#[derive(Debug, Serialize, TS)]
pub struct FollowUpDraftResponse {
    pub task_attempt_id: Uuid,
    pub prompt: String,
    pub queued: bool,
    pub variant: Option<String>,
    pub image_ids: Option<Vec<Uuid>>, // attachments
    pub version: i64,
}

#[derive(Debug, Deserialize, TS)]
pub struct UpdateFollowUpDraftRequest {
    pub prompt: Option<String>,
    // Present with null explicitly clears variant; absent leaves unchanged
    pub variant: Option<Option<String>>,
    pub image_ids: Option<Vec<Uuid>>, // send empty array to clear; omit to leave unchanged
    pub version: Option<i64>,         // optimistic concurrency
}

#[derive(Debug, Deserialize, TS)]
pub struct SetQueueRequest {
    pub queued: bool,
    pub expected_queued: Option<bool>,
    pub expected_version: Option<i64>,
}

async fn has_running_processes_for_attempt(
    pool: &sqlx::SqlitePool,
    attempt_id: Uuid,
) -> Result<bool, ApiError> {
    let processes = ExecutionProcess::find_by_task_attempt_id(pool, attempt_id, false).await?;
    Ok(processes.into_iter().any(|p| {
        matches!(
            p.status,
            db::models::execution_process::ExecutionProcessStatus::Running
        )
    }))
}

#[axum::debug_handler]
pub async fn get_follow_up_draft(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<FollowUpDraftResponse>>, ApiError> {
    let pool = &deployment.db().pool;
    let draft = FollowUpDraft::find_by_task_attempt_id(pool, task_attempt.id)
        .await?
        .map(|d| FollowUpDraftResponse {
            task_attempt_id: d.task_attempt_id,
            prompt: d.prompt,
            queued: d.queued,
            variant: d.variant,
            image_ids: d.image_ids,
            version: d.version,
        })
        .unwrap_or(FollowUpDraftResponse {
            task_attempt_id: task_attempt.id,
            prompt: "".to_string(),
            queued: false,
            variant: None,
            image_ids: None,
            version: 0,
        });
    Ok(ResponseJson(ApiResponse::success(draft)))
}

#[axum::debug_handler]
pub async fn save_follow_up_draft(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<UpdateFollowUpDraftRequest>,
) -> Result<ResponseJson<ApiResponse<FollowUpDraftResponse>>, ApiError> {
    let pool = &deployment.db().pool;

    // Enforce: cannot edit while queued
    let d = match FollowUpDraft::find_by_task_attempt_id(pool, task_attempt.id).await? {
        Some(d) => d,
        None => {
            // Create empty draft implicitly
            let id = uuid::Uuid::new_v4();
            sqlx::query(
                r#"INSERT INTO follow_up_drafts (id, task_attempt_id, prompt, queued, sending)
                   VALUES (?, ?, '', 0, 0)"#,
            )
            .bind(id)
            .bind(task_attempt.id)
            .execute(pool)
            .await?;
            FollowUpDraft::find_by_task_attempt_id(pool, task_attempt.id)
                .await?
                .ok_or(SqlxError::RowNotFound)?
        }
    };
    if d.queued {
        return Err(ApiError::Conflict(
            "Draft is queued; click Edit to unqueue before editing".to_string(),
        ));
    }

    // Optimistic concurrency check
    if let Some(expected_version) = payload.version
        && d.version != expected_version
    {
        return Err(ApiError::Conflict(
            "Draft changed, please retry with latest".to_string(),
        ));
    }

    if payload.prompt.is_none() && payload.variant.is_none() && payload.image_ids.is_none() {
        // nothing to change; return current
    } else {
        // Build a conservative UPDATE using positional binds to avoid SQL builder quirks
        let mut set_clauses: Vec<&str> = Vec::new();
        let mut has_variant_null = false;
        if payload.prompt.is_some() {
            set_clauses.push("prompt = ?");
        }
        if let Some(variant_opt) = &payload.variant {
            match variant_opt {
                Some(_) => set_clauses.push("variant = ?"),
                None => {
                    has_variant_null = true;
                    set_clauses.push("variant = NULL");
                }
            }
        }
        if payload.image_ids.is_some() {
            set_clauses.push("image_ids = ?");
        }
        // Always bump metadata when something changes
        set_clauses.push("updated_at = CURRENT_TIMESTAMP");
        set_clauses.push("version = version + 1");

        let mut sql = String::from("UPDATE follow_up_drafts SET ");
        sql.push_str(&set_clauses.join(", "));
        sql.push_str(" WHERE task_attempt_id = ?");

        let mut q = sqlx::query(&sql);
        if let Some(prompt) = &payload.prompt {
            q = q.bind(prompt);
        }
        if let Some(variant_opt) = &payload.variant
            && let Some(v) = variant_opt
        {
            q = q.bind(v);
        }
        if let Some(image_ids) = &payload.image_ids {
            let image_ids_json =
                serde_json::to_string(image_ids).unwrap_or_else(|_| "[]".to_string());
            q = q.bind(image_ids_json);
        }
        // WHERE bind
        q = q.bind(task_attempt.id);
        q.execute(pool).await?;
        let _ = has_variant_null; // silence unused (document intent)
    }

    // Ensure images are associated with the task for preview/loading
    if let Some(image_ids) = &payload.image_ids
        && !image_ids.is_empty()
    {
        // get parent task
        let task = task_attempt
            .parent_task(&deployment.db().pool)
            .await?
            .ok_or(SqlxError::RowNotFound)?;
        TaskImage::associate_many_dedup(pool, task.id, image_ids).await?;
    }

    // If queued and no process running for this attempt, attempt to start immediately.
    // Use an atomic sending lock to prevent duplicate starts when concurrent requests occur.
    let current = FollowUpDraft::find_by_task_attempt_id(pool, task_attempt.id).await?;
    let should_consider_start = current.as_ref().map(|c| c.queued).unwrap_or(false)
        && !has_running_processes_for_attempt(pool, task_attempt.id).await?;
    if should_consider_start {
        if FollowUpDraft::try_mark_sending(pool, task_attempt.id)
            .await
            .unwrap_or(false)
        {
            // Start follow up with saved draft
            let _ =
                start_follow_up_from_draft(&deployment, &task_attempt, current.as_ref().unwrap())
                    .await;
        } else {
            tracing::debug!(
                "Follow-up draft for attempt {} already being sent or not eligible",
                task_attempt.id
            );
        }
    }

    // Return current draft state (may have been cleared if started immediately)
    let current = FollowUpDraft::find_by_task_attempt_id(pool, task_attempt.id)
        .await?
        .map(|d| FollowUpDraftResponse {
            task_attempt_id: d.task_attempt_id,
            prompt: d.prompt,
            queued: d.queued,
            variant: d.variant,
            image_ids: d.image_ids,
            version: d.version,
        })
        .unwrap_or(FollowUpDraftResponse {
            task_attempt_id: task_attempt.id,
            prompt: "".to_string(),
            queued: false,
            variant: None,
            image_ids: None,
            version: 0,
        });

    Ok(ResponseJson(ApiResponse::success(current)))
}

#[axum::debug_handler]
pub async fn stream_follow_up_draft_ws(
    ws: WebSocketUpgrade,
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
) -> impl IntoResponse {
    ws.on_upgrade(move |socket| async move {
        if let Err(e) = handle_follow_up_draft_ws(socket, deployment, task_attempt.id).await {
            tracing::warn!("follow-up draft WS closed: {}", e);
        }
    })
}

async fn handle_follow_up_draft_ws(
    socket: WebSocket,
    deployment: DeploymentImpl,
    task_attempt_id: uuid::Uuid,
) -> anyhow::Result<()> {
    use futures_util::{SinkExt, StreamExt, TryStreamExt};

    let mut stream = deployment
        .events()
        .stream_follow_up_draft_for_attempt_raw(task_attempt_id)
        .await?
        .map_ok(|msg| msg.to_ws_message_unchecked());

    // Split socket into sender and receiver
    let (mut sender, mut receiver) = socket.split();

    // Drain (and ignore) any client->server messages so pings/pongs work
    tokio::spawn(async move { while let Some(Ok(_)) = receiver.next().await {} });

    // Forward server messages
    while let Some(item) = stream.next().await {
        match item {
            Ok(msg) => {
                if sender.send(msg).await.is_err() {
                    break;
                }
            }
            Err(e) => {
                tracing::error!("stream error: {}", e);
                break;
            }
        }
    }
    Ok(())
}

#[axum::debug_handler]
pub async fn set_follow_up_queue(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<SetQueueRequest>,
) -> Result<ResponseJson<ApiResponse<FollowUpDraftResponse>>, ApiError> {
    let pool = &deployment.db().pool;
    let Some(d) = FollowUpDraft::find_by_task_attempt_id(pool, task_attempt.id).await? else {
        return Err(ApiError::Conflict("No draft to queue".to_string()));
    };

    // Optimistic concurrency: ensure caller's view matches current state (if provided)
    if let Some(expected) = payload.expected_queued
        && d.queued != expected
    {
        return Err(ApiError::Conflict(
            "Draft state changed, please refresh and try again".to_string(),
        ));
    }
    if let Some(expected_v) = payload.expected_version
        && d.version != expected_v
    {
        return Err(ApiError::Conflict(
            "Draft changed, please refresh and try again".to_string(),
        ));
    }

    if payload.queued {
        let should_queue = !d.prompt.trim().is_empty();
        sqlx::query(
            r#"UPDATE follow_up_drafts
                   SET queued = ?, updated_at = CURRENT_TIMESTAMP, version = version + 1
                 WHERE task_attempt_id = ?"#,
        )
        .bind(should_queue as i64)
        .bind(task_attempt.id)
        .execute(pool)
        .await?;
    } else {
        // Unqueue
        sqlx::query(
            r#"UPDATE follow_up_drafts
                   SET queued = 0, updated_at = CURRENT_TIMESTAMP, version = version + 1
                 WHERE task_attempt_id = ?"#,
        )
        .bind(task_attempt.id)
        .execute(pool)
        .await?;
    }

    // If queued and no process running for this attempt, attempt to start immediately.
    let current = FollowUpDraft::find_by_task_attempt_id(pool, task_attempt.id).await?;
    let should_consider_start = current.as_ref().map(|c| c.queued).unwrap_or(false)
        && !has_running_processes_for_attempt(pool, task_attempt.id).await?;
    if should_consider_start {
        if FollowUpDraft::try_mark_sending(pool, task_attempt.id)
            .await
            .unwrap_or(false)
        {
            let _ =
                start_follow_up_from_draft(&deployment, &task_attempt, current.as_ref().unwrap())
                    .await;
        } else {
            // Schedule a short delayed recheck to handle timing edges
            let deployment_clone = deployment.clone();
            let task_attempt_clone = task_attempt.clone();
            tokio::spawn(async move {
                use std::time::Duration;
                tokio::time::sleep(Duration::from_millis(1200)).await;
                let pool = &deployment_clone.db().pool;
                // Still no running process?
                let running = match ExecutionProcess::find_by_task_attempt_id(
                    pool,
                    task_attempt_clone.id,
                    false,
                )
                .await
                {
                    Ok(procs) => procs.into_iter().any(|p| {
                        matches!(
                            p.status,
                            db::models::execution_process::ExecutionProcessStatus::Running
                        )
                    }),
                    Err(_) => true, // assume running on error to avoid duplicate starts
                };
                if running {
                    return;
                }
                // Still queued and eligible?
                let draft =
                    match FollowUpDraft::find_by_task_attempt_id(pool, task_attempt_clone.id).await
                    {
                        Ok(Some(d)) if d.queued && !d.sending && !d.prompt.trim().is_empty() => d,
                        _ => return,
                    };
                if FollowUpDraft::try_mark_sending(pool, task_attempt_clone.id)
                    .await
                    .unwrap_or(false)
                {
                    let _ =
                        start_follow_up_from_draft(&deployment_clone, &task_attempt_clone, &draft)
                            .await;
                }
            });
        }
    }

    let d = FollowUpDraft::find_by_task_attempt_id(pool, task_attempt.id)
        .await?
        .ok_or(SqlxError::RowNotFound)?;
    let resp = FollowUpDraftResponse {
        task_attempt_id: d.task_attempt_id,
        prompt: d.prompt,
        queued: d.queued,
        variant: d.variant,
        image_ids: d.image_ids,
        version: d.version,
    };
    Ok(ResponseJson(ApiResponse::success(resp)))
}

async fn start_follow_up_from_draft(
    deployment: &DeploymentImpl,
    task_attempt: &TaskAttempt,
    draft: &FollowUpDraft,
) -> Result<ExecutionProcess, ApiError> {
    // Ensure worktree exists
    deployment
        .container()
        .ensure_container_exists(task_attempt)
        .await?;

    // Get latest session id (ignoring dropped)
    let session_id = ExecutionProcess::find_latest_session_id_by_task_attempt(
        &deployment.db().pool,
        task_attempt.id,
    )
    .await?
    .ok_or(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
        "Couldn't find a prior session_id, please create a new task attempt".to_string(),
    )))?;

    // Get latest coding agent process to inherit executor profile
    let latest_execution_process = ExecutionProcess::find_latest_by_task_attempt_and_run_reason(
        &deployment.db().pool,
        task_attempt.id,
        &ExecutionProcessRunReason::CodingAgent,
    )
    .await?
    .ok_or(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
        "Couldn't find initial coding agent process, has it run yet?".to_string(),
    )))?;
    let initial_executor_profile_id = match &latest_execution_process
        .executor_action()
        .map_err(|e| ApiError::TaskAttempt(TaskAttemptError::ValidationError(e.to_string())))?
        .typ
    {
        ExecutorActionType::CodingAgentInitialRequest(request) => {
            Ok(request.executor_profile_id.clone())
        }
        ExecutorActionType::CodingAgentFollowUpRequest(request) => {
            Ok(request.executor_profile_id.clone())
        }
        _ => Err(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
            "Couldn't find profile from initial request".to_string(),
        ))),
    }?;

    // Inherit executor profile; override variant if provided in draft
    let executor_profile_id = ExecutorProfileId {
        executor: initial_executor_profile_id.executor,
        variant: draft.variant.clone(),
    };

    // Get parent task -> project and cleanup action
    let task = task_attempt
        .parent_task(&deployment.db().pool)
        .await?
        .ok_or(SqlxError::RowNotFound)?;
    let project = task
        .parent_project(&deployment.db().pool)
        .await?
        .ok_or(SqlxError::RowNotFound)?;

    let cleanup_action = project.cleanup_script.map(|script| {
        Box::new(ExecutorAction::new(
            ExecutorActionType::ScriptRequest(ScriptRequest {
                script,
                language: ScriptRequestLanguage::Bash,
                context: ScriptContext::CleanupScript,
            }),
            None,
        ))
    });

    // Handle images: associate to task, copy to worktree, and canonicalize paths in prompt
    let mut prompt = draft.prompt.clone();
    if let Some(image_ids) = &draft.image_ids {
        TaskImage::associate_many_dedup(&deployment.db().pool, task_attempt.task_id, image_ids)
            .await?;
        if let Some(container_ref) = &task_attempt.container_ref {
            let worktree_path = std::path::PathBuf::from(container_ref);
            deployment
                .image()
                .copy_images_by_ids_to_worktree(&worktree_path, image_ids)
                .await?;
            prompt = ImageService::canonicalise_image_paths(&prompt, &worktree_path);
        }
    }

    let follow_up_request = CodingAgentFollowUpRequest {
        prompt,
        session_id,
        executor_profile_id,
    };

    let follow_up_action = ExecutorAction::new(
        ExecutorActionType::CodingAgentFollowUpRequest(follow_up_request),
        cleanup_action,
    );

    let execution_process = deployment
        .container()
        .start_execution(
            task_attempt,
            &follow_up_action,
            &ExecutionProcessRunReason::CodingAgent,
        )
        .await?;

    // Best-effort: clear the draft after scheduling the execution
    let _ = FollowUpDraft::clear_after_send(&deployment.db().pool, task_attempt.id).await;

    Ok(execution_process)
}

#[axum::debug_handler]
pub async fn replace_process(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<ReplaceProcessRequest>,
) -> Result<ResponseJson<ApiResponse<ReplaceProcessResult>>, ApiError> {
    let pool = &deployment.db().pool;
    let proc_id = payload.process_id;
    let force_when_dirty = payload.force_when_dirty.unwrap_or(false);
    let perform_git_reset = payload.perform_git_reset.unwrap_or(true);

    // Validate process belongs to attempt
    let process =
        ExecutionProcess::find_by_id(pool, proc_id)
            .await?
            .ok_or(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
                "Process not found".to_string(),
            )))?;
    if process.task_attempt_id != task_attempt.id {
        return Err(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
            "Process does not belong to this attempt".to_string(),
        )));
    }

    // Determine target reset OID: before the target process
    let mut target_before_oid = process.before_head_commit.clone();
    if target_before_oid.is_none() {
        // Fallback: previous process's after_head_commit
        target_before_oid =
            ExecutionProcess::find_prev_after_head_commit(pool, task_attempt.id, proc_id).await?;
    }

    // Decide if Git reset is needed and apply it
    let mut git_reset_needed = false;
    let mut git_reset_applied = false;
    if perform_git_reset {
        if let Some(target_oid) = &target_before_oid {
            let container_ref = deployment
                .container()
                .ensure_container_exists(&task_attempt)
                .await?;
            let wt = std::path::Path::new(&container_ref);
            let head_oid = deployment.git().get_head_info(wt).ok().map(|h| h.oid);
            let is_dirty = deployment
                .container()
                .is_container_clean(&task_attempt)
                .await
                .map(|is_clean| !is_clean)
                .unwrap_or(false);
            if head_oid.as_deref() != Some(target_oid.as_str()) || is_dirty {
                git_reset_needed = true;
                if is_dirty && !force_when_dirty {
                    git_reset_applied = false; // cannot reset now
                } else if let Err(e) =
                    deployment
                        .git()
                        .reset_worktree_to_commit(wt, target_oid, force_when_dirty)
                {
                    tracing::error!("Failed to reset worktree: {}", e);
                    git_reset_applied = false;
                } else {
                    git_reset_applied = true;
                }
            }
        }
    } else {
        // Only compute necessity
        if let Some(target_oid) = &target_before_oid {
            let container_ref = deployment
                .container()
                .ensure_container_exists(&task_attempt)
                .await?;
            let wt = std::path::Path::new(&container_ref);
            let head_oid = deployment.git().get_head_info(wt).ok().map(|h| h.oid);
            let is_dirty = deployment
                .container()
                .is_container_clean(&task_attempt)
                .await
                .map(|is_clean| !is_clean)
                .unwrap_or(false);
            if head_oid.as_deref() != Some(target_oid.as_str()) || is_dirty {
                git_reset_needed = true;
            }
        }
    }

    // Stop any running processes for this attempt
    deployment.container().try_stop(&task_attempt).await;

    // Soft-drop the target process and all later processes
    let deleted_count = ExecutionProcess::drop_at_and_after(pool, task_attempt.id, proc_id).await?;

    // Build follow-up executor action using the original process profile
    let initial_executor_profile_id = match &process
        .executor_action()
        .map_err(|e| ApiError::TaskAttempt(TaskAttemptError::ValidationError(e.to_string())))?
        .typ
    {
        ExecutorActionType::CodingAgentInitialRequest(request) => {
            Ok(request.executor_profile_id.clone())
        }
        ExecutorActionType::CodingAgentFollowUpRequest(request) => {
            Ok(request.executor_profile_id.clone())
        }
        _ => Err(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
            "Couldn't find profile from executor action".to_string(),
        ))),
    }?;

    let executor_profile_id = ExecutorProfileId {
        executor: initial_executor_profile_id.executor,
        variant: payload
            .variant
            .or(initial_executor_profile_id.variant.clone()),
    };

    // Use latest session_id from remaining (earlier) processes; if none exists, start a fresh initial request
    let latest_session_id =
        ExecutionProcess::find_latest_session_id_by_task_attempt(pool, task_attempt.id).await?;

    let action = if let Some(session_id) = latest_session_id {
        let follow_up_request = CodingAgentFollowUpRequest {
            prompt: payload.prompt.clone(),
            session_id,
            executor_profile_id,
        };
        ExecutorAction::new(
            ExecutorActionType::CodingAgentFollowUpRequest(follow_up_request),
            None,
        )
    } else {
        // No prior session (e.g., replacing the first run) → start a fresh initial request
        ExecutorAction::new(
            ExecutorActionType::CodingAgentInitialRequest(
                executors::actions::coding_agent_initial::CodingAgentInitialRequest {
                    prompt: payload.prompt.clone(),
                    executor_profile_id,
                },
            ),
            None,
        )
    };

    let execution_process = deployment
        .container()
        .start_execution(
            &task_attempt,
            &action,
            &ExecutionProcessRunReason::CodingAgent,
        )
        .await?;

    Ok(ResponseJson(ApiResponse::success(ReplaceProcessResult {
        deleted_count,
        git_reset_needed,
        git_reset_applied,
        target_before_oid,
        new_execution_id: Some(execution_process.id),
    })))
}

pub async fn get_task_attempt_diff(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
    // ) -> Result<ResponseJson<ApiResponse<Diff>>, ApiError> {
) -> Result<Sse<impl futures_util::Stream<Item = Result<Event, BoxError>>>, ApiError> {
    let stream = deployment.container().get_diff(&task_attempt).await?;

    Ok(Sse::new(stream.map_err(|e| -> BoxError { e.into() })).keep_alive(KeepAlive::default()))
}

#[derive(Debug, Serialize, TS)]
pub struct CommitInfo {
    pub sha: String,
    pub subject: String,
}

pub async fn get_commit_info(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
    Query(params): Query<std::collections::HashMap<String, String>>,
) -> Result<ResponseJson<ApiResponse<CommitInfo>>, ApiError> {
    let Some(sha) = params.get("sha").cloned() else {
        return Err(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
            "Missing sha param".to_string(),
        )));
    };
    let container_ref = deployment
        .container()
        .ensure_container_exists(&task_attempt)
        .await?;
    let wt = std::path::Path::new(&container_ref);
    let subject = deployment.git().get_commit_subject(wt, &sha)?;
    Ok(ResponseJson(ApiResponse::success(CommitInfo {
        sha,
        subject,
    })))
}

#[derive(Debug, Serialize, TS)]
pub struct CommitCompareResult {
    pub head_oid: String,
    pub target_oid: String,
    pub ahead_from_head: usize,
    pub behind_from_head: usize,
    pub is_linear: bool,
}

pub async fn compare_commit_to_head(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
    Query(params): Query<std::collections::HashMap<String, String>>,
) -> Result<ResponseJson<ApiResponse<CommitCompareResult>>, ApiError> {
    let Some(target_oid) = params.get("sha").cloned() else {
        return Err(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
            "Missing sha param".to_string(),
        )));
    };
    let container_ref = deployment
        .container()
        .ensure_container_exists(&task_attempt)
        .await?;
    let wt = std::path::Path::new(&container_ref);
    let head_info = deployment.git().get_head_info(wt)?;
    let (ahead_from_head, behind_from_head) =
        deployment
            .git()
            .ahead_behind_commits_by_oid(wt, &head_info.oid, &target_oid)?;
    let is_linear = behind_from_head == 0;
    Ok(ResponseJson(ApiResponse::success(CommitCompareResult {
        head_oid: head_info.oid,
        target_oid,
        ahead_from_head,
        behind_from_head,
        is_linear,
    })))
}

#[axum::debug_handler]
pub async fn merge_task_attempt(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    let pool = &deployment.db().pool;

    let task = task_attempt
        .parent_task(pool)
        .await?
        .ok_or(ApiError::TaskAttempt(TaskAttemptError::TaskNotFound))?;
    let ctx = TaskAttempt::load_context(pool, task_attempt.id, task.id, task.project_id).await?;

    let container_ref = deployment
        .container()
        .ensure_container_exists(&task_attempt)
        .await?;
    let worktree_path = std::path::Path::new(&container_ref);

    let task_uuid_str = task.id.to_string();
    let first_uuid_section = task_uuid_str.split('-').next().unwrap_or(&task_uuid_str);

    // Create commit message with task title and description
    let mut commit_message = format!("{} (vibe-kanban {})", ctx.task.title, first_uuid_section);

    // Add description on next line if it exists
    if let Some(description) = &ctx.task.description
        && !description.trim().is_empty()
    {
        commit_message.push_str("\n\n");
        commit_message.push_str(description);
    }

    // Get branch name from task attempt
    let branch_name = ctx.task_attempt.branch.as_ref().ok_or_else(|| {
        ApiError::TaskAttempt(TaskAttemptError::ValidationError(
            "No branch found for task attempt".to_string(),
        ))
    })?;

    let merge_commit_id = deployment.git().merge_changes(
        &ctx.project.git_repo_path,
        worktree_path,
        branch_name,
        &ctx.task_attempt.base_branch,
        &commit_message,
    )?;

    Merge::create_direct(
        pool,
        task_attempt.id,
        &ctx.task_attempt.base_branch,
        &merge_commit_id,
    )
    .await?;
    Task::update_status(pool, ctx.task.id, TaskStatus::Done).await?;

    deployment
        .track_if_analytics_allowed(
            "task_attempt_merged",
            serde_json::json!({
                "task_id": ctx.task.id.to_string(),
                "project_id": ctx.project.id.to_string(),
                "attempt_id": task_attempt.id.to_string(),
            }),
        )
        .await;

    Ok(ResponseJson(ApiResponse::success(())))
}

pub async fn push_task_attempt_branch(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    let github_config = deployment.config().read().await.github.clone();
    let Some(github_token) = github_config.token() else {
        return Err(GitHubServiceError::TokenInvalid.into());
    };

    let github_service = GitHubService::new(&github_token)?;
    github_service.check_token().await?;

    let branch_name = task_attempt.branch.as_ref().ok_or_else(|| {
        ApiError::TaskAttempt(TaskAttemptError::ValidationError(
            "No branch found for task attempt".to_string(),
        ))
    })?;
    let ws_path = PathBuf::from(
        deployment
            .container()
            .ensure_container_exists(&task_attempt)
            .await?,
    );

    deployment
        .git()
        .push_to_github(&ws_path, branch_name, &github_token)?;
    Ok(ResponseJson(ApiResponse::success(())))
}

pub async fn create_github_pr(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
    Json(request): Json<CreateGitHubPrRequest>,
) -> Result<ResponseJson<ApiResponse<String, GitHubServiceError>>, ApiError> {
    let github_config = deployment.config().read().await.github.clone();
    let Some(github_token) = github_config.token() else {
        return Ok(ResponseJson(ApiResponse::error_with_data(
            GitHubServiceError::TokenInvalid,
        )));
    };
    // Create GitHub service instance
    let github_service = GitHubService::new(&github_token)?;
    // Get the task attempt to access the stored base branch
    let base_branch = request.base_branch.unwrap_or_else(|| {
        // Use the stored base branch from the task attempt as the default
        // Fall back to config default or "main" only if stored base branch is somehow invalid
        if !task_attempt.base_branch.trim().is_empty() {
            task_attempt.base_branch.clone()
        } else {
            github_config
                .default_pr_base
                .as_ref()
                .map_or_else(|| "main".to_string(), |b| b.to_string())
        }
    });

    let pool = &deployment.db().pool;
    let task = task_attempt
        .parent_task(pool)
        .await?
        .ok_or(ApiError::TaskAttempt(TaskAttemptError::TaskNotFound))?;
    let project = Project::find_by_id(pool, task.project_id)
        .await?
        .ok_or(ApiError::Project(ProjectError::ProjectNotFound))?;

    // Get branch name from task attempt
    let branch_name = task_attempt.branch.as_ref().ok_or_else(|| {
        ApiError::TaskAttempt(TaskAttemptError::ValidationError(
            "No branch found for task attempt".to_string(),
        ))
    })?;
    let workspace_path = PathBuf::from(
        deployment
            .container()
            .ensure_container_exists(&task_attempt)
            .await?,
    );

    // Push the branch to GitHub first
    if let Err(e) = deployment
        .git()
        .push_to_github(&workspace_path, branch_name, &github_token)
    {
        tracing::error!("Failed to push branch to GitHub: {}", e);
        let gh_e = GitHubServiceError::from(e);
        if gh_e.is_api_data() {
            return Ok(ResponseJson(ApiResponse::error_with_data(gh_e)));
        } else {
            return Ok(ResponseJson(ApiResponse::error(
                format!("Failed to push branch to GitHub: {}", gh_e).as_str(),
            )));
        }
    }

    let norm_base_branch_name = if matches!(
        deployment
            .git()
            .find_branch_type(&project.git_repo_path, &base_branch)?,
        BranchType::Remote
    ) {
        // Remote branches are formatted as {remote}/{branch} locally.
        // For PR APIs, we must provide just the branch name.
        let remote = deployment
            .git()
            .get_remote_name_from_branch_name(&workspace_path, &base_branch)?;
        let remote_prefix = format!("{}/", remote);
        base_branch
            .strip_prefix(&remote_prefix)
            .unwrap_or(&base_branch)
            .to_string()
    } else {
        base_branch
    };
    // Create the PR using GitHub service
    let pr_request = CreatePrRequest {
        title: request.title.clone(),
        body: request.body.clone(),
        head_branch: branch_name.clone(),
        base_branch: norm_base_branch_name.clone(),
    };
    // Use GitService to get the remote URL, then create GitHubRepoInfo
    let repo_info = deployment
        .git()
        .get_github_repo_info(&project.git_repo_path)?;

    match github_service.create_pr(&repo_info, &pr_request).await {
        Ok(pr_info) => {
            // Update the task attempt with PR information
            if let Err(e) = Merge::create_pr(
                pool,
                task_attempt.id,
                &norm_base_branch_name,
                pr_info.number,
                &pr_info.url,
            )
            .await
            {
                tracing::error!("Failed to update task attempt PR status: {}", e);
            }

            // Auto-open PR in browser
            if let Err(e) = utils::browser::open_browser(&pr_info.url).await {
                tracing::warn!("Failed to open PR in browser: {}", e);
            }
            deployment
                .track_if_analytics_allowed(
                    "github_pr_created",
                    serde_json::json!({
                        "task_id": task.id.to_string(),
                        "project_id": project.id.to_string(),
                        "attempt_id": task_attempt.id.to_string(),
                    }),
                )
                .await;

            Ok(ResponseJson(ApiResponse::success(pr_info.url)))
        }
        Err(e) => {
            tracing::error!(
                "Failed to create GitHub PR for attempt {}: {}",
                task_attempt.id,
                e
            );
            if e.is_api_data() {
                Ok(ResponseJson(ApiResponse::error_with_data(e)))
            } else {
                Ok(ResponseJson(ApiResponse::error(
                    format!("Failed to create PR: {}", e).as_str(),
                )))
            }
        }
    }
}

#[derive(serde::Deserialize)]
pub struct OpenEditorRequest {
    editor_type: Option<String>,
    file_path: Option<String>,
}

pub async fn open_task_attempt_in_editor(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<Option<OpenEditorRequest>>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    // Get the task attempt to access the worktree path
    let attempt = &task_attempt;
    let base_path = attempt.container_ref.as_ref().ok_or_else(|| {
        tracing::error!(
            "No container ref found for task attempt {}",
            task_attempt.id
        );
        ApiError::TaskAttempt(TaskAttemptError::ValidationError(
            "No container ref found".to_string(),
        ))
    })?;

    // If a specific file path is provided, use it; otherwise use the base path
    let path = if let Some(file_path) = payload.as_ref().and_then(|req| req.file_path.as_ref()) {
        std::path::Path::new(base_path).join(file_path)
    } else {
        std::path::PathBuf::from(base_path)
    };

    let editor_config = {
        let config = deployment.config().read().await;
        let editor_type_str = payload.as_ref().and_then(|req| req.editor_type.as_deref());
        config.editor.with_override(editor_type_str)
    };

    match editor_config.open_file(&path.to_string_lossy()) {
        Ok(_) => {
            tracing::info!(
                "Opened editor for task attempt {} at path: {}",
                task_attempt.id,
                path.display()
            );
            Ok(ResponseJson(ApiResponse::success(())))
        }
        Err(e) => {
            tracing::error!(
                "Failed to open editor for attempt {}: {}",
                task_attempt.id,
                e
            );
            Err(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
                format!("Failed to open editor: {}", e),
            )))
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct BranchStatus {
    pub commits_behind: Option<usize>,
    pub commits_ahead: Option<usize>,
    pub has_uncommitted_changes: Option<bool>,
    pub head_oid: Option<String>,
    pub uncommitted_count: Option<usize>,
    pub untracked_count: Option<usize>,
    pub base_branch_name: String,
    pub remote_commits_behind: Option<usize>,
    pub remote_commits_ahead: Option<usize>,
    pub merges: Vec<Merge>,
    /// True if a `git rebase` is currently in progress in this worktree
    pub is_rebase_in_progress: bool,
    /// Current conflict operation if any
    pub conflict_op: Option<ConflictOp>,
    /// List of files currently in conflicted (unmerged) state
    pub conflicted_files: Vec<String>,
}

pub async fn get_task_attempt_branch_status(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<BranchStatus>>, ApiError> {
    let pool = &deployment.db().pool;

    let task = task_attempt
        .parent_task(pool)
        .await?
        .ok_or(ApiError::TaskAttempt(TaskAttemptError::TaskNotFound))?;
    let ctx = TaskAttempt::load_context(pool, task_attempt.id, task.id, task.project_id).await?;
    let has_uncommitted_changes = deployment
        .container()
        .is_container_clean(&task_attempt)
        .await
        .ok()
        .map(|is_clean| !is_clean);
    let head_oid = {
        let container_ref = deployment
            .container()
            .ensure_container_exists(&task_attempt)
            .await?;
        let wt = std::path::Path::new(&container_ref);
        deployment.git().get_head_info(wt).ok().map(|h| h.oid)
    };
    // Detect conflicts and operation in progress (best-effort)
    let (is_rebase_in_progress, conflicted_files, conflict_op) = {
        let container_ref = deployment
            .container()
            .ensure_container_exists(&task_attempt)
            .await?;
        let wt = std::path::Path::new(&container_ref);
        let in_rebase = deployment.git().is_rebase_in_progress(wt).unwrap_or(false);
        let conflicts = deployment
            .git()
            .get_conflicted_files(wt)
            .unwrap_or_default();
        let op = if conflicts.is_empty() {
            None
        } else {
            deployment.git().detect_conflict_op(wt).unwrap_or(None)
        };
        (in_rebase, conflicts, op)
    };
    let (uncommitted_count, untracked_count) = {
        let container_ref = deployment
            .container()
            .ensure_container_exists(&task_attempt)
            .await?;
        let wt = std::path::Path::new(&container_ref);
        match deployment.git().get_worktree_change_counts(wt) {
            Ok((a, b)) => (Some(a), Some(b)),
            Err(_) => (None, None),
        }
    };

    let task_branch =
        task_attempt
            .branch
            .ok_or(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
                "No branch found for task attempt".to_string(),
            )))?;
    let base_branch_type = deployment
        .git()
        .find_branch_type(&ctx.project.git_repo_path, &task_attempt.base_branch)?;

    let (commits_ahead, commits_behind) = if matches!(base_branch_type, BranchType::Local) {
        let (a, b) = deployment.git().get_branch_status(
            &ctx.project.git_repo_path,
            &task_branch,
            &task_attempt.base_branch,
        )?;
        (Some(a), Some(b))
    } else {
        (None, None)
    };
    // Fetch merges for this task attempt and add to branch status
    let merges = Merge::find_by_task_attempt_id(pool, task_attempt.id).await?;
    let mut branch_status = BranchStatus {
        commits_ahead,
        commits_behind,
        has_uncommitted_changes,
        head_oid,
        uncommitted_count,
        untracked_count,
        remote_commits_ahead: None,
        remote_commits_behind: None,
        merges,
        base_branch_name: task_attempt.base_branch.clone(),
        is_rebase_in_progress,
        conflict_op,
        conflicted_files,
    };
    let has_open_pr = branch_status.merges.first().is_some_and(|m| {
        matches!(
            m,
            Merge::Pr(PrMerge {
                pr_info: PullRequestInfo {
                    status: MergeStatus::Open,
                    ..
                },
                ..
            })
        )
    });

    // check remote status if the attempt has an open PR or the base_branch is a remote branch
    if has_open_pr || base_branch_type == BranchType::Remote {
        let github_config = deployment.config().read().await.github.clone();
        let token = github_config
            .token()
            .ok_or(ApiError::GitHubService(GitHubServiceError::TokenInvalid))?;

        // For an attempt with a remote base branch, we compare against that
        // After opening a PR, the attempt has a remote branch itself, so we use that
        let remote_base_branch = if base_branch_type == BranchType::Remote && !has_open_pr {
            Some(task_attempt.base_branch)
        } else {
            None
        };
        let (remote_commits_ahead, remote_commits_behind) =
            deployment.git().get_remote_branch_status(
                &ctx.project.git_repo_path,
                &task_branch,
                remote_base_branch.as_deref(),
                token,
            )?;
        branch_status.remote_commits_ahead = Some(remote_commits_ahead);
        branch_status.remote_commits_behind = Some(remote_commits_behind);
    }
    Ok(ResponseJson(ApiResponse::success(branch_status)))
}

#[axum::debug_handler]
pub async fn rebase_task_attempt(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
    request_body: Option<Json<RebaseTaskAttemptRequest>>,
) -> Result<ResponseJson<ApiResponse<(), GitOperationError>>, ApiError> {
    // Extract new base branch from request body if provided
    let new_base_branch = request_body.and_then(|body| body.new_base_branch.clone());

    let github_config = deployment.config().read().await.github.clone();

    let pool = &deployment.db().pool;

    let task = task_attempt
        .parent_task(pool)
        .await?
        .ok_or(ApiError::TaskAttempt(TaskAttemptError::TaskNotFound))?;
    let ctx = TaskAttempt::load_context(pool, task_attempt.id, task.id, task.project_id).await?;

    // Use the stored base branch if no new base branch is provided
    let effective_base_branch =
        new_base_branch.or_else(|| Some(ctx.task_attempt.base_branch.clone()));

    let container_ref = deployment
        .container()
        .ensure_container_exists(&task_attempt)
        .await?;
    let worktree_path = std::path::Path::new(&container_ref);

    let result = deployment.git().rebase_branch(
        &ctx.project.git_repo_path,
        worktree_path,
        effective_base_branch.clone().as_deref(),
        &ctx.task_attempt.base_branch.clone(),
        github_config.token(),
    );
    if let Err(e) = result {
        use services::services::git::GitServiceError;
        return match e {
            GitServiceError::MergeConflicts(msg) => Ok(ResponseJson(ApiResponse::<
                (),
                GitOperationError,
            >::error_with_data(
                GitOperationError::MergeConflicts {
                    message: msg,
                    op: ConflictOp::Rebase,
                },
            ))),
            GitServiceError::RebaseInProgress => Ok(ResponseJson(ApiResponse::<
                (),
                GitOperationError,
            >::error_with_data(
                GitOperationError::RebaseInProgress,
            ))),
            other => Err(ApiError::GitService(other)),
        };
    }

    if let Some(new_base_branch) = &effective_base_branch
        && new_base_branch != &ctx.task_attempt.base_branch
    {
        TaskAttempt::update_base_branch(&deployment.db().pool, task_attempt.id, new_base_branch)
            .await?;
    }

    Ok(ResponseJson(ApiResponse::success(())))
}

#[axum::debug_handler]
pub async fn abort_conflicts_task_attempt(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    // Resolve worktree path for this attempt
    let container_ref = deployment
        .container()
        .ensure_container_exists(&task_attempt)
        .await?;
    let worktree_path = std::path::Path::new(&container_ref);

    deployment.git().abort_conflicts(worktree_path)?;

    Ok(ResponseJson(ApiResponse::success(())))
}

#[derive(serde::Deserialize)]
pub struct DeleteFileQuery {
    file_path: String,
}

#[axum::debug_handler]
pub async fn delete_task_attempt_file(
    Extension(task_attempt): Extension<TaskAttempt>,
    Query(query): Query<DeleteFileQuery>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    let container_ref = deployment
        .container()
        .ensure_container_exists(&task_attempt)
        .await?;
    let worktree_path = std::path::Path::new(&container_ref);

    // Use GitService to delete file and commit
    let _commit_id = deployment
        .git()
        .delete_file_and_commit(worktree_path, &query.file_path)
        .map_err(|e| {
            tracing::error!(
                "Failed to delete file '{}' from task attempt {}: {}",
                query.file_path,
                task_attempt.id,
                e
            );
            ApiError::GitService(e)
        })?;

    Ok(ResponseJson(ApiResponse::success(())))
}

#[axum::debug_handler]
pub async fn start_dev_server(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    let pool = &deployment.db().pool;

    // Get parent task
    let task = task_attempt
        .parent_task(&deployment.db().pool)
        .await?
        .ok_or(SqlxError::RowNotFound)?;

    // Get parent project
    let project = task
        .parent_project(&deployment.db().pool)
        .await?
        .ok_or(SqlxError::RowNotFound)?;

    // Stop any existing dev servers for this project
    let existing_dev_servers =
        match ExecutionProcess::find_running_dev_servers_by_project(pool, project.id).await {
            Ok(servers) => servers,
            Err(e) => {
                tracing::error!(
                    "Failed to find running dev servers for project {}: {}",
                    project.id,
                    e
                );
                return Err(ApiError::TaskAttempt(TaskAttemptError::ValidationError(
                    e.to_string(),
                )));
            }
        };

    for dev_server in existing_dev_servers {
        tracing::info!(
            "Stopping existing dev server {} for project {}",
            dev_server.id,
            project.id
        );

        if let Err(e) = deployment.container().stop_execution(&dev_server).await {
            tracing::error!("Failed to stop dev server {}: {}", dev_server.id, e);
        }
    }

    if let Some(dev_server) = project.dev_script {
        // TODO: Derive script language from system config
        let executor_action = ExecutorAction::new(
            ExecutorActionType::ScriptRequest(ScriptRequest {
                script: dev_server,
                language: ScriptRequestLanguage::Bash,
                context: ScriptContext::DevServer,
            }),
            None,
        );

        deployment
            .container()
            .start_execution(
                &task_attempt,
                &executor_action,
                &ExecutionProcessRunReason::DevServer,
            )
            .await?
    } else {
        return Ok(ResponseJson(ApiResponse::error(
            "No dev server script configured for this project",
        )));
    };

    Ok(ResponseJson(ApiResponse::success(())))
}

pub async fn get_task_attempt_children(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<TaskRelationships>>, StatusCode> {
    match Task::find_relationships_for_attempt(&deployment.db().pool, &task_attempt).await {
        Ok(relationships) => Ok(ResponseJson(ApiResponse::success(relationships))),
        Err(e) => {
            tracing::error!(
                "Failed to fetch relationships for task attempt {}: {}",
                task_attempt.id,
                e
            );
            Err(StatusCode::INTERNAL_SERVER_ERROR)
        }
    }
}

pub async fn stop_task_attempt_execution(
    Extension(task_attempt): Extension<TaskAttempt>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    deployment.container().try_stop(&task_attempt).await;
    Ok(ResponseJson(ApiResponse::success(())))
}

pub fn router(deployment: &DeploymentImpl) -> Router<DeploymentImpl> {
    let task_attempt_id_router = Router::new()
        .route("/", get(get_task_attempt))
        .route("/follow-up", post(follow_up))
        .route(
            "/follow-up-draft",
            get(get_follow_up_draft).put(save_follow_up_draft),
        )
        .route("/follow-up-draft/stream/ws", get(stream_follow_up_draft_ws))
        .route("/follow-up-draft/queue", post(set_follow_up_queue))
        .route("/replace-process", post(replace_process))
        .route("/commit-info", get(get_commit_info))
        .route("/commit-compare", get(compare_commit_to_head))
        .route("/start-dev-server", post(start_dev_server))
        .route("/branch-status", get(get_task_attempt_branch_status))
        .route("/diff", get(get_task_attempt_diff))
        .route("/merge", post(merge_task_attempt))
        .route("/push", post(push_task_attempt_branch))
        .route("/rebase", post(rebase_task_attempt))
        .route("/conflicts/abort", post(abort_conflicts_task_attempt))
        .route("/pr", post(create_github_pr))
        .route("/open-editor", post(open_task_attempt_in_editor))
        .route("/delete-file", post(delete_task_attempt_file))
        .route("/children", get(get_task_attempt_children))
        .route("/stop", post(stop_task_attempt_execution))
        .layer(from_fn_with_state(
            deployment.clone(),
            load_task_attempt_middleware,
        ));

    let task_attempts_router = Router::new()
        .route("/", get(get_task_attempts).post(create_task_attempt))
        .nest("/{id}", task_attempt_id_router);

    Router::new().nest("/task-attempts", task_attempts_router)
}
</file>

<file path="crates/server/src/routes/task_templates.rs">
use axum::{
    Extension, Json, Router,
    extract::{Query, State},
    middleware::from_fn_with_state,
    response::Json as ResponseJson,
    routing::get,
};
use db::models::task_template::{CreateTaskTemplate, TaskTemplate, UpdateTaskTemplate};
use deployment::Deployment;
use serde::Deserialize;
use sqlx::Error as SqlxError;
use utils::response::ApiResponse;
use uuid::Uuid;

use crate::{DeploymentImpl, error::ApiError, middleware::load_task_template_middleware};

#[derive(Debug, Deserialize)]
pub struct TaskTemplateQuery {
    global: Option<bool>,
    project_id: Option<Uuid>,
}

pub async fn get_templates(
    State(deployment): State<DeploymentImpl>,
    Query(query): Query<TaskTemplateQuery>,
) -> Result<ResponseJson<ApiResponse<Vec<TaskTemplate>>>, ApiError> {
    let templates = match (query.global, query.project_id) {
        // All templates: Global and project-specific
        (None, None) => TaskTemplate::find_all(&deployment.db().pool).await?,
        // Only global templates
        (Some(true), None) => TaskTemplate::find_by_project_id(&deployment.db().pool, None).await?,
        // Only project-specific templates
        (None | Some(false), Some(project_id)) => {
            TaskTemplate::find_by_project_id(&deployment.db().pool, Some(project_id)).await?
        }
        // No global templates, but project_id is None, return empty list
        (Some(false), None) => vec![],
        // Invalid combination: Cannot query both global and project-specific templates
        (Some(_), Some(_)) => {
            return Err(ApiError::Database(SqlxError::InvalidArgument(
                "Cannot query both global and project-specific templates".to_string(),
            )));
        }
    };
    Ok(ResponseJson(ApiResponse::success(templates)))
}

pub async fn get_template(
    Extension(template): Extension<TaskTemplate>,
) -> Result<ResponseJson<ApiResponse<TaskTemplate>>, ApiError> {
    Ok(Json(ApiResponse::success(template)))
}

pub async fn create_template(
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<CreateTaskTemplate>,
) -> Result<ResponseJson<ApiResponse<TaskTemplate>>, ApiError> {
    Ok(ResponseJson(ApiResponse::success(
        TaskTemplate::create(&deployment.db().pool, &payload).await?,
    )))
}

pub async fn update_template(
    Extension(template): Extension<TaskTemplate>,
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<UpdateTaskTemplate>,
) -> Result<ResponseJson<ApiResponse<TaskTemplate>>, ApiError> {
    Ok(ResponseJson(ApiResponse::success(
        TaskTemplate::update(&deployment.db().pool, template.id, &payload).await?,
    )))
}

pub async fn delete_template(
    Extension(template): Extension<TaskTemplate>,
    State(deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    let rows_affected = TaskTemplate::delete(&deployment.db().pool, template.id).await?;
    if rows_affected == 0 {
        Err(ApiError::Database(SqlxError::RowNotFound))
    } else {
        Ok(ResponseJson(ApiResponse::success(())))
    }
}

pub fn router(deployment: &DeploymentImpl) -> Router<DeploymentImpl> {
    let task_template_router = Router::new()
        .route(
            "/",
            get(get_template)
                .put(update_template)
                .delete(delete_template),
        )
        .layer(from_fn_with_state(
            deployment.clone(),
            load_task_template_middleware,
        ));

    let inner = Router::new()
        .route("/", get(get_templates).post(create_template))
        .nest("/{template_id}", task_template_router);

    Router::new().nest("/templates", inner)
}
</file>

<file path="crates/server/src/routes/tasks.rs">
use std::path::PathBuf;

use anyhow;
use axum::{
    Extension, Json, Router,
    extract::{
        Query, State,
        ws::{WebSocket, WebSocketUpgrade},
    },
    http::StatusCode,
    middleware::from_fn_with_state,
    response::{IntoResponse, Json as ResponseJson},
    routing::{get, post},
};
use db::models::{
    image::TaskImage,
    task::{CreateTask, Task, TaskWithAttemptStatus, UpdateTask},
    task_attempt::{CreateTaskAttempt, TaskAttempt},
};
use deployment::Deployment;
use executors::profile::ExecutorProfileId;
use futures_util::{SinkExt, StreamExt, TryStreamExt};
use serde::Deserialize;
use services::services::container::{
    ContainerService, WorktreeCleanupData, cleanup_worktrees_direct,
};
use sqlx::Error as SqlxError;
use ts_rs::TS;
use utils::response::ApiResponse;
use uuid::Uuid;

use crate::{DeploymentImpl, error::ApiError, middleware::load_task_middleware};

#[derive(Debug, Deserialize)]
pub struct TaskQuery {
    pub project_id: Uuid,
}

pub async fn get_tasks(
    State(deployment): State<DeploymentImpl>,
    Query(query): Query<TaskQuery>,
) -> Result<ResponseJson<ApiResponse<Vec<TaskWithAttemptStatus>>>, ApiError> {
    let tasks =
        Task::find_by_project_id_with_attempt_status(&deployment.db().pool, query.project_id)
            .await?;

    Ok(ResponseJson(ApiResponse::success(tasks)))
}

pub async fn stream_tasks_ws(
    ws: WebSocketUpgrade,
    State(deployment): State<DeploymentImpl>,
    Query(query): Query<TaskQuery>,
) -> impl IntoResponse {
    ws.on_upgrade(move |socket| async move {
        if let Err(e) = handle_tasks_ws(socket, deployment, query.project_id).await {
            tracing::warn!("tasks WS closed: {}", e);
        }
    })
}

async fn handle_tasks_ws(
    socket: WebSocket,
    deployment: DeploymentImpl,
    project_id: Uuid,
) -> anyhow::Result<()> {
    // Get the raw stream and convert LogMsg to WebSocket messages
    let mut stream = deployment
        .events()
        .stream_tasks_raw(project_id)
        .await?
        .map_ok(|msg| msg.to_ws_message_unchecked());

    // Split socket into sender and receiver
    let (mut sender, mut receiver) = socket.split();

    // Drain (and ignore) any client->server messages so pings/pongs work
    tokio::spawn(async move { while let Some(Ok(_)) = receiver.next().await {} });

    // Forward server messages
    while let Some(item) = stream.next().await {
        match item {
            Ok(msg) => {
                if sender.send(msg).await.is_err() {
                    break; // client disconnected
                }
            }
            Err(e) => {
                tracing::error!("stream error: {}", e);
                break;
            }
        }
    }
    Ok(())
}

pub async fn get_task(
    Extension(task): Extension<Task>,
    State(_deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<Task>>, ApiError> {
    Ok(ResponseJson(ApiResponse::success(task)))
}

pub async fn create_task(
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<CreateTask>,
) -> Result<ResponseJson<ApiResponse<Task>>, ApiError> {
    let id = Uuid::new_v4();

    tracing::debug!(
        "Creating task '{}' in project {}",
        payload.title,
        payload.project_id
    );

    let task = Task::create(&deployment.db().pool, &payload, id).await?;

    if let Some(image_ids) = &payload.image_ids {
        TaskImage::associate_many_dedup(&deployment.db().pool, task.id, image_ids).await?;
    }

    deployment
        .track_if_analytics_allowed(
            "task_created",
            serde_json::json!({
            "task_id": task.id.to_string(),
            "project_id": payload.project_id,
            "has_description": task.description.is_some(),
            "has_images": payload.image_ids.is_some(),
            }),
        )
        .await;

    Ok(ResponseJson(ApiResponse::success(task)))
}

#[derive(Debug, Deserialize, TS)]
pub struct CreateAndStartTaskRequest {
    pub task: CreateTask,
    pub executor_profile_id: ExecutorProfileId,
    pub base_branch: String,
}

pub async fn create_task_and_start(
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<CreateAndStartTaskRequest>,
) -> Result<ResponseJson<ApiResponse<TaskWithAttemptStatus>>, ApiError> {
    let task_id = Uuid::new_v4();
    let task = Task::create(&deployment.db().pool, &payload.task, task_id).await?;

    if let Some(image_ids) = &payload.task.image_ids {
        TaskImage::associate_many(&deployment.db().pool, task.id, image_ids).await?;
    }

    deployment
        .track_if_analytics_allowed(
            "task_created",
            serde_json::json!({
                "task_id": task.id.to_string(),
                "project_id": task.project_id,
                "has_description": task.description.is_some(),
                "has_images": payload.task.image_ids.is_some(),
            }),
        )
        .await;

    let task_attempt = TaskAttempt::create(
        &deployment.db().pool,
        &CreateTaskAttempt {
            executor: payload.executor_profile_id.executor,
            base_branch: payload.base_branch,
        },
        task.id,
    )
    .await?;
    let execution_process = deployment
        .container()
        .start_attempt(&task_attempt, payload.executor_profile_id.clone())
        .await?;
    deployment
        .track_if_analytics_allowed(
            "task_attempt_started",
            serde_json::json!({
                "task_id": task.id.to_string(),
                "executor": &payload.executor_profile_id.executor,
                "variant": &payload.executor_profile_id.variant,
                "attempt_id": task_attempt.id.to_string(),
            }),
        )
        .await;

    let task = Task::find_by_id(&deployment.db().pool, task.id)
        .await?
        .ok_or(ApiError::Database(SqlxError::RowNotFound))?;

    tracing::info!("Started execution process {}", execution_process.id);
    Ok(ResponseJson(ApiResponse::success(TaskWithAttemptStatus {
        task,
        has_in_progress_attempt: true,
        has_merged_attempt: false,
        last_attempt_failed: false,
        executor: task_attempt.executor,
    })))
}

pub async fn update_task(
    Extension(existing_task): Extension<Task>,
    State(deployment): State<DeploymentImpl>,
    Json(payload): Json<UpdateTask>,
) -> Result<ResponseJson<ApiResponse<Task>>, ApiError> {
    // Use existing values if not provided in update
    let title = payload.title.unwrap_or(existing_task.title);
    let description = payload.description.or(existing_task.description);
    let status = payload.status.unwrap_or(existing_task.status);
    let parent_task_attempt = payload
        .parent_task_attempt
        .or(existing_task.parent_task_attempt);

    let task = Task::update(
        &deployment.db().pool,
        existing_task.id,
        existing_task.project_id,
        title,
        description,
        status,
        parent_task_attempt,
    )
    .await?;

    if let Some(image_ids) = &payload.image_ids {
        TaskImage::delete_by_task_id(&deployment.db().pool, task.id).await?;
        TaskImage::associate_many_dedup(&deployment.db().pool, task.id, image_ids).await?;
    }

    Ok(ResponseJson(ApiResponse::success(task)))
}

pub async fn delete_task(
    Extension(task): Extension<Task>,
    State(deployment): State<DeploymentImpl>,
) -> Result<(StatusCode, ResponseJson<ApiResponse<()>>), ApiError> {
    // Validate no running execution processes
    if deployment
        .container()
        .has_running_processes(task.id)
        .await?
    {
        return Err(ApiError::Conflict("Task has running execution processes. Please wait for them to complete or stop them first.".to_string()));
    }

    // Gather task attempts data needed for background cleanup
    let attempts = TaskAttempt::fetch_all(&deployment.db().pool, Some(task.id))
        .await
        .map_err(|e| {
            tracing::error!("Failed to fetch task attempts for task {}: {}", task.id, e);
            ApiError::TaskAttempt(e)
        })?;

    // Gather cleanup data before deletion
    let project = task
        .parent_project(&deployment.db().pool)
        .await?
        .ok_or_else(|| ApiError::Database(SqlxError::RowNotFound))?;

    let cleanup_data: Vec<WorktreeCleanupData> = attempts
        .iter()
        .filter_map(|attempt| {
            attempt
                .container_ref
                .as_ref()
                .map(|worktree_path| WorktreeCleanupData {
                    attempt_id: attempt.id,
                    worktree_path: PathBuf::from(worktree_path),
                    git_repo_path: Some(project.git_repo_path.clone()),
                })
        })
        .collect();

    // Delete task from database (FK CASCADE will handle task_attempts)
    let rows_affected = Task::delete(&deployment.db().pool, task.id).await?;

    if rows_affected == 0 {
        return Err(ApiError::Database(SqlxError::RowNotFound));
    }

    // Spawn background worktree cleanup task
    let task_id = task.id;
    tokio::spawn(async move {
        let span = tracing::info_span!("background_worktree_cleanup", task_id = %task_id);
        let _enter = span.enter();

        tracing::info!(
            "Starting background cleanup for task {} ({} worktrees)",
            task_id,
            cleanup_data.len()
        );

        if let Err(e) = cleanup_worktrees_direct(&cleanup_data).await {
            tracing::error!(
                "Background worktree cleanup failed for task {}: {}",
                task_id,
                e
            );
        } else {
            tracing::info!("Background cleanup completed for task {}", task_id);
        }
    });

    // Return 202 Accepted to indicate deletion was scheduled
    Ok((StatusCode::ACCEPTED, ResponseJson(ApiResponse::success(()))))
}

pub fn router(deployment: &DeploymentImpl) -> Router<DeploymentImpl> {
    let task_id_router = Router::new()
        .route("/", get(get_task).put(update_task).delete(delete_task))
        .layer(from_fn_with_state(deployment.clone(), load_task_middleware));

    let inner = Router::new()
        .route("/", get(get_tasks).post(create_task))
        .route("/stream/ws", get(stream_tasks_ws))
        .route("/create-and-start", post(create_task_and_start))
        .nest("/{task_id}", task_id_router);

    // mount under /projects/:project_id/tasks
    Router::new().nest("/tasks", inner)
}
</file>

<file path="crates/server/src/error.rs">
use axum::{
    Json,
    extract::multipart::MultipartError,
    http::StatusCode,
    response::{IntoResponse, Response},
};
use db::models::{
    execution_process::ExecutionProcessError, project::ProjectError, task_attempt::TaskAttemptError,
};
use deployment::DeploymentError;
use executors::executors::ExecutorError;
use git2::Error as Git2Error;
use services::services::{
    auth::AuthError, config::ConfigError, container::ContainerError, git::GitServiceError,
    github_service::GitHubServiceError, image::ImageError, worktree_manager::WorktreeError,
};
use thiserror::Error;
use utils::response::ApiResponse;

#[derive(Debug, Error, ts_rs::TS)]
#[ts(type = "string")]
pub enum ApiError {
    #[error(transparent)]
    Project(#[from] ProjectError),
    #[error(transparent)]
    TaskAttempt(#[from] TaskAttemptError),
    #[error(transparent)]
    ExecutionProcess(#[from] ExecutionProcessError),
    #[error(transparent)]
    GitService(#[from] GitServiceError),
    #[error(transparent)]
    GitHubService(#[from] GitHubServiceError),
    #[error(transparent)]
    Auth(#[from] AuthError),
    #[error(transparent)]
    Deployment(#[from] DeploymentError),
    #[error(transparent)]
    Container(#[from] ContainerError),
    #[error(transparent)]
    Executor(#[from] ExecutorError),
    #[error(transparent)]
    Database(#[from] sqlx::Error),
    #[error(transparent)]
    Worktree(#[from] WorktreeError),
    #[error(transparent)]
    Config(#[from] ConfigError),
    #[error(transparent)]
    Image(#[from] ImageError),
    #[error("Multipart error: {0}")]
    Multipart(#[from] MultipartError),
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    #[error("Conflict: {0}")]
    Conflict(String),
}

impl From<Git2Error> for ApiError {
    fn from(err: Git2Error) -> Self {
        ApiError::GitService(GitServiceError::from(err))
    }
}

impl IntoResponse for ApiError {
    fn into_response(self) -> Response {
        let (status_code, error_type) = match &self {
            ApiError::Project(_) => (StatusCode::INTERNAL_SERVER_ERROR, "ProjectError"),
            ApiError::TaskAttempt(_) => (StatusCode::INTERNAL_SERVER_ERROR, "TaskAttemptError"),
            ApiError::ExecutionProcess(err) => match err {
                ExecutionProcessError::ExecutionProcessNotFound => {
                    (StatusCode::NOT_FOUND, "ExecutionProcessError")
                }
                _ => (StatusCode::INTERNAL_SERVER_ERROR, "ExecutionProcessError"),
            },
            // Promote certain GitService errors to conflict status with concise messages
            ApiError::GitService(git_err) => match git_err {
                services::services::git::GitServiceError::MergeConflicts(_) => {
                    (StatusCode::CONFLICT, "GitServiceError")
                }
                services::services::git::GitServiceError::RebaseInProgress => {
                    (StatusCode::CONFLICT, "GitServiceError")
                }
                _ => (StatusCode::INTERNAL_SERVER_ERROR, "GitServiceError"),
            },
            ApiError::GitHubService(_) => (StatusCode::INTERNAL_SERVER_ERROR, "GitHubServiceError"),
            ApiError::Auth(_) => (StatusCode::INTERNAL_SERVER_ERROR, "AuthError"),
            ApiError::Deployment(_) => (StatusCode::INTERNAL_SERVER_ERROR, "DeploymentError"),
            ApiError::Container(_) => (StatusCode::INTERNAL_SERVER_ERROR, "ContainerError"),
            ApiError::Executor(_) => (StatusCode::INTERNAL_SERVER_ERROR, "ExecutorError"),
            ApiError::Database(_) => (StatusCode::INTERNAL_SERVER_ERROR, "DatabaseError"),
            ApiError::Worktree(_) => (StatusCode::INTERNAL_SERVER_ERROR, "WorktreeError"),
            ApiError::Config(_) => (StatusCode::INTERNAL_SERVER_ERROR, "ConfigError"),
            ApiError::Image(img_err) => match img_err {
                ImageError::InvalidFormat => (StatusCode::BAD_REQUEST, "InvalidImageFormat"),
                ImageError::TooLarge(_, _) => (StatusCode::PAYLOAD_TOO_LARGE, "ImageTooLarge"),
                ImageError::NotFound => (StatusCode::NOT_FOUND, "ImageNotFound"),
                _ => (StatusCode::INTERNAL_SERVER_ERROR, "ImageError"),
            },
            ApiError::Io(_) => (StatusCode::INTERNAL_SERVER_ERROR, "IoError"),
            ApiError::Multipart(_) => (StatusCode::BAD_REQUEST, "MultipartError"),
            ApiError::Conflict(_) => (StatusCode::CONFLICT, "ConflictError"),
        };

        let error_message = match &self {
            ApiError::Image(img_err) => match img_err {
                ImageError::InvalidFormat => "This file type is not supported. Please upload an image file (PNG, JPG, GIF, WebP, or BMP).".to_string(),
                ImageError::TooLarge(size, max) => format!(
                    "This image is too large ({:.1} MB). Maximum file size is {:.1} MB.",
                    *size as f64 / 1_048_576.0,
                    *max as f64 / 1_048_576.0
                ),
                ImageError::NotFound => "Image not found.".to_string(),
                _ => {
                    "Failed to process image. Please try again.".to_string()
                }
            },
            ApiError::GitService(git_err) => match git_err {
                services::services::git::GitServiceError::MergeConflicts(msg) => msg.clone(),
                services::services::git::GitServiceError::RebaseInProgress => {
                    "A rebase is already in progress. Resolve conflicts or abort the rebase, then retry.".to_string()
                }
                _ => format!("{}: {}", error_type, self),
            },
            ApiError::Multipart(_) => "Failed to upload file. Please ensure the file is valid and try again.".to_string(),
            ApiError::Conflict(msg) => msg.clone(),
            _ => format!("{}: {}", error_type, self),
        };
        let response = ApiResponse::<()>::error(&error_message);
        (status_code, Json(response)).into_response()
    }
}
</file>

<file path="crates/server/src/lib.rs">
pub mod error;
pub mod mcp;
pub mod middleware;
pub mod routes;

// #[cfg(feature = "cloud")]
// type DeploymentImpl = vibe_kanban_cloud::deployment::CloudDeployment;
// #[cfg(not(feature = "cloud"))]
pub type DeploymentImpl = local_deployment::LocalDeployment;
</file>

<file path="crates/server/src/main.rs">
use anyhow::{self, Error as AnyhowError};
use deployment::{Deployment, DeploymentError};
use server::{DeploymentImpl, routes};
use sqlx::Error as SqlxError;
use strip_ansi_escapes::strip;
use thiserror::Error;
use tracing_subscriber::{EnvFilter, prelude::*};
use utils::{
    assets::asset_dir, browser::open_browser, port_file::write_port_file, sentry::sentry_layer,
};

#[derive(Debug, Error)]
pub enum VibeKanbanError {
    #[error(transparent)]
    Io(#[from] std::io::Error),
    #[error(transparent)]
    Sqlx(#[from] SqlxError),
    #[error(transparent)]
    Deployment(#[from] DeploymentError),
    #[error(transparent)]
    Other(#[from] AnyhowError),
}

#[tokio::main]
async fn main() -> Result<(), VibeKanbanError> {
    let log_level = std::env::var("RUST_LOG").unwrap_or_else(|_| "info".to_string());
    let filter_string = format!(
        "warn,server={level},services={level},db={level},executors={level},deployment={level},local_deployment={level},utils={level}",
        level = log_level
    );
    let env_filter = EnvFilter::try_new(filter_string).expect("Failed to create tracing filter");
    tracing_subscriber::registry()
        .with(tracing_subscriber::fmt::layer().with_filter(env_filter))
        .with(sentry_layer())
        .init();

    // Create asset directory if it doesn't exist
    if !asset_dir().exists() {
        std::fs::create_dir_all(asset_dir())?;
    }

    let deployment = DeploymentImpl::new().await?;
    deployment.update_sentry_scope().await?;
    deployment.cleanup_orphan_executions().await?;
    deployment.backfill_before_head_commits().await?;
    deployment.spawn_pr_monitor_service().await;
    deployment
        .track_if_analytics_allowed("session_start", serde_json::json!({}))
        .await;

    // Pre-warm file search cache for most active projects
    let deployment_for_cache = deployment.clone();
    tokio::spawn(async move {
        if let Err(e) = deployment_for_cache
            .file_search_cache()
            .warm_most_active(&deployment_for_cache.db().pool, 3)
            .await
        {
            tracing::warn!("Failed to warm file search cache: {}", e);
        }
    });

    let app_router = routes::router(deployment);

    let port = std::env::var("BACKEND_PORT")
        .or_else(|_| std::env::var("PORT"))
        .ok()
        .and_then(|s| {
            // remove any ANSI codes, then turn into String
            let cleaned =
                String::from_utf8(strip(s.as_bytes())).expect("UTF-8 after stripping ANSI");
            cleaned.trim().parse::<u16>().ok()
        })
        .unwrap_or_else(|| {
            tracing::info!("No PORT environment variable set, using port 0 for auto-assignment");
            0
        }); // Use 0 to find free port if no specific port provided

    let host = std::env::var("HOST").unwrap_or_else(|_| "127.0.0.1".to_string());
    let listener = tokio::net::TcpListener::bind(format!("{host}:{port}")).await?;
    let actual_port = listener.local_addr()?.port(); // get → 53427 (example)

    // Write port file for discovery if prod, warn on fail
    if !cfg!(debug_assertions)
        && let Err(e) = write_port_file(actual_port).await
    {
        tracing::warn!("Failed to write port file: {}", e);
    }

    tracing::info!("Server running on http://{host}:{actual_port}");

    if !cfg!(debug_assertions) {
        tracing::info!("Opening browser...");
        tokio::spawn(async move {
            if let Err(e) = open_browser(&format!("http://127.0.0.1:{actual_port}")).await {
                tracing::warn!(
                    "Failed to open browser automatically: {}. Please open http://127.0.0.1:{} manually.",
                    e,
                    actual_port
                );
            }
        });
    }

    axum::serve(listener, app_router).await?;
    Ok(())
}
</file>

<file path="crates/server/build.rs">
use std::{fs, path::Path};

fn main() {
    dotenv::dotenv().ok();

    if let Ok(api_key) = std::env::var("POSTHOG_API_KEY") {
        println!("cargo:rustc-env=POSTHOG_API_KEY={}", api_key);
    }
    if let Ok(api_endpoint) = std::env::var("POSTHOG_API_ENDPOINT") {
        println!("cargo:rustc-env=POSTHOG_API_ENDPOINT={}", api_endpoint);
    }
    if let Ok(api_key) = std::env::var("GITHUB_APP_ID") {
        println!("cargo:rustc-env=GITHUB_APP_ID={}", api_key);
    }
    if let Ok(api_endpoint) = std::env::var("GITHUB_APP_CLIENT_ID") {
        println!("cargo:rustc-env=GITHUB_APP_CLIENT_ID={}", api_endpoint);
    }

    // Create frontend/dist directory if it doesn't exist
    let dist_path = Path::new("../../frontend/dist");
    if !dist_path.exists() {
        println!("cargo:warning=Creating dummy frontend/dist directory for compilation");
        fs::create_dir_all(dist_path).unwrap();

        // Create a dummy index.html
        let dummy_html = r#"<!DOCTYPE html>
<html><head><title>Build frontend first</title></head>
<body><h1>Please build the frontend</h1></body></html>"#;

        fs::write(dist_path.join("index.html"), dummy_html).unwrap();
    }
}
</file>

<file path="crates/server/Cargo.toml">
[package]
name = "server"
version = "0.0.94"
edition = "2024"
default-run = "server"

[lints.clippy]
uninlined-format-args = "allow"

[dependencies]
deployment = { path = "../deployment" }
executors = { path = "../executors" }
local-deployment = { path = "../local-deployment" }
utils = { path = "../utils" }
db = { path = "../db" }
services = { path = "../services" }
tokio = { workspace = true }
tokio-util = { version = "0.7", features = ["io"] }
axum = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
anyhow = { workspace = true }
tracing = { workspace = true }
tracing-subscriber = { workspace = true }
sqlx = { version = "0.8.6", features = ["runtime-tokio-rustls", "sqlite", "chrono", "uuid"] }
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1.0", features = ["v4", "serde"] }
ts-rs = { workspace = true, features = ["serde-json-impl"]}
async-trait = "0.1"
command-group = { version = "5.0", features = ["with-tokio"] }
nix = { version = "0.29", features = ["signal", "process"] }
openssl-sys = { workspace = true }
rmcp = { version = "0.5.0", features = ["server", "transport-io"] }
schemars = { workspace = true }
regex = "1.11.1"
toml = "0.8"
sentry = { version = "0.41.0", features = ["anyhow", "backtrace", "panic", "debug-images"] }
sentry-tracing = { version = "0.41.0", features = ["backtrace"] }
reqwest = { version = "0.12", features = ["json"] }
strip-ansi-escapes = "0.2.1"
thiserror = { workspace = true }
os_info = "3.12.0"
futures-util = "0.3"
ignore = "0.4"
git2 = "0.18"
mime_guess = "2.0"
rust-embed = "8.2"
octocrab = "0.44"
dirs = "5.0"

[dev-dependencies]
tempfile = "3.8"
tower = { version = "0.4", features = ["util"] }

[build-dependencies]
dotenv = "0.15"
</file>

<file path="crates/services/src/services/config/versions/mod.rs">
pub(super) mod v1;
pub(super) mod v2;
pub(super) mod v3;
pub(super) mod v4;
pub(super) mod v5;
pub(super) mod v6;
</file>

<file path="crates/services/src/services/config/versions/v1.rs">
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub(super) struct Config {
    pub(super) theme: ThemeMode,
    pub(super) executor: ExecutorConfig,
    pub(super) disclaimer_acknowledged: bool,
    pub(super) onboarding_acknowledged: bool,
    pub(super) github_login_acknowledged: bool,
    pub(super) telemetry_acknowledged: bool,
    pub(super) sound_alerts: bool,
    pub(super) sound_file: SoundFile,
    pub(super) push_notifications: bool,
    pub(super) editor: EditorConfig,
    pub(super) github: GitHubConfig,
    pub(super) analytics_enabled: Option<bool>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type", rename_all = "kebab-case")]
pub(super) enum ExecutorConfig {
    Echo,
    Claude,
    ClaudePlan,
    Amp,
    Gemini,
    #[serde(alias = "setup_script")]
    SetupScript {
        script: String,
    },
    ClaudeCodeRouter,
    #[serde(alias = "charmopencode")]
    CharmOpencode,
    #[serde(alias = "opencode")]
    SstOpencode,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub(super) enum ThemeMode {
    Light,
    Dark,
    System,
    Purple,
    Green,
    Blue,
    Orange,
    Red,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub(super) struct EditorConfig {
    pub editor_type: EditorType,
    pub custom_command: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub(super) struct GitHubConfig {
    pub pat: Option<String>,
    pub token: Option<String>,
    pub username: Option<String>,
    pub primary_email: Option<String>,
    pub default_pr_base: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub(super) enum EditorType {
    VsCode,
    Cursor,
    Windsurf,
    IntelliJ,
    Zed,
    Custom,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "kebab-case")]
pub(super) enum SoundFile {
    AbstractSound1,
    AbstractSound2,
    AbstractSound3,
    AbstractSound4,
    CowMooing,
    PhoneVibration,
    Rooster,
}
</file>

<file path="crates/services/src/services/config/versions/v2.rs">
use std::{path::PathBuf, str::FromStr};

use anyhow::Error;
use serde::{Deserialize, Serialize};
use strum_macros::EnumString;
use ts_rs::TS;
use utils::{assets::SoundAssets, cache_dir};

use crate::services::config::versions::v1;

#[derive(Clone, Debug, Serialize, Deserialize, TS)]
pub struct Config {
    pub config_version: String,
    pub theme: ThemeMode,
    pub profile: String,
    pub disclaimer_acknowledged: bool,
    pub onboarding_acknowledged: bool,
    pub github_login_acknowledged: bool,
    pub telemetry_acknowledged: bool,
    pub notifications: NotificationConfig,
    pub editor: EditorConfig,
    pub github: GitHubConfig,
    pub analytics_enabled: Option<bool>,
    pub workspace_dir: Option<String>,
}

impl Config {
    pub fn from_previous_version(raw_config: &str) -> Result<Self, Error> {
        let old_config = match serde_json::from_str::<v1::Config>(raw_config) {
            Ok(cfg) => cfg,
            Err(e) => {
                tracing::error!("❌ Failed to parse config: {}", e);
                tracing::error!("   at line {}, column {}", e.line(), e.column());
                return Err(e.into());
            }
        };

        let old_config_clone = old_config.clone();

        let mut onboarding_acknowledged = old_config.onboarding_acknowledged;

        // Map old executors to new profiles
        let profile: &str = match old_config.executor {
            v1::ExecutorConfig::Claude => "claude-code",
            v1::ExecutorConfig::ClaudeCodeRouter => "claude-code",
            v1::ExecutorConfig::ClaudePlan => "claude-code-plan",
            v1::ExecutorConfig::Amp => "amp",
            v1::ExecutorConfig::Gemini => "gemini",
            v1::ExecutorConfig::SstOpencode => "opencode",
            _ => {
                onboarding_acknowledged = false; // Reset the user's onboarding if executor is not supported
                "claude-code"
            }
        };

        Ok(Self {
            config_version: "v2".to_string(),
            theme: ThemeMode::from(old_config.theme), // Now SCREAMING_SNAKE_CASE
            profile: profile.to_string(),
            disclaimer_acknowledged: old_config.disclaimer_acknowledged,
            onboarding_acknowledged,
            github_login_acknowledged: old_config.github_login_acknowledged,
            telemetry_acknowledged: old_config.telemetry_acknowledged,
            notifications: NotificationConfig::from(old_config_clone),
            editor: EditorConfig::from(old_config.editor),
            github: GitHubConfig::from(old_config.github),
            analytics_enabled: None,
            workspace_dir: None,
        })
    }
}

impl From<String> for Config {
    fn from(raw_config: String) -> Self {
        if let Ok(config) = serde_json::from_str(&raw_config) {
            config
        } else if let Ok(config) = Self::from_previous_version(&raw_config) {
            tracing::info!("Config upgraded from previous version");
            config
        } else {
            tracing::warn!("Config reset to default");
            Self::default()
        }
    }
}

impl Default for Config {
    fn default() -> Self {
        Self {
            config_version: "v2".to_string(),
            theme: ThemeMode::System,
            profile: String::from("claude-code"),
            disclaimer_acknowledged: false,
            onboarding_acknowledged: false,
            github_login_acknowledged: false,
            telemetry_acknowledged: false,
            notifications: NotificationConfig::default(),
            editor: EditorConfig::default(),
            github: GitHubConfig::default(),
            analytics_enabled: None,
            workspace_dir: None,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct GitHubConfig {
    pub pat: Option<String>,
    pub oauth_token: Option<String>,
    pub username: Option<String>,
    pub primary_email: Option<String>,
    pub default_pr_base: Option<String>,
}

impl From<v1::GitHubConfig> for GitHubConfig {
    fn from(old: v1::GitHubConfig) -> Self {
        Self {
            pat: old.pat,
            oauth_token: old.token, // Map to new field name
            username: old.username,
            primary_email: old.primary_email,
            default_pr_base: old.default_pr_base,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct NotificationConfig {
    pub sound_enabled: bool,
    pub push_enabled: bool,
    pub sound_file: SoundFile,
}

impl From<v1::Config> for NotificationConfig {
    fn from(old: v1::Config) -> Self {
        Self {
            sound_enabled: old.sound_alerts,
            push_enabled: old.push_notifications,
            sound_file: SoundFile::from(old.sound_file), // Now SCREAMING_SNAKE_CASE
        }
    }
}

impl Default for NotificationConfig {
    fn default() -> Self {
        Self {
            sound_enabled: true,
            push_enabled: true,
            sound_file: SoundFile::CowMooing,
        }
    }
}

impl Default for GitHubConfig {
    fn default() -> Self {
        Self {
            pat: None,
            oauth_token: None,
            username: None,
            primary_email: None,
            default_pr_base: Some("main".to_string()),
        }
    }
}

impl GitHubConfig {
    pub fn token(&self) -> Option<String> {
        self.pat
            .as_deref()
            .or(self.oauth_token.as_deref())
            .map(|s| s.to_string())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, TS, EnumString)]
#[ts(use_ts_enum)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
#[strum(serialize_all = "SCREAMING_SNAKE_CASE")]
pub enum SoundFile {
    AbstractSound1,
    AbstractSound2,
    AbstractSound3,
    AbstractSound4,
    CowMooing,
    PhoneVibration,
    Rooster,
}

impl SoundFile {
    pub fn to_filename(&self) -> &'static str {
        match self {
            SoundFile::AbstractSound1 => "abstract-sound1.wav",
            SoundFile::AbstractSound2 => "abstract-sound2.wav",
            SoundFile::AbstractSound3 => "abstract-sound3.wav",
            SoundFile::AbstractSound4 => "abstract-sound4.wav",
            SoundFile::CowMooing => "cow-mooing.wav",
            SoundFile::PhoneVibration => "phone-vibration.wav",
            SoundFile::Rooster => "rooster.wav",
        }
    }

    // load the sound file from the embedded assets or cache
    pub async fn serve(&self) -> Result<rust_embed::EmbeddedFile, Error> {
        match SoundAssets::get(self.to_filename()) {
            Some(content) => Ok(content),
            None => {
                tracing::error!("Sound file not found: {}", self.to_filename());
                Err(anyhow::anyhow!(
                    "Sound file not found: {}",
                    self.to_filename()
                ))
            }
        }
    }
    /// Get or create a cached sound file with the embedded sound data
    pub async fn get_path(&self) -> Result<PathBuf, Box<dyn std::error::Error + Send + Sync>> {
        use std::io::Write;

        let filename = self.to_filename();
        let cache_dir = cache_dir();
        let cached_path = cache_dir.join(format!("sound-{filename}"));

        // Check if cached file already exists and is valid
        if cached_path.exists() {
            // Verify file has content (basic validation)
            if let Ok(metadata) = std::fs::metadata(&cached_path)
                && metadata.len() > 0
            {
                return Ok(cached_path);
            }
        }

        // File doesn't exist or is invalid, create it
        let sound_data = SoundAssets::get(filename)
            .ok_or_else(|| format!("Embedded sound file not found: {filename}"))?
            .data;

        // Ensure cache directory exists
        std::fs::create_dir_all(&cache_dir)
            .map_err(|e| format!("Failed to create cache directory: {e}"))?;

        let mut file = std::fs::File::create(&cached_path)
            .map_err(|e| format!("Failed to create cached sound file: {e}"))?;

        file.write_all(&sound_data)
            .map_err(|e| format!("Failed to write sound data to cached file: {e}"))?;

        drop(file); // Ensure file is closed

        Ok(cached_path)
    }
}

impl From<v1::SoundFile> for SoundFile {
    fn from(old: v1::SoundFile) -> Self {
        match old {
            v1::SoundFile::AbstractSound1 => SoundFile::AbstractSound1,
            v1::SoundFile::AbstractSound2 => SoundFile::AbstractSound2,
            v1::SoundFile::AbstractSound3 => SoundFile::AbstractSound3,
            v1::SoundFile::AbstractSound4 => SoundFile::AbstractSound4,
            v1::SoundFile::CowMooing => SoundFile::CowMooing,
            v1::SoundFile::PhoneVibration => SoundFile::PhoneVibration,
            v1::SoundFile::Rooster => SoundFile::Rooster,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct EditorConfig {
    editor_type: EditorType,
    custom_command: Option<String>,
}

impl From<v1::EditorConfig> for EditorConfig {
    fn from(old: v1::EditorConfig) -> Self {
        Self {
            editor_type: EditorType::from(old.editor_type), // Now SCREAMING_SNAKE_CASE
            custom_command: old.custom_command,
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, TS, EnumString)]
#[ts(use_ts_enum)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
#[strum(serialize_all = "SCREAMING_SNAKE_CASE")]
pub enum EditorType {
    VsCode,
    Cursor,
    Windsurf,
    IntelliJ,
    Zed,
    Xcode,
    Custom,
}

impl From<v1::EditorType> for EditorType {
    fn from(old: v1::EditorType) -> Self {
        match old {
            v1::EditorType::VsCode => EditorType::VsCode,
            v1::EditorType::Cursor => EditorType::Cursor,
            v1::EditorType::Windsurf => EditorType::Windsurf,
            v1::EditorType::IntelliJ => EditorType::IntelliJ,
            v1::EditorType::Zed => EditorType::Zed,
            v1::EditorType::Custom => EditorType::Custom,
        }
    }
}

impl Default for EditorConfig {
    fn default() -> Self {
        Self {
            editor_type: EditorType::VsCode,
            custom_command: None,
        }
    }
}

impl EditorConfig {
    pub fn get_command(&self) -> Vec<String> {
        match &self.editor_type {
            EditorType::VsCode => vec!["code".to_string()],
            EditorType::Cursor => vec!["cursor".to_string()],
            EditorType::Windsurf => vec!["windsurf".to_string()],
            EditorType::IntelliJ => vec!["idea".to_string()],
            EditorType::Zed => vec!["zed".to_string()],
            EditorType::Xcode => vec!["xed".to_string()],
            EditorType::Custom => {
                if let Some(custom) = &self.custom_command {
                    custom.split_whitespace().map(|s| s.to_string()).collect()
                } else {
                    vec!["code".to_string()] // fallback to VSCode
                }
            }
        }
    }

    pub fn open_file(&self, path: &str) -> Result<(), std::io::Error> {
        let mut command = self.get_command();

        if command.is_empty() {
            return Err(std::io::Error::new(
                std::io::ErrorKind::InvalidInput,
                "No editor command configured",
            ));
        }

        if cfg!(windows) {
            command[0] =
                utils::shell::resolve_executable_path(&command[0]).ok_or(std::io::Error::new(
                    std::io::ErrorKind::NotFound,
                    format!("Editor command '{}' not found", command[0]),
                ))?;
        }

        let mut cmd = std::process::Command::new(&command[0]);
        for arg in &command[1..] {
            cmd.arg(arg);
        }
        cmd.arg(path);
        cmd.spawn()?;
        Ok(())
    }

    pub fn with_override(&self, editor_type_str: Option<&str>) -> Self {
        if let Some(editor_type_str) = editor_type_str {
            let editor_type =
                EditorType::from_str(editor_type_str).unwrap_or(self.editor_type.clone());
            EditorConfig {
                editor_type,
                custom_command: self.custom_command.clone(),
            }
        } else {
            self.clone()
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, TS, EnumString)]
#[ts(use_ts_enum)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
#[strum(serialize_all = "SCREAMING_SNAKE_CASE")]
pub enum ThemeMode {
    Light,
    Dark,
    System,
    Purple,
    Green,
    Blue,
    Orange,
    Red,
}

impl From<v1::ThemeMode> for ThemeMode {
    fn from(old: v1::ThemeMode) -> Self {
        match old {
            v1::ThemeMode::Light => ThemeMode::Light,
            v1::ThemeMode::Dark => ThemeMode::Dark,
            v1::ThemeMode::System => ThemeMode::System,
            v1::ThemeMode::Purple => ThemeMode::Purple,
            v1::ThemeMode::Green => ThemeMode::Green,
            v1::ThemeMode::Blue => ThemeMode::Blue,
            v1::ThemeMode::Orange => ThemeMode::Orange,
            v1::ThemeMode::Red => ThemeMode::Red,
        }
    }
}
</file>

<file path="crates/services/src/services/config/versions/v3.rs">
use anyhow::Error;
use serde::{Deserialize, Serialize};
use ts_rs::TS;
pub use v2::{EditorConfig, EditorType, GitHubConfig, NotificationConfig, SoundFile, ThemeMode};

use crate::services::config::versions::v2;

#[derive(Clone, Debug, Serialize, Deserialize, TS)]
pub struct Config {
    pub config_version: String,
    pub theme: ThemeMode,
    pub profile: String,
    pub disclaimer_acknowledged: bool,
    pub onboarding_acknowledged: bool,
    pub github_login_acknowledged: bool,
    pub telemetry_acknowledged: bool,
    pub notifications: NotificationConfig,
    pub editor: EditorConfig,
    pub github: GitHubConfig,
    pub analytics_enabled: Option<bool>,
    pub workspace_dir: Option<String>,
}

impl Config {
    pub fn from_previous_version(raw_config: &str) -> Result<Self, Error> {
        let old_config = match serde_json::from_str::<v2::Config>(raw_config) {
            Ok(cfg) => cfg,
            Err(e) => {
                tracing::error!("❌ Failed to parse config: {}", e);
                tracing::error!("   at line {}, column {}", e.line(), e.column());
                return Err(e.into());
            }
        };

        Ok(Self {
            config_version: "v3".to_string(),
            theme: old_config.theme,
            profile: old_config.profile,
            disclaimer_acknowledged: old_config.disclaimer_acknowledged,
            onboarding_acknowledged: old_config.onboarding_acknowledged,
            github_login_acknowledged: old_config.github_login_acknowledged,
            telemetry_acknowledged: false,
            notifications: old_config.notifications,
            editor: old_config.editor,
            github: old_config.github,
            analytics_enabled: old_config.analytics_enabled,
            workspace_dir: old_config.workspace_dir,
        })
    }
}

impl From<String> for Config {
    fn from(raw_config: String) -> Self {
        if let Ok(config) = serde_json::from_str::<Config>(&raw_config)
            && config.config_version == "v3"
        {
            return config;
        }

        match Self::from_previous_version(&raw_config) {
            Ok(config) => {
                tracing::info!("Config upgraded to v3");
                config
            }
            Err(e) => {
                tracing::warn!("Config migration failed: {}, using default", e);
                Self::default()
            }
        }
    }
}

impl Default for Config {
    fn default() -> Self {
        Self {
            config_version: "v3".to_string(),
            theme: ThemeMode::System,
            profile: String::from("claude-code"),
            disclaimer_acknowledged: false,
            onboarding_acknowledged: false,
            github_login_acknowledged: false,
            telemetry_acknowledged: false,
            notifications: NotificationConfig::default(),
            editor: EditorConfig::default(),
            github: GitHubConfig::default(),
            analytics_enabled: None,
            workspace_dir: None,
        }
    }
}
</file>

<file path="crates/services/src/services/config/versions/v4.rs">
use anyhow::Error;
use serde::{Deserialize, Serialize};
use ts_rs::TS;
pub use v3::{EditorConfig, EditorType, GitHubConfig, NotificationConfig, SoundFile, ThemeMode};

use crate::services::config::versions::v3;

// DEPRECATED
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, TS)]
pub struct ProfileVariantLabel {
    pub profile: String,
    pub variant: Option<String>,
}

impl ProfileVariantLabel {
    pub fn default(profile: String) -> Self {
        Self {
            profile,
            variant: None,
        }
    }
    pub fn with_variant(profile: String, mode: String) -> Self {
        Self {
            profile,
            variant: Some(mode),
        }
    }
}

#[derive(Clone, Debug, Serialize, Deserialize, TS)]
pub struct Config {
    pub config_version: String,
    pub theme: ThemeMode,
    pub profile: ProfileVariantLabel,
    pub disclaimer_acknowledged: bool,
    pub onboarding_acknowledged: bool,
    pub github_login_acknowledged: bool,
    pub telemetry_acknowledged: bool,
    pub notifications: NotificationConfig,
    pub editor: EditorConfig,
    pub github: GitHubConfig,
    pub analytics_enabled: Option<bool>,
    pub workspace_dir: Option<String>,
}

impl Config {
    pub fn from_previous_version(raw_config: &str) -> Result<Self, Error> {
        let old_config = match serde_json::from_str::<v3::Config>(raw_config) {
            Ok(cfg) => cfg,
            Err(e) => {
                tracing::error!("❌ Failed to parse config: {}", e);
                tracing::error!("   at line {}, column {}", e.line(), e.column());
                return Err(e.into());
            }
        };
        let mut onboarding_acknowledged = old_config.onboarding_acknowledged;
        let profile = match old_config.profile.as_str() {
            "claude-code" => ProfileVariantLabel::default("claude-code".to_string()),
            "claude-code-plan" => {
                ProfileVariantLabel::with_variant("claude-code".to_string(), "plan".to_string())
            }
            "claude-code-router" => {
                ProfileVariantLabel::with_variant("claude-code".to_string(), "router".to_string())
            }
            "amp" => ProfileVariantLabel::default("amp".to_string()),
            "gemini" => ProfileVariantLabel::default("gemini".to_string()),
            "codex" => ProfileVariantLabel::default("codex".to_string()),
            "opencode" => ProfileVariantLabel::default("opencode".to_string()),
            "qwen-code" => ProfileVariantLabel::default("qwen-code".to_string()),
            _ => {
                onboarding_acknowledged = false; // Reset the user's onboarding if executor is not supported
                ProfileVariantLabel::default("claude-code".to_string())
            }
        };

        Ok(Self {
            config_version: "v4".to_string(),
            theme: old_config.theme,
            profile,
            disclaimer_acknowledged: old_config.disclaimer_acknowledged,
            onboarding_acknowledged,
            github_login_acknowledged: old_config.github_login_acknowledged,
            telemetry_acknowledged: old_config.telemetry_acknowledged,
            notifications: old_config.notifications,
            editor: old_config.editor,
            github: old_config.github,
            analytics_enabled: old_config.analytics_enabled,
            workspace_dir: old_config.workspace_dir,
        })
    }
}

impl From<String> for Config {
    fn from(raw_config: String) -> Self {
        if let Ok(config) = serde_json::from_str::<Config>(&raw_config)
            && config.config_version == "v4"
        {
            return config;
        }

        match Self::from_previous_version(&raw_config) {
            Ok(config) => {
                tracing::info!("Config upgraded to v3");
                config
            }
            Err(e) => {
                tracing::warn!("Config migration failed: {}, using default", e);
                Self::default()
            }
        }
    }
}

impl Default for Config {
    fn default() -> Self {
        Self {
            config_version: "v4".to_string(),
            theme: ThemeMode::System,
            profile: ProfileVariantLabel::default("claude-code".to_string()),
            disclaimer_acknowledged: false,
            onboarding_acknowledged: false,
            github_login_acknowledged: false,
            telemetry_acknowledged: false,
            notifications: NotificationConfig::default(),
            editor: EditorConfig::default(),
            github: GitHubConfig::default(),
            analytics_enabled: None,
            workspace_dir: None,
        }
    }
}
</file>

<file path="crates/services/src/services/config/versions/v5.rs">
use anyhow::Error;
use serde::{Deserialize, Serialize};
use ts_rs::TS;
pub use v4::{EditorConfig, EditorType, GitHubConfig, NotificationConfig, SoundFile, ThemeMode};

use crate::services::config::versions::v4::{self, ProfileVariantLabel};

#[derive(Clone, Debug, Serialize, Deserialize, TS)]
pub struct Config {
    pub config_version: String,
    pub theme: ThemeMode,
    pub profile: ProfileVariantLabel,
    pub disclaimer_acknowledged: bool,
    pub onboarding_acknowledged: bool,
    pub github_login_acknowledged: bool,
    pub telemetry_acknowledged: bool,
    pub notifications: NotificationConfig,
    pub editor: EditorConfig,
    pub github: GitHubConfig,
    pub analytics_enabled: Option<bool>,
    pub workspace_dir: Option<String>,
    pub last_app_version: Option<String>,
    pub show_release_notes: bool,
}

impl Config {
    pub fn from_previous_version(raw_config: &str) -> Result<Self, Error> {
        let old_config = match serde_json::from_str::<v4::Config>(raw_config) {
            Ok(cfg) => cfg,
            Err(e) => {
                tracing::error!("❌ Failed to parse config: {}", e);
                tracing::error!("   at line {}, column {}", e.line(), e.column());
                return Err(e.into());
            }
        };

        Ok(Self {
            config_version: "v5".to_string(),
            theme: old_config.theme,
            profile: old_config.profile,
            disclaimer_acknowledged: old_config.disclaimer_acknowledged,
            onboarding_acknowledged: old_config.onboarding_acknowledged,
            github_login_acknowledged: old_config.github_login_acknowledged,
            telemetry_acknowledged: old_config.telemetry_acknowledged,
            notifications: old_config.notifications,
            editor: old_config.editor,
            github: old_config.github,
            analytics_enabled: old_config.analytics_enabled,
            workspace_dir: old_config.workspace_dir,
            last_app_version: None,
            show_release_notes: false,
        })
    }
}

impl From<String> for Config {
    fn from(raw_config: String) -> Self {
        if let Ok(config) = serde_json::from_str::<Config>(&raw_config)
            && config.config_version == "v5"
        {
            return config;
        }

        match Self::from_previous_version(&raw_config) {
            Ok(config) => {
                tracing::info!("Config upgraded to v5");
                config
            }
            Err(e) => {
                tracing::warn!("Config migration failed: {}, using default", e);
                Self::default()
            }
        }
    }
}

impl Default for Config {
    fn default() -> Self {
        Self {
            config_version: "v5".to_string(),
            theme: ThemeMode::System,
            profile: ProfileVariantLabel::default("claude-code".to_string()),
            disclaimer_acknowledged: false,
            onboarding_acknowledged: false,
            github_login_acknowledged: false,
            telemetry_acknowledged: false,
            notifications: NotificationConfig::default(),
            editor: EditorConfig::default(),
            github: GitHubConfig::default(),
            analytics_enabled: None,
            workspace_dir: None,
            last_app_version: None,
            show_release_notes: false,
        }
    }
}
</file>

<file path="crates/services/src/services/config/versions/v6.rs">
use std::str::FromStr;

use anyhow::Error;
use executors::{executors::BaseCodingAgent, profile::ExecutorProfileId};
use serde::{Deserialize, Serialize};
use ts_rs::TS;
use utils;
pub use v5::{EditorConfig, EditorType, GitHubConfig, NotificationConfig, SoundFile, ThemeMode};

use crate::services::config::versions::v5;

#[derive(Clone, Copy, Debug, Serialize, Deserialize, TS, Default)]
#[ts(export)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
pub enum UiLanguage {
    #[default]
    Browser, // Detect from browser
    En, // Force English
    Ja, // Force Japanese
}

#[derive(Clone, Debug, Serialize, Deserialize, TS)]
pub struct Config {
    pub config_version: String,
    pub theme: ThemeMode,
    pub executor_profile: ExecutorProfileId,
    pub disclaimer_acknowledged: bool,
    pub onboarding_acknowledged: bool,
    pub github_login_acknowledged: bool,
    pub telemetry_acknowledged: bool,
    pub notifications: NotificationConfig,
    pub editor: EditorConfig,
    pub github: GitHubConfig,
    pub analytics_enabled: Option<bool>,
    pub workspace_dir: Option<String>,
    pub last_app_version: Option<String>,
    pub show_release_notes: bool,
    #[serde(default)]
    pub language: UiLanguage,
}

impl Config {
    pub fn from_previous_version(raw_config: &str) -> Result<Self, Error> {
        let old_config = match serde_json::from_str::<v5::Config>(raw_config) {
            Ok(cfg) => cfg,
            Err(e) => {
                tracing::error!("❌ Failed to parse config: {}", e);
                tracing::error!("   at line {}, column {}", e.line(), e.column());
                return Err(e.into());
            }
        };

        // Backup custom profiles.json if it exists (v6 migration may break compatibility)
        let profiles_path = utils::assets::profiles_path();
        if profiles_path.exists() {
            let backup_name = format!(
                "profiles_v5_backup_{}.json",
                std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap()
                    .as_secs()
            );
            let backup_path = profiles_path.parent().unwrap().join(backup_name);

            if let Err(e) = std::fs::rename(&profiles_path, &backup_path) {
                tracing::warn!("Failed to backup profiles.json: {}", e);
            } else {
                tracing::info!("Custom profiles.json backed up to {:?}", backup_path);
                tracing::info!("Please review your custom profiles after migration to v6");
            }
        }

        // Validate and convert ProfileVariantLabel
        let old_coding_agent = old_config.profile.profile.to_uppercase();
        let base_coding_agent =
            BaseCodingAgent::from_str(&old_coding_agent).unwrap_or(BaseCodingAgent::ClaudeCode);
        let executor_profile = ExecutorProfileId::new(base_coding_agent);

        Ok(Self {
            config_version: "v6".to_string(),
            theme: old_config.theme,
            executor_profile,
            disclaimer_acknowledged: old_config.disclaimer_acknowledged,
            onboarding_acknowledged: old_config.onboarding_acknowledged,
            github_login_acknowledged: old_config.github_login_acknowledged,
            telemetry_acknowledged: old_config.telemetry_acknowledged,
            notifications: old_config.notifications,
            editor: old_config.editor,
            github: old_config.github,
            analytics_enabled: old_config.analytics_enabled,
            workspace_dir: old_config.workspace_dir,
            last_app_version: old_config.last_app_version,
            show_release_notes: old_config.show_release_notes,
            language: UiLanguage::default(),
        })
    }
}

impl From<String> for Config {
    fn from(raw_config: String) -> Self {
        if let Ok(config) = serde_json::from_str::<Config>(&raw_config)
            && config.config_version == "v6"
        {
            return config;
        }

        match Self::from_previous_version(&raw_config) {
            Ok(config) => {
                tracing::info!("Config upgraded to v6");
                config
            }
            Err(e) => {
                tracing::warn!("Config migration failed: {}, using default", e);
                Self::default()
            }
        }
    }
}

impl Default for Config {
    fn default() -> Self {
        Self {
            config_version: "v6".to_string(),
            theme: ThemeMode::System,
            executor_profile: ExecutorProfileId::new(BaseCodingAgent::ClaudeCode),
            disclaimer_acknowledged: false,
            onboarding_acknowledged: false,
            github_login_acknowledged: false,
            telemetry_acknowledged: false,
            notifications: NotificationConfig::default(),
            editor: EditorConfig::default(),
            github: GitHubConfig::default(),
            analytics_enabled: None,
            workspace_dir: None,
            last_app_version: None,
            show_release_notes: false,
            language: UiLanguage::default(),
        }
    }
}
</file>

<file path="crates/services/src/services/config/mod.rs">
use std::path::PathBuf;

use thiserror::Error;

mod versions;

#[derive(Debug, Error)]
pub enum ConfigError {
    #[error(transparent)]
    Io(#[from] std::io::Error),
    #[error(transparent)]
    Json(#[from] serde_json::Error),
    #[error("Validation error: {0}")]
    ValidationError(String),
}

pub type Config = versions::v6::Config;
pub type NotificationConfig = versions::v6::NotificationConfig;
pub type EditorConfig = versions::v6::EditorConfig;
pub type ThemeMode = versions::v6::ThemeMode;
pub type SoundFile = versions::v6::SoundFile;
pub type EditorType = versions::v6::EditorType;
pub type GitHubConfig = versions::v6::GitHubConfig;
pub type UiLanguage = versions::v6::UiLanguage;

/// Will always return config, trying old schemas or eventually returning default
pub async fn load_config_from_file(config_path: &PathBuf) -> Config {
    match std::fs::read_to_string(config_path) {
        Ok(raw_config) => Config::from(raw_config),
        Err(_) => {
            tracing::info!("No config file found, creating one");
            Config::default()
        }
    }
}

/// Saves the config to the given path
pub async fn save_config_to_file(
    config: &Config,
    config_path: &PathBuf,
) -> Result<(), ConfigError> {
    let raw_config = serde_json::to_string_pretty(config)?;
    std::fs::write(config_path, raw_config)?;
    Ok(())
}
</file>

<file path="crates/services/src/services/analytics.rs">
use std::{
    collections::hash_map::DefaultHasher,
    hash::{Hash, Hasher},
    time::Duration,
};

use os_info;
use serde_json::{Value, json};

#[derive(Debug, Clone)]
pub struct AnalyticsContext {
    pub user_id: String,
    pub analytics_service: AnalyticsService,
}

#[derive(Debug, Clone)]
pub struct AnalyticsConfig {
    pub posthog_api_key: String,
    pub posthog_api_endpoint: String,
}

impl AnalyticsConfig {
    pub fn new() -> Option<Self> {
        let api_key = option_env!("POSTHOG_API_KEY")
            .map(|s| s.to_string())
            .or_else(|| std::env::var("POSTHOG_API_KEY").ok())?;
        let api_endpoint = option_env!("POSTHOG_API_ENDPOINT")
            .map(|s| s.to_string())
            .or_else(|| std::env::var("POSTHOG_API_ENDPOINT").ok())?;

        Some(Self {
            posthog_api_key: api_key,
            posthog_api_endpoint: api_endpoint,
        })
    }
}

#[derive(Clone, Debug)]
pub struct AnalyticsService {
    config: AnalyticsConfig,
    client: reqwest::Client,
}

impl AnalyticsService {
    pub fn new(config: AnalyticsConfig) -> Self {
        let client = reqwest::Client::builder()
            .timeout(Duration::from_secs(30))
            .build()
            .unwrap();

        Self { config, client }
    }

    pub fn track_event(&self, user_id: &str, event_name: &str, properties: Option<Value>) {
        let endpoint = format!(
            "{}/capture/",
            self.config.posthog_api_endpoint.trim_end_matches('/')
        );

        let mut payload = json!({
            "api_key": self.config.posthog_api_key,
            "event": event_name,
            "distinct_id": user_id,
        });
        if event_name == "$identify" {
            // For $identify, set person properties in $set
            if let Some(props) = properties {
                payload["$set"] = props;
            }
        } else {
            // For other events, use properties as before
            let mut event_properties = properties.unwrap_or_else(|| json!({}));
            if let Some(props) = event_properties.as_object_mut() {
                props.insert(
                    "timestamp".to_string(),
                    json!(chrono::Utc::now().to_rfc3339()),
                );
                props.insert("version".to_string(), json!(env!("CARGO_PKG_VERSION")));
                props.insert("device".to_string(), get_device_info());
            }
            payload["properties"] = event_properties;
        }

        let client = self.client.clone();
        let event_name = event_name.to_string();

        tokio::spawn(async move {
            match client
                .post(&endpoint)
                .header("Content-Type", "application/json")
                .json(&payload)
                .send()
                .await
            {
                Ok(response) => {
                    if response.status().is_success() {
                        tracing::debug!("Event '{}' sent successfully", event_name);
                    } else {
                        let status = response.status();
                        let response_text = response.text().await.unwrap_or_default();
                        tracing::error!(
                            "Failed to send event. Status: {}. Response: {}",
                            status,
                            response_text
                        );
                    }
                }
                Err(e) => {
                    tracing::error!("Error sending event '{}': {}", event_name, e);
                }
            }
        });
    }
}

/// Generates a consistent, anonymous user ID for npm package telemetry.
/// Returns a hex string prefixed with "npm_user_"
pub fn generate_user_id() -> String {
    let mut hasher = DefaultHasher::new();

    #[cfg(target_os = "macos")]
    {
        // Use ioreg to get hardware UUID
        if let Ok(output) = std::process::Command::new("ioreg")
            .args(["-rd1", "-c", "IOPlatformExpertDevice"])
            .output()
        {
            let stdout = String::from_utf8_lossy(&output.stdout);
            if let Some(line) = stdout.lines().find(|l| l.contains("IOPlatformUUID")) {
                line.hash(&mut hasher);
            }
        }
    }

    #[cfg(target_os = "linux")]
    {
        if let Ok(machine_id) = std::fs::read_to_string("/etc/machine-id") {
            machine_id.trim().hash(&mut hasher);
        }
    }

    #[cfg(target_os = "windows")]
    {
        // Use PowerShell to get machine GUID from registry
        if let Ok(output) = std::process::Command::new("powershell")
            .args(&[
                "-NoProfile",
                "-Command",
                "(Get-ItemProperty -Path 'HKLM:\\SOFTWARE\\Microsoft\\Cryptography').MachineGuid",
            ])
            .output()
        {
            if output.status.success() {
                output.stdout.hash(&mut hasher);
            }
        }
    }

    // Add username for per-user differentiation
    if let Ok(user) = std::env::var("USER").or_else(|_| std::env::var("USERNAME")) {
        user.hash(&mut hasher);
    }

    // Add home directory for additional entropy
    if let Ok(home) = std::env::var("HOME").or_else(|_| std::env::var("USERPROFILE")) {
        home.hash(&mut hasher);
    }

    format!("npm_user_{:016x}", hasher.finish())
}

fn get_device_info() -> Value {
    let info = os_info::get();

    json!({
        "os_type": info.os_type().to_string(),
        "os_version": info.version().to_string(),
        "architecture": info.architecture().unwrap_or("unknown").to_string(),
        "bitness": info.bitness().to_string(),
    })
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_generate_user_id_format() {
        let id = generate_user_id();
        assert!(id.starts_with("npm_user_"));
        assert_eq!(id.len(), 25);
    }

    #[test]
    fn test_consistency() {
        let id1 = generate_user_id();
        let id2 = generate_user_id();
        assert_eq!(id1, id2, "ID should be consistent across calls");
    }
}
</file>

<file path="crates/services/src/services/auth.rs">
use std::sync::Arc;

use anyhow::Error as AnyhowError;
use axum::http::{HeaderName, header::ACCEPT};
use octocrab::{
    OctocrabBuilder,
    auth::{Continue, DeviceCodes, OAuth},
};
use secrecy::{ExposeSecret, SecretString};
use serde::{Deserialize, Serialize};
use thiserror::Error;
use tokio::sync::RwLock;
use ts_rs::TS;

#[derive(Clone)]
pub struct AuthService {
    pub client_id: String,
    pub device_codes: Arc<RwLock<Option<DeviceCodes>>>,
}

#[derive(Debug, Error)]
pub enum AuthError {
    #[error(transparent)]
    GitHubClient(#[from] octocrab::Error),
    #[error(transparent)]
    Parse(#[from] serde_json::Error),
    #[error("Device flow not started")]
    DeviceFlowNotStarted,
    #[error("Device flow pending")]
    Pending(Continue),
    #[error(transparent)]
    Other(#[from] AnyhowError),
}

#[derive(Serialize, Deserialize, TS)]
pub struct DeviceFlowStartResponse {
    pub user_code: String,
    pub verification_uri: String,
    pub expires_in: u32,
    pub interval: u32,
}

pub struct UserInfo {
    pub username: String,
    pub primary_email: Option<String>,
    pub token: String,
}

#[derive(Deserialize)]
pub struct GitHubEmailEntry {
    pub email: String,
    pub primary: bool,
}

impl Default for AuthService {
    fn default() -> Self {
        Self::new()
    }
}

impl AuthService {
    pub fn new() -> Self {
        let client_id_str = option_env!("GITHUB_CLIENT_ID").unwrap_or("Ov23li9bxz3kKfPOIsGm");
        AuthService {
            client_id: client_id_str.to_string(),
            device_codes: Arc::new(RwLock::new(None)), // Initially no device codes
        }
    }

    pub async fn device_start(&self) -> Result<DeviceFlowStartResponse, AuthError> {
        let client = OctocrabBuilder::new()
            .base_uri("https://github.com")?
            .add_header(ACCEPT, "application/json".to_string())
            .build()?;
        let device_codes = client
            .authenticate_as_device(
                &SecretString::from(self.client_id.clone()),
                ["user:email", "repo"],
            )
            .await?;
        self.device_codes
            .write()
            .await
            .replace(device_codes.clone()); // Store the device codes for later polling
        Ok(DeviceFlowStartResponse {
            user_code: device_codes.user_code,
            verification_uri: device_codes.verification_uri,
            expires_in: device_codes.expires_in as u32,
            interval: device_codes.interval as u32,
        })
    }

    pub async fn device_poll(&self) -> Result<UserInfo, AuthError> {
        let device_codes = {
            let guard = self.device_codes.read().await;
            guard
                .as_ref()
                .ok_or(AuthError::DeviceFlowNotStarted)?
                .clone()
        };
        let client = OctocrabBuilder::new()
            .base_uri("https://github.com")?
            .add_header(ACCEPT, "application/json".to_string())
            .build()?;
        let poll_response = device_codes
            .poll_once(&client, &SecretString::from(self.client_id.clone()))
            .await?;
        let access_token = poll_response.either(
            |OAuth { access_token, .. }| Ok(access_token),
            |c| Err(AuthError::Pending(c)),
        )?;
        let client = OctocrabBuilder::new()
            .add_header(
                HeaderName::try_from("User-Agent").unwrap(),
                "vibe-kanban-app".to_string(),
            )
            .personal_token(access_token.clone())
            .build()?;
        let user = client.current().user().await?;
        let emails: Vec<GitHubEmailEntry> = client.get("/user/emails", None::<&()>).await?;
        let primary_email = emails
            .iter()
            .find(|entry| entry.primary)
            .map(|entry| entry.email.clone());
        Ok(UserInfo {
            username: user.login,
            primary_email,
            token: access_token.expose_secret().to_string(),
        })
    }
}
</file>

<file path="crates/services/src/services/container.rs">
use std::{
    collections::HashMap,
    path::{Path, PathBuf},
    sync::Arc,
};

use anyhow::{Error as AnyhowError, anyhow};
use async_trait::async_trait;
use axum::response::sse::Event;
use db::{
    DBService,
    models::{
        execution_process::{
            CreateExecutionProcess, ExecutionContext, ExecutionProcess, ExecutionProcessRunReason,
            ExecutionProcessStatus,
        },
        execution_process_logs::ExecutionProcessLogs,
        executor_session::{CreateExecutorSession, ExecutorSession},
        task::{Task, TaskStatus},
        task_attempt::{TaskAttempt, TaskAttemptError},
    },
};
use executors::{
    actions::{
        ExecutorAction, ExecutorActionType,
        coding_agent_initial::CodingAgentInitialRequest,
        script::{ScriptContext, ScriptRequest, ScriptRequestLanguage},
    },
    executors::{ExecutorError, StandardCodingAgentExecutor},
    profile::{ExecutorConfigs, ExecutorProfileId},
};
use futures::{StreamExt, future};
use sqlx::Error as SqlxError;
use thiserror::Error;
use tokio::{sync::RwLock, task::JoinHandle};
use utils::{log_msg::LogMsg, msg_store::MsgStore};
use uuid::Uuid;

use crate::services::{
    git::{GitService, GitServiceError},
    image::ImageService,
    worktree_manager::{WorktreeError, WorktreeManager},
};
pub type ContainerRef = String;

/// Data needed for background worktree cleanup (doesn't require DB access)
#[derive(Debug, Clone)]
pub struct WorktreeCleanupData {
    pub attempt_id: Uuid,
    pub worktree_path: PathBuf,
    pub git_repo_path: Option<PathBuf>,
}

/// Cleanup worktrees without requiring database access
pub async fn cleanup_worktrees_direct(data: &[WorktreeCleanupData]) -> Result<(), ContainerError> {
    for cleanup_data in data {
        tracing::debug!(
            "Cleaning up worktree for attempt {}: {:?}",
            cleanup_data.attempt_id,
            cleanup_data.worktree_path
        );

        if let Err(e) = WorktreeManager::cleanup_worktree(
            &cleanup_data.worktree_path,
            cleanup_data.git_repo_path.as_deref(),
        )
        .await
        {
            tracing::error!(
                "Failed to cleanup worktree for task attempt {}: {}",
                cleanup_data.attempt_id,
                e
            );
            // Continue with other cleanups even if one fails
        }
    }
    Ok(())
}

#[derive(Debug, Error)]
pub enum ContainerError {
    #[error(transparent)]
    GitServiceError(#[from] GitServiceError),
    #[error(transparent)]
    Sqlx(#[from] SqlxError),
    #[error(transparent)]
    ExecutorError(#[from] ExecutorError),
    #[error(transparent)]
    Worktree(#[from] WorktreeError),
    #[error("Io error: {0}")]
    Io(#[from] std::io::Error),
    #[error("Failed to kill process: {0}")]
    KillFailed(std::io::Error),
    #[error(transparent)]
    TaskAttemptError(#[from] TaskAttemptError),
    #[error(transparent)]
    Other(#[from] AnyhowError), // Catches any unclassified errors
}

#[async_trait]
pub trait ContainerService {
    fn msg_stores(&self) -> &Arc<RwLock<HashMap<Uuid, Arc<MsgStore>>>>;

    fn db(&self) -> &DBService;

    fn git(&self) -> &GitService;

    fn task_attempt_to_current_dir(&self, task_attempt: &TaskAttempt) -> PathBuf;

    async fn create(&self, task_attempt: &TaskAttempt) -> Result<ContainerRef, ContainerError>;

    async fn delete(&self, task_attempt: &TaskAttempt) -> Result<(), ContainerError> {
        self.try_stop(task_attempt).await;
        self.delete_inner(task_attempt).await
    }

    /// Check if a task has any running execution processes
    async fn has_running_processes(&self, task_id: Uuid) -> Result<bool, ContainerError> {
        let attempts = TaskAttempt::fetch_all(&self.db().pool, Some(task_id)).await?;

        for attempt in attempts {
            if let Ok(processes) =
                ExecutionProcess::find_by_task_attempt_id(&self.db().pool, attempt.id, false).await
            {
                for process in processes {
                    if process.status == ExecutionProcessStatus::Running {
                        return Ok(true);
                    }
                }
            }
        }

        Ok(false)
    }

    /// Stop execution processes for task attempts without cleanup
    async fn stop_task_processes(
        &self,
        task_attempts: &[TaskAttempt],
    ) -> Result<(), ContainerError> {
        for attempt in task_attempts {
            self.try_stop(attempt).await;
        }
        Ok(())
    }

    async fn try_stop(&self, task_attempt: &TaskAttempt) {
        // stop all execution processes for this attempt
        if let Ok(processes) =
            ExecutionProcess::find_by_task_attempt_id(&self.db().pool, task_attempt.id, false).await
        {
            for process in processes {
                if process.status == ExecutionProcessStatus::Running {
                    self.stop_execution(&process).await.unwrap_or_else(|e| {
                        tracing::debug!(
                            "Failed to stop execution process {} for task attempt {}: {}",
                            process.id,
                            task_attempt.id,
                            e
                        );
                    });
                }
            }
        }
    }

    async fn delete_inner(&self, task_attempt: &TaskAttempt) -> Result<(), ContainerError>;

    async fn ensure_container_exists(
        &self,
        task_attempt: &TaskAttempt,
    ) -> Result<ContainerRef, ContainerError>;
    async fn is_container_clean(&self, task_attempt: &TaskAttempt) -> Result<bool, ContainerError>;

    async fn start_execution_inner(
        &self,
        task_attempt: &TaskAttempt,
        execution_process: &ExecutionProcess,
        executor_action: &ExecutorAction,
    ) -> Result<(), ContainerError>;

    async fn stop_execution(
        &self,
        execution_process: &ExecutionProcess,
    ) -> Result<(), ContainerError>;

    async fn try_commit_changes(&self, ctx: &ExecutionContext) -> Result<bool, ContainerError>;

    async fn copy_project_files(
        &self,
        source_dir: &Path,
        target_dir: &Path,
        copy_files: &str,
    ) -> Result<(), ContainerError>;

    async fn get_diff(
        &self,
        task_attempt: &TaskAttempt,
    ) -> Result<futures::stream::BoxStream<'static, Result<Event, std::io::Error>>, ContainerError>;

    /// Fetch the MsgStore for a given execution ID, panicking if missing.
    async fn get_msg_store_by_id(&self, uuid: &Uuid) -> Option<Arc<MsgStore>> {
        let map = self.msg_stores().read().await;
        map.get(uuid).cloned()
    }

    async fn stream_raw_logs(
        &self,
        id: &Uuid,
    ) -> Option<futures::stream::BoxStream<'static, Result<LogMsg, std::io::Error>>> {
        if let Some(store) = self.get_msg_store_by_id(id).await {
            // First try in-memory store
            return Some(
                store
                    .history_plus_stream()
                    .filter(|msg| {
                        future::ready(matches!(
                            msg,
                            Ok(LogMsg::Stdout(..) | LogMsg::Stderr(..) | LogMsg::Finished)
                        ))
                    })
                    .boxed(),
            );
        } else {
            // Fallback: load from DB and create direct stream
            let logs_record =
                match ExecutionProcessLogs::find_by_execution_id(&self.db().pool, *id).await {
                    Ok(Some(record)) => record,
                    Ok(None) => return None, // No logs exist
                    Err(e) => {
                        tracing::error!("Failed to fetch logs for execution {}: {}", id, e);
                        return None;
                    }
                };

            let messages = match logs_record.parse_logs() {
                Ok(msgs) => msgs,
                Err(e) => {
                    tracing::error!("Failed to parse logs for execution {}: {}", id, e);
                    return None;
                }
            };

            // Direct stream from parsed messages
            let stream = futures::stream::iter(
                messages
                    .into_iter()
                    .filter(|m| matches!(m, LogMsg::Stdout(_) | LogMsg::Stderr(_)))
                    .chain(std::iter::once(LogMsg::Finished))
                    .map(Ok::<_, std::io::Error>),
            )
            .boxed();

            Some(stream)
        }
    }

    async fn stream_normalized_logs(
        &self,
        id: &Uuid,
    ) -> Option<futures::stream::BoxStream<'static, Result<LogMsg, std::io::Error>>> {
        // First try in-memory store (existing behavior)
        if let Some(store) = self.get_msg_store_by_id(id).await {
            Some(
                store
                    .history_plus_stream() // BoxStream<Result<LogMsg, io::Error>>
                    .filter(|msg| future::ready(matches!(msg, Ok(LogMsg::JsonPatch(..)))))
                    .chain(futures::stream::once(async {
                        Ok::<_, std::io::Error>(LogMsg::Finished)
                    }))
                    .boxed(),
            )
        } else {
            // Fallback: load from DB and normalize
            let logs_record =
                match ExecutionProcessLogs::find_by_execution_id(&self.db().pool, *id).await {
                    Ok(Some(record)) => record,
                    Ok(None) => return None, // No logs exist
                    Err(e) => {
                        tracing::error!("Failed to fetch logs for execution {}: {}", id, e);
                        return None;
                    }
                };

            let raw_messages = match logs_record.parse_logs() {
                Ok(msgs) => msgs,
                Err(e) => {
                    tracing::error!("Failed to parse logs for execution {}: {}", id, e);
                    return None;
                }
            };

            // Create temporary store and populate
            let temp_store = Arc::new(MsgStore::new());
            for msg in raw_messages {
                if matches!(msg, LogMsg::Stdout(_) | LogMsg::Stderr(_)) {
                    temp_store.push(msg);
                }
            }
            temp_store.push_finished();

            let process = match ExecutionProcess::find_by_id(&self.db().pool, *id).await {
                Ok(Some(process)) => process,
                Ok(None) => {
                    tracing::error!("No execution process found for ID: {}", id);
                    return None;
                }
                Err(e) => {
                    tracing::error!("Failed to fetch execution process {}: {}", id, e);
                    return None;
                }
            };

            // Get the task attempt to determine correct directory
            let task_attempt = match process.parent_task_attempt(&self.db().pool).await {
                Ok(Some(task_attempt)) => task_attempt,
                Ok(None) => {
                    tracing::error!("No task attempt found for ID: {}", process.task_attempt_id);
                    return None;
                }
                Err(e) => {
                    tracing::error!(
                        "Failed to fetch task attempt {}: {}",
                        process.task_attempt_id,
                        e
                    );
                    return None;
                }
            };

            if let Err(err) = self.ensure_container_exists(&task_attempt).await {
                tracing::warn!(
                    "Failed to recreate worktree before log normalization for task attempt {}: {}",
                    task_attempt.id,
                    err
                );
            }

            let current_dir = self.task_attempt_to_current_dir(&task_attempt);

            let executor_action = if let Ok(executor_action) = process.executor_action() {
                executor_action
            } else {
                tracing::error!(
                    "Failed to parse executor action: {:?}",
                    process.executor_action()
                );
                return None;
            };

            // Spawn normalizer on populated store
            match executor_action.typ() {
                ExecutorActionType::CodingAgentInitialRequest(request) => {
                    let executor = ExecutorConfigs::get_cached()
                        .get_coding_agent_or_default(&request.executor_profile_id);
                    executor.normalize_logs(temp_store.clone(), &current_dir);
                }
                ExecutorActionType::CodingAgentFollowUpRequest(request) => {
                    let executor = ExecutorConfigs::get_cached()
                        .get_coding_agent_or_default(&request.executor_profile_id);
                    executor.normalize_logs(temp_store.clone(), &current_dir);
                }
                _ => {
                    tracing::debug!(
                        "Executor action doesn't support log normalization: {:?}",
                        process.executor_action()
                    );
                    return None;
                }
            }
            Some(
                temp_store
                    .history_plus_stream()
                    .filter(|msg| future::ready(matches!(msg, Ok(LogMsg::JsonPatch(..)))))
                    .chain(futures::stream::once(async {
                        Ok::<_, std::io::Error>(LogMsg::Finished)
                    }))
                    .boxed(),
            )
        }
    }

    fn spawn_stream_raw_logs_to_db(&self, execution_id: &Uuid) -> JoinHandle<()> {
        let execution_id = *execution_id;
        let msg_stores = self.msg_stores().clone();
        let db = self.db().clone();

        tokio::spawn(async move {
            // Get the message store for this execution
            let store = {
                let map = msg_stores.read().await;
                map.get(&execution_id).cloned()
            };

            if let Some(store) = store {
                let mut stream = store.history_plus_stream();

                while let Some(Ok(msg)) = stream.next().await {
                    match &msg {
                        LogMsg::Stdout(_) | LogMsg::Stderr(_) => {
                            // Serialize this individual message as a JSONL line
                            match serde_json::to_string(&msg) {
                                Ok(jsonl_line) => {
                                    let jsonl_line_with_newline = format!("{jsonl_line}\n");

                                    // Append this line to the database
                                    if let Err(e) = ExecutionProcessLogs::append_log_line(
                                        &db.pool,
                                        execution_id,
                                        &jsonl_line_with_newline,
                                    )
                                    .await
                                    {
                                        tracing::error!(
                                            "Failed to append log line for execution {}: {}",
                                            execution_id,
                                            e
                                        );
                                    }
                                }
                                Err(e) => {
                                    tracing::error!(
                                        "Failed to serialize log message for execution {}: {}",
                                        execution_id,
                                        e
                                    );
                                }
                            }
                        }
                        LogMsg::SessionId(session_id) => {
                            // Append this line to the database
                            if let Err(e) = ExecutorSession::update_session_id(
                                &db.pool,
                                execution_id,
                                session_id,
                            )
                            .await
                            {
                                tracing::error!(
                                    "Failed to update session_id {} for execution process {}: {}",
                                    session_id,
                                    execution_id,
                                    e
                                );
                            }
                        }
                        LogMsg::Finished => {
                            break;
                        }
                        LogMsg::JsonPatch(_) => continue,
                    }
                }
            }
        })
    }

    async fn start_attempt(
        &self,
        task_attempt: &TaskAttempt,
        executor_profile_id: ExecutorProfileId,
    ) -> Result<ExecutionProcess, ContainerError> {
        // Create container
        self.create(task_attempt).await?;

        // Get parent task
        let task = task_attempt
            .parent_task(&self.db().pool)
            .await?
            .ok_or(SqlxError::RowNotFound)?;

        // Get parent project
        let project = task
            .parent_project(&self.db().pool)
            .await?
            .ok_or(SqlxError::RowNotFound)?;

        // // Get latest version of task attempt
        let task_attempt = TaskAttempt::find_by_id(&self.db().pool, task_attempt.id)
            .await?
            .ok_or(SqlxError::RowNotFound)?;

        // TODO: this implementation will not work in cloud
        let worktree_path = PathBuf::from(
            task_attempt
                .container_ref
                .as_ref()
                .ok_or_else(|| ContainerError::Other(anyhow!("Container ref not found")))?,
        );
        let prompt = ImageService::canonicalise_image_paths(&task.to_prompt(), &worktree_path);

        let cleanup_action = project.cleanup_script.map(|script| {
            Box::new(ExecutorAction::new(
                ExecutorActionType::ScriptRequest(ScriptRequest {
                    script,
                    language: ScriptRequestLanguage::Bash,
                    context: ScriptContext::CleanupScript,
                }),
                None,
            ))
        });

        // Choose whether to execute the setup_script or coding agent first
        let execution_process = if let Some(setup_script) = project.setup_script {
            let executor_action = ExecutorAction::new(
                ExecutorActionType::ScriptRequest(ScriptRequest {
                    script: setup_script,
                    language: ScriptRequestLanguage::Bash,
                    context: ScriptContext::SetupScript,
                }),
                // once the setup script is done, run the initial coding agent request
                Some(Box::new(ExecutorAction::new(
                    ExecutorActionType::CodingAgentInitialRequest(CodingAgentInitialRequest {
                        prompt,
                        executor_profile_id: executor_profile_id.clone(),
                    }),
                    cleanup_action,
                ))),
            );

            self.start_execution(
                &task_attempt,
                &executor_action,
                &ExecutionProcessRunReason::SetupScript,
            )
            .await?
        } else {
            let executor_action = ExecutorAction::new(
                ExecutorActionType::CodingAgentInitialRequest(CodingAgentInitialRequest {
                    prompt,
                    executor_profile_id: executor_profile_id.clone(),
                }),
                cleanup_action,
            );

            self.start_execution(
                &task_attempt,
                &executor_action,
                &ExecutionProcessRunReason::CodingAgent,
            )
            .await?
        };
        Ok(execution_process)
    }

    async fn start_execution(
        &self,
        task_attempt: &TaskAttempt,
        executor_action: &ExecutorAction,
        run_reason: &ExecutionProcessRunReason,
    ) -> Result<ExecutionProcess, ContainerError> {
        // Update task status to InProgress when starting an attempt
        let task = task_attempt
            .parent_task(&self.db().pool)
            .await?
            .ok_or(SqlxError::RowNotFound)?;
        if task.status != TaskStatus::InProgress
            && run_reason != &ExecutionProcessRunReason::DevServer
        {
            Task::update_status(&self.db().pool, task.id, TaskStatus::InProgress).await?;
        }
        // Create new execution process record
        // Capture current HEAD as the "before" commit for this execution
        let before_head_commit = {
            if let Some(container_ref) = &task_attempt.container_ref {
                let wt = std::path::Path::new(container_ref);
                self.git().get_head_info(wt).ok().map(|h| h.oid)
            } else {
                None
            }
        };
        let create_execution_process = CreateExecutionProcess {
            task_attempt_id: task_attempt.id,
            executor_action: executor_action.clone(),
            run_reason: run_reason.clone(),
        };

        let execution_process = ExecutionProcess::create(
            &self.db().pool,
            &create_execution_process,
            Uuid::new_v4(),
            before_head_commit.as_deref(),
        )
        .await?;

        if let Some(prompt) = match executor_action.typ() {
            ExecutorActionType::CodingAgentInitialRequest(coding_agent_request) => {
                Some(coding_agent_request.prompt.clone())
            }
            ExecutorActionType::CodingAgentFollowUpRequest(follow_up_request) => {
                Some(follow_up_request.prompt.clone())
            }
            _ => None,
        } {
            let create_executor_data = CreateExecutorSession {
                task_attempt_id: task_attempt.id,
                execution_process_id: execution_process.id,
                prompt: Some(prompt),
            };

            let executor_session_record_id = Uuid::new_v4();

            ExecutorSession::create(
                &self.db().pool,
                &create_executor_data,
                executor_session_record_id,
            )
            .await?;
        }

        let _ = self
            .start_execution_inner(task_attempt, &execution_process, executor_action)
            .await?;

        // Start processing normalised logs for executor requests and follow ups
        match executor_action.typ() {
            ExecutorActionType::CodingAgentInitialRequest(request) => {
                if let Some(msg_store) = self.get_msg_store_by_id(&execution_process.id).await {
                    if let Some(executor) =
                        ExecutorConfigs::get_cached().get_coding_agent(&request.executor_profile_id)
                    {
                        executor.normalize_logs(
                            msg_store,
                            &self.task_attempt_to_current_dir(task_attempt),
                        );
                    } else {
                        tracing::error!(
                            "Failed to resolve profile '{:?}' for normalization",
                            request.executor_profile_id
                        );
                    }
                }
            }
            ExecutorActionType::CodingAgentFollowUpRequest(request) => {
                if let Some(msg_store) = self.get_msg_store_by_id(&execution_process.id).await {
                    if let Some(executor) =
                        ExecutorConfigs::get_cached().get_coding_agent(&request.executor_profile_id)
                    {
                        executor.normalize_logs(
                            msg_store,
                            &self.task_attempt_to_current_dir(task_attempt),
                        );
                    } else {
                        tracing::error!(
                            "Failed to resolve profile '{:?}' for normalization",
                            request.get_executor_profile_id()
                        );
                    }
                }
            }
            _ => {}
        };

        self.spawn_stream_raw_logs_to_db(&execution_process.id);
        Ok(execution_process)
    }

    async fn try_start_next_action(&self, ctx: &ExecutionContext) -> Result<(), ContainerError> {
        let action = ctx.execution_process.executor_action()?;
        let next_action = if let Some(next_action) = action.next_action() {
            next_action
        } else if matches!(
            ctx.execution_process.run_reason,
            ExecutionProcessRunReason::SetupScript
        ) {
            return Err(ContainerError::Other(anyhow::anyhow!(
                "No next action configured for SetupScript"
            )));
        } else {
            tracing::debug!("No next action configured");
            return Ok(());
        };

        // Determine the run reason of the next action
        let next_run_reason = match ctx.execution_process.run_reason {
            ExecutionProcessRunReason::SetupScript => ExecutionProcessRunReason::CodingAgent,
            ExecutionProcessRunReason::CodingAgent => ExecutionProcessRunReason::CleanupScript,
            _ => {
                tracing::warn!(
                    "Unexpected run reason: {:?}, defaulting to current reason",
                    ctx.execution_process.run_reason
                );
                ctx.execution_process.run_reason.clone()
            }
        };

        self.start_execution(&ctx.task_attempt, next_action, &next_run_reason)
            .await?;

        tracing::debug!("Started next action: {:?}", next_action);
        Ok(())
    }
}
</file>

<file path="crates/services/src/services/events.rs">
use std::{str::FromStr, sync::Arc};

use anyhow::Error as AnyhowError;
use db::{
    DBService,
    models::{
        execution_process::ExecutionProcess,
        task::{Task, TaskWithAttemptStatus},
        task_attempt::TaskAttempt,
    },
};
use futures::StreamExt;
use json_patch::{AddOperation, Patch, PatchOperation, RemoveOperation, ReplaceOperation};
use serde::{Deserialize, Serialize};
use serde_json::json;
use sqlx::{Error as SqlxError, SqlitePool, sqlite::SqliteOperation};
use strum_macros::{Display, EnumString};
use thiserror::Error;
use tokio::sync::RwLock;
use tokio_stream::wrappers::BroadcastStream;
use ts_rs::TS;
use utils::{log_msg::LogMsg, msg_store::MsgStore};
use uuid::Uuid;

#[derive(Debug, Error)]
pub enum EventError {
    #[error(transparent)]
    Sqlx(#[from] SqlxError),
    #[error(transparent)]
    Parse(#[from] serde_json::Error),
    #[error(transparent)]
    Other(#[from] AnyhowError), // Catches any unclassified errors
}

/// Helper functions for creating task-specific patches
pub mod task_patch {
    use super::*;

    /// Escape JSON Pointer special characters
    fn escape_pointer_segment(s: &str) -> String {
        s.replace('~', "~0").replace('/', "~1")
    }

    /// Create path for task operation
    fn task_path(task_id: Uuid) -> String {
        format!("/tasks/{}", escape_pointer_segment(&task_id.to_string()))
    }

    /// Create patch for adding a new task
    pub fn add(task: &TaskWithAttemptStatus) -> Patch {
        Patch(vec![PatchOperation::Add(AddOperation {
            path: task_path(task.id)
                .try_into()
                .expect("Task path should be valid"),
            value: serde_json::to_value(task).expect("Task serialization should not fail"),
        })])
    }

    /// Create patch for updating an existing task
    pub fn replace(task: &TaskWithAttemptStatus) -> Patch {
        Patch(vec![PatchOperation::Replace(ReplaceOperation {
            path: task_path(task.id)
                .try_into()
                .expect("Task path should be valid"),
            value: serde_json::to_value(task).expect("Task serialization should not fail"),
        })])
    }

    /// Create patch for removing a task
    pub fn remove(task_id: Uuid) -> Patch {
        Patch(vec![PatchOperation::Remove(RemoveOperation {
            path: task_path(task_id)
                .try_into()
                .expect("Task path should be valid"),
        })])
    }
}

/// Helper functions for creating execution process-specific patches
pub mod execution_process_patch {
    use db::models::execution_process::ExecutionProcess;

    use super::*;

    /// Escape JSON Pointer special characters
    fn escape_pointer_segment(s: &str) -> String {
        s.replace('~', "~0").replace('/', "~1")
    }

    /// Create path for execution process operation
    fn execution_process_path(process_id: Uuid) -> String {
        format!(
            "/execution_processes/{}",
            escape_pointer_segment(&process_id.to_string())
        )
    }

    /// Create patch for adding a new execution process
    pub fn add(process: &ExecutionProcess) -> Patch {
        Patch(vec![PatchOperation::Add(AddOperation {
            path: execution_process_path(process.id)
                .try_into()
                .expect("Execution process path should be valid"),
            value: serde_json::to_value(process)
                .expect("Execution process serialization should not fail"),
        })])
    }

    /// Create patch for updating an existing execution process
    pub fn replace(process: &ExecutionProcess) -> Patch {
        Patch(vec![PatchOperation::Replace(ReplaceOperation {
            path: execution_process_path(process.id)
                .try_into()
                .expect("Execution process path should be valid"),
            value: serde_json::to_value(process)
                .expect("Execution process serialization should not fail"),
        })])
    }

    /// Create patch for removing an execution process
    pub fn remove(process_id: Uuid) -> Patch {
        Patch(vec![PatchOperation::Remove(RemoveOperation {
            path: execution_process_path(process_id)
                .try_into()
                .expect("Execution process path should be valid"),
        })])
    }
}

#[derive(Clone)]
pub struct EventService {
    msg_store: Arc<MsgStore>,
    db: DBService,
    #[allow(dead_code)]
    entry_count: Arc<RwLock<usize>>,
}

#[derive(EnumString, Display)]
enum HookTables {
    #[strum(to_string = "tasks")]
    Tasks,
    #[strum(to_string = "task_attempts")]
    TaskAttempts,
    #[strum(to_string = "execution_processes")]
    ExecutionProcesses,
    #[strum(to_string = "follow_up_drafts")]
    FollowUpDrafts,
}

#[derive(Serialize, Deserialize, TS)]
#[serde(tag = "type", content = "data", rename_all = "SCREAMING_SNAKE_CASE")]
pub enum RecordTypes {
    Task(Task),
    TaskAttempt(TaskAttempt),
    ExecutionProcess(ExecutionProcess),
    FollowUpDraft(db::models::follow_up_draft::FollowUpDraft),
    DeletedTask {
        rowid: i64,
        project_id: Option<Uuid>,
        task_id: Option<Uuid>,
    },
    DeletedTaskAttempt {
        rowid: i64,
        task_id: Option<Uuid>,
    },
    DeletedExecutionProcess {
        rowid: i64,
        task_attempt_id: Option<Uuid>,
        process_id: Option<Uuid>,
    },
    DeletedFollowUpDraft {
        rowid: i64,
        task_attempt_id: Option<Uuid>,
    },
}

#[derive(Serialize, Deserialize, TS)]
pub struct EventPatchInner {
    db_op: String,
    record: RecordTypes,
}

#[derive(Serialize, Deserialize, TS)]
pub struct EventPatch {
    op: String,
    path: String,
    value: EventPatchInner,
}

impl EventService {
    /// Creates a new EventService that will work with a DBService configured with hooks
    pub fn new(db: DBService, msg_store: Arc<MsgStore>, entry_count: Arc<RwLock<usize>>) -> Self {
        Self {
            msg_store,
            db,
            entry_count,
        }
    }

    async fn push_task_update_for_task(
        pool: &SqlitePool,
        msg_store: Arc<MsgStore>,
        task_id: Uuid,
    ) -> Result<(), SqlxError> {
        if let Some(task) = Task::find_by_id(pool, task_id).await? {
            let tasks = Task::find_by_project_id_with_attempt_status(pool, task.project_id).await?;

            if let Some(task_with_status) = tasks
                .into_iter()
                .find(|task_with_status| task_with_status.id == task_id)
            {
                msg_store.push_patch(task_patch::replace(&task_with_status));
            }
        }

        Ok(())
    }

    async fn push_task_update_for_attempt(
        pool: &SqlitePool,
        msg_store: Arc<MsgStore>,
        attempt_id: Uuid,
    ) -> Result<(), SqlxError> {
        if let Some(attempt) = TaskAttempt::find_by_id(pool, attempt_id).await? {
            Self::push_task_update_for_task(pool, msg_store, attempt.task_id).await?;
        }

        Ok(())
    }

    /// Creates the hook function that should be used with DBService::new_with_after_connect
    pub fn create_hook(
        msg_store: Arc<MsgStore>,
        entry_count: Arc<RwLock<usize>>,
        db_service: DBService,
    ) -> impl for<'a> Fn(
        &'a mut sqlx::sqlite::SqliteConnection,
    ) -> std::pin::Pin<
        Box<dyn std::future::Future<Output = Result<(), sqlx::Error>> + Send + 'a>,
    > + Send
    + Sync
    + 'static {
        move |conn: &mut sqlx::sqlite::SqliteConnection| {
            let msg_store_for_hook = msg_store.clone();
            let entry_count_for_hook = entry_count.clone();
            let db_for_hook = db_service.clone();

            Box::pin(async move {
                let mut handle = conn.lock_handle().await?;
                let runtime_handle = tokio::runtime::Handle::current();
                handle.set_update_hook(move |hook: sqlx::sqlite::UpdateHookResult<'_>| {
                    let runtime_handle = runtime_handle.clone();
                    let entry_count_for_hook = entry_count_for_hook.clone();
                    let msg_store_for_hook = msg_store_for_hook.clone();
                    let db = db_for_hook.clone();

                    if let Ok(table) = HookTables::from_str(hook.table) {
                        let rowid = hook.rowid;
                        runtime_handle.spawn(async move {
                            let record_type: RecordTypes = match (table, hook.operation.clone()) {
                                (HookTables::Tasks, SqliteOperation::Delete) => {
                                    // Try to get task before deletion to capture project_id and task_id
                                    let task_info =
                                        Task::find_by_rowid(&db.pool, rowid).await.ok().flatten();
                                    RecordTypes::DeletedTask {
                                        rowid,
                                        project_id: task_info.as_ref().map(|t| t.project_id),
                                        task_id: task_info.as_ref().map(|t| t.id),
                                    }
                                }
                                (HookTables::TaskAttempts, SqliteOperation::Delete) => {
                                    // Try to get task_attempt before deletion to capture task_id
                                    let task_id = TaskAttempt::find_by_rowid(&db.pool, rowid)
                                        .await
                                        .ok()
                                        .flatten()
                                        .map(|attempt| attempt.task_id);
                                    RecordTypes::DeletedTaskAttempt { rowid, task_id }
                                }
                                (HookTables::ExecutionProcesses, SqliteOperation::Delete) => {
                                    // Try to get execution_process before deletion to capture full process data
                                    if let Ok(Some(process)) =
                                        ExecutionProcess::find_by_rowid(&db.pool, rowid).await
                                    {
                                        RecordTypes::DeletedExecutionProcess {
                                            rowid,
                                            task_attempt_id: Some(process.task_attempt_id),
                                            process_id: Some(process.id),
                                        }
                                    } else {
                                        RecordTypes::DeletedExecutionProcess {
                                            rowid,
                                            task_attempt_id: None,
                                            process_id: None,
                                        }
                                    }
                                }
                                (HookTables::Tasks, _) => {
                                    match Task::find_by_rowid(&db.pool, rowid).await {
                                        Ok(Some(task)) => RecordTypes::Task(task),
                                        Ok(None) => RecordTypes::DeletedTask {
                                            rowid,
                                            project_id: None,
                                            task_id: None,
                                        },
                                        Err(e) => {
                                            tracing::error!("Failed to fetch task: {:?}", e);
                                            return;
                                        }
                                    }
                                }
                                (HookTables::TaskAttempts, _) => {
                                    match TaskAttempt::find_by_rowid(&db.pool, rowid).await {
                                        Ok(Some(attempt)) => RecordTypes::TaskAttempt(attempt),
                                        Ok(None) => RecordTypes::DeletedTaskAttempt {
                                            rowid,
                                            task_id: None,
                                        },
                                        Err(e) => {
                                            tracing::error!(
                                                "Failed to fetch task_attempt: {:?}",
                                                e
                                            );
                                            return;
                                        }
                                    }
                                }
                                (HookTables::ExecutionProcesses, _) => {
                                    match ExecutionProcess::find_by_rowid(&db.pool, rowid).await {
                                        Ok(Some(process)) => RecordTypes::ExecutionProcess(process),
                                        Ok(None) => RecordTypes::DeletedExecutionProcess {
                                            rowid,
                                            task_attempt_id: None,
                                            process_id: None,
                                        },
                                        Err(e) => {
                                            tracing::error!(
                                                "Failed to fetch execution_process: {:?}",
                                                e
                                            );
                                            return;
                                        }
                                    }
                                }
                                (HookTables::FollowUpDrafts, SqliteOperation::Delete) => {
                                    // Try to get draft before deletion to capture attempt id
                                    let attempt_id =
                                        db::models::follow_up_draft::FollowUpDraft::find_by_rowid(
                                            &db.pool, rowid,
                                        )
                                        .await
                                        .ok()
                                        .flatten()
                                        .map(|d| d.task_attempt_id);
                                    RecordTypes::DeletedFollowUpDraft {
                                        rowid,
                                        task_attempt_id: attempt_id,
                                    }
                                }
                                (HookTables::FollowUpDrafts, _) => {
                                    match db::models::follow_up_draft::FollowUpDraft::find_by_rowid(
                                        &db.pool, rowid,
                                    )
                                    .await
                                    {
                                        Ok(Some(draft)) => RecordTypes::FollowUpDraft(draft),
                                        Ok(None) => RecordTypes::DeletedFollowUpDraft {
                                            rowid,
                                            task_attempt_id: None,
                                        },
                                        Err(e) => {
                                            tracing::error!(
                                                "Failed to fetch follow_up_draft: {:?}",
                                                e
                                            );
                                            return;
                                        }
                                    }
                                }
                            };

                            let db_op: &str = match hook.operation {
                                SqliteOperation::Insert => "insert",
                                SqliteOperation::Delete => "delete",
                                SqliteOperation::Update => "update",
                                SqliteOperation::Unknown(_) => "unknown",
                            };

                            // Handle task-related operations with direct patches
                            match &record_type {
                                RecordTypes::Task(task) => {
                                    // Convert Task to TaskWithAttemptStatus
                                    if let Ok(task_list) =
                                        Task::find_by_project_id_with_attempt_status(
                                            &db.pool,
                                            task.project_id,
                                        )
                                        .await
                                        && let Some(task_with_status) =
                                            task_list.into_iter().find(|t| t.id == task.id)
                                    {
                                        let patch = match hook.operation {
                                            SqliteOperation::Insert => {
                                                task_patch::add(&task_with_status)
                                            }
                                            SqliteOperation::Update => {
                                                task_patch::replace(&task_with_status)
                                            }
                                            _ => task_patch::replace(&task_with_status), // fallback
                                        };
                                        msg_store_for_hook.push_patch(patch);
                                        return;
                                    }
                                }
                                RecordTypes::DeletedTask {
                                    task_id: Some(task_id),
                                    ..
                                } => {
                                    let patch = task_patch::remove(*task_id);
                                    msg_store_for_hook.push_patch(patch);
                                    return;
                                }
                                RecordTypes::TaskAttempt(attempt) => {
                                    // Task attempts should update the parent task with fresh data
                                    if let Ok(Some(task)) =
                                        Task::find_by_id(&db.pool, attempt.task_id).await
                                        && let Ok(task_list) =
                                            Task::find_by_project_id_with_attempt_status(
                                                &db.pool,
                                                task.project_id,
                                            )
                                            .await
                                        && let Some(task_with_status) =
                                            task_list.into_iter().find(|t| t.id == attempt.task_id)
                                    {
                                        let patch = task_patch::replace(&task_with_status);
                                        msg_store_for_hook.push_patch(patch);
                                        return;
                                    }
                                }
                                RecordTypes::DeletedTaskAttempt {
                                    task_id: Some(task_id),
                                    ..
                                } => {
                                    // Task attempt deletion should update the parent task with fresh data
                                    if let Ok(Some(task)) =
                                        Task::find_by_id(&db.pool, *task_id).await
                                        && let Ok(task_list) =
                                            Task::find_by_project_id_with_attempt_status(
                                                &db.pool,
                                                task.project_id,
                                            )
                                            .await
                                        && let Some(task_with_status) =
                                            task_list.into_iter().find(|t| t.id == *task_id)
                                    {
                                        let patch = task_patch::replace(&task_with_status);
                                        msg_store_for_hook.push_patch(patch);
                                        return;
                                    }
                                }
                                RecordTypes::ExecutionProcess(process) => {
                                    let patch = match hook.operation {
                                        SqliteOperation::Insert => {
                                            execution_process_patch::add(process)
                                        }
                                        SqliteOperation::Update => {
                                            execution_process_patch::replace(process)
                                        }
                                        _ => execution_process_patch::replace(process), // fallback
                                    };
                                    msg_store_for_hook.push_patch(patch);

                                    if let Err(err) = EventService::push_task_update_for_attempt(
                                        &db.pool,
                                        msg_store_for_hook.clone(),
                                        process.task_attempt_id,
                                    )
                                    .await
                                    {
                                        tracing::error!(
                                            "Failed to push task update after execution process change: {:?}",
                                            err
                                        );
                                    }

                                    return;
                                }
                                RecordTypes::DeletedExecutionProcess {
                                    process_id: Some(process_id),
                                    task_attempt_id,
                                    ..
                                } => {
                                    let patch = execution_process_patch::remove(*process_id);
                                    msg_store_for_hook.push_patch(patch);

                                    if let Some(task_attempt_id) = task_attempt_id
                                        && let Err(err) =
                                            EventService::push_task_update_for_attempt(
                                                &db.pool,
                                                msg_store_for_hook.clone(),
                                                *task_attempt_id,
                                            )
                                            .await
                                        {
                                            tracing::error!(
                                                "Failed to push task update after execution process removal: {:?}",
                                                err
                                            );
                                        }

                                    return;
                                }
                                _ => {}
                            }

                            // Fallback: use the old entries format for other record types
                            let next_entry_count = {
                                let mut entry_count = entry_count_for_hook.write().await;
                                *entry_count += 1;
                                *entry_count
                            };

                            let event_patch: EventPatch = EventPatch {
                                op: "add".to_string(),
                                path: format!("/entries/{next_entry_count}"),
                                value: EventPatchInner {
                                    db_op: db_op.to_string(),
                                    record: record_type,
                                },
                            };

                            let patch =
                                serde_json::from_value(json!([
                                    serde_json::to_value(event_patch).unwrap()
                                ]))
                                .unwrap();

                            msg_store_for_hook.push_patch(patch);
                        });
                    }
                });

                Ok(())
            })
        }
    }

    pub fn msg_store(&self) -> &Arc<MsgStore> {
        &self.msg_store
    }

    /// Stream raw task messages for a specific project with initial snapshot
    pub async fn stream_tasks_raw(
        &self,
        project_id: Uuid,
    ) -> Result<futures::stream::BoxStream<'static, Result<LogMsg, std::io::Error>>, EventError>
    {
        // Get initial snapshot of tasks
        let tasks = Task::find_by_project_id_with_attempt_status(&self.db.pool, project_id).await?;

        // Convert task array to object keyed by task ID
        let tasks_map: serde_json::Map<String, serde_json::Value> = tasks
            .into_iter()
            .map(|task| (task.id.to_string(), serde_json::to_value(task).unwrap()))
            .collect();

        let initial_patch = json!([{
            "op": "replace",
            "path": "/tasks",
            "value": tasks_map
        }]);
        let initial_msg = LogMsg::JsonPatch(serde_json::from_value(initial_patch).unwrap());

        // Clone necessary data for the async filter
        let db_pool = self.db.pool.clone();

        // Get filtered event stream
        let filtered_stream =
            BroadcastStream::new(self.msg_store.get_receiver()).filter_map(move |msg_result| {
                let db_pool = db_pool.clone();
                async move {
                    match msg_result {
                        Ok(LogMsg::JsonPatch(patch)) => {
                            // Filter events based on project_id
                            if let Some(patch_op) = patch.0.first() {
                                // Check if this is a direct task patch (new format)
                                if patch_op.path().starts_with("/tasks/") {
                                    match patch_op {
                                        json_patch::PatchOperation::Add(op) => {
                                            // Parse task data directly from value
                                            if let Ok(task) =
                                                serde_json::from_value::<TaskWithAttemptStatus>(
                                                    op.value.clone(),
                                                )
                                                && task.project_id == project_id
                                            {
                                                return Some(Ok(LogMsg::JsonPatch(patch)));
                                            }
                                        }
                                        json_patch::PatchOperation::Replace(op) => {
                                            // Parse task data directly from value
                                            if let Ok(task) =
                                                serde_json::from_value::<TaskWithAttemptStatus>(
                                                    op.value.clone(),
                                                )
                                                && task.project_id == project_id
                                            {
                                                return Some(Ok(LogMsg::JsonPatch(patch)));
                                            }
                                        }
                                        json_patch::PatchOperation::Remove(_) => {
                                            // For remove operations, we need to check project membership differently
                                            // We could cache this information or let it pass through for now
                                            // Since we don't have the task data, we'll allow all removals
                                            // and let the client handle filtering
                                            return Some(Ok(LogMsg::JsonPatch(patch)));
                                        }
                                        _ => {}
                                    }
                                } else if let Ok(event_patch_value) = serde_json::to_value(patch_op)
                                    && let Ok(event_patch) =
                                        serde_json::from_value::<EventPatch>(event_patch_value)
                                {
                                    // Handle old EventPatch format for non-task records
                                    match &event_patch.value.record {
                                        RecordTypes::Task(task) => {
                                            if task.project_id == project_id {
                                                return Some(Ok(LogMsg::JsonPatch(patch)));
                                            }
                                        }
                                        RecordTypes::DeletedTask {
                                            project_id: Some(deleted_project_id),
                                            ..
                                        } => {
                                            if *deleted_project_id == project_id {
                                                return Some(Ok(LogMsg::JsonPatch(patch)));
                                            }
                                        }
                                        RecordTypes::TaskAttempt(attempt) => {
                                            // Check if this task_attempt belongs to a task in our project
                                            if let Ok(Some(task)) =
                                                Task::find_by_id(&db_pool, attempt.task_id).await
                                                && task.project_id == project_id
                                            {
                                                return Some(Ok(LogMsg::JsonPatch(patch)));
                                            }
                                        }
                                        RecordTypes::DeletedTaskAttempt {
                                            task_id: Some(deleted_task_id),
                                            ..
                                        } => {
                                            // Check if deleted attempt belonged to a task in our project
                                            if let Ok(Some(task)) =
                                                Task::find_by_id(&db_pool, *deleted_task_id).await
                                                && task.project_id == project_id
                                            {
                                                return Some(Ok(LogMsg::JsonPatch(patch)));
                                            }
                                        }
                                        _ => {}
                                    }
                                }
                            }
                            None
                        }
                        Ok(other) => Some(Ok(other)), // Pass through non-patch messages
                        Err(_) => None,               // Filter out broadcast errors
                    }
                }
            });

        // Start with initial snapshot, then live updates
        let initial_stream = futures::stream::once(async move { Ok(initial_msg) });
        let combined_stream = initial_stream.chain(filtered_stream).boxed();

        Ok(combined_stream)
    }

    /// Stream execution processes for a specific task attempt with initial snapshot (raw LogMsg format for WebSocket)
    pub async fn stream_execution_processes_for_attempt_raw(
        &self,
        task_attempt_id: Uuid,
        show_soft_deleted: bool,
    ) -> Result<futures::stream::BoxStream<'static, Result<LogMsg, std::io::Error>>, EventError>
    {
        // Get initial snapshot of execution processes (filtering at SQL level)
        let processes = ExecutionProcess::find_by_task_attempt_id(
            &self.db.pool,
            task_attempt_id,
            show_soft_deleted,
        )
        .await?;

        // Convert processes array to object keyed by process ID
        let processes_map: serde_json::Map<String, serde_json::Value> = processes
            .into_iter()
            .map(|process| {
                (
                    process.id.to_string(),
                    serde_json::to_value(process).unwrap(),
                )
            })
            .collect();

        let initial_patch = json!([{
            "op": "replace",
            "path": "/execution_processes",
            "value": processes_map
        }]);
        let initial_msg = LogMsg::JsonPatch(serde_json::from_value(initial_patch).unwrap());

        // Get filtered event stream
        let filtered_stream = BroadcastStream::new(self.msg_store.get_receiver()).filter_map(
            move |msg_result| async move {
                match msg_result {
                    Ok(LogMsg::JsonPatch(patch)) => {
                        // Filter events based on task_attempt_id
                        if let Some(patch_op) = patch.0.first() {
                            // Check if this is a modern execution process patch
                            if patch_op.path().starts_with("/execution_processes/") {
                                match patch_op {
                                    json_patch::PatchOperation::Add(op) => {
                                        // Parse execution process data directly from value
                                        if let Ok(process) =
                                            serde_json::from_value::<ExecutionProcess>(
                                                op.value.clone(),
                                            )
                                            && process.task_attempt_id == task_attempt_id
                                        {
                                            if !show_soft_deleted && process.dropped {
                                                return None;
                                            }
                                            return Some(Ok(LogMsg::JsonPatch(patch)));
                                        }
                                    }
                                    json_patch::PatchOperation::Replace(op) => {
                                        // Parse execution process data directly from value
                                        if let Ok(process) =
                                            serde_json::from_value::<ExecutionProcess>(
                                                op.value.clone(),
                                            )
                                            && process.task_attempt_id == task_attempt_id
                                        {
                                            if !show_soft_deleted && process.dropped {
                                                let remove_patch =
                                                    execution_process_patch::remove(process.id);
                                                return Some(Ok(LogMsg::JsonPatch(remove_patch)));
                                            }
                                            return Some(Ok(LogMsg::JsonPatch(patch)));
                                        }
                                    }
                                    json_patch::PatchOperation::Remove(_) => {
                                        // For remove operations, we can't verify task_attempt_id
                                        // so we allow all removals and let the client handle filtering
                                        return Some(Ok(LogMsg::JsonPatch(patch)));
                                    }
                                    _ => {}
                                }
                            }
                            // Fallback to legacy EventPatch format for backward compatibility
                            else if let Ok(event_patch_value) = serde_json::to_value(patch_op)
                                && let Ok(event_patch) =
                                    serde_json::from_value::<EventPatch>(event_patch_value)
                            {
                                match &event_patch.value.record {
                                    RecordTypes::ExecutionProcess(process) => {
                                        if process.task_attempt_id == task_attempt_id {
                                            if !show_soft_deleted && process.dropped {
                                                let remove_patch =
                                                    execution_process_patch::remove(process.id);
                                                return Some(Ok(LogMsg::JsonPatch(remove_patch)));
                                            }
                                            return Some(Ok(LogMsg::JsonPatch(patch)));
                                        }
                                    }
                                    RecordTypes::DeletedExecutionProcess {
                                        task_attempt_id: Some(deleted_attempt_id),
                                        ..
                                    } => {
                                        if *deleted_attempt_id == task_attempt_id {
                                            return Some(Ok(LogMsg::JsonPatch(patch)));
                                        }
                                    }
                                    _ => {}
                                }
                            }
                        }
                        None
                    }
                    Ok(other) => Some(Ok(other)), // Pass through non-patch messages
                    Err(_) => None,               // Filter out broadcast errors
                }
            },
        );

        // Start with initial snapshot, then live updates
        let initial_stream = futures::stream::once(async move { Ok(initial_msg) });
        let combined_stream = initial_stream.chain(filtered_stream).boxed();

        Ok(combined_stream)
    }

    /// Stream follow-up draft for a specific task attempt (raw LogMsg format for WebSocket)
    pub async fn stream_follow_up_draft_for_attempt_raw(
        &self,
        task_attempt_id: Uuid,
    ) -> Result<futures::stream::BoxStream<'static, Result<LogMsg, std::io::Error>>, EventError>
    {
        // Get initial snapshot of follow-up draft
        let draft = db::models::follow_up_draft::FollowUpDraft::find_by_task_attempt_id(
            &self.db.pool,
            task_attempt_id,
        )
        .await?
        .unwrap_or(db::models::follow_up_draft::FollowUpDraft {
            id: uuid::Uuid::new_v4(),
            task_attempt_id,
            prompt: String::new(),
            queued: false,
            sending: false,
            variant: None,
            image_ids: None,
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
            version: 0,
        });

        let initial_patch = json!([
            {
                "op": "replace",
                "path": "/",
                "value": { "follow_up_draft": draft }
            }
        ]);
        let initial_msg = LogMsg::JsonPatch(serde_json::from_value(initial_patch).unwrap());

        // Filtered live stream, mapped into direct JSON patches that update /follow_up_draft
        let filtered_stream = BroadcastStream::new(self.msg_store.get_receiver()).filter_map(
            move |msg_result| async move {
                match msg_result {
                    Ok(LogMsg::JsonPatch(patch)) => {
                        if let Some(event_patch_op) = patch.0.first()
                            && let Ok(event_patch_value) = serde_json::to_value(event_patch_op)
                            && let Ok(event_patch) =
                                serde_json::from_value::<EventPatch>(event_patch_value)
                        {
                            match &event_patch.value.record {
                                RecordTypes::FollowUpDraft(draft) => {
                                    if draft.task_attempt_id == task_attempt_id {
                                        // Build a direct patch to replace /follow_up_draft
                                        let direct = json!([
                                            {
                                                "op": "replace",
                                                "path": "/follow_up_draft",
                                                "value": draft
                                            }
                                        ]);
                                        let direct_patch = serde_json::from_value(direct).unwrap();
                                        return Some(Ok(LogMsg::JsonPatch(direct_patch)));
                                    }
                                }
                                RecordTypes::DeletedFollowUpDraft {
                                    task_attempt_id: Some(id),
                                    ..
                                } => {
                                    if *id == task_attempt_id {
                                        // Replace with empty draft state
                                        let empty = json!({
                                            "id": uuid::Uuid::new_v4(),
                                            "task_attempt_id": id,
                                            "prompt": "",
                                            "queued": false,
                                            "sending": false,
                                            "variant": null,
                                            "image_ids": null,
                                            "created_at": chrono::Utc::now(),
                                            "updated_at": chrono::Utc::now(),
                                            "version": 0
                                        });
                                        let direct = json!([
                                            {
                                                "op": "replace",
                                                "path": "/follow_up_draft",
                                                "value": empty
                                            }
                                        ]);
                                        let direct_patch = serde_json::from_value(direct).unwrap();
                                        return Some(Ok(LogMsg::JsonPatch(direct_patch)));
                                    }
                                }
                                _ => {}
                            }
                        }
                        None
                    }
                    Ok(other) => Some(Ok(other)),
                    Err(_) => None,
                }
            },
        );

        let initial_stream = futures::stream::once(async move { Ok(initial_msg) });
        let combined_stream = initial_stream.chain(filtered_stream).boxed();

        Ok(combined_stream)
    }
}
</file>

<file path="crates/services/src/services/file_ranker.rs">
use std::{
    collections::HashMap,
    path::{Path, PathBuf},
    sync::Arc,
};

use chrono::{DateTime, Utc};
use dashmap::DashMap;
use db::models::project::{SearchMatchType, SearchResult};
use once_cell::sync::Lazy;
use tokio::task;

use super::git::{GitService, GitServiceError};

/// Statistics for a single file based on git history
#[derive(Clone, Debug)]
pub struct FileStat {
    /// Index in the commit history (0 = HEAD, 1 = parent of HEAD, ...)
    pub last_index: usize,
    /// Number of times this file was changed in recent commits
    pub commit_count: u32,
    /// Timestamp of the most recent change
    pub last_time: DateTime<Utc>,
}

/// File statistics for a repository
pub type FileStats = HashMap<String, FileStat>;

/// Cache entry for repository history
#[derive(Clone)]
struct RepoHistoryCache {
    head_sha: String,
    stats: Arc<FileStats>,
}

/// Global cache for file ranking statistics
static FILE_STATS_CACHE: Lazy<DashMap<PathBuf, RepoHistoryCache>> = Lazy::new(DashMap::new);

/// Configuration constants for ranking algorithm
const DEFAULT_COMMIT_LIMIT: usize = 100;
const BASE_MATCH_SCORE_FILENAME: i64 = 100;
const BASE_MATCH_SCORE_DIRNAME: i64 = 10;
const BASE_MATCH_SCORE_FULLPATH: i64 = 1;
const RECENCY_WEIGHT: i64 = 2;
const FREQUENCY_WEIGHT: i64 = 1;

/// Service for ranking files based on git history
#[derive(Clone)]
pub struct FileRanker {
    git_service: GitService,
}

impl Default for FileRanker {
    fn default() -> Self {
        Self::new()
    }
}

impl FileRanker {
    pub fn new() -> Self {
        Self {
            git_service: GitService::new(),
        }
    }

    /// Get file statistics for a repository, using cache when possible
    pub async fn get_stats(&self, repo_path: &Path) -> Result<Arc<FileStats>, GitServiceError> {
        let repo_path = repo_path.to_path_buf();

        // Check if we have a valid cache entry
        if let Some(cache_entry) = FILE_STATS_CACHE.get(&repo_path) {
            // Verify cache is still valid by checking HEAD
            if let Ok(head_info) = self.git_service.get_head_info(&repo_path)
                && head_info.oid == cache_entry.head_sha
            {
                return Ok(Arc::clone(&cache_entry.stats));
            }
        }

        // Cache miss or invalid - compute new stats
        let stats = self.compute_stats(&repo_path).await?;
        Ok(stats)
    }

    /// Re-rank search results based on git history statistics
    pub fn rerank(&self, results: &mut [SearchResult], stats: &FileStats) {
        results.sort_by(|a, b| {
            let score_a = self.calculate_score(a, stats);
            let score_b = self.calculate_score(b, stats);
            score_b.cmp(&score_a) // Higher scores first
        });
    }

    /// Calculate relevance score for a search result
    fn calculate_score(&self, result: &SearchResult, stats: &FileStats) -> i64 {
        let base_score = match result.match_type {
            SearchMatchType::FileName => BASE_MATCH_SCORE_FILENAME,
            SearchMatchType::DirectoryName => BASE_MATCH_SCORE_DIRNAME,
            SearchMatchType::FullPath => BASE_MATCH_SCORE_FULLPATH,
        };

        if let Some(stat) = stats.get(&result.path) {
            let recency_bonus = (100 - stat.last_index.min(99) as i64) * RECENCY_WEIGHT;
            let frequency_bonus = stat.commit_count as i64 * FREQUENCY_WEIGHT;

            // Multiply base score to maintain hierarchy, add git-based bonuses
            base_score * 1000 + recency_bonus * 10 + frequency_bonus
        } else {
            // Files not in git history get base score only
            base_score * 1000
        }
    }

    /// Compute file statistics from git history
    async fn compute_stats(&self, repo_path: &Path) -> Result<Arc<FileStats>, GitServiceError> {
        let repo_path = repo_path.to_path_buf();
        let repo_path_for_error = repo_path.clone();
        let git_service = self.git_service.clone();

        // Run git analysis in blocking task to avoid blocking async runtime
        let stats = task::spawn_blocking(move || {
            git_service.collect_recent_file_stats(&repo_path, DEFAULT_COMMIT_LIMIT)
        })
        .await
        .map_err(|e| GitServiceError::InvalidRepository(format!("Task join error: {e}")))?;

        let stats = match stats {
            Ok(s) => s,
            Err(e) => {
                tracing::warn!(
                    "Failed to collect file stats for {:?}: {}",
                    repo_path_for_error,
                    e
                );
                // Return empty stats on error - search will still work without ranking
                HashMap::new()
            }
        };

        let stats_arc = Arc::new(stats);

        // Update cache
        if let Ok(head_info) = self.git_service.get_head_info(&repo_path_for_error) {
            FILE_STATS_CACHE.insert(
                repo_path_for_error,
                RepoHistoryCache {
                    head_sha: head_info.oid,
                    stats: Arc::clone(&stats_arc),
                },
            );
        }

        Ok(stats_arc)
    }
}
</file>

<file path="crates/services/src/services/file_search_cache.rs">
use std::{
    path::{Path, PathBuf},
    sync::Arc,
    time::{Duration, Instant},
};

use dashmap::DashMap;
use db::models::project::{SearchMatchType, SearchResult};
use fst::{Map, MapBuilder};
use ignore::WalkBuilder;
use moka::future::Cache;
use notify::{RecommendedWatcher, RecursiveMode};
use notify_debouncer_full::{DebounceEventResult, new_debouncer};
use serde::{Deserialize, Serialize};
use sqlx::SqlitePool;
use thiserror::Error;
use tokio::sync::mpsc;
use tracing::{error, info, warn};
use ts_rs::TS;

use super::{
    file_ranker::{FileRanker, FileStats},
    git::GitService,
};

/// Search mode for different use cases
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[serde(rename_all = "lowercase")]
#[derive(Default)]
pub enum SearchMode {
    #[default]
    TaskForm, // Default: exclude ignored files (clean results)
    Settings, // Include ignored files (for project config like .env)
}

/// Search query parameters for typed Axum extraction
#[derive(Debug, Deserialize)]
pub struct SearchQuery {
    pub q: String,
    #[serde(default)]
    pub mode: SearchMode,
}

/// FST-indexed file search result
#[derive(Clone, Debug)]
pub struct IndexedFile {
    pub path: String,
    pub is_file: bool,
    pub match_type: SearchMatchType,
    pub path_lowercase: Arc<str>,
    pub is_ignored: bool, // Track if file is gitignored
}

/// File index build result containing indexed files and FST map
#[derive(Debug)]
pub struct FileIndex {
    pub files: Vec<IndexedFile>,
    pub map: Map<Vec<u8>>,
}

/// Errors that can occur during file index building
#[derive(Error, Debug)]
pub enum FileIndexError {
    #[error(transparent)]
    Io(#[from] std::io::Error),
    #[error(transparent)]
    Fst(#[from] fst::Error),
    #[error(transparent)]
    Walk(#[from] ignore::Error),
    #[error(transparent)]
    StripPrefix(#[from] std::path::StripPrefixError),
}

/// Cached repository data with FST index and git stats
#[derive(Clone)]
pub struct CachedRepo {
    pub head_sha: String,
    pub fst_index: Map<Vec<u8>>,
    pub indexed_files: Vec<IndexedFile>,
    pub stats: Arc<FileStats>,
    pub build_ts: Instant,
}

/// Cache miss error
#[derive(Debug)]
pub enum CacheError {
    Miss,
    BuildError(String),
}

/// File search cache with FST indexing
pub struct FileSearchCache {
    cache: Cache<PathBuf, CachedRepo>,
    git_service: GitService,
    file_ranker: FileRanker,
    build_queue: mpsc::UnboundedSender<PathBuf>,
    watchers: DashMap<PathBuf, RecommendedWatcher>,
}

impl FileSearchCache {
    pub fn new() -> Self {
        let (build_sender, build_receiver) = mpsc::unbounded_channel();

        // Create cache with 100MB limit and 1 hour TTL
        let cache = Cache::builder()
            .max_capacity(50) // Max 50 repos
            .time_to_live(Duration::from_secs(3600)) // 1 hour TTL
            .build();

        let cache_for_worker = cache.clone();
        let git_service = GitService::new();
        let file_ranker = FileRanker::new();

        // Spawn background worker
        let worker_git_service = git_service.clone();
        let worker_file_ranker = file_ranker.clone();
        tokio::spawn(async move {
            Self::background_worker(
                build_receiver,
                cache_for_worker,
                worker_git_service,
                worker_file_ranker,
            )
            .await;
        });

        Self {
            cache,
            git_service,
            file_ranker,
            build_queue: build_sender,
            watchers: DashMap::new(),
        }
    }

    /// Search files in repository using cache
    pub async fn search(
        &self,
        repo_path: &Path,
        query: &str,
        mode: SearchMode,
    ) -> Result<Vec<SearchResult>, CacheError> {
        let repo_path_buf = repo_path.to_path_buf();

        // Check if we have a valid cache entry
        if let Some(cached) = self.cache.get(&repo_path_buf).await
            && let Ok(head_info) = self.git_service.get_head_info(&repo_path_buf)
            && head_info.oid == cached.head_sha
        {
            // Cache hit - perform fast search with mode-based filtering
            return Ok(self.search_in_cache(&cached, query, mode).await);
        }

        // Cache miss - trigger background refresh and return error
        if let Err(e) = self.build_queue.send(repo_path_buf) {
            warn!("Failed to enqueue cache build: {}", e);
        }

        Err(CacheError::Miss)
    }

    /// Pre-warm cache for given repositories
    pub async fn warm_repos(&self, repo_paths: Vec<PathBuf>) -> Result<(), String> {
        for repo_path in repo_paths {
            if let Err(e) = self.build_queue.send(repo_path.clone()) {
                error!(
                    "Failed to enqueue repo for warming: {:?} - {}",
                    repo_path, e
                );
            }
        }
        Ok(())
    }

    /// Pre-warm cache for most active projects
    pub async fn warm_most_active(&self, db_pool: &SqlitePool, limit: i32) -> Result<(), String> {
        use db::models::project::Project;

        info!("Starting file search cache warming...");

        // Get most active projects
        let active_projects = Project::find_most_active(db_pool, limit)
            .await
            .map_err(|e| format!("Failed to fetch active projects: {e}"))?;

        if active_projects.is_empty() {
            info!("No active projects found, skipping cache warming");
            return Ok(());
        }

        let repo_paths: Vec<PathBuf> = active_projects
            .iter()
            .map(|p| PathBuf::from(&p.git_repo_path))
            .collect();

        info!(
            "Warming cache for {} projects: {:?}",
            repo_paths.len(),
            repo_paths
        );

        // Warm the cache
        self.warm_repos(repo_paths.clone())
            .await
            .map_err(|e| format!("Failed to warm cache: {e}"))?;

        // Setup watchers for active projects
        for repo_path in &repo_paths {
            if let Err(e) = self.setup_watcher(repo_path).await {
                warn!("Failed to setup watcher for {:?}: {}", repo_path, e);
            }
        }

        info!("File search cache warming completed");
        Ok(())
    }

    /// Search within cached index with mode-based filtering
    async fn search_in_cache(
        &self,
        cached: &CachedRepo,
        query: &str,
        mode: SearchMode,
    ) -> Vec<SearchResult> {
        let query_lower = query.to_lowercase();
        let mut results = Vec::new();

        // Search through indexed files with mode-based filtering
        for indexed_file in &cached.indexed_files {
            if indexed_file.path_lowercase.contains(&query_lower) {
                // Apply mode-based filtering
                match mode {
                    SearchMode::TaskForm => {
                        // Exclude ignored files for task forms
                        if indexed_file.is_ignored {
                            continue;
                        }
                    }
                    SearchMode::Settings => {
                        // Include all files (including ignored) for project settings
                        // No filtering needed
                    }
                }

                results.push(SearchResult {
                    path: indexed_file.path.clone(),
                    is_file: indexed_file.is_file,
                    match_type: indexed_file.match_type.clone(),
                });
            }
        }

        // Apply git history-based ranking
        self.file_ranker.rerank(&mut results, &cached.stats);

        // Limit to top 10 results
        results.truncate(10);
        results
    }

    /// Build cache entry for a repository
    async fn build_repo_cache(&self, repo_path: &Path) -> Result<CachedRepo, String> {
        let repo_path_buf = repo_path.to_path_buf();

        info!("Building cache for repo: {:?}", repo_path);

        // Get current HEAD
        let head_info = self
            .git_service
            .get_head_info(&repo_path_buf)
            .map_err(|e| format!("Failed to get HEAD info: {e}"))?;

        // Get git stats
        let stats = self
            .file_ranker
            .get_stats(repo_path)
            .await
            .map_err(|e| format!("Failed to get git stats: {e}"))?;

        // Build file index
        let file_index = Self::build_file_index(repo_path)
            .map_err(|e| format!("Failed to build file index: {e}"))?;

        Ok(CachedRepo {
            head_sha: head_info.oid,
            fst_index: file_index.map,
            indexed_files: file_index.files,
            stats,
            build_ts: Instant::now(),
        })
    }

    /// Build FST index from filesystem traversal using superset approach
    fn build_file_index(repo_path: &Path) -> Result<FileIndex, FileIndexError> {
        let mut indexed_files = Vec::new();
        let mut fst_keys = Vec::new();

        // Build superset walker - include ignored files but exclude .git and performance killers
        let mut builder = WalkBuilder::new(repo_path);
        builder
            .git_ignore(false) // Include all files initially
            .git_global(false)
            .git_exclude(false)
            .hidden(false) // Show hidden files like .env
            .filter_entry(|entry| {
                let name = entry.file_name().to_string_lossy();
                // Always exclude .git directories
                if name == ".git" {
                    return false;
                }
                // Exclude performance killers even when including ignored files
                if name == "node_modules" || name == "target" || name == "dist" || name == "build" {
                    return false;
                }
                true
            });

        let walker = builder.build();

        // Create a second walker for checking ignore status
        let ignore_walker = WalkBuilder::new(repo_path)
            .git_ignore(true) // This will tell us what's ignored
            .git_global(true)
            .git_exclude(true)
            .hidden(false)
            .filter_entry(|entry| {
                let name = entry.file_name().to_string_lossy();
                name != ".git"
            })
            .build();

        // Collect paths from ignore-aware walker to know what's NOT ignored
        let mut non_ignored_paths = std::collections::HashSet::new();
        for result in ignore_walker {
            if let Ok(entry) = result
                && let Ok(relative_path) = entry.path().strip_prefix(repo_path)
            {
                non_ignored_paths.insert(relative_path.to_path_buf());
            }
        }

        // Now walk all files and determine their ignore status
        for result in walker {
            let entry = result?;
            let path = entry.path();

            if path == repo_path {
                continue;
            }

            let relative_path = path.strip_prefix(repo_path)?;
            let relative_path_str = relative_path.to_string_lossy().to_string();
            let relative_path_lower = relative_path_str.to_lowercase();

            // Skip empty paths
            if relative_path_lower.is_empty() {
                continue;
            }

            // Determine if this file is ignored
            let is_ignored = !non_ignored_paths.contains(relative_path);

            let file_name = path
                .file_name()
                .map(|name| name.to_string_lossy().to_lowercase())
                .unwrap_or_default();

            // Determine match type
            let match_type = if !file_name.is_empty() {
                SearchMatchType::FileName
            } else if path
                .parent()
                .and_then(|p| p.file_name())
                .map(|name| name.to_string_lossy().to_lowercase())
                .unwrap_or_default()
                != relative_path_lower
            {
                SearchMatchType::DirectoryName
            } else {
                SearchMatchType::FullPath
            };

            let indexed_file = IndexedFile {
                path: relative_path_str,
                is_file: path.is_file(),
                match_type,
                path_lowercase: Arc::from(relative_path_lower.as_str()),
                is_ignored,
            };

            // Store the key for FST along with file index
            let file_index = indexed_files.len() as u64;
            fst_keys.push((relative_path_lower, file_index));
            indexed_files.push(indexed_file);
        }

        // Sort keys for FST (required for building)
        fst_keys.sort_by(|a, b| a.0.cmp(&b.0));

        // Remove duplicates (keep first occurrence)
        fst_keys.dedup_by(|a, b| a.0 == b.0);

        // Build FST
        let mut fst_builder = MapBuilder::memory();
        for (key, value) in fst_keys {
            fst_builder.insert(&key, value)?;
        }

        let fst_map = fst_builder.into_map();
        Ok(FileIndex {
            files: indexed_files,
            map: fst_map,
        })
    }

    /// Background worker for cache building
    async fn background_worker(
        mut build_receiver: mpsc::UnboundedReceiver<PathBuf>,
        cache: Cache<PathBuf, CachedRepo>,
        git_service: GitService,
        file_ranker: FileRanker,
    ) {
        while let Some(repo_path) = build_receiver.recv().await {
            let cache_builder = FileSearchCache {
                cache: cache.clone(),
                git_service: git_service.clone(),
                file_ranker: file_ranker.clone(),
                build_queue: mpsc::unbounded_channel().0, // Dummy sender
                watchers: DashMap::new(),
            };

            match cache_builder.build_repo_cache(&repo_path).await {
                Ok(cached_repo) => {
                    cache.insert(repo_path.clone(), cached_repo).await;
                    info!("Successfully cached repo: {:?}", repo_path);
                }
                Err(e) => {
                    error!("Failed to cache repo {:?}: {}", repo_path, e);
                }
            }
        }
    }

    /// Setup file watcher for repository
    pub async fn setup_watcher(&self, repo_path: &Path) -> Result<(), String> {
        let repo_path_buf = repo_path.to_path_buf();

        if self.watchers.contains_key(&repo_path_buf) {
            return Ok(()); // Already watching
        }

        let git_dir = repo_path.join(".git");
        if !git_dir.exists() {
            return Err("Not a git repository".to_string());
        }

        let build_queue = self.build_queue.clone();
        let watched_path = repo_path_buf.clone();

        let (tx, mut rx) = mpsc::unbounded_channel();

        let mut debouncer = new_debouncer(
            Duration::from_millis(500),
            None,
            move |res: DebounceEventResult| {
                if let Ok(events) = res {
                    for event in events {
                        // Check if any path contains HEAD file
                        for path in &event.event.paths {
                            if path.file_name().is_some_and(|name| name == "HEAD") {
                                if let Err(e) = tx.send(()) {
                                    error!("Failed to send HEAD change event: {}", e);
                                }
                                break;
                            }
                        }
                    }
                }
            },
        )
        .map_err(|e| format!("Failed to create file watcher: {e}"))?;

        debouncer
            .watch(git_dir.join("HEAD"), RecursiveMode::NonRecursive)
            .map_err(|e| format!("Failed to watch HEAD file: {e}"))?;

        // Spawn task to handle HEAD changes
        tokio::spawn(async move {
            while rx.recv().await.is_some() {
                info!("HEAD changed for repo: {:?}", watched_path);
                if let Err(e) = build_queue.send(watched_path.clone()) {
                    error!("Failed to enqueue cache refresh: {}", e);
                }
            }
        });

        info!("Setup file watcher for repo: {:?}", repo_path);
        Ok(())
    }
}

impl Default for FileSearchCache {
    fn default() -> Self {
        Self::new()
    }
}
</file>

<file path="crates/services/src/services/filesystem_watcher.rs">
use std::{
    path::{Path, PathBuf},
    sync::Arc,
    time::Duration,
};

use futures::{
    SinkExt,
    channel::mpsc::{Receiver, channel},
};
use ignore::{
    WalkBuilder,
    gitignore::{Gitignore, GitignoreBuilder},
};
use notify::{RecommendedWatcher, RecursiveMode};
use notify_debouncer_full::{
    DebounceEventResult, DebouncedEvent, Debouncer, RecommendedCache, new_debouncer,
};
use thiserror::Error;

pub type WatcherComponents = (
    Debouncer<RecommendedWatcher, RecommendedCache>,
    Receiver<DebounceEventResult>,
    PathBuf,
);

#[derive(Debug, Error)]
pub enum FilesystemWatcherError {
    #[error(transparent)]
    Notify(#[from] notify::Error),
    #[error(transparent)]
    Ignore(#[from] ignore::Error),
    #[error(transparent)]
    IoError(#[from] std::io::Error),
    #[error("Failed to build gitignore: {0}")]
    GitignoreBuilder(String),
    #[error("Invalid path: {0}")]
    InvalidPath(String),
}

fn canonicalize_lossy(path: &Path) -> PathBuf {
    dunce::canonicalize(path).unwrap_or_else(|_| path.to_path_buf())
}

fn build_gitignore_set(root: &Path) -> Result<Gitignore, FilesystemWatcherError> {
    let mut builder = GitignoreBuilder::new(root);

    // Walk once to collect all .gitignore files under root
    for result in WalkBuilder::new(root)
        .follow_links(false)
        .hidden(false) // we *want* to see .gitignore
        .standard_filters(false) // do not apply default ignores while walking
        .git_ignore(false) // we'll add them manually
        .git_exclude(false)
        .build()
    {
        let dir_entry = result?;
        if dir_entry
            .file_type()
            .map(|ft| ft.is_file())
            .unwrap_or(false)
            && dir_entry
                .path()
                .file_name()
                .is_some_and(|name| name == ".gitignore")
        {
            builder.add(dir_entry.path());
        }
    }

    // Optionally include repo-local excludes
    let info_exclude = root.join(".git/info/exclude");
    if info_exclude.exists() {
        builder.add(info_exclude);
    }

    Ok(builder.build()?)
}

fn path_allowed(path: &Path, gi: &Gitignore, canonical_root: &Path) -> bool {
    let canonical_path = canonicalize_lossy(path);

    // Convert absolute path to relative path from the gitignore root
    let relative_path = match canonical_path.strip_prefix(canonical_root) {
        Ok(rel_path) => rel_path,
        Err(_) => {
            // Path is outside the watched root, don't ignore it
            return true;
        }
    };

    // Heuristic: assume paths without extensions are directories
    // This works for most cases and avoids filesystem syscalls
    let is_dir = relative_path.extension().is_none();
    let matched = gi.matched_path_or_any_parents(relative_path, is_dir);

    !matched.is_ignore()
}

fn debounced_should_forward(event: &DebouncedEvent, gi: &Gitignore, canonical_root: &Path) -> bool {
    // DebouncedEvent is a struct that wraps the underlying notify::Event
    // We can check its paths field to determine if the event should be forwarded
    event
        .paths
        .iter()
        .all(|path| path_allowed(path, gi, canonical_root))
}

pub fn async_watcher(root: PathBuf) -> Result<WatcherComponents, FilesystemWatcherError> {
    let canonical_root = canonicalize_lossy(&root);
    let gi_set = Arc::new(build_gitignore_set(&canonical_root)?);
    let (mut tx, rx) = channel(64); // Increased capacity for error bursts

    let gi_clone = gi_set.clone();
    let root_clone = canonical_root.clone();

    let mut debouncer = new_debouncer(
        Duration::from_millis(200),
        None, // Use default config
        move |res: DebounceEventResult| {
            match res {
                Ok(events) => {
                    // Filter events and only send allowed ones
                    let filtered_events: Vec<DebouncedEvent> = events
                        .into_iter()
                        .filter(|ev| debounced_should_forward(ev, &gi_clone, &root_clone))
                        .collect();

                    if !filtered_events.is_empty() {
                        let filtered_result = Ok(filtered_events);
                        futures::executor::block_on(async {
                            tx.send(filtered_result).await.ok();
                        });
                    }
                }
                Err(errors) => {
                    // Always forward errors
                    futures::executor::block_on(async {
                        tx.send(Err(errors)).await.ok();
                    });
                }
            }
        },
    )?;

    // Start watching the root directory
    debouncer.watch(&canonical_root, RecursiveMode::Recursive)?;

    Ok((debouncer, rx, canonical_root))
}
</file>

<file path="crates/services/src/services/filesystem.rs">
use std::{
    collections::HashSet,
    fs,
    path::{Path, PathBuf},
};

use ignore::WalkBuilder;
use serde::Serialize;
use thiserror::Error;
use tokio_util::sync::CancellationToken;
use ts_rs::TS;

#[derive(Clone)]
pub struct FilesystemService {}

#[derive(Debug, Error)]
pub enum FilesystemError {
    #[error("Directory does not exist")]
    DirectoryDoesNotExist,
    #[error("Path is not a directory")]
    PathIsNotDirectory,
    #[error("Failed to read directory: {0}")]
    Io(#[from] std::io::Error),
}
#[derive(Debug, Serialize, TS)]
pub struct DirectoryListResponse {
    pub entries: Vec<DirectoryEntry>,
    pub current_path: String,
}

#[derive(Debug, Serialize, TS)]
pub struct DirectoryEntry {
    pub name: String,
    pub path: PathBuf,
    pub is_directory: bool,
    pub is_git_repo: bool,
    pub last_modified: Option<u64>,
}

impl Default for FilesystemService {
    fn default() -> Self {
        Self::new()
    }
}

impl FilesystemService {
    pub fn new() -> Self {
        FilesystemService {}
    }

    fn get_directories_to_skip() -> HashSet<String> {
        let mut skip_dirs = HashSet::from(
            [
                "node_modules",
                "target",
                "build",
                "dist",
                ".next",
                ".nuxt",
                ".cache",
                ".npm",
                ".yarn",
                ".pnpm-store",
                "Library",
                "AppData",
                "Applications",
            ]
            .map(String::from),
        );

        [
            dirs::executable_dir(),
            dirs::data_dir(),
            dirs::download_dir(),
            dirs::picture_dir(),
            dirs::video_dir(),
            dirs::audio_dir(),
        ]
        .into_iter()
        .flatten()
        .filter_map(|path| path.file_name()?.to_str().map(String::from))
        .for_each(|name| {
            skip_dirs.insert(name);
        });

        skip_dirs
    }

    pub async fn list_git_repos(
        &self,
        path: Option<String>,
        timeout_ms: u64,
        hard_timeout_ms: u64,
        max_depth: Option<usize>,
    ) -> Result<Vec<DirectoryEntry>, FilesystemError> {
        let base_path = path
            .map(PathBuf::from)
            .unwrap_or_else(Self::get_home_directory);
        Self::verify_directory(&base_path)?;
        self.list_git_repos_with_timeout(vec![base_path], timeout_ms, hard_timeout_ms, max_depth)
            .await
    }

    async fn list_git_repos_with_timeout(
        &self,
        paths: Vec<PathBuf>,
        timeout_ms: u64,
        hard_timeout_ms: u64,
        max_depth: Option<usize>,
    ) -> Result<Vec<DirectoryEntry>, FilesystemError> {
        let cancel_token = CancellationToken::new();
        let cancel_after_delay = cancel_token.clone();
        tokio::spawn(async move {
            tokio::time::sleep(std::time::Duration::from_millis(timeout_ms)).await;
            cancel_after_delay.cancel();
        });
        let service = self.clone();
        let cancel_for_scan = cancel_token.clone();
        let mut scan_handle = tokio::spawn(async move {
            service
                .list_git_repos_inner(paths, max_depth, Some(&cancel_for_scan))
                .await
        });

        let hard_timeout = tokio::time::sleep(std::time::Duration::from_millis(hard_timeout_ms));
        tokio::pin!(hard_timeout);

        tokio::select! {
            res = &mut scan_handle => {
                match res {
                    Ok(Ok(repos)) => Ok(repos),
                    Ok(Err(err)) => Err(err),
                    Err(join_err) => Err(FilesystemError::Io(
                        std::io::Error::other(join_err.to_string())))
                }
                }
            _ = &mut hard_timeout => {
                scan_handle.abort();
                tracing::warn!("list_git_repos_with_timeout: hard timeout reached after {}ms", hard_timeout_ms);
                Err(FilesystemError::Io(std::io::Error::new(
                    std::io::ErrorKind::TimedOut,
                    "Operation forcibly terminated due to hard timeout",
                )))
            }
        }
    }

    pub async fn list_common_git_repos(
        &self,
        timeout_ms: u64,
        hard_timeout_ms: u64,
        max_depth: Option<usize>,
    ) -> Result<Vec<DirectoryEntry>, FilesystemError> {
        let search_strings = ["repos", "dev", "work", "code", "projects"];
        let home_dir = Self::get_home_directory();
        let mut paths: Vec<PathBuf> = search_strings
            .iter()
            .map(|s| home_dir.join(s))
            .filter(|p| p.exists() && p.is_dir())
            .collect();
        paths.insert(0, home_dir);
        if let Some(cwd) = std::env::current_dir().ok()
            && cwd.exists()
            && cwd.is_dir()
        {
            paths.insert(0, cwd);
        }
        self.list_git_repos_with_timeout(paths, timeout_ms, hard_timeout_ms, max_depth)
            .await
    }

    async fn list_git_repos_inner(
        &self,
        path: Vec<PathBuf>,
        max_depth: Option<usize>,
        cancel: Option<&CancellationToken>,
    ) -> Result<Vec<DirectoryEntry>, FilesystemError> {
        let base_dir = match path.first() {
            Some(dir) => dir,
            None => return Ok(vec![]),
        };
        let skip_dirs = Self::get_directories_to_skip();
        let mut walker_builder = WalkBuilder::new(base_dir);
        walker_builder
            .follow_links(false)
            .hidden(true) // true to skip hidden files
            .git_ignore(true)
            .filter_entry({
                let cancel = cancel.cloned();
                move |entry| {
                    if let Some(token) = cancel.as_ref()
                        && token.is_cancelled()
                    {
                        tracing::debug!("Cancellation token triggered");
                        return false;
                    }

                    let path = entry.path();
                    if !path.is_dir() {
                        return false;
                    }

                    // Skip common non-git folders
                    if let Some(name) = path.file_name().and_then(|n| n.to_str())
                        && skip_dirs.contains(name)
                    {
                        return false;
                    }

                    true
                }
            })
            .max_depth(max_depth)
            .git_exclude(true);
        for p in path.iter().skip(1) {
            walker_builder.add(p);
        }
        let mut git_repos: Vec<DirectoryEntry> = walker_builder
            .build()
            .filter_map(|entry| {
                let entry = entry.ok()?;
                let name = entry.file_name().to_str()?;
                if !entry.path().join(".git").exists() {
                    return None;
                }
                let last_modified = entry
                    .metadata()
                    .ok()
                    .and_then(|m| m.modified().ok())
                    .map(|t| t.elapsed().unwrap_or_default().as_secs());
                Some(DirectoryEntry {
                    name: name.to_string(),
                    path: entry.into_path(),
                    is_directory: true,
                    is_git_repo: true,
                    last_modified,
                })
            })
            .collect();
        git_repos.sort_by_key(|entry| entry.last_modified.unwrap_or(0));
        Ok(git_repos)
    }

    fn get_home_directory() -> PathBuf {
        dirs::home_dir()
            .or_else(dirs::desktop_dir)
            .or_else(dirs::document_dir)
            .unwrap_or_else(|| {
                if cfg!(windows) {
                    std::env::var("USERPROFILE")
                        .map(PathBuf::from)
                        .unwrap_or_else(|_| PathBuf::from("C:\\"))
                } else {
                    PathBuf::from("/")
                }
            })
    }

    fn verify_directory(path: &Path) -> Result<(), FilesystemError> {
        if !path.exists() {
            return Err(FilesystemError::DirectoryDoesNotExist);
        }
        if !path.is_dir() {
            return Err(FilesystemError::PathIsNotDirectory);
        }
        Ok(())
    }

    pub async fn list_directory(
        &self,
        path: Option<String>,
    ) -> Result<DirectoryListResponse, FilesystemError> {
        let path = path
            .map(PathBuf::from)
            .unwrap_or_else(Self::get_home_directory);
        Self::verify_directory(&path)?;

        let entries = fs::read_dir(&path)?;
        let mut directory_entries = Vec::new();

        for entry in entries.flatten() {
            let path = entry.path();
            let metadata = entry.metadata().ok();
            if let Some(name) = path.file_name().and_then(|n| n.to_str()) {
                // Skip hidden files/directories
                if name.starts_with('.') && name != ".." {
                    continue;
                }

                let is_directory = metadata.is_some_and(|m| m.is_dir());
                let is_git_repo = if is_directory {
                    path.join(".git").exists()
                } else {
                    false
                };

                directory_entries.push(DirectoryEntry {
                    name: name.to_string(),
                    path,
                    is_directory,
                    is_git_repo,
                    last_modified: None,
                });
            }
        }
        // Sort: directories first, then files, both alphabetically
        directory_entries.sort_by(|a, b| match (a.is_directory, b.is_directory) {
            (true, false) => std::cmp::Ordering::Less,
            (false, true) => std::cmp::Ordering::Greater,
            _ => a.name.to_lowercase().cmp(&b.name.to_lowercase()),
        });

        Ok(DirectoryListResponse {
            entries: directory_entries,
            current_path: path.to_string_lossy().to_string(),
        })
    }
}
</file>

<file path="crates/services/src/services/git_cli.rs">
//! Why we prefer the Git CLI here
//!
//! - Safer working-tree semantics: the `git` CLI refuses to clobber uncommitted
//!   tracked changes and untracked files during checkout/merge/rebase unless you
//!   explicitly force it. libgit2 does not enforce those protections by default,
//!   which means callers must re‑implement a lot of safety checks to avoid data loss.
//! - Sparse‑checkout correctness: the CLI natively respects sparse‑checkout.
//!   libgit2 does not yet support sparse‑checkout semantics the same way, which
//!   led to incorrect diffs and staging in our workflows.
//! - Cross‑platform stability: we observed libgit2 corrupt repositories shared
//!   between WSL and Windows in scenarios where the `git` CLI did not. Delegating
//!   working‑tree mutations to the CLI has proven more reliable in practice.
//!
//! Given these reasons, this module centralizes destructive or working‑tree‑
//! touching operations (rebase, merge, checkout, add/commit, etc.) through the
//! `git` CLI, while keeping libgit2 for read‑only graph queries and credentialed
//! network operations when useful.
use std::{
    ffi::{OsStr, OsString},
    path::Path,
    process::Command,
};

use base64::{Engine, engine::general_purpose::STANDARD as BASE64_STANDARD};
use thiserror::Error;
use utils::shell::resolve_executable_path;

#[derive(Debug, Error)]
pub enum GitCliError {
    #[error("git executable not found or not runnable")]
    NotAvailable,
    #[error("git command failed: {0}")]
    CommandFailed(String),
    #[error("authentication failed: {0}")]
    AuthFailed(String),
    #[error("push rejected: {0}")]
    PushRejected(String),
    #[error("rebase in progress in this worktree")]
    RebaseInProgress,
}

#[derive(Clone, Default)]
pub struct GitCli;

/// Parsed change type from `git diff --name-status` output
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ChangeType {
    Added,
    Modified,
    Deleted,
    Renamed,
    Copied,
    TypeChanged,
    Unmerged,
    Unknown(String),
}

/// One entry from a status diff (name-status + paths)
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct StatusDiffEntry {
    pub change: ChangeType,
    pub path: String,
    pub old_path: Option<String>,
}

/// Parsed worktree entry from `git worktree list --porcelain`
#[derive(Debug, Clone)]
pub struct WorktreeEntry {
    pub path: String,
    pub head_sha: String,
    pub branch: Option<String>,
}

#[derive(Debug, Clone, Default)]
pub struct StatusDiffOptions {
    pub path_filter: Option<Vec<String>>, // pathspecs to limit diff
}

impl GitCli {
    pub fn new() -> Self {
        Self {}
    }

    /// Run `git -C <repo> worktree add <path> <branch>` (optionally creating the branch with -b)
    pub fn worktree_add(
        &self,
        repo_path: &Path,
        worktree_path: &Path,
        branch: &str,
        create_branch: bool,
    ) -> Result<(), GitCliError> {
        self.ensure_available()?;

        let mut args: Vec<OsString> = vec!["worktree".into(), "add".into()];
        if create_branch {
            args.push("-b".into());
            args.push(OsString::from(branch));
        }
        args.push(worktree_path.as_os_str().into());
        args.push(OsString::from(branch));
        self.git(repo_path, args)?;

        // Good practice: reapply sparse-checkout in the new worktree to ensure materialization matches
        // Non-fatal if it fails or not configured.
        let _ = self.git(worktree_path, ["sparse-checkout", "reapply"]);

        Ok(())
    }

    /// Run `git -C <repo> worktree remove <path>`
    pub fn worktree_remove(
        &self,
        repo_path: &Path,
        worktree_path: &Path,
        force: bool,
    ) -> Result<(), GitCliError> {
        self.ensure_available()?;
        let mut args: Vec<OsString> = vec!["worktree".into(), "remove".into()];
        if force {
            args.push("--force".into());
        }
        args.push(worktree_path.as_os_str().into());
        self.git(repo_path, args)?;
        Ok(())
    }

    /// Prune stale worktree metadata
    pub fn worktree_prune(&self, repo_path: &Path) -> Result<(), GitCliError> {
        self.git(repo_path, ["worktree", "prune"])?;
        Ok(())
    }

    /// Return true if there are any changes in the working tree (staged or unstaged).
    pub fn has_changes(&self, worktree_path: &Path) -> Result<bool, GitCliError> {
        let out = self.git(worktree_path, ["status", "--porcelain"])?;
        Ok(!out.is_empty())
    }

    /// Diff status vs a base branch using a temporary index (always includes untracked).
    /// Path filter limits the reported paths.
    pub fn diff_status(
        &self,
        worktree_path: &Path,
        base_branch: &str,
        opts: StatusDiffOptions,
    ) -> Result<Vec<StatusDiffEntry>, GitCliError> {
        // Create a temp index file
        let tmp_dir = tempfile::TempDir::new()
            .map_err(|e| GitCliError::CommandFailed(format!("temp dir create failed: {e}")))?;
        let tmp_index = tmp_dir.path().join("index");
        let envs = vec![(
            OsString::from("GIT_INDEX_FILE"),
            tmp_index.as_os_str().to_os_string(),
        )];

        // Use a temp index from HEAD to accurately track renames in untracked files
        let _ = self.git_with_env(worktree_path, ["read-tree", "HEAD"], &envs)?;

        // Stage all in temp index
        let _ = self.git_with_env(worktree_path, ["add", "-A"], &envs)?;

        // git diff --cached
        let mut args: Vec<OsString> = vec![
            "-c".into(),
            "core.quotepath=false".into(),
            "diff".into(),
            "--cached".into(),
            "-M".into(),
            "--name-status".into(),
            OsString::from(base_branch),
        ];
        if let Some(paths) = &opts.path_filter {
            let non_empty_paths: Vec<&str> = paths
                .iter()
                .map(|s| s.as_str())
                .filter(|p| !p.trim().is_empty())
                .collect();
            if !non_empty_paths.is_empty() {
                args.push("--".into());
                for p in non_empty_paths {
                    args.push(OsString::from(p));
                }
            }
        }
        let out = self.git_with_env(worktree_path, args, &envs)?;
        Ok(Self::parse_name_status(&out))
    }

    /// Return `git status --porcelain` parsed into a structured summary
    pub fn get_worktree_status(&self, worktree_path: &Path) -> Result<WorktreeStatus, GitCliError> {
        let out = self.git(worktree_path, ["status", "--porcelain"])?;
        let mut entries: Vec<StatusEntry> = Vec::new();
        let mut uncommitted_tracked = 0usize;
        let mut untracked = 0usize;
        for line in out.lines() {
            let l = line.trim_end();
            if l.is_empty() {
                continue;
            }
            // Two columns (XY) + space + path(s), or '?? path' for untracked
            if let Some(rest) = l.strip_prefix("?? ") {
                untracked += 1;
                entries.push(StatusEntry {
                    staged: '?',
                    unstaged: '?',
                    path: rest.to_string(),
                    orig_path: None,
                    is_untracked: true,
                });
                continue;
            }
            // At least 3 chars (X, Y, space)
            let (xy, tail) = l.split_at(2);
            let (_, pathspec) = tail.split_at(1); // skip the space
            let staged = xy.chars().nth(0).unwrap_or(' ');
            let unstaged = xy.chars().nth(1).unwrap_or(' ');
            // Rename shows as 'R ' with `old -> new`
            let (path, orig_path) = if pathspec.contains(" -> ") {
                let mut parts = pathspec.splitn(2, " -> ");
                let oldp = parts.next().unwrap_or("").to_string();
                let newp = parts.next().unwrap_or("").to_string();
                (newp, Some(oldp))
            } else {
                (pathspec.to_string(), None)
            };
            // Count as tracked change if either column indicates a change
            if staged != ' ' || unstaged != ' ' {
                uncommitted_tracked += 1;
            }
            entries.push(StatusEntry {
                staged,
                unstaged,
                path,
                orig_path,
                is_untracked: false,
            });
        }
        Ok(WorktreeStatus {
            uncommitted_tracked,
            untracked,
            entries,
        })
    }

    /// Stage all changes in the working tree (respects sparse-checkout semantics).
    pub fn add_all(&self, worktree_path: &Path) -> Result<(), GitCliError> {
        self.git(worktree_path, ["add", "-A"])?;
        Ok(())
    }

    pub fn list_worktrees(&self, repo_path: &Path) -> Result<Vec<WorktreeEntry>, GitCliError> {
        let out = self.git(repo_path, ["worktree", "list", "--porcelain"])?;
        let mut entries = Vec::new();
        let mut current_path: Option<String> = None;
        let mut current_head: Option<String> = None;
        let mut current_branch: Option<String> = None;

        for line in out.lines() {
            let line = line.trim();

            if line.is_empty() {
                // End of current worktree entry, save it if we have required data
                if let (Some(path), Some(head)) = (current_path.take(), current_head.take()) {
                    entries.push(WorktreeEntry {
                        path,
                        head_sha: head,
                        branch: current_branch.take(),
                    });
                }
            } else if let Some(path) = line.strip_prefix("worktree ") {
                current_path = Some(path.to_string());
            } else if let Some(head) = line.strip_prefix("HEAD ") {
                current_head = Some(head.to_string());
            } else if let Some(branch_ref) = line.strip_prefix("branch ") {
                // Extract branch name from refs/heads/branch-name
                current_branch = branch_ref
                    .strip_prefix("refs/heads/")
                    .map(|name| name.to_string());
            }
        }

        // Handle the last entry if no trailing empty line
        if let (Some(path), Some(head)) = (current_path, current_head) {
            entries.push(WorktreeEntry {
                path,
                head_sha: head,
                branch: current_branch,
            });
        }

        Ok(entries)
    }

    /// Commit staged changes with the given message.
    pub fn commit(&self, worktree_path: &Path, message: &str) -> Result<(), GitCliError> {
        self.git(worktree_path, ["commit", "-m", message])?;
        Ok(())
    }
    /// Fetch a branch to the given remote using an HTTPS token for authentication.
    pub fn fetch_with_token_and_refspec(
        &self,
        repo_path: &Path,
        remote_url: &str,
        refspec: &str,
        token: &str,
    ) -> Result<(), GitCliError> {
        let auth_header = self.build_auth_header(token);
        let envs = self.build_token_env(&auth_header);

        let args = [
            OsString::from("-c"),
            OsString::from("credential.helper="),
            OsString::from("--config-env"),
            OsString::from("http.extraHeader=GIT_HTTP_EXTRAHEADER"),
            OsString::from("fetch"),
            OsString::from(remote_url),
            OsString::from(refspec),
        ];

        match self.git_with_env(repo_path, args, &envs) {
            Ok(_) => Ok(()),
            Err(GitCliError::CommandFailed(msg)) => Err(self.classify_cli_error(msg)),
            Err(err) => Err(err),
        }
    }

    /// Push a branch to the given remote using an HTTPS token for authentication.
    pub fn push_with_token(
        &self,
        repo_path: &Path,
        remote_url: &str,
        branch: &str,
        token: &str,
    ) -> Result<(), GitCliError> {
        let refspec = format!("refs/heads/{branch}:refs/heads/{branch}");
        let auth_header = self.build_auth_header(token);
        let envs = self.build_token_env(&auth_header);

        let args = [
            OsString::from("-c"),
            OsString::from("credential.helper="),
            OsString::from("--config-env"),
            OsString::from("http.extraHeader=GIT_HTTP_EXTRAHEADER"),
            OsString::from("push"),
            OsString::from(remote_url),
            OsString::from(refspec),
        ];

        match self.git_with_env(repo_path, args, &envs) {
            Ok(_) => Ok(()),
            Err(GitCliError::CommandFailed(msg)) => Err(self.classify_cli_error(msg)),
            Err(err) => Err(err),
        }
    }

    // Parse `git diff --name-status` output into structured entries.
    // Handles rename/copy scores like `R100` by matching the first letter.
    fn parse_name_status(output: &str) -> Vec<StatusDiffEntry> {
        let mut out = Vec::new();
        for line in output.lines() {
            let line = line.trim_end();
            if line.is_empty() {
                continue;
            }
            let mut parts = line.split('\t');
            let code = parts.next().unwrap_or("");
            let change = match code.chars().next().unwrap_or('?') {
                'A' => ChangeType::Added,
                'M' => ChangeType::Modified,
                'D' => ChangeType::Deleted,
                'R' => ChangeType::Renamed,
                'C' => ChangeType::Copied,
                'T' => ChangeType::TypeChanged,
                'U' => ChangeType::Unmerged,
                other => ChangeType::Unknown(other.to_string()),
            };

            match change {
                ChangeType::Renamed | ChangeType::Copied => {
                    if let (Some(old), Some(newp)) = (parts.next(), parts.next()) {
                        out.push(StatusDiffEntry {
                            change,
                            path: newp.to_string(),
                            old_path: Some(old.to_string()),
                        });
                    }
                }
                _ => {
                    if let Some(p) = parts.next() {
                        out.push(StatusDiffEntry {
                            change,
                            path: p.to_string(),
                            old_path: None,
                        });
                    }
                }
            }
        }
        out
    }

    /// Perform `git rebase --onto <new_base> <old_base>` on the current branch in `worktree_path`.
    pub fn rebase_onto(
        &self,
        worktree_path: &Path,
        new_base: &str,
        old_base: &str,
    ) -> Result<(), GitCliError> {
        // If a rebase is in progress, refuse to proceed. The caller can
        // choose to abort or continue; we avoid destructive actions here.
        if self.is_rebase_in_progress(worktree_path).unwrap_or(false) {
            return Err(GitCliError::RebaseInProgress);
        }
        self.git(worktree_path, ["rebase", "--onto", new_base, old_base])?;
        Ok(())
    }

    /// Return true if there is a rebase in progress in this worktree.
    /// We treat this as true when either of Git's rebase state directories exists:
    /// - rebase-merge (interactive rebase)
    /// - rebase-apply (am-based rebase)
    pub fn is_rebase_in_progress(&self, worktree_path: &Path) -> Result<bool, GitCliError> {
        let rebase_merge = self.git(worktree_path, ["rev-parse", "--git-path", "rebase-merge"])?;
        let rebase_apply = self.git(worktree_path, ["rev-parse", "--git-path", "rebase-apply"])?;
        let rm_exists = std::path::Path::new(rebase_merge.trim()).exists();
        let ra_exists = std::path::Path::new(rebase_apply.trim()).exists();
        Ok(rm_exists || ra_exists)
    }

    /// Return true if a merge is in progress (MERGE_HEAD exists).
    pub fn is_merge_in_progress(&self, worktree_path: &Path) -> Result<bool, GitCliError> {
        match self.git(worktree_path, ["rev-parse", "--verify", "MERGE_HEAD"]) {
            Ok(_) => Ok(true),
            Err(GitCliError::CommandFailed(_)) => Ok(false),
            Err(e) => Err(e),
        }
    }

    /// Return true if a cherry-pick is in progress (CHERRY_PICK_HEAD exists).
    pub fn is_cherry_pick_in_progress(&self, worktree_path: &Path) -> Result<bool, GitCliError> {
        match self.git(worktree_path, ["rev-parse", "--verify", "CHERRY_PICK_HEAD"]) {
            Ok(_) => Ok(true),
            Err(GitCliError::CommandFailed(_)) => Ok(false),
            Err(e) => Err(e),
        }
    }

    /// Return true if a revert is in progress (REVERT_HEAD exists).
    pub fn is_revert_in_progress(&self, worktree_path: &Path) -> Result<bool, GitCliError> {
        match self.git(worktree_path, ["rev-parse", "--verify", "REVERT_HEAD"]) {
            Ok(_) => Ok(true),
            Err(GitCliError::CommandFailed(_)) => Ok(false),
            Err(e) => Err(e),
        }
    }

    /// Abort an in-progress rebase in this worktree. If no rebase is in progress,
    /// this is a no-op and returns Ok(()).
    pub fn abort_rebase(&self, worktree_path: &Path) -> Result<(), GitCliError> {
        // If nothing to abort, return success
        if !self.is_rebase_in_progress(worktree_path)? {
            return Ok(());
        }
        // Best-effort: if `git rebase --abort` fails, surface the error message
        self.git(worktree_path, ["rebase", "--abort"]).map(|_| ())
    }

    /// Quit an in-progress rebase (cleanup metadata without modifying commits).
    /// If no rebase is in progress, it's a no-op.
    pub fn quit_rebase(&self, worktree_path: &Path) -> Result<(), GitCliError> {
        if !self.is_rebase_in_progress(worktree_path)? {
            return Ok(());
        }
        self.git(worktree_path, ["rebase", "--quit"]).map(|_| ())
    }

    /// Return true if there are staged changes (index differs from HEAD)
    pub fn has_staged_changes(&self, repo_path: &Path) -> Result<bool, GitCliError> {
        // `git diff --cached --quiet` returns exit code 1 if there are differences
        let out = Command::new(resolve_executable_path("git").ok_or(GitCliError::NotAvailable)?)
            .arg("-C")
            .arg(repo_path)
            .arg("diff")
            .arg("--cached")
            .arg("--quiet")
            .output()
            .map_err(|e| GitCliError::CommandFailed(e.to_string()))?;
        match out.status.code() {
            Some(0) => Ok(false),
            Some(1) => Ok(true),
            _ => Err(GitCliError::CommandFailed(
                String::from_utf8_lossy(&out.stderr).trim().to_string(),
            )),
        }
    }

    /// Reset index to HEAD (mixed reset). Does not modify working tree.
    pub fn reset(&self, repo_path: &Path) -> Result<(), GitCliError> {
        self.git(repo_path, ["reset"]).map(|_| ())
    }

    /// Checkout base branch, squash-merge from_branch, and commit with message. Returns new HEAD sha.
    pub fn merge_squash_commit(
        &self,
        repo_path: &Path,
        base_branch: &str,
        from_branch: &str,
        message: &str,
    ) -> Result<String, GitCliError> {
        self.git(repo_path, ["checkout", base_branch]).map(|_| ())?;
        self.git(repo_path, ["merge", "--squash", "--no-commit", from_branch])
            .map(|_| ())?;
        self.git(repo_path, ["commit", "-m", message]).map(|_| ())?;
        let sha = self
            .git(repo_path, ["rev-parse", "HEAD"])?
            .trim()
            .to_string();
        Ok(sha)
    }

    /// Update a ref to a specific sha in the repo.
    pub fn update_ref(
        &self,
        repo_path: &Path,
        refname: &str,
        sha: &str,
    ) -> Result<(), GitCliError> {
        self.git(repo_path, ["update-ref", refname, sha])
            .map(|_| ())
    }

    pub fn abort_merge(&self, worktree_path: &Path) -> Result<(), GitCliError> {
        if !self.is_merge_in_progress(worktree_path)? {
            return Ok(());
        }
        self.git(worktree_path, ["merge", "--abort"]).map(|_| ())
    }

    pub fn abort_cherry_pick(&self, worktree_path: &Path) -> Result<(), GitCliError> {
        if !self.is_cherry_pick_in_progress(worktree_path)? {
            return Ok(());
        }
        self.git(worktree_path, ["cherry-pick", "--abort"])
            .map(|_| ())
    }

    pub fn abort_revert(&self, worktree_path: &Path) -> Result<(), GitCliError> {
        if !self.is_revert_in_progress(worktree_path)? {
            return Ok(());
        }
        self.git(worktree_path, ["revert", "--abort"]).map(|_| ())
    }

    /// List files currently in a conflicted (unmerged) state in the worktree.
    pub fn get_conflicted_files(&self, worktree_path: &Path) -> Result<Vec<String>, GitCliError> {
        // `--diff-filter=U` lists paths with unresolved conflicts
        let out = self.git(worktree_path, ["diff", "--name-only", "--diff-filter=U"])?;
        let mut files = Vec::new();
        for line in out.lines() {
            let p = line.trim();
            if !p.is_empty() {
                files.push(p.to_string());
            }
        }
        Ok(files)
    }
}

// Private methods
impl GitCli {
    fn classify_cli_error(&self, msg: String) -> GitCliError {
        let lower = msg.to_ascii_lowercase();
        if lower.contains("authentication failed")
            || lower.contains("could not read username")
            || lower.contains("invalid username or password")
        {
            GitCliError::AuthFailed(msg)
        } else if lower.contains("non-fast-forward")
            || lower.contains("failed to push some refs")
            || lower.contains("fetch first")
            || lower.contains("updates were rejected because the tip")
        {
            GitCliError::PushRejected(msg)
        } else {
            GitCliError::CommandFailed(msg)
        }
    }

    fn build_auth_header(&self, token: &str) -> String {
        let auth_value = BASE64_STANDARD.encode(format!("x-access-token:{token}"));
        format!("Authorization: Basic {auth_value}")
    }

    fn build_token_env(&self, auth_header: &str) -> Vec<(OsString, OsString)> {
        vec![
            (OsString::from("GIT_TERMINAL_PROMPT"), OsString::from("0")),
            (OsString::from("GIT_ASKPASS"), OsString::from("")),
            (OsString::from("SSH_ASKPASS"), OsString::from("")),
            (
                OsString::from("GIT_HTTP_EXTRAHEADER"),
                OsString::from(auth_header),
            ),
        ]
    }

    /// Ensure `git` is available on PATH
    fn ensure_available(&self) -> Result<(), GitCliError> {
        let git = resolve_executable_path("git").ok_or(GitCliError::NotAvailable)?;
        let out = Command::new(&git)
            .arg("--version")
            .output()
            .map_err(|_| GitCliError::NotAvailable)?;
        if out.status.success() {
            Ok(())
        } else {
            Err(GitCliError::NotAvailable)
        }
    }

    /// Run `git -C <repo_path> <args...>` and return stdout on success.
    /// Prefer adding specific helpers (e.g. `get_worktree_status`, `diff_status`)
    /// instead of calling this directly, so all parsing and command choices are
    /// centralized here. This makes it easier to change the underlying commands
    /// without adjusting callers. Use this low-level method directly only in
    /// tests or when no dedicated helper exists yet.
    ///
    /// About `OsStr`/`OsString` usage:
    /// - `Command` and `Path` operate on `OsStr` to support non‑UTF‑8 paths and
    ///   arguments across platforms. Using `String` would force lossy conversion
    ///   or partial failures. This API accepts anything that implements
    ///   `AsRef<OsStr>` so typical call sites can still pass `&str` literals or
    ///   owned `String`s without friction.
    pub fn git<I, S>(&self, repo_path: &Path, args: I) -> Result<String, GitCliError>
    where
        I: IntoIterator<Item = S>,
        S: AsRef<OsStr>,
    {
        self.ensure_available()?;
        let git = resolve_executable_path("git").ok_or(GitCliError::NotAvailable)?;
        let mut cmd = Command::new(&git);
        cmd.arg("-C").arg(repo_path);
        for a in args {
            cmd.arg(a);
        }
        let out = cmd
            .output()
            .map_err(|e| GitCliError::CommandFailed(e.to_string()))?;
        if !out.status.success() {
            let stderr = String::from_utf8_lossy(&out.stderr).trim().to_string();
            return Err(GitCliError::CommandFailed(stderr));
        }
        Ok(String::from_utf8_lossy(&out.stdout).to_string())
    }

    /// Like `git`, but allows passing additional environment variables.
    fn git_with_env<I, S>(
        &self,
        repo_path: &Path,
        args: I,
        envs: &[(OsString, OsString)],
    ) -> Result<String, GitCliError>
    where
        I: IntoIterator<Item = S>,
        S: AsRef<OsStr>,
    {
        self.ensure_available()?;
        let git = resolve_executable_path("git").ok_or(GitCliError::NotAvailable)?;
        let mut cmd = Command::new(&git);
        cmd.arg("-C").arg(repo_path);
        for (k, v) in envs {
            cmd.env(k, v);
        }
        for a in args {
            cmd.arg(a);
        }
        let out = cmd
            .output()
            .map_err(|e| GitCliError::CommandFailed(e.to_string()))?;
        if !out.status.success() {
            let stderr = String::from_utf8_lossy(&out.stderr).trim().to_string();
            return Err(GitCliError::CommandFailed(stderr));
        }
        Ok(String::from_utf8_lossy(&out.stdout).to_string())
    }
}
/// Parsed entry from `git status --porcelain`
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct StatusEntry {
    /// Single-letter staged status (column X) or '?' for untracked
    pub staged: char,
    /// Single-letter unstaged status (column Y) or '?' for untracked
    pub unstaged: char,
    /// Current path
    pub path: String,
    /// Original path (for renames)
    pub orig_path: Option<String>,
    /// True if this entry is untracked ("??")
    pub is_untracked: bool,
}

/// Summary + entries for a working tree status
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct WorktreeStatus {
    pub uncommitted_tracked: usize,
    pub untracked: usize,
    pub entries: Vec<StatusEntry>,
}
</file>

<file path="crates/services/src/services/git.rs">
use std::{collections::HashMap, path::Path};

use chrono::{DateTime, Utc};
use git2::{
    BranchType, Delta, DiffFindOptions, DiffOptions, Error as GitError, Reference, Remote,
    Repository, Sort, build::CheckoutBuilder,
};
use serde::{Deserialize, Serialize};
use thiserror::Error;
use ts_rs::TS;
use utils::diff::{Diff, DiffChangeKind, FileDiffDetails};

// Import for file ranking functionality
use super::file_ranker::FileStat;
use super::git_cli::{ChangeType, GitCli, GitCliError, StatusDiffEntry, StatusDiffOptions};
use crate::services::github_service::GitHubRepoInfo;

#[derive(Debug, Error)]
pub enum GitServiceError {
    #[error(transparent)]
    Git(#[from] GitError),
    #[error(transparent)]
    GitCLI(#[from] GitCliError),
    #[error(transparent)]
    IoError(#[from] std::io::Error),
    #[error("Invalid repository: {0}")]
    InvalidRepository(String),
    #[error("Branch not found: {0}")]
    BranchNotFound(String),
    #[error("Merge conflicts: {0}")]
    MergeConflicts(String),
    #[error("Branches diverged: {0}")]
    BranchesDiverged(String),
    #[error("{0} has uncommitted changes: {1}")]
    WorktreeDirty(String, String),
    #[error("No GitHub token available.")]
    TokenUnavailable,
    #[error("Rebase in progress; resolve or abort it before retrying")]
    RebaseInProgress,
}
/// Service for managing Git operations in task execution workflows
#[derive(Clone)]
pub struct GitService {}

// Max inline diff size for UI (in bytes). Files larger than this will have
// their contents omitted from the diff stream to avoid UI crashes.
const MAX_INLINE_DIFF_BYTES: usize = 50 * 1024; // ~50KB

#[derive(Debug, Clone, Serialize, Deserialize, TS, PartialEq, Eq)]
#[serde(rename_all = "snake_case")]
#[ts(rename_all = "snake_case")]
pub enum ConflictOp {
    Rebase,
    Merge,
    CherryPick,
    Revert,
}

#[derive(Debug, Serialize, TS)]
pub struct GitBranch {
    pub name: String,
    pub is_current: bool,
    pub is_remote: bool,
    #[ts(type = "Date")]
    pub last_commit_date: DateTime<Utc>,
}

#[derive(Debug, Clone)]
pub struct HeadInfo {
    pub branch: String,
    pub oid: String,
}

/// Target for diff generation
pub enum DiffTarget<'p> {
    /// Work-in-progress branch checked out in this worktree
    Worktree {
        worktree_path: &'p Path,
        branch_name: &'p str,
        base_branch: &'p str,
    },
    /// Fully committed branch vs base branch
    Branch {
        repo_path: &'p Path,
        branch_name: &'p str,
        base_branch: &'p str,
    },
    /// Specific commit vs base branch
    Commit {
        repo_path: &'p Path,
        commit_sha: &'p str,
    },
}

impl Default for GitService {
    fn default() -> Self {
        Self::new()
    }
}

impl GitService {
    /// Create a new GitService for the given repository path
    pub fn new() -> Self {
        Self {}
    }

    /// Open the repository
    fn open_repo(&self, repo_path: &Path) -> Result<Repository, GitServiceError> {
        Repository::open(repo_path).map_err(GitServiceError::from)
    }

    /// Ensure local (repo-scoped) identity exists for CLI commits.
    /// Sets user.name/email only if missing in the repo config.
    fn ensure_cli_commit_identity(&self, repo_path: &Path) -> Result<(), GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        let cfg = repo.config()?;
        let has_name = cfg.get_string("user.name").is_ok();
        let has_email = cfg.get_string("user.email").is_ok();
        if !(has_name && has_email) {
            let mut cfg = repo.config()?;
            cfg.set_str("user.name", "Vibe Kanban")?;
            cfg.set_str("user.email", "noreply@vibekanban.com")?;
        }
        Ok(())
    }

    /// Get a signature for libgit2 commits with a safe fallback identity.
    fn signature_with_fallback<'a>(
        &self,
        repo: &'a Repository,
    ) -> Result<git2::Signature<'a>, GitServiceError> {
        match repo.signature() {
            Ok(sig) => Ok(sig),
            Err(_) => git2::Signature::now("Vibe Kanban", "noreply@vibekanban.com")
                .map_err(GitServiceError::from),
        }
    }

    pub fn default_remote_name(&self, repo: &Repository) -> String {
        if let Ok(repos) = repo.remotes() {
            repos
                .iter()
                .flatten()
                .next()
                .map(|r| r.to_owned())
                .unwrap_or_else(|| "origin".to_string())
        } else {
            "origin".to_string()
        }
    }

    /// Initialize a new git repository with a main branch and initial commit
    pub fn initialize_repo_with_main_branch(
        &self,
        repo_path: &Path,
    ) -> Result<(), GitServiceError> {
        // Create directory if it doesn't exist
        if !repo_path.exists() {
            std::fs::create_dir_all(repo_path)?;
        }

        // Initialize git repository with main branch
        let repo = Repository::init_opts(
            repo_path,
            git2::RepositoryInitOptions::new()
                .initial_head("main")
                .mkdir(true),
        )?;

        // Create initial commit
        self.create_initial_commit(&repo)?;

        Ok(())
    }

    /// Ensure an existing repository has a main branch (for empty repos)
    pub fn ensure_main_branch_exists(&self, repo_path: &Path) -> Result<(), GitServiceError> {
        let repo = self.open_repo(repo_path)?;

        // Only create initial commit if repository is empty
        if repo.is_empty()? {
            self.create_initial_commit(&repo)?;
        }

        Ok(())
    }

    pub fn create_initial_commit(&self, repo: &Repository) -> Result<(), GitServiceError> {
        let signature = self.signature_with_fallback(repo)?;

        let tree_id = {
            let tree_builder = repo.treebuilder(None)?;
            tree_builder.write()?
        };
        let tree = repo.find_tree(tree_id)?;

        // Create initial commit on main branch
        let _commit_id = repo.commit(
            Some("refs/heads/main"),
            &signature,
            &signature,
            "Initial commit",
            &tree,
            &[],
        )?;

        // Set HEAD to point to main branch
        repo.set_head("refs/heads/main")?;

        Ok(())
    }

    pub fn commit(&self, path: &Path, message: &str) -> Result<bool, GitServiceError> {
        // Use Git CLI to respect sparse-checkout semantics for staging and commit
        let git = GitCli::new();
        let has_changes = git
            .has_changes(path)
            .map_err(|e| GitServiceError::InvalidRepository(format!("git status failed: {e}")))?;
        if !has_changes {
            tracing::debug!("No changes to commit!");
            return Ok(false);
        }

        git.add_all(path)
            .map_err(|e| GitServiceError::InvalidRepository(format!("git add failed: {e}")))?;
        // Only ensure identity once we know we're about to commit
        self.ensure_cli_commit_identity(path)?;
        git.commit(path, message)
            .map_err(|e| GitServiceError::InvalidRepository(format!("git commit failed: {e}")))?;
        Ok(true)
    }

    /// Get diffs between branches or worktree changes
    pub fn get_diffs(
        &self,
        target: DiffTarget,
        path_filter: Option<&[&str]>,
    ) -> Result<Vec<Diff>, GitServiceError> {
        match target {
            DiffTarget::Worktree {
                worktree_path,
                branch_name: _,
                base_branch,
            } => {
                // Use Git CLI to compute diff vs base to avoid sparse false deletions
                let repo = Repository::open(worktree_path)?;
                let base_git_branch = GitService::find_branch(&repo, base_branch)?;
                let base_tree = base_git_branch.get().peel_to_commit()?.tree()?;

                let git = GitCli::new();
                let cli_opts = StatusDiffOptions {
                    path_filter: path_filter.map(|fs| fs.iter().map(|s| s.to_string()).collect()),
                };
                let entries = git
                    .diff_status(worktree_path, base_branch, cli_opts)
                    .map_err(|e| {
                        GitServiceError::InvalidRepository(format!("git diff failed: {e}"))
                    })?;
                Ok(entries
                    .into_iter()
                    .map(|e| Self::status_entry_to_diff(&repo, &base_tree, e))
                    .collect())
            }
            DiffTarget::Branch {
                repo_path,
                branch_name,
                base_branch,
            } => {
                let repo = self.open_repo(repo_path)?;
                let base_tree = Self::find_branch(&repo, base_branch)?
                    .get()
                    .peel_to_commit()?
                    .tree()?;
                let branch_tree = Self::find_branch(&repo, branch_name)?
                    .get()
                    .peel_to_commit()?
                    .tree()?;

                let mut diff_opts = DiffOptions::new();
                diff_opts.include_typechange(true);

                // Add path filtering if specified
                if let Some(paths) = path_filter {
                    for path in paths {
                        diff_opts.pathspec(*path);
                    }
                }

                let mut diff = repo.diff_tree_to_tree(
                    Some(&base_tree),
                    Some(&branch_tree),
                    Some(&mut diff_opts),
                )?;

                // Enable rename detection
                let mut find_opts = DiffFindOptions::new();
                diff.find_similar(Some(&mut find_opts))?;

                self.convert_diff_to_file_diffs(diff, &repo)
            }
            DiffTarget::Commit {
                repo_path,
                commit_sha,
            } => {
                let repo = self.open_repo(repo_path)?;

                // Resolve commit and its baseline (the parent before the squash landed)
                let commit_oid = git2::Oid::from_str(commit_sha).map_err(|_| {
                    GitServiceError::InvalidRepository(format!("Invalid commit SHA: {commit_sha}"))
                })?;
                let commit = repo.find_commit(commit_oid)?;
                let parent = commit.parent(0).map_err(|_| {
                    GitServiceError::InvalidRepository(
                        "Commit has no parent; cannot diff a squash merge without a baseline"
                            .into(),
                    )
                })?;

                let parent_tree = parent.tree()?;
                let commit_tree = commit.tree()?;

                // Diff options
                let mut diff_opts = git2::DiffOptions::new();
                diff_opts.include_typechange(true);

                // Optional path filtering
                if let Some(paths) = path_filter {
                    for path in paths {
                        diff_opts.pathspec(*path);
                    }
                }

                // Compute the diff parent -> commit
                let mut diff = repo.diff_tree_to_tree(
                    Some(&parent_tree),
                    Some(&commit_tree),
                    Some(&mut diff_opts),
                )?;

                // Enable rename detection
                let mut find_opts = git2::DiffFindOptions::new();
                diff.find_similar(Some(&mut find_opts))?;

                self.convert_diff_to_file_diffs(diff, &repo)
            }
        }
    }

    /// Convert git2::Diff to our Diff structs
    fn convert_diff_to_file_diffs(
        &self,
        diff: git2::Diff,
        repo: &Repository,
    ) -> Result<Vec<Diff>, GitServiceError> {
        let mut file_diffs = Vec::new();

        let mut delta_index: usize = 0;
        diff.foreach(
            &mut |delta, _| {
                if delta.status() == Delta::Unreadable {
                    return true;
                }

                let status = delta.status();

                // Decide if we should omit content due to size
                let mut content_omitted = false;
                // Check old blob size when applicable
                if !matches!(status, Delta::Added) {
                    let oid = delta.old_file().id();
                    if !oid.is_zero()
                        && let Ok(blob) = repo.find_blob(oid)
                        && !blob.is_binary()
                        && blob.size() > MAX_INLINE_DIFF_BYTES
                    {
                        content_omitted = true;
                    }
                }
                // Check new blob size when applicable
                if !matches!(status, Delta::Deleted) {
                    let oid = delta.new_file().id();
                    if !oid.is_zero()
                        && let Ok(blob) = repo.find_blob(oid)
                        && !blob.is_binary()
                        && blob.size() > MAX_INLINE_DIFF_BYTES
                    {
                        content_omitted = true;
                    }
                }

                // Only build old/new content if not omitted
                let (old_path, old_content) = if matches!(status, Delta::Added) {
                    (None, None)
                } else {
                    let path_opt = delta
                        .old_file()
                        .path()
                        .map(|p| p.to_string_lossy().to_string());
                    if content_omitted {
                        (path_opt, None)
                    } else {
                        let details = delta
                            .old_file()
                            .path()
                            .map(|p| self.create_file_details(p, &delta.old_file().id(), repo));
                        (
                            details.as_ref().and_then(|f| f.file_name.clone()),
                            details.and_then(|f| f.content),
                        )
                    }
                };

                let (new_path, new_content) = if matches!(status, Delta::Deleted) {
                    (None, None)
                } else {
                    let path_opt = delta
                        .new_file()
                        .path()
                        .map(|p| p.to_string_lossy().to_string());
                    if content_omitted {
                        (path_opt, None)
                    } else {
                        let details = delta
                            .new_file()
                            .path()
                            .map(|p| self.create_file_details(p, &delta.new_file().id(), repo));
                        (
                            details.as_ref().and_then(|f| f.file_name.clone()),
                            details.and_then(|f| f.content),
                        )
                    }
                };

                let mut change = match status {
                    Delta::Added => DiffChangeKind::Added,
                    Delta::Deleted => DiffChangeKind::Deleted,
                    Delta::Modified => DiffChangeKind::Modified,
                    Delta::Renamed => DiffChangeKind::Renamed,
                    Delta::Copied => DiffChangeKind::Copied,
                    Delta::Untracked => DiffChangeKind::Added,
                    _ => DiffChangeKind::Modified,
                };

                // Detect pure mode changes (e.g., chmod +/-x) and classify as PermissionChange
                if matches!(status, Delta::Modified)
                    && delta.old_file().mode() != delta.new_file().mode()
                {
                    // Only downgrade to PermissionChange if we KNOW content is unchanged
                    if old_content.is_some() && new_content.is_some() && old_content == new_content
                    {
                        change = DiffChangeKind::PermissionChange;
                    }
                }

                // If contents are omitted, try to compute line stats via libgit2 Patch
                let mut additions: Option<usize> = None;
                let mut deletions: Option<usize> = None;
                if content_omitted
                    && let Ok(Some(patch)) = git2::Patch::from_diff(&diff, delta_index)
                    && let Ok((_ctx, adds, dels)) = patch.line_stats()
                {
                    additions = Some(adds);
                    deletions = Some(dels);
                }

                file_diffs.push(Diff {
                    change,
                    old_path,
                    new_path,
                    old_content,
                    new_content,
                    content_omitted,
                    additions,
                    deletions,
                });

                delta_index += 1;
                true
            },
            None,
            None,
            None,
        )?;

        Ok(file_diffs)
    }

    /// Extract file path from a Diff (for indexing and ConversationPatch)
    pub fn diff_path(diff: &Diff) -> String {
        diff.new_path
            .clone()
            .or_else(|| diff.old_path.clone())
            .unwrap_or_default()
    }

    /// Helper function to convert blob to string content
    fn blob_to_string(blob: &git2::Blob) -> Option<String> {
        if blob.is_binary() {
            None // Skip binary files
        } else {
            std::str::from_utf8(blob.content())
                .ok()
                .map(|s| s.to_string())
        }
    }

    /// Helper function to read file content from filesystem with safety guards
    fn read_file_to_string(repo: &Repository, rel_path: &Path) -> Option<String> {
        let workdir = repo.workdir()?;
        let abs_path = workdir.join(rel_path);

        // Read file from filesystem
        let bytes = match std::fs::read(&abs_path) {
            Ok(bytes) => bytes,
            Err(e) => {
                tracing::debug!("Failed to read file from filesystem: {:?}: {}", abs_path, e);
                return None;
            }
        };

        // Size guard - skip files larger than UI inline threshold
        if bytes.len() > MAX_INLINE_DIFF_BYTES {
            tracing::debug!(
                "Skipping large file ({}KB): {:?}",
                bytes.len() / 1024,
                abs_path
            );
            return None;
        }

        // Binary guard - skip files containing null bytes
        if bytes.contains(&0) {
            tracing::debug!("Skipping binary file: {:?}", abs_path);
            return None;
        }

        // UTF-8 validation
        match String::from_utf8(bytes) {
            Ok(content) => Some(content),
            Err(e) => {
                tracing::debug!("File is not valid UTF-8: {:?}: {}", abs_path, e);
                None
            }
        }
    }

    /// Create FileDiffDetails from path and blob with filesystem fallback
    fn create_file_details(
        &self,
        path: &Path,
        blob_id: &git2::Oid,
        repo: &Repository,
    ) -> FileDiffDetails {
        let file_name = path.to_string_lossy().to_string();

        // Try to get content from blob first (for non-zero OIDs)
        let content = if !blob_id.is_zero() {
            repo.find_blob(*blob_id)
                .ok()
                .and_then(|blob| Self::blob_to_string(&blob))
                .or_else(|| {
                    // Fallback to filesystem for unstaged changes
                    tracing::debug!(
                        "Blob not found for non-zero OID, reading from filesystem: {}",
                        file_name
                    );
                    Self::read_file_to_string(repo, path)
                })
        } else {
            // For zero OIDs, check filesystem directly (covers new/untracked files)
            Self::read_file_to_string(repo, path)
        };

        FileDiffDetails {
            file_name: Some(file_name),
            content,
        }
    }

    /// Create Diff entries from git_cli::StatusDiffEntry
    /// New Diff format is flattened with change kind, paths, and optional contents.
    fn status_entry_to_diff(repo: &Repository, base_tree: &git2::Tree, e: StatusDiffEntry) -> Diff {
        // Map ChangeType to DiffChangeKind
        let mut change = match e.change {
            ChangeType::Added => DiffChangeKind::Added,
            ChangeType::Deleted => DiffChangeKind::Deleted,
            ChangeType::Modified => DiffChangeKind::Modified,
            ChangeType::Renamed => DiffChangeKind::Renamed,
            ChangeType::Copied => DiffChangeKind::Copied,
            // Treat type changes and unmerged as modified for now
            ChangeType::TypeChanged | ChangeType::Unmerged => DiffChangeKind::Modified,
            ChangeType::Unknown(_) => DiffChangeKind::Modified,
        };

        // Determine old/new paths based on change
        let (old_path_opt, new_path_opt): (Option<String>, Option<String>) = match e.change {
            ChangeType::Added => (None, Some(e.path.clone())),
            ChangeType::Deleted => (Some(e.old_path.unwrap_or(e.path.clone())), None),
            ChangeType::Modified | ChangeType::TypeChanged | ChangeType::Unmerged => (
                Some(e.old_path.unwrap_or(e.path.clone())),
                Some(e.path.clone()),
            ),
            ChangeType::Renamed | ChangeType::Copied => (e.old_path.clone(), Some(e.path.clone())),
            ChangeType::Unknown(_) => (e.old_path.clone(), Some(e.path.clone())),
        };

        // Decide if we should omit content by size (either side)
        let mut content_omitted = false;
        // Old side (from base tree)
        if let Some(ref oldp) = old_path_opt {
            let rel = std::path::Path::new(oldp);
            if let Ok(entry) = base_tree.get_path(rel)
                && entry.kind() == Some(git2::ObjectType::Blob)
                && let Ok(blob) = repo.find_blob(entry.id())
                && !blob.is_binary()
                && blob.size() > MAX_INLINE_DIFF_BYTES
            {
                content_omitted = true;
            }
        }
        // New side (from filesystem)
        if let Some(ref newp) = new_path_opt
            && let Some(workdir) = repo.workdir()
        {
            let abs = workdir.join(newp);
            if let Ok(md) = std::fs::metadata(&abs)
                && (md.len() as usize) > MAX_INLINE_DIFF_BYTES
            {
                content_omitted = true;
            }
        }

        // Load contents only if not omitted
        let (old_content, new_content) = if content_omitted {
            (None, None)
        } else {
            // Load old content from base tree if possible
            let old_content = if let Some(ref oldp) = old_path_opt {
                let rel = std::path::Path::new(oldp);
                match base_tree.get_path(rel) {
                    Ok(entry) if entry.kind() == Some(git2::ObjectType::Blob) => repo
                        .find_blob(entry.id())
                        .ok()
                        .and_then(|b| Self::blob_to_string(&b)),
                    _ => None,
                }
            } else {
                None
            };

            // Load new content from filesystem (worktree) when available
            let new_content = if let Some(ref newp) = new_path_opt {
                let rel = std::path::Path::new(newp);
                Self::read_file_to_string(repo, rel)
            } else {
                None
            };
            (old_content, new_content)
        };

        // If reported as Modified but content is identical, treat as a permission-only change
        if matches!(change, DiffChangeKind::Modified)
            && old_content.is_some()
            && new_content.is_some()
            && old_content == new_content
        {
            change = DiffChangeKind::PermissionChange;
        }

        Diff {
            change,
            old_path: old_path_opt,
            new_path: new_path_opt,
            old_content,
            new_content,
            content_omitted,
            additions: None,
            deletions: None,
        }
    }

    /// Find where a branch is currently checked out
    fn find_checkout_path_for_branch(
        &self,
        repo_path: &Path,
        branch_name: &str,
    ) -> Result<Option<std::path::PathBuf>, GitServiceError> {
        let git_cli = GitCli::new();
        let worktrees = git_cli.list_worktrees(repo_path).map_err(|e| {
            GitServiceError::InvalidRepository(format!("git worktree list failed: {e}"))
        })?;

        for worktree in worktrees {
            if let Some(ref branch) = worktree.branch
                && branch == branch_name
            {
                return Ok(Some(std::path::PathBuf::from(worktree.path)));
            }
        }
        Ok(None)
    }

    /// Merge changes from a task branch into the base branch.
    pub fn merge_changes(
        &self,
        base_worktree_path: &Path,
        task_worktree_path: &Path,
        task_branch_name: &str,
        base_branch_name: &str,
        commit_message: &str,
    ) -> Result<String, GitServiceError> {
        // Open the repositories
        let task_repo = self.open_repo(task_worktree_path)?;
        let base_repo = self.open_repo(base_worktree_path)?;

        // Check if base branch is ahead of task branch - this indicates the base has moved
        // ahead since the task was created, which should block the merge
        let (_, task_behind) =
            self.get_branch_status(base_worktree_path, task_branch_name, base_branch_name)?;

        if task_behind > 0 {
            return Err(GitServiceError::BranchesDiverged(format!(
                "Cannot merge: base branch '{base_branch_name}' is {task_behind} commits ahead of task branch '{task_branch_name}'. The base branch has moved forward since the task was created.",
            )));
        }

        // Check where base branch is checked out (if anywhere)
        match self.find_checkout_path_for_branch(base_worktree_path, base_branch_name)? {
            Some(base_checkout_path) => {
                // base branch is checked out somewhere - use CLI merge
                let git_cli = GitCli::new();

                // Safety check: base branch has no staged changes
                if git_cli
                    .has_staged_changes(&base_checkout_path)
                    .map_err(|e| {
                        GitServiceError::InvalidRepository(format!("git diff --cached failed: {e}"))
                    })?
                {
                    return Err(GitServiceError::WorktreeDirty(
                        base_branch_name.to_string(),
                        "staged changes present".to_string(),
                    ));
                }

                // Use CLI merge in base context
                self.ensure_cli_commit_identity(&base_checkout_path)?;
                let sha = git_cli
                    .merge_squash_commit(
                        &base_checkout_path,
                        base_branch_name,
                        task_branch_name,
                        commit_message,
                    )
                    .map_err(|e| {
                        GitServiceError::InvalidRepository(format!("CLI merge failed: {e}"))
                    })?;

                // Update task branch ref for continuity
                let task_refname = format!("refs/heads/{task_branch_name}");
                git_cli
                    .update_ref(base_worktree_path, &task_refname, &sha)
                    .map_err(|e| {
                        GitServiceError::InvalidRepository(format!("git update-ref failed: {e}"))
                    })?;

                Ok(sha)
            }
            None => {
                // base branch not checked out anywhere - use libgit2 pure ref operations
                let task_branch = Self::find_branch(&task_repo, task_branch_name)?;
                let base_branch = Self::find_branch(&task_repo, base_branch_name)?;

                // Resolve commits
                let base_commit = base_branch.get().peel_to_commit()?;
                let task_commit = task_branch.get().peel_to_commit()?;

                // Create the squash commit in-memory (no checkout) and update the base branch ref
                let signature = self.signature_with_fallback(&task_repo)?;
                let squash_commit_id = self.perform_squash_merge(
                    &task_repo,
                    &base_commit,
                    &task_commit,
                    &signature,
                    commit_message,
                    base_branch_name,
                )?;

                // Update the task branch to the new squash commit so follow-up
                // work can continue from the merged state without conflicts.
                let task_refname = format!("refs/heads/{task_branch_name}");
                base_repo.reference(
                    &task_refname,
                    squash_commit_id,
                    true,
                    "Reset task branch after squash merge",
                )?;

                Ok(squash_commit_id.to_string())
            }
        }
    }
    fn get_branch_status_inner(
        &self,
        repo: &Repository,
        branch_ref: &Reference,
        base_branch_ref: &Reference,
    ) -> Result<(usize, usize), GitServiceError> {
        let (a, b) = repo.graph_ahead_behind(
            branch_ref.target().ok_or(GitServiceError::BranchNotFound(
                "Branch not found".to_string(),
            ))?,
            base_branch_ref
                .target()
                .ok_or(GitServiceError::BranchNotFound(
                    "Branch not found".to_string(),
                ))?,
        )?;
        Ok((a, b))
    }

    pub fn get_branch_status(
        &self,
        repo_path: &Path,
        branch_name: &str,
        base_branch_name: &str,
    ) -> Result<(usize, usize), GitServiceError> {
        let repo = Repository::open(repo_path)?;
        let branch = Self::find_branch(&repo, branch_name)?;
        let base_branch = Self::find_branch(&repo, base_branch_name)?;
        self.get_branch_status_inner(
            &repo,
            &branch.into_reference(),
            &base_branch.into_reference(),
        )
    }

    pub fn get_remote_branch_status(
        &self,
        repo_path: &Path,
        branch_name: &str,
        base_branch_name: Option<&str>,
        github_token: String,
    ) -> Result<(usize, usize), GitServiceError> {
        let repo = Repository::open(repo_path)?;
        let branch_ref = Self::find_branch(&repo, branch_name)?.into_reference();
        // base branch is either given or upstream of branch_name
        let base_branch_ref = if let Some(bn) = base_branch_name {
            Self::find_branch(&repo, bn)?
        } else {
            repo.find_branch(branch_name, BranchType::Local)?
                .upstream()?
        }
        .into_reference();
        let remote = self.get_remote_from_branch_ref(&repo, &base_branch_ref)?;
        self.fetch_all_from_remote(&repo, &github_token, &remote)?;
        self.get_branch_status_inner(&repo, &branch_ref, &base_branch_ref)
    }

    pub fn is_worktree_clean(&self, worktree_path: &Path) -> Result<bool, GitServiceError> {
        let repo = self.open_repo(worktree_path)?;
        match self.check_worktree_clean(&repo) {
            Ok(()) => Ok(true),
            Err(GitServiceError::WorktreeDirty(_, _)) => Ok(false),
            Err(e) => Err(e),
        }
    }

    /// Check if the worktree is clean (no uncommitted changes to tracked files)
    fn check_worktree_clean(&self, repo: &Repository) -> Result<(), GitServiceError> {
        let mut status_options = git2::StatusOptions::new();
        status_options
            .include_untracked(false) // Don't include untracked files
            .include_ignored(false); // Don't include ignored files

        let statuses = repo.statuses(Some(&mut status_options))?;

        if !statuses.is_empty() {
            let mut dirty_files = Vec::new();
            for entry in statuses.iter() {
                let status = entry.status();
                // Only consider files that are actually tracked and modified
                if status.intersects(
                    git2::Status::INDEX_MODIFIED
                        | git2::Status::INDEX_NEW
                        | git2::Status::INDEX_DELETED
                        | git2::Status::INDEX_RENAMED
                        | git2::Status::INDEX_TYPECHANGE
                        | git2::Status::WT_MODIFIED
                        | git2::Status::WT_DELETED
                        | git2::Status::WT_RENAMED
                        | git2::Status::WT_TYPECHANGE,
                ) && let Some(path) = entry.path()
                {
                    dirty_files.push(path.to_string());
                }
            }

            if !dirty_files.is_empty() {
                let branch_name = repo
                    .head()
                    .ok()
                    .and_then(|h| h.shorthand().map(|s| s.to_string()))
                    .unwrap_or_else(|| "unknown branch".to_string());
                return Err(GitServiceError::WorktreeDirty(
                    branch_name,
                    dirty_files.join(", "),
                ));
            }
        }

        Ok(())
    }

    /// Get current HEAD information including branch name and commit OID
    pub fn get_head_info(&self, repo_path: &Path) -> Result<HeadInfo, GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        let head = repo.head()?;

        let branch = if let Some(branch_name) = head.shorthand() {
            branch_name.to_string()
        } else {
            "HEAD".to_string()
        };

        let oid = if let Some(target_oid) = head.target() {
            target_oid.to_string()
        } else {
            // Handle case where HEAD exists but has no target (empty repo)
            return Err(GitServiceError::InvalidRepository(
                "Repository HEAD has no target commit".to_string(),
            ));
        };

        Ok(HeadInfo { branch, oid })
    }

    pub fn get_current_branch(&self, repo_path: &Path) -> Result<String, git2::Error> {
        // Thin wrapper for backward compatibility
        match self.get_head_info(repo_path) {
            Ok(head_info) => Ok(head_info.branch),
            Err(GitServiceError::Git(git_err)) => Err(git_err),
            Err(_) => Err(git2::Error::from_str("Failed to get head info")),
        }
    }

    /// Get the commit OID (as hex string) for a given branch without modifying HEAD
    pub fn get_branch_oid(
        &self,
        repo_path: &Path,
        branch_name: &str,
    ) -> Result<String, GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        let branch = Self::find_branch(&repo, branch_name)?;
        let oid = branch.get().peel_to_commit()?.id().to_string();
        Ok(oid)
    }

    /// Get the author name and email for the given commit OID (hex)
    pub fn get_commit_author(
        &self,
        repo_path: &Path,
        commit_sha: &str,
    ) -> Result<(Option<String>, Option<String>), GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        let oid = git2::Oid::from_str(commit_sha)
            .map_err(|_| GitServiceError::InvalidRepository("Invalid commit SHA".into()))?;
        let commit = repo.find_commit(oid)?;
        let author = commit.author();
        Ok((
            author.name().map(|s| s.to_string()),
            author.email().map(|s| s.to_string()),
        ))
    }

    /// Get the subject/summary line for a given commit OID
    pub fn get_commit_subject(
        &self,
        repo_path: &Path,
        commit_sha: &str,
    ) -> Result<String, GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        let oid = git2::Oid::from_str(commit_sha)
            .map_err(|_| GitServiceError::InvalidRepository("Invalid commit SHA".into()))?;
        let commit = repo.find_commit(oid)?;
        Ok(commit.summary().unwrap_or("(no subject)").to_string())
    }

    /// Compare two OIDs and return (ahead, behind) counts: how many commits
    /// `from_oid` is ahead of and behind `to_oid`.
    pub fn ahead_behind_commits_by_oid(
        &self,
        repo_path: &Path,
        from_oid: &str,
        to_oid: &str,
    ) -> Result<(usize, usize), GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        let from = git2::Oid::from_str(from_oid)
            .map_err(|_| GitServiceError::InvalidRepository("Invalid from OID".into()))?;
        let to = git2::Oid::from_str(to_oid)
            .map_err(|_| GitServiceError::InvalidRepository("Invalid to OID".into()))?;
        let (ahead, behind) = repo.graph_ahead_behind(from, to)?;
        Ok((ahead, behind))
    }

    /// Return (uncommitted_tracked_changes, untracked_files) counts in worktree
    pub fn get_worktree_change_counts(
        &self,
        worktree_path: &Path,
    ) -> Result<(usize, usize), GitServiceError> {
        let cli = super::git_cli::GitCli::new();
        let st = cli
            .get_worktree_status(worktree_path)
            .map_err(|e| GitServiceError::InvalidRepository(format!("git status failed: {e}")))?;
        Ok((st.uncommitted_tracked, st.untracked))
    }

    /// Expose full worktree status details (CLI porcelain parsing)
    pub fn get_worktree_status(
        &self,
        worktree_path: &Path,
    ) -> Result<super::git_cli::WorktreeStatus, GitServiceError> {
        let cli = super::git_cli::GitCli::new();
        cli.get_worktree_status(worktree_path)
            .map_err(|e| GitServiceError::InvalidRepository(format!("git status failed: {e}")))
    }

    /// Reset the given worktree to the specified commit SHA.
    /// If `force` is false and the worktree is dirty, returns WorktreeDirty error.
    pub fn reset_worktree_to_commit(
        &self,
        worktree_path: &Path,
        commit_sha: &str,
        force: bool,
    ) -> Result<(), GitServiceError> {
        let repo = self.open_repo(worktree_path)?;
        if !force {
            // Avoid clobbering uncommitted changes unless explicitly forced
            self.check_worktree_clean(&repo)?;
        }
        let cli = super::git_cli::GitCli::new();
        cli.git(worktree_path, ["reset", "--hard", commit_sha])
            .map_err(|e| {
                GitServiceError::InvalidRepository(format!("git reset --hard failed: {e}"))
            })?;
        // Reapply sparse-checkout if configured (non-fatal)
        let _ = cli.git(worktree_path, ["sparse-checkout", "reapply"]);
        Ok(())
    }

    /// Convenience: Get author of HEAD commit
    pub fn get_head_author(
        &self,
        repo_path: &Path,
    ) -> Result<(Option<String>, Option<String>), GitServiceError> {
        let head = self.get_head_info(repo_path)?;
        self.get_commit_author(repo_path, &head.oid)
    }

    /// Configure local user identity for committing via CLI
    pub fn configure_user(
        &self,
        repo_path: &Path,
        name: &str,
        email: &str,
    ) -> Result<(), GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        let mut cfg = repo.config()?;
        cfg.set_str("user.name", name)?;
        cfg.set_str("user.email", email)?;
        Ok(())
    }

    /// Create a local branch at the current HEAD
    pub fn create_branch(
        &self,
        repo_path: &Path,
        branch_name: &str,
    ) -> Result<(), GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        let head_commit = repo.head()?.peel_to_commit()?;
        repo.branch(branch_name, &head_commit, true)?;
        Ok(())
    }

    /// Checkout a local branch in the given working tree
    pub fn checkout_branch(
        &self,
        repo_path: &Path,
        branch_name: &str,
    ) -> Result<(), GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        let refname = format!("refs/heads/{branch_name}");
        repo.set_head(&refname)?;
        let mut co = CheckoutBuilder::new();
        co.force();
        repo.checkout_head(Some(&mut co))?;
        Ok(())
    }

    /// Add a worktree for a branch, optionally creating the branch
    pub fn add_worktree(
        &self,
        repo_path: &Path,
        worktree_path: &Path,
        branch: &str,
        create_branch: bool,
    ) -> Result<(), GitServiceError> {
        let git = GitCli::new();
        git.worktree_add(repo_path, worktree_path, branch, create_branch)
            .map_err(|e| GitServiceError::InvalidRepository(e.to_string()))?;
        Ok(())
    }

    /// Set or add a remote URL
    pub fn set_remote(
        &self,
        repo_path: &Path,
        name: &str,
        url: &str,
    ) -> Result<(), GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        match repo.find_remote(name) {
            Ok(_) => repo.remote_set_url(name, url)?,
            Err(_) => {
                repo.remote(name, url)?;
            }
        }
        Ok(())
    }

    /// Stage a specific path (wrapper over git add)
    pub fn add_path(&self, repo_path: &Path, path: &str) -> Result<(), GitServiceError> {
        let git = GitCli::new();
        git.git(repo_path, ["add", path])
            .map(|_| ())
            .map_err(|e| GitServiceError::InvalidRepository(e.to_string()))
    }

    /// Detach HEAD to the current commit (for testing commit on detached HEAD)
    pub fn detach_head_current(&self, repo_path: &Path) -> Result<(), GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        let oid = repo
            .head()?
            .target()
            .ok_or_else(|| GitServiceError::InvalidRepository("HEAD has no target".into()))?;
        repo.set_head_detached(oid)?;
        Ok(())
    }

    pub fn get_all_branches(&self, repo_path: &Path) -> Result<Vec<GitBranch>, git2::Error> {
        let repo = Repository::open(repo_path)?;
        let current_branch = self.get_current_branch(repo_path).unwrap_or_default();
        let mut branches = Vec::new();

        // Helper function to get last commit date for a branch
        let get_last_commit_date = |branch: &git2::Branch| -> Result<DateTime<Utc>, git2::Error> {
            if let Some(target) = branch.get().target()
                && let Ok(commit) = repo.find_commit(target)
            {
                let timestamp = commit.time().seconds();
                return Ok(DateTime::from_timestamp(timestamp, 0).unwrap_or_else(Utc::now));
            }
            Ok(Utc::now()) // Default to now if we can't get the commit date
        };

        // Get local branches
        let local_branches = repo.branches(Some(BranchType::Local))?;
        for branch_result in local_branches {
            let (branch, _) = branch_result?;
            if let Some(name) = branch.name()? {
                let last_commit_date = get_last_commit_date(&branch)?;
                branches.push(GitBranch {
                    name: name.to_string(),
                    is_current: name == current_branch,
                    is_remote: false,
                    last_commit_date,
                });
            }
        }

        // Get remote branches
        let remote_branches = repo.branches(Some(BranchType::Remote))?;
        for branch_result in remote_branches {
            let (branch, _) = branch_result?;
            if let Some(name) = branch.name()? {
                // Skip remote HEAD references
                if !name.ends_with("/HEAD") {
                    let last_commit_date = get_last_commit_date(&branch)?;
                    branches.push(GitBranch {
                        name: name.to_string(),
                        is_current: false,
                        is_remote: true,
                        last_commit_date,
                    });
                }
            }
        }

        // Sort branches: current first, then by most recent commit date
        branches.sort_by(|a, b| {
            if a.is_current && !b.is_current {
                std::cmp::Ordering::Less
            } else if !a.is_current && b.is_current {
                std::cmp::Ordering::Greater
            } else {
                // Sort by most recent commit date (newest first)
                b.last_commit_date.cmp(&a.last_commit_date)
            }
        });

        Ok(branches)
    }

    /// Perform a squash merge of task branch into base branch, but fail on conflicts
    fn perform_squash_merge(
        &self,
        repo: &Repository,
        base_commit: &git2::Commit,
        task_commit: &git2::Commit,
        signature: &git2::Signature,
        commit_message: &str,
        base_branch_name: &str,
    ) -> Result<git2::Oid, GitServiceError> {
        // In-memory merge to detect conflicts without touching the working tree
        let mut merge_opts = git2::MergeOptions::new();
        // Safety and correctness options
        merge_opts.find_renames(true); // improve rename handling
        merge_opts.fail_on_conflict(true); // bail out instead of generating conflicted index
        let mut index = repo.merge_commits(base_commit, task_commit, Some(&merge_opts))?;

        // If there are conflicts, return an error
        if index.has_conflicts() {
            return Err(GitServiceError::MergeConflicts(
                "Merge failed due to conflicts. Please resolve conflicts manually.".to_string(),
            ));
        }

        // Write the merged tree back to the repository
        let tree_id = index.write_tree_to(repo)?;
        let tree = repo.find_tree(tree_id)?;

        // Create a squash commit: use merged tree with base_commit as sole parent
        let squash_commit_id = repo.commit(
            None,           // Don't update any reference yet
            signature,      // Author
            signature,      // Committer
            commit_message, // Custom message
            &tree,          // Merged tree content
            &[base_commit], // Single parent: base branch commit
        )?;

        // Update the base branch reference to point to the new commit
        let refname = format!("refs/heads/{base_branch_name}");
        repo.reference(&refname, squash_commit_id, true, "Squash merge")?;

        Ok(squash_commit_id)
    }

    /// Rebase a worktree branch onto a new base
    pub fn rebase_branch(
        &self,
        repo_path: &Path,
        worktree_path: &Path,
        new_base_branch: Option<&str>,
        old_base_branch: &str,
        github_token: Option<String>,
    ) -> Result<String, GitServiceError> {
        let worktree_repo = Repository::open(worktree_path)?;
        let main_repo = self.open_repo(repo_path)?;

        // Safety guard: never operate on a dirty worktree. This preserves any
        // uncommitted changes to tracked files by failing fast instead of
        // resetting or cherry-picking over them. Untracked files are allowed.
        self.check_worktree_clean(&worktree_repo)?;

        // If a rebase is already in progress, refuse to proceed instead of
        // aborting (which might destroy user changes mid-rebase).
        let git = GitCli::new();
        if git.is_rebase_in_progress(worktree_path).unwrap_or(false) {
            return Err(GitServiceError::RebaseInProgress);
        }

        // Get the target base branch reference
        let new_base_branch_name = match new_base_branch {
            Some(branch) => branch.to_string(),
            None => main_repo
                .head()
                .ok()
                .and_then(|head| head.shorthand().map(|s| s.to_string()))
                .unwrap_or_else(|| "main".to_string()),
        };
        let nbr = Self::find_branch(&main_repo, &new_base_branch_name)?.into_reference();
        // If the target base is remote, update it first so CLI sees latest
        if nbr.is_remote() {
            let github_token = github_token.ok_or(GitServiceError::TokenUnavailable)?;
            let remote = self.get_remote_from_branch_ref(&main_repo, &nbr)?;
            // First, fetch the latest changes from remote
            self.fetch_branch_from_remote(
                &main_repo,
                &github_token,
                &remote,
                &new_base_branch_name,
            )?;
        }

        // Ensure identity for any commits produced by rebase
        self.ensure_cli_commit_identity(worktree_path)?;
        // Use git CLI rebase to carry out the operation safely
        match git.rebase_onto(worktree_path, &new_base_branch_name, old_base_branch) {
            Ok(()) => {}
            Err(GitCliError::RebaseInProgress) => {
                return Err(GitServiceError::RebaseInProgress);
            }
            Err(GitCliError::CommandFailed(stderr)) => {
                // If the CLI indicates conflicts, return a concise, actionable error.
                let looks_like_conflict = stderr.contains("could not apply")
                    || stderr.contains("CONFLICT")
                    || stderr.to_lowercase().contains("resolve all conflicts");
                if looks_like_conflict {
                    // Determine current attempt branch name for clarity
                    let attempt_branch = worktree_repo
                        .head()
                        .ok()
                        .and_then(|h| h.shorthand().map(|s| s.to_string()))
                        .unwrap_or_else(|| "(unknown)".to_string());
                    // List conflicted files (best-effort)
                    let conflicts = git.get_conflicted_files(worktree_path).unwrap_or_default();
                    let files_part = if conflicts.is_empty() {
                        "".to_string()
                    } else {
                        let mut sample = conflicts.clone();
                        let total = sample.len();
                        sample.truncate(10);
                        let list = sample.join(", ");
                        if total > sample.len() {
                            format!(
                                " Conflicted files (showing {} of {}): {}.",
                                sample.len(),
                                total,
                                list
                            )
                        } else {
                            format!(" Conflicted files: {list}.")
                        }
                    };
                    let msg = format!(
                        "Rebase encountered merge conflicts while rebasing '{attempt_branch}' onto '{new_base_branch_name}'.{files_part} Resolve conflicts and then continue or abort."
                    );
                    return Err(GitServiceError::MergeConflicts(msg));
                }
                return Err(GitServiceError::InvalidRepository(format!(
                    "Rebase failed: {}",
                    stderr.lines().next().unwrap_or("")
                )));
            }
            Err(e) => {
                return Err(GitServiceError::InvalidRepository(format!(
                    "git rebase failed: {e}"
                )));
            }
        }

        // Return resulting HEAD commit
        let final_commit = worktree_repo.head()?.peel_to_commit()?;
        Ok(final_commit.id().to_string())
    }

    pub fn find_branch_type(
        &self,
        repo_path: &Path,
        branch_name: &str,
    ) -> Result<BranchType, GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        // Try to find the branch as a local branch first
        match repo.find_branch(branch_name, BranchType::Local) {
            Ok(_) => Ok(BranchType::Local),
            Err(_) => {
                // If not found, try to find it as a remote branch
                match repo.find_branch(branch_name, BranchType::Remote) {
                    Ok(_) => Ok(BranchType::Remote),
                    Err(_) => Err(GitServiceError::BranchNotFound(branch_name.to_string())),
                }
            }
        }
    }

    /// Return true if a rebase is currently in progress in this worktree.
    pub fn is_rebase_in_progress(&self, worktree_path: &Path) -> Result<bool, GitServiceError> {
        let git = GitCli::new();
        git.is_rebase_in_progress(worktree_path).map_err(|e| {
            GitServiceError::InvalidRepository(format!("git rebase state check failed: {e}"))
        })
    }

    pub fn detect_conflict_op(
        &self,
        worktree_path: &Path,
    ) -> Result<Option<ConflictOp>, GitServiceError> {
        let git = GitCli::new();
        if git.is_rebase_in_progress(worktree_path).unwrap_or(false) {
            return Ok(Some(ConflictOp::Rebase));
        }
        if git.is_merge_in_progress(worktree_path).unwrap_or(false) {
            return Ok(Some(ConflictOp::Merge));
        }
        if git
            .is_cherry_pick_in_progress(worktree_path)
            .unwrap_or(false)
        {
            return Ok(Some(ConflictOp::CherryPick));
        }
        if git.is_revert_in_progress(worktree_path).unwrap_or(false) {
            return Ok(Some(ConflictOp::Revert));
        }
        Ok(None)
    }

    /// List conflicted (unmerged) files in the worktree.
    pub fn get_conflicted_files(
        &self,
        worktree_path: &Path,
    ) -> Result<Vec<String>, GitServiceError> {
        let git = GitCli::new();
        git.get_conflicted_files(worktree_path).map_err(|e| {
            GitServiceError::InvalidRepository(format!("git diff for conflicts failed: {e}"))
        })
    }

    /// Abort an in-progress rebase in this worktree (no-op if none).
    pub fn abort_rebase(&self, worktree_path: &Path) -> Result<(), GitServiceError> {
        let git = GitCli::new();
        git.abort_rebase(worktree_path).map_err(|e| {
            GitServiceError::InvalidRepository(format!("git rebase --abort failed: {e}"))
        })
    }

    pub fn abort_conflicts(&self, worktree_path: &Path) -> Result<(), GitServiceError> {
        let git = GitCli::new();
        if git.is_rebase_in_progress(worktree_path).unwrap_or(false) {
            // If there are no conflicted files, prefer `git rebase --quit` to clean up metadata
            let has_conflicts = !self
                .get_conflicted_files(worktree_path)
                .unwrap_or_default()
                .is_empty();
            if has_conflicts {
                return self.abort_rebase(worktree_path);
            } else {
                return git.quit_rebase(worktree_path).map_err(|e| {
                    GitServiceError::InvalidRepository(format!("git rebase --quit failed: {e}"))
                });
            }
        }
        if git.is_merge_in_progress(worktree_path).unwrap_or(false) {
            return git.abort_merge(worktree_path).map_err(|e| {
                GitServiceError::InvalidRepository(format!("git merge --abort failed: {e}"))
            });
        }
        if git
            .is_cherry_pick_in_progress(worktree_path)
            .unwrap_or(false)
        {
            return git.abort_cherry_pick(worktree_path).map_err(|e| {
                GitServiceError::InvalidRepository(format!("git cherry-pick --abort failed: {e}"))
            });
        }
        if git.is_revert_in_progress(worktree_path).unwrap_or(false) {
            return git.abort_revert(worktree_path).map_err(|e| {
                GitServiceError::InvalidRepository(format!("git revert --abort failed: {e}"))
            });
        }
        Ok(())
    }

    pub fn find_branch<'a>(
        repo: &'a Repository,
        branch_name: &str,
    ) -> Result<git2::Branch<'a>, GitServiceError> {
        // Try to find the branch as a local branch first
        match repo.find_branch(branch_name, BranchType::Local) {
            Ok(branch) => Ok(branch),
            Err(_) => {
                // If not found, try to find it as a remote branch
                match repo.find_branch(branch_name, BranchType::Remote) {
                    Ok(branch) => Ok(branch),
                    Err(_) => Err(GitServiceError::BranchNotFound(branch_name.to_string())),
                }
            }
        }
    }

    /// Delete a file from the repository and commit the change
    pub fn delete_file_and_commit(
        &self,
        worktree_path: &Path,
        file_path: &str,
    ) -> Result<String, GitServiceError> {
        let repo = Repository::open(worktree_path)?;

        // Get the absolute path to the file within the worktree
        let file_full_path = worktree_path.join(file_path);

        // Check if file exists and delete it
        if file_full_path.exists() {
            std::fs::remove_file(&file_full_path).map_err(|e| {
                GitServiceError::IoError(std::io::Error::other(format!(
                    "Failed to delete file {file_path}: {e}"
                )))
            })?;
        }

        // Stage the deletion
        let mut index = repo.index()?;
        index.remove_path(Path::new(file_path))?;
        index.write()?;

        // Create a commit for the file deletion
        let signature = self.signature_with_fallback(&repo)?;
        let tree_id = index.write_tree()?;
        let tree = repo.find_tree(tree_id)?;

        // Get the current HEAD commit
        let head = repo.head()?;
        let parent_commit = head.peel_to_commit()?;

        let commit_message = format!("Delete file: {file_path}");
        let commit_id = repo.commit(
            Some("HEAD"),
            &signature,
            &signature,
            &commit_message,
            &tree,
            &[&parent_commit],
        )?;

        Ok(commit_id.to_string())
    }

    /// Get the default branch name for the repository
    pub fn get_default_branch_name(&self, repo_path: &Path) -> Result<String, GitServiceError> {
        let repo = self.open_repo(repo_path)?;

        match repo.head() {
            Ok(head_ref) => Ok(head_ref.shorthand().unwrap_or("main").to_string()),
            Err(e)
                if e.class() == git2::ErrorClass::Reference
                    && e.code() == git2::ErrorCode::UnbornBranch =>
            {
                Ok("main".to_string()) // Repository has no commits yet
            }
            Err(_) => Ok("main".to_string()), // Fallback
        }
    }

    /// Extract GitHub owner and repo name from git repo path
    pub fn get_github_repo_info(
        &self,
        repo_path: &Path,
    ) -> Result<GitHubRepoInfo, GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        let remote_name = self.default_remote_name(&repo);
        let remote = repo.find_remote(&remote_name).map_err(|_| {
            GitServiceError::InvalidRepository(format!("No '{remote_name}' remote found"))
        })?;

        let url = remote
            .url()
            .ok_or_else(|| GitServiceError::InvalidRepository("Remote has no URL".to_string()))?;
        GitHubRepoInfo::from_remote_url(url).map_err(|e| {
            GitServiceError::InvalidRepository(format!("Failed to parse remote URL: {e}"))
        })
    }

    pub fn get_remote_name_from_branch_name(
        &self,
        repo_path: &Path,
        branch_name: &str,
    ) -> Result<String, GitServiceError> {
        let repo = Repository::open(repo_path)?;
        let branch_ref = Self::find_branch(&repo, branch_name)?.into_reference();
        let default_remote = self.default_remote_name(&repo);
        self.get_remote_from_branch_ref(&repo, &branch_ref)
            .map(|r| r.name().unwrap_or(&default_remote).to_string())
    }

    fn get_remote_from_branch_ref<'a>(
        &self,
        repo: &'a Repository,
        branch_ref: &Reference,
    ) -> Result<Remote<'a>, GitServiceError> {
        let branch_name = branch_ref
            .name()
            .map(|name| name.to_string())
            .ok_or_else(|| GitServiceError::InvalidRepository("Invalid branch ref".into()))?;
        let remote_name_buf = repo.branch_remote_name(&branch_name)?;

        let remote_name = str::from_utf8(&remote_name_buf)
            .map_err(|e| {
                GitServiceError::InvalidRepository(format!(
                    "Invalid remote name for branch {branch_name}: {e}"
                ))
            })?
            .to_string();
        repo.find_remote(&remote_name).map_err(|_| {
            GitServiceError::InvalidRepository(format!(
                "Remote '{remote_name}' for branch '{branch_name}' not found"
            ))
        })
    }

    pub fn push_to_github(
        &self,
        worktree_path: &Path,
        branch_name: &str,
        github_token: &str,
    ) -> Result<(), GitServiceError> {
        let repo = Repository::open(worktree_path)?;
        self.check_worktree_clean(&repo)?;

        // Get the remote
        let remote_name = self.default_remote_name(&repo);
        let remote = repo.find_remote(&remote_name)?;

        let remote_url = remote
            .url()
            .ok_or_else(|| GitServiceError::InvalidRepository("Remote has no URL".to_string()))?;
        let https_url = self.convert_to_https_url(remote_url);
        let git_cli = GitCli::new();
        if let Err(e) =
            git_cli.push_with_token(worktree_path, &https_url, branch_name, github_token)
        {
            tracing::error!("Push to GitHub failed: {}", e);
            return Err(e.into());
        }

        let mut branch = Self::find_branch(&repo, branch_name)?;
        if !branch.get().is_remote() {
            if let Some(branch_target) = branch.get().target() {
                let remote_ref = format!("refs/remotes/{remote_name}/{branch_name}");
                repo.reference(
                    &remote_ref,
                    branch_target,
                    true,
                    "update remote tracking branch",
                )?;
            }
            branch.set_upstream(Some(&format!("{remote_name}/{branch_name}")))?;
        }

        Ok(())
    }

    pub fn convert_to_https_url(&self, url: &str) -> String {
        // Convert SSH URL to HTTPS URL if necessary
        let new_url = if url.starts_with("git@github.com:") {
            // Convert git@github.com:owner/repo.git to https://github.com/owner/repo.git
            url.replace("git@github.com:", "https://github.com/")
        } else if url.starts_with("ssh://git@github.com/") {
            // Convert ssh://git@github.com/owner/repo.git to https://github.com/owner/repo.git
            url.replace("ssh://git@github.com/", "https://github.com/")
        } else {
            url.to_string()
        };
        let mut normalized = new_url.trim_end_matches('/').to_string();
        if !normalized.ends_with(".git") {
            normalized.push_str(".git");
        }

        normalized
    }

    /// Fetch from remote repository using GitHub token authentication
    fn fetch_from_remote(
        &self,
        repo: &Repository,
        github_token: &str,
        remote: &Remote,
        refspec: &str,
    ) -> Result<(), GitServiceError> {
        // Get the remote
        let remote_url = remote
            .url()
            .ok_or_else(|| GitServiceError::InvalidRepository("Remote has no URL".to_string()))?;

        let https_url = self.convert_to_https_url(remote_url);
        // Create temporary HTTPS remote
        let git_cli = GitCli::new();
        if let Err(e) =
            git_cli.fetch_with_token_and_refspec(repo.path(), &https_url, refspec, github_token)
        {
            tracing::error!("Fetch from GitHub failed: {}", e);
            return Err(e.into());
        }
        Ok(())
    }

    /// Fetch from remote repository using GitHub token authentication
    fn fetch_branch_from_remote(
        &self,
        repo: &Repository,
        github_token: &str,
        remote: &Remote,
        branch_name: &str,
    ) -> Result<(), GitServiceError> {
        let default_remote_name = self.default_remote_name(repo);
        let remote_name = remote.name().unwrap_or(&default_remote_name);
        let refspec = format!("+refs/heads/{branch_name}:refs/remotes/{remote_name}/{branch_name}");
        self.fetch_from_remote(repo, github_token, remote, &refspec)
    }

    /// Fetch from remote repository using GitHub token authentication
    fn fetch_all_from_remote(
        &self,
        repo: &Repository,
        github_token: &str,
        remote: &Remote,
    ) -> Result<(), GitServiceError> {
        let default_remote_name = self.default_remote_name(repo);
        let remote_name = remote.name().unwrap_or(&default_remote_name);
        let refspec = format!("+refs/heads/*:refs/remotes/{remote_name}/*");
        self.fetch_from_remote(repo, github_token, remote, &refspec)
    }

    /// Clone a repository to the specified directory
    #[cfg(feature = "cloud")]
    pub fn clone_repository(
        clone_url: &str,
        target_path: &Path,
        token: Option<&str>,
    ) -> Result<Repository, GitServiceError> {
        if let Some(parent) = target_path.parent() {
            std::fs::create_dir_all(parent)?;
        }

        // Set up callbacks for authentication if token is provided
        let mut callbacks = RemoteCallbacks::new();
        if let Some(token) = token {
            callbacks.credentials(|_url, username_from_url, _allowed_types| {
                Cred::userpass_plaintext(username_from_url.unwrap_or("git"), token)
            });
        } else {
            // Fallback to SSH agent and key file authentication
            callbacks.credentials(|_url, username_from_url, _| {
                // Try SSH agent first
                if let Some(username) = username_from_url {
                    if let Ok(cred) = Cred::ssh_key_from_agent(username) {
                        return Ok(cred);
                    }
                }
                // Fallback to key file (~/.ssh/id_rsa)
                let home = dirs::home_dir()
                    .ok_or_else(|| git2::Error::from_str("Could not find home directory"))?;
                let key_path = home.join(".ssh").join("id_rsa");
                Cred::ssh_key(username_from_url.unwrap_or("git"), None, &key_path, None)
            });
        }

        // Set up fetch options with our callbacks
        let mut fetch_opts = FetchOptions::new();
        fetch_opts.remote_callbacks(callbacks);

        // Create a repository builder with fetch options
        let mut builder = git2::build::RepoBuilder::new();
        builder.fetch_options(fetch_opts);

        let repo = builder.clone(clone_url, target_path)?;

        tracing::info!(
            "Successfully cloned repository from {} to {}",
            clone_url,
            target_path.display()
        );

        Ok(repo)
    }

    /// Collect file statistics from recent commits for ranking purposes
    pub fn collect_recent_file_stats(
        &self,
        repo_path: &Path,
        commit_limit: usize,
    ) -> Result<HashMap<String, FileStat>, GitServiceError> {
        let repo = self.open_repo(repo_path)?;
        let mut stats: HashMap<String, FileStat> = HashMap::new();

        // Set up revision walk from HEAD
        let mut revwalk = repo.revwalk()?;
        revwalk.push_head()?;
        revwalk.set_sorting(Sort::TIME)?;

        // Iterate through recent commits
        for (commit_index, oid_result) in revwalk.take(commit_limit).enumerate() {
            let oid = oid_result?;
            let commit = repo.find_commit(oid)?;

            // Get commit timestamp
            let commit_time = {
                let time = commit.time();
                DateTime::from_timestamp(time.seconds(), 0).unwrap_or_else(Utc::now)
            };

            // Get the commit tree
            let commit_tree = commit.tree()?;

            // For the first commit (no parent), diff against empty tree
            let parent_tree = if commit.parent_count() == 0 {
                None
            } else {
                Some(commit.parent(0)?.tree()?)
            };

            // Create diff between parent and current commit
            let diff = repo.diff_tree_to_tree(parent_tree.as_ref(), Some(&commit_tree), None)?;

            // Process each changed file in this commit
            diff.foreach(
                &mut |delta, _progress| {
                    // Get the file path - prefer new file path, fall back to old
                    if let Some(path) = delta.new_file().path().or_else(|| delta.old_file().path())
                    {
                        let path_str = path.to_string_lossy().to_string();

                        // Update or insert file stats
                        let stat = stats.entry(path_str).or_insert(FileStat {
                            last_index: commit_index,
                            commit_count: 0,
                            last_time: commit_time,
                        });

                        // Increment commit count
                        stat.commit_count += 1;

                        // Keep the most recent change (smallest index)
                        if commit_index < stat.last_index {
                            stat.last_index = commit_index;
                            stat.last_time = commit_time;
                        }
                    }

                    true // Continue iteration
                },
                None, // No binary callback
                None, // No hunk callback
                None, // No line callback
            )?;
        }

        Ok(stats)
    }
}

// #[cfg(test)]
// mod tests {
//     use tempfile::TempDir;

//     use super::*;

//     fn create_test_repo() -> (TempDir, Repository) {
//         let temp_dir = TempDir::new().unwrap();
//         let repo = Repository::init(temp_dir.path()).unwrap();

//         // Configure the repository
//         let mut config = repo.config().unwrap();
//         config.set_str("user.name", "Test User").unwrap();
//         config.set_str("user.email", "test@example.com").unwrap();

//         (temp_dir, repo)
//     }

//     #[test]
//     fn test_git_service_creation() {
//         let (temp_dir, _repo) = create_test_repo();
//         let _git_service = GitService::new(temp_dir.path()).unwrap();
//     }

//     #[test]
//     fn test_invalid_repository_path() {
//         let result = GitService::new("/nonexistent/path");
//         assert!(result.is_err());
//     }

//     #[test]
//     fn test_default_branch_name() {
//         let (temp_dir, _repo) = create_test_repo();
//         let git_service = GitService::new(temp_dir.path()).unwrap();
//         let branch_name = git_service.get_default_branch_name().unwrap();
//         assert_eq!(branch_name, "main");
//     }
// }
</file>

<file path="crates/services/src/services/github_service.rs">
use std::time::Duration;

use backon::{ExponentialBuilder, Retryable};
use db::models::merge::{MergeStatus, PullRequestInfo};
use octocrab::{Octocrab, OctocrabBuilder, models::IssueState};
use regex::Regex;
use serde::{Deserialize, Serialize};
use thiserror::Error;
use tracing::info;
use ts_rs::TS;

use crate::services::{git::GitServiceError, git_cli::GitCliError};

#[derive(Debug, Error, Serialize, Deserialize, TS)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
#[ts(use_ts_enum)]
pub enum GitHubServiceError {
    #[ts(skip)]
    #[serde(skip)]
    #[error(transparent)]
    Client(octocrab::Error),
    #[ts(skip)]
    #[error("Repository error: {0}")]
    Repository(String),
    #[ts(skip)]
    #[error("Pull request error: {0}")]
    PullRequest(String),
    #[ts(skip)]
    #[error("Branch error: {0}")]
    Branch(String),
    #[error("GitHub token is invalid or expired.")]
    TokenInvalid,
    #[error("Insufficient permissions")]
    InsufficientPermissions,
    #[error("GitHub repository not found or no access")]
    RepoNotFoundOrNoAccess,
    #[ts(skip)]
    #[serde(skip)]
    #[error(transparent)]
    GitService(GitServiceError),
}

impl From<octocrab::Error> for GitHubServiceError {
    fn from(err: octocrab::Error) -> Self {
        match &err {
            octocrab::Error::GitHub { source, .. } => {
                let status = source.status_code.as_u16();
                let msg = source.message.to_ascii_lowercase();
                if status == 401 || msg.contains("bad credentials") || msg.contains("token expired")
                {
                    GitHubServiceError::TokenInvalid
                } else if status == 403 {
                    GitHubServiceError::InsufficientPermissions
                } else {
                    GitHubServiceError::Client(err)
                }
            }
            _ => GitHubServiceError::Client(err),
        }
    }
}
impl From<GitServiceError> for GitHubServiceError {
    fn from(error: GitServiceError) -> Self {
        match error {
            GitServiceError::GitCLI(GitCliError::AuthFailed(_)) => Self::TokenInvalid,
            GitServiceError::GitCLI(GitCliError::CommandFailed(msg)) => {
                let lower = msg.to_ascii_lowercase();
                if lower.contains("the requested url returned error: 403") {
                    Self::InsufficientPermissions
                } else if lower.contains("the requested url returned error: 404") {
                    Self::RepoNotFoundOrNoAccess
                } else {
                    Self::GitService(GitServiceError::GitCLI(GitCliError::CommandFailed(msg)))
                }
            }
            other => Self::GitService(other),
        }
    }
}

impl GitHubServiceError {
    pub fn is_api_data(&self) -> bool {
        matches!(
            self,
            GitHubServiceError::TokenInvalid
                | GitHubServiceError::InsufficientPermissions
                | GitHubServiceError::RepoNotFoundOrNoAccess
        )
    }

    pub fn should_retry(&self) -> bool {
        !self.is_api_data()
    }
}

#[derive(Debug, Clone)]
pub struct GitHubRepoInfo {
    pub owner: String,
    pub repo_name: String,
}
impl GitHubRepoInfo {
    pub fn from_remote_url(remote_url: &str) -> Result<Self, GitHubServiceError> {
        // Supports SSH, HTTPS and PR GitHub URLs. See tests for examples.
        let re = Regex::new(r"github\.com[:/](?P<owner>[^/]+)/(?P<repo>[^/]+?)(?:\.git)?(?:/|$)")
            .map_err(|e| {
            GitHubServiceError::Repository(format!("Failed to compile regex: {e}"))
        })?;

        let caps = re.captures(remote_url).ok_or_else(|| {
            GitHubServiceError::Repository(format!("Invalid GitHub URL format: {remote_url}"))
        })?;

        Ok(Self {
            owner: caps.name("owner").unwrap().as_str().to_string(),
            repo_name: caps.name("repo").unwrap().as_str().to_string(),
        })
    }
}

#[derive(Debug, Clone)]
pub struct CreatePrRequest {
    pub title: String,
    pub body: Option<String>,
    pub head_branch: String,
    pub base_branch: String,
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
pub struct RepositoryInfo {
    pub id: i64,
    pub name: String,
    pub full_name: String,
    pub owner: String,
    pub description: Option<String>,
    pub clone_url: String,
    pub ssh_url: String,
    pub default_branch: String,
    pub private: bool,
}

#[derive(Debug, Clone)]
pub struct GitHubService {
    client: Octocrab,
}

impl GitHubService {
    /// Create a new GitHub service with authentication
    pub fn new(github_token: &str) -> Result<Self, GitHubServiceError> {
        let client = OctocrabBuilder::new()
            .personal_token(github_token.to_string())
            .build()?;

        Ok(Self { client })
    }

    pub async fn check_token(&self) -> Result<(), GitHubServiceError> {
        self.client.current().user().await?;
        Ok(())
    }

    /// Create a pull request on GitHub
    pub async fn create_pr(
        &self,
        repo_info: &GitHubRepoInfo,
        request: &CreatePrRequest,
    ) -> Result<PullRequestInfo, GitHubServiceError> {
        (|| async { self.create_pr_internal(repo_info, request).await })
            .retry(
                &ExponentialBuilder::default()
                    .with_min_delay(Duration::from_secs(1))
                    .with_max_delay(Duration::from_secs(30))
                    .with_max_times(3)
                    .with_jitter(),
            )
            .when(|e| e.should_retry())
            .notify(|err: &GitHubServiceError, dur: Duration| {
                tracing::warn!(
                    "GitHub API call failed, retrying after {:.2}s: {}",
                    dur.as_secs_f64(),
                    err
                );
            })
            .await
    }

    async fn create_pr_internal(
        &self,
        repo_info: &GitHubRepoInfo,
        request: &CreatePrRequest,
    ) -> Result<PullRequestInfo, GitHubServiceError> {
        // Verify repository access
        self.client
            .repos(&repo_info.owner, &repo_info.repo_name)
            .get()
            .await
            .map_err(|error| match GitHubServiceError::from(error) {
                GitHubServiceError::Client(source) => GitHubServiceError::Repository(format!(
                    "Cannot access repository {}/{}: {}",
                    repo_info.owner, repo_info.repo_name, source
                )),
                other => other,
            })?;

        // Check if the base branch exists
        self.client
            .repos(&repo_info.owner, &repo_info.repo_name)
            .get_ref(&octocrab::params::repos::Reference::Branch(
                request.base_branch.to_string(),
            ))
            .await
            .map_err(|err| match GitHubServiceError::from(err) {
                GitHubServiceError::Client(source) => {
                    let hint = if request.base_branch != "main" {
                        " Perhaps you meant to use main as your base branch instead?"
                    } else {
                        ""
                    };
                    GitHubServiceError::Branch(format!(
                        "Base branch '{}' does not exist: {}{}",
                        request.base_branch, source, hint
                    ))
                }
                other => other,
            })?;

        // Check if the head branch exists
        self.client
            .repos(&repo_info.owner, &repo_info.repo_name)
            .get_ref(&octocrab::params::repos::Reference::Branch(
                request.head_branch.to_string(),
            ))
            .await
            .map_err(|err| match GitHubServiceError::from(err) {
                GitHubServiceError::Client(source) => GitHubServiceError::Branch(format!(
                    "Head branch '{}' does not exist: {}",
                    request.head_branch, source
                )),
                other => other,
            })?;

        // Create the pull request
        let pr_info = self
            .client
            .pulls(&repo_info.owner, &repo_info.repo_name)
            .create(&request.title, &request.head_branch, &request.base_branch)
            .body(request.body.as_deref().unwrap_or(""))
            .send()
            .await
            .map(Self::map_pull_request)
            .map_err(|err| match GitHubServiceError::from(err) {
                GitHubServiceError::Client(source) => GitHubServiceError::PullRequest(format!(
                    "Failed to create PR for '{} -> {}': {}",
                    request.head_branch, request.base_branch, source
                )),
                other => other,
            })?;

        info!(
            "Created GitHub PR #{} for branch {} in {}/{}",
            pr_info.number, request.head_branch, repo_info.owner, repo_info.repo_name
        );

        Ok(pr_info)
    }

    /// Update and get the status of a pull request
    pub async fn update_pr_status(
        &self,
        repo_info: &GitHubRepoInfo,
        pr_number: i64,
    ) -> Result<PullRequestInfo, GitHubServiceError> {
        (|| async {
            self.client
                .pulls(&repo_info.owner, &repo_info.repo_name)
                .get(pr_number as u64)
                .await
                .map(Self::map_pull_request)
                .map_err(|err| match GitHubServiceError::from(err) {
                    GitHubServiceError::Client(source) => GitHubServiceError::PullRequest(format!(
                        "Failed to get PR #{pr_number}: {source}",
                    )),
                    other => other,
                })
        })
        .retry(
            &ExponentialBuilder::default()
                .with_min_delay(Duration::from_secs(1))
                .with_max_delay(Duration::from_secs(30))
                .with_max_times(3)
                .with_jitter(),
        )
        .when(|err| err.should_retry())
        .notify(|err: &GitHubServiceError, dur: Duration| {
            tracing::warn!(
                "GitHub API call failed, retrying after {:.2}s: {}",
                dur.as_secs_f64(),
                err
            );
        })
        .await
    }

    fn map_pull_request(pr: octocrab::models::pulls::PullRequest) -> PullRequestInfo {
        let state = match pr.state {
            Some(IssueState::Open) => MergeStatus::Open,
            Some(IssueState::Closed) => {
                if pr.merged_at.is_some() {
                    MergeStatus::Merged
                } else {
                    MergeStatus::Closed
                }
            }
            None => MergeStatus::Unknown,
            Some(_) => MergeStatus::Unknown,
        };

        PullRequestInfo {
            number: pr.number as i64,
            url: pr.html_url.map(|url| url.to_string()).unwrap_or_default(),
            status: state,
            merged_at: pr.merged_at.map(|dt| dt.naive_utc().and_utc()),
            merge_commit_sha: pr.merge_commit_sha,
        }
    }

    /// List repositories for the authenticated user with pagination
    #[cfg(feature = "cloud")]
    pub async fn list_repositories(
        &self,
        page: u8,
    ) -> Result<Vec<RepositoryInfo>, GitHubServiceError> {
        (|| async { self.list_repositories_internal(page).await })
            .retry(
                &ExponentialBuilder::default()
                    .with_min_delay(Duration::from_secs(1))
                    .with_max_delay(Duration::from_secs(30))
                    .with_max_times(3)
                    .with_jitter(),
            )
            .when(|err| err.should_retry())
            .notify(|err: &GitHubServiceError, dur: Duration| {
                tracing::warn!(
                    "GitHub API call failed, retrying after {:.2}s: {}",
                    dur.as_secs_f64(),
                    err
                );
            })
            .await
    }

    #[cfg(feature = "cloud")]
    async fn list_repositories_internal(
        &self,
        page: u8,
    ) -> Result<Vec<RepositoryInfo>, GitHubServiceError> {
        let repos_page = self
            .client
            .current()
            .list_repos_for_authenticated_user()
            .type_("all")
            .sort("updated")
            .direction("desc")
            .per_page(50)
            .page(page)
            .send()
            .await
            .map_err(|e| {
                GitHubServiceError::Repository(format!("Failed to list repositories: {}", e))
            })?;

        let repositories: Vec<RepositoryInfo> = repos_page
            .items
            .into_iter()
            .map(|repo| RepositoryInfo {
                id: repo.id.0 as i64,
                name: repo.name,
                full_name: repo.full_name.unwrap_or_default(),
                owner: repo.owner.map(|o| o.login).unwrap_or_default(),
                description: repo.description,
                clone_url: repo
                    .clone_url
                    .map(|url| url.to_string())
                    .unwrap_or_default(),
                ssh_url: repo.ssh_url.unwrap_or_default(),
                default_branch: repo.default_branch.unwrap_or_else(|| "main".to_string()),
                private: repo.private.unwrap_or(false),
            })
            .collect();

        tracing::info!(
            "Retrieved {} repositories from GitHub (page {})",
            repositories.len(),
            page
        );
        Ok(repositories)
    }
}
</file>

<file path="crates/services/src/services/image.rs">
use std::{
    fs,
    path::{Path, PathBuf},
};

use db::models::image::{CreateImage, Image};
use regex::{Captures, Regex};
use sha2::{Digest, Sha256};
use sqlx::SqlitePool;
use uuid::Uuid;

#[derive(Debug, thiserror::Error)]
pub enum ImageError {
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),

    #[error("Database error: {0}")]
    Database(#[from] sqlx::Error),

    #[error("Invalid image format")]
    InvalidFormat,

    #[error("Image too large: {0} bytes (max: {1} bytes)")]
    TooLarge(u64, u64),

    #[error("Image not found")]
    NotFound,

    #[error("Failed to build response: {0}")]
    ResponseBuildError(String),
}

#[derive(Clone)]
pub struct ImageService {
    cache_dir: PathBuf,
    pool: SqlitePool,
    max_size_bytes: u64,
}

impl ImageService {
    pub fn new(pool: SqlitePool) -> Result<Self, ImageError> {
        let cache_dir = utils::cache_dir().join("images");
        fs::create_dir_all(&cache_dir)?;
        Ok(Self {
            cache_dir,
            pool,
            max_size_bytes: 20 * 1024 * 1024, // 20MB default
        })
    }

    pub async fn store_image(
        &self,
        data: &[u8],
        original_filename: &str,
    ) -> Result<Image, ImageError> {
        let file_size = data.len() as u64;

        if file_size > self.max_size_bytes {
            return Err(ImageError::TooLarge(file_size, self.max_size_bytes));
        }

        let hash = format!("{:x}", Sha256::digest(data));

        // Extract extension from original filename
        let extension = Path::new(original_filename)
            .extension()
            .and_then(|e| e.to_str())
            .unwrap_or("png");

        let mime_type = match extension.to_lowercase().as_str() {
            "png" => Some("image/png".to_string()),
            "jpg" | "jpeg" => Some("image/jpeg".to_string()),
            "gif" => Some("image/gif".to_string()),
            "webp" => Some("image/webp".to_string()),
            "bmp" => Some("image/bmp".to_string()),
            "svg" => Some("image/svg+xml".to_string()),
            _ => None,
        };

        if mime_type.is_none() {
            return Err(ImageError::InvalidFormat);
        }

        let existing_image = Image::find_by_hash(&self.pool, &hash).await?;

        if let Some(existing) = existing_image {
            tracing::debug!("Reusing existing image record with hash {}", hash);
            return Ok(existing);
        }

        let new_filename = format!("{}.{}", Uuid::new_v4(), extension);
        let cached_path = self.cache_dir.join(&new_filename);
        fs::write(&cached_path, data)?;

        let image = Image::create(
            &self.pool,
            &CreateImage {
                file_path: new_filename,
                original_name: original_filename.to_string(),
                mime_type,
                size_bytes: file_size as i64,
                hash,
            },
        )
        .await?;
        Ok(image)
    }

    pub async fn delete_orphaned_images(&self) -> Result<(), ImageError> {
        let orphaned_images = Image::find_orphaned_images(&self.pool).await?;
        if orphaned_images.is_empty() {
            tracing::debug!("No orphaned images found during cleanup");
            return Ok(());
        }

        tracing::debug!(
            "Found {} orphaned images to clean up",
            orphaned_images.len()
        );
        let mut deleted_count = 0;
        let mut failed_count = 0;

        for image in orphaned_images {
            match self.delete_image(image.id).await {
                Ok(_) => {
                    deleted_count += 1;
                    tracing::debug!("Deleted orphaned image: {}", image.id);
                }
                Err(e) => {
                    failed_count += 1;
                    tracing::error!("Failed to delete orphaned image {}: {}", image.id, e);
                }
            }
        }

        tracing::info!(
            "Image cleanup completed: {} deleted, {} failed",
            deleted_count,
            failed_count
        );

        Ok(())
    }

    pub fn get_absolute_path(&self, image: &Image) -> PathBuf {
        self.cache_dir.join(&image.file_path)
    }

    pub async fn get_image(&self, id: Uuid) -> Result<Option<Image>, ImageError> {
        Ok(Image::find_by_id(&self.pool, id).await?)
    }

    pub async fn delete_image(&self, id: Uuid) -> Result<(), ImageError> {
        if let Some(image) = Image::find_by_id(&self.pool, id).await? {
            let file_path = self.cache_dir.join(&image.file_path);
            if file_path.exists() {
                fs::remove_file(file_path)?;
            }

            Image::delete(&self.pool, id).await?;
        }

        Ok(())
    }

    pub async fn copy_images_by_task_to_worktree(
        &self,
        worktree_path: &Path,
        task_id: Uuid,
    ) -> Result<(), ImageError> {
        let images = Image::find_by_task_id(&self.pool, task_id).await?;
        self.copy_images(worktree_path, images)
    }

    pub async fn copy_images_by_ids_to_worktree(
        &self,
        worktree_path: &Path,
        image_ids: &[Uuid],
    ) -> Result<(), ImageError> {
        let mut images = Vec::new();
        for id in image_ids {
            if let Some(image) = Image::find_by_id(&self.pool, *id).await? {
                images.push(image);
            }
        }
        self.copy_images(worktree_path, images)
    }

    fn copy_images(&self, worktree_path: &Path, images: Vec<Image>) -> Result<(), ImageError> {
        if images.is_empty() {
            return Ok(());
        }

        let images_dir = worktree_path.join(utils::path::VIBE_IMAGES_DIR);
        std::fs::create_dir_all(&images_dir)?;

        // Create .gitignore to ignore all files in this directory
        let gitignore_path = images_dir.join(".gitignore");
        if !gitignore_path.exists() {
            std::fs::write(&gitignore_path, "*\n")?;
        }

        for image in images {
            let src = self.cache_dir.join(&image.file_path);
            let dst = images_dir.join(&image.file_path);
            if src.exists() {
                if let Err(e) = std::fs::copy(&src, &dst) {
                    tracing::error!("Failed to copy {}: {}", image.file_path, e);
                } else {
                    tracing::debug!("Copied {}", image.file_path);
                }
            } else {
                tracing::warn!("Missing cache file: {}", src.display());
            }
        }

        Ok(())
    }

    pub fn canonicalise_image_paths(prompt: &str, worktree_path: &Path) -> String {
        let pattern = format!(
            r#"!\[([^\]]*)\]\(({}/[^)\s]+)\)"#,
            regex::escape(utils::path::VIBE_IMAGES_DIR)
        );
        let re = Regex::new(&pattern).unwrap();

        re.replace_all(prompt, |caps: &Captures| {
            let alt = &caps[1];
            let rel = &caps[2];
            let abs = worktree_path.join(rel);
            let abs = abs.to_string_lossy().replace('\\', "/");
            format!("![{alt}]({abs})")
        })
        .into_owned()
    }
}
</file>

<file path="crates/services/src/services/mod.rs">
pub mod analytics;
pub mod auth;
pub mod config;
pub mod container;
pub mod events;
pub mod file_ranker;
pub mod file_search_cache;
pub mod filesystem;
pub mod filesystem_watcher;
pub mod git;
pub mod git_cli;
pub mod github_service;
pub mod image;
pub mod notification;
pub mod pr_monitor;
pub mod sentry;
pub mod worktree_manager;
</file>

<file path="crates/services/src/services/notification.rs">
use std::sync::OnceLock;

use db::models::execution_process::{ExecutionContext, ExecutionProcessStatus};
use utils;

use crate::services::config::SoundFile;

/// Service for handling cross-platform notifications including sound alerts and push notifications
#[derive(Debug, Clone)]
pub struct NotificationService {}
use crate::services::config::NotificationConfig;

/// Cache for WSL root path from PowerShell
static WSL_ROOT_PATH_CACHE: OnceLock<Option<String>> = OnceLock::new();

impl NotificationService {
    pub async fn notify_execution_halted(mut config: NotificationConfig, ctx: &ExecutionContext) {
        // If the process was intentionally killed by user, suppress sound
        if matches!(ctx.execution_process.status, ExecutionProcessStatus::Killed) {
            config.sound_enabled = false;
        }

        let title = format!("Task Complete: {}", ctx.task.title);
        let message = match ctx.execution_process.status {
            ExecutionProcessStatus::Completed => format!(
                "✅ '{}' completed successfully\nBranch: {:?}\nExecutor: {}",
                ctx.task.title, ctx.task_attempt.branch, ctx.task_attempt.executor
            ),
            ExecutionProcessStatus::Failed => format!(
                "❌ '{}' execution failed\nBranch: {:?}\nExecutor: {}",
                ctx.task.title, ctx.task_attempt.branch, ctx.task_attempt.executor
            ),
            ExecutionProcessStatus::Killed => format!(
                "🛑 '{}' execution cancelled by user\nBranch: {:?}\nExecutor: {}",
                ctx.task.title, ctx.task_attempt.branch, ctx.task_attempt.executor
            ),
            _ => {
                tracing::warn!(
                    "Tried to notify attempt completion for {} but process is still running!",
                    ctx.task_attempt.id
                );
                return;
            }
        };
        Self::notify(config, &title, &message).await;
    }

    /// Send both sound and push notifications if enabled
    pub async fn notify(config: NotificationConfig, title: &str, message: &str) {
        if config.sound_enabled {
            Self::play_sound_notification(&config.sound_file).await;
        }

        if config.push_enabled {
            Self::send_push_notification(title, message).await;
        }
    }

    /// Play a system sound notification across platforms
    async fn play_sound_notification(sound_file: &SoundFile) {
        let file_path = match sound_file.get_path().await {
            Ok(path) => path,
            Err(e) => {
                tracing::error!("Failed to create cached sound file: {}", e);
                return;
            }
        };

        // Use platform-specific sound notification
        // Note: spawn() calls are intentionally not awaited - sound notifications should be fire-and-forget
        if cfg!(target_os = "macos") {
            let _ = tokio::process::Command::new("afplay")
                .arg(&file_path)
                .spawn();
        } else if cfg!(target_os = "linux") && !utils::is_wsl2() {
            // Try different Linux audio players
            if tokio::process::Command::new("paplay")
                .arg(&file_path)
                .spawn()
                .is_ok()
            {
                // Success with paplay
            } else if tokio::process::Command::new("aplay")
                .arg(&file_path)
                .spawn()
                .is_ok()
            {
                // Success with aplay
            } else {
                // Try system bell as fallback
                let _ = tokio::process::Command::new("echo")
                    .arg("-e")
                    .arg("\\a")
                    .spawn();
            }
        } else if cfg!(target_os = "windows") || (cfg!(target_os = "linux") && utils::is_wsl2()) {
            // Convert WSL path to Windows path if in WSL2
            let file_path = if utils::is_wsl2() {
                if let Some(windows_path) = Self::wsl_to_windows_path(&file_path).await {
                    windows_path
                } else {
                    file_path.to_string_lossy().to_string()
                }
            } else {
                file_path.to_string_lossy().to_string()
            };

            let _ = tokio::process::Command::new("powershell.exe")
                .arg("-c")
                .arg(format!(
                    r#"(New-Object Media.SoundPlayer "{file_path}").PlaySync()"#
                ))
                .spawn();
        }
    }

    /// Send a cross-platform push notification
    async fn send_push_notification(title: &str, message: &str) {
        if cfg!(target_os = "macos") {
            Self::send_macos_notification(title, message).await;
        } else if cfg!(target_os = "linux") && !utils::is_wsl2() {
            Self::send_linux_notification(title, message).await;
        } else if cfg!(target_os = "windows") || (cfg!(target_os = "linux") && utils::is_wsl2()) {
            Self::send_windows_notification(title, message).await;
        }
    }

    /// Send macOS notification using osascript
    async fn send_macos_notification(title: &str, message: &str) {
        let script = format!(
            r#"display notification "{message}" with title "{title}" sound name "Glass""#,
            message = message.replace('"', r#"\""#),
            title = title.replace('"', r#"\""#)
        );

        let _ = tokio::process::Command::new("osascript")
            .arg("-e")
            .arg(script)
            .spawn();
    }

    /// Send Linux notification using notify-rust
    async fn send_linux_notification(title: &str, message: &str) {
        use notify_rust::Notification;

        let title = title.to_string();
        let message = message.to_string();

        let _handle = tokio::task::spawn_blocking(move || {
            if let Err(e) = Notification::new()
                .summary(&title)
                .body(&message)
                .timeout(10000)
                .show()
            {
                tracing::error!("Failed to send Linux notification: {}", e);
            }
        });
        drop(_handle); // Don't await, fire-and-forget
    }

    /// Send Windows/WSL notification using PowerShell toast script
    async fn send_windows_notification(title: &str, message: &str) {
        let script_path = match utils::get_powershell_script().await {
            Ok(path) => path,
            Err(e) => {
                tracing::error!("Failed to get PowerShell script: {}", e);
                return;
            }
        };

        // Convert WSL path to Windows path if in WSL2
        let script_path_str = if utils::is_wsl2() {
            if let Some(windows_path) = Self::wsl_to_windows_path(&script_path).await {
                windows_path
            } else {
                script_path.to_string_lossy().to_string()
            }
        } else {
            script_path.to_string_lossy().to_string()
        };

        let _ = tokio::process::Command::new("powershell.exe")
            .arg("-NoProfile")
            .arg("-ExecutionPolicy")
            .arg("Bypass")
            .arg("-File")
            .arg(script_path_str)
            .arg("-Title")
            .arg(title)
            .arg("-Message")
            .arg(message)
            .spawn();
    }

    /// Get WSL root path via PowerShell (cached)
    async fn get_wsl_root_path() -> Option<String> {
        if let Some(cached) = WSL_ROOT_PATH_CACHE.get() {
            return cached.clone();
        }

        match tokio::process::Command::new("powershell.exe")
            .arg("-c")
            .arg("(Get-Location).Path -replace '^.*::', ''")
            .current_dir("/")
            .output()
            .await
        {
            Ok(output) => {
                match String::from_utf8(output.stdout) {
                    Ok(pwd_str) => {
                        let pwd = pwd_str.trim();
                        tracing::info!("WSL root path detected: {}", pwd);

                        // Cache the result
                        let _ = WSL_ROOT_PATH_CACHE.set(Some(pwd.to_string()));
                        return Some(pwd.to_string());
                    }
                    Err(e) => {
                        tracing::error!("Failed to parse PowerShell pwd output as UTF-8: {}", e);
                    }
                }
            }
            Err(e) => {
                tracing::error!("Failed to execute PowerShell pwd command: {}", e);
            }
        }

        // Cache the failure result
        let _ = WSL_ROOT_PATH_CACHE.set(None);
        None
    }

    /// Convert WSL path to Windows UNC path for PowerShell
    async fn wsl_to_windows_path(wsl_path: &std::path::Path) -> Option<String> {
        let path_str = wsl_path.to_string_lossy();

        // Relative paths work fine as-is in PowerShell
        if !path_str.starts_with('/') {
            tracing::debug!("Using relative path as-is: {}", path_str);
            return Some(path_str.to_string());
        }

        // Get cached WSL root path from PowerShell
        if let Some(wsl_root) = Self::get_wsl_root_path().await {
            // Simply concatenate WSL root with the absolute path - PowerShell doesn't mind /
            let windows_path = format!("{wsl_root}{path_str}");
            tracing::debug!("WSL path converted: {} -> {}", path_str, windows_path);
            Some(windows_path)
        } else {
            tracing::error!(
                "Failed to determine WSL root path for conversion: {}",
                path_str
            );
            None
        }
    }
}
</file>

<file path="crates/services/src/services/pr_monitor.rs">
use std::{sync::Arc, time::Duration};

use db::{
    DBService,
    models::{
        merge::{Merge, MergeStatus, PrMerge},
        task::{Task, TaskStatus},
        task_attempt::{TaskAttempt, TaskAttemptError},
    },
};
use sqlx::error::Error as SqlxError;
use thiserror::Error;
use tokio::{sync::RwLock, time::interval};
use tracing::{debug, error, info};

use crate::services::{
    config::Config,
    github_service::{GitHubRepoInfo, GitHubService, GitHubServiceError},
};

#[derive(Debug, Error)]
enum PrMonitorError {
    #[error("No GitHub token configured")]
    NoGitHubToken,
    #[error(transparent)]
    GitHubServiceError(#[from] GitHubServiceError),
    #[error(transparent)]
    TaskAttemptError(#[from] TaskAttemptError),
    #[error(transparent)]
    Sqlx(#[from] SqlxError),
}

/// Service to monitor GitHub PRs and update task status when they are merged
pub struct PrMonitorService {
    db: DBService,
    config: Arc<RwLock<Config>>,
    poll_interval: Duration,
}

impl PrMonitorService {
    pub async fn spawn(db: DBService, config: Arc<RwLock<Config>>) -> tokio::task::JoinHandle<()> {
        let service = Self {
            db,
            config,
            poll_interval: Duration::from_secs(60), // Check every minute
        };
        tokio::spawn(async move {
            service.start().await;
        })
    }

    async fn start(&self) {
        info!(
            "Starting PR monitoring service with interval {:?}",
            self.poll_interval
        );

        let mut interval = interval(self.poll_interval);

        loop {
            interval.tick().await;
            if let Err(e) = self.check_all_open_prs().await {
                error!("Error checking open PRs: {}", e);
            }
        }
    }

    /// Check all open PRs for updates with the provided GitHub token
    async fn check_all_open_prs(&self) -> Result<(), PrMonitorError> {
        let open_prs = Merge::get_open_prs(&self.db.pool).await?;

        if open_prs.is_empty() {
            debug!("No open PRs to check");
            return Ok(());
        }

        info!("Checking {} open PRs", open_prs.len());

        for pr_merge in open_prs {
            if let Err(e) = self.check_pr_status(&pr_merge).await {
                error!(
                    "Error checking PR #{} for attempt {}: {}",
                    pr_merge.pr_info.number, pr_merge.task_attempt_id, e
                );
            }
        }
        Ok(())
    }

    /// Check the status of a specific PR
    async fn check_pr_status(&self, pr_merge: &PrMerge) -> Result<(), PrMonitorError> {
        let github_config = self.config.read().await.github.clone();
        let github_token = github_config.token().ok_or(PrMonitorError::NoGitHubToken)?;

        let github_service = GitHubService::new(&github_token)?;

        let repo_info = GitHubRepoInfo::from_remote_url(&pr_merge.pr_info.url)?;

        let pr_status = github_service
            .update_pr_status(&repo_info, pr_merge.pr_info.number)
            .await?;

        debug!(
            "PR #{} status: {:?} (was open)",
            pr_merge.pr_info.number, pr_status.status
        );

        // Update the PR status in the database
        if !matches!(&pr_status.status, MergeStatus::Open) {
            // Update merge status with the latest information from GitHub
            Merge::update_status(
                &self.db.pool,
                pr_merge.id,
                pr_status.status.clone(),
                pr_status.merge_commit_sha,
            )
            .await?;

            // If the PR was merged, update the task status to done
            if matches!(&pr_status.status, MergeStatus::Merged)
                && let Some(task_attempt) =
                    TaskAttempt::find_by_id(&self.db.pool, pr_merge.task_attempt_id).await?
            {
                info!(
                    "PR #{} was merged, updating task {} to done",
                    pr_merge.pr_info.number, task_attempt.task_id
                );
                Task::update_status(&self.db.pool, task_attempt.task_id, TaskStatus::Done).await?;
            }
        }

        Ok(())
    }
}
</file>

<file path="crates/services/src/services/sentry.rs">
#[derive(Clone)]
pub struct SentryService {}

impl Default for SentryService {
    fn default() -> Self {
        Self::new()
    }
}

impl SentryService {
    pub fn new() -> Self {
        SentryService {}
    }

    pub async fn update_scope(&self, user_id: &str, username: Option<&str>, email: Option<&str>) {
        let sentry_user = match (username, email) {
            (Some(user), Some(email)) => sentry::User {
                id: Some(user_id.to_string()),
                username: Some(user.to_string()),
                email: Some(email.to_string()),
                ..Default::default()
            },
            _ => sentry::User {
                id: Some(user_id.to_string()),
                ..Default::default()
            },
        };

        sentry::configure_scope(|scope| {
            scope.set_user(Some(sentry_user));
        });
    }
}
</file>

<file path="crates/services/src/services/worktree_manager.rs">
use std::{
    collections::HashMap,
    path::{Path, PathBuf},
    sync::{Arc, Mutex},
};

use git2::{Error as GitError, Repository};
use thiserror::Error;
use tracing::{debug, info};
use utils::shell::get_shell_command;

use super::{
    git::{GitService, GitServiceError},
    git_cli::GitCli,
};

// Global synchronization for worktree creation to prevent race conditions
lazy_static::lazy_static! {
    static ref WORKTREE_CREATION_LOCKS: Arc<Mutex<HashMap<String, Arc<tokio::sync::Mutex<()>>>>> =
        Arc::new(Mutex::new(HashMap::new()));
}

#[derive(Debug, Error)]
pub enum WorktreeError {
    #[error(transparent)]
    Git(#[from] GitError),
    #[error(transparent)]
    GitService(#[from] GitServiceError),
    #[error("Git CLI error: {0}")]
    GitCli(String),
    #[error("Task join error: {0}")]
    TaskJoin(String),
    #[error("Invalid path: {0}")]
    InvalidPath(String),
    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),
    #[error("Branch not found: {0}")]
    BranchNotFound(String),
    #[error("Repository error: {0}")]
    Repository(String),
}

pub struct WorktreeManager;

impl WorktreeManager {
    /// Create a worktree with a new branch
    pub async fn create_worktree(
        repo_path: &Path,
        branch_name: &str,
        worktree_path: &Path,
        base_branch: &str,
        create_branch: bool,
    ) -> Result<(), WorktreeError> {
        if create_branch {
            let repo_path_owned = repo_path.to_path_buf();
            let branch_name_owned = branch_name.to_string();
            let base_branch_owned = base_branch.to_string();

            tokio::task::spawn_blocking(move || {
                let repo = Repository::open(&repo_path_owned)?;
                let base_branch_ref =
                    GitService::find_branch(&repo, &base_branch_owned)?.into_reference();
                repo.branch(
                    &branch_name_owned,
                    &base_branch_ref.peel_to_commit()?,
                    false,
                )?;
                Ok::<(), GitServiceError>(())
            })
            .await
            .map_err(|e| WorktreeError::TaskJoin(format!("Task join error: {e}")))??;
        }

        Self::ensure_worktree_exists(repo_path, branch_name, worktree_path).await
    }

    /// Ensure worktree exists, recreating if necessary with proper synchronization
    /// This is the main entry point for ensuring a worktree exists and prevents race conditions
    pub async fn ensure_worktree_exists(
        repo_path: &Path,
        branch_name: &str,
        worktree_path: &Path,
    ) -> Result<(), WorktreeError> {
        let path_str = worktree_path.to_string_lossy().to_string();

        // Get or create a lock for this specific worktree path
        let lock = {
            let mut locks = WORKTREE_CREATION_LOCKS.lock().unwrap();
            locks
                .entry(path_str.clone())
                .or_insert_with(|| Arc::new(tokio::sync::Mutex::new(())))
                .clone()
        };

        // Acquire the lock for this specific worktree path
        let _guard = lock.lock().await;

        // Check if worktree already exists and is properly set up
        if Self::is_worktree_properly_set_up(repo_path, worktree_path).await? {
            debug!("Worktree already properly set up at path: {}", path_str);
            return Ok(());
        }

        // If worktree doesn't exist or isn't properly set up, recreate it
        info!("Worktree needs recreation at path: {}", path_str);
        Self::recreate_worktree_internal(repo_path, branch_name, worktree_path).await
    }

    /// Internal worktree recreation function (always recreates)
    async fn recreate_worktree_internal(
        repo_path: &Path,
        branch_name: &str,
        worktree_path: &Path,
    ) -> Result<(), WorktreeError> {
        let path_str = worktree_path.to_string_lossy().to_string();
        let branch_name_owned = branch_name.to_string();
        let worktree_path_owned = worktree_path.to_path_buf();

        // Use the provided repo path
        let git_repo_path = repo_path;

        // Get the worktree name for metadata operations
        let worktree_name = worktree_path
            .file_name()
            .and_then(|n| n.to_str())
            .ok_or_else(|| WorktreeError::InvalidPath("Invalid worktree path".to_string()))?
            .to_string();

        info!(
            "Creating worktree {} at path {}",
            branch_name_owned, path_str
        );

        // Step 1: Comprehensive cleanup of existing worktree and metadata (non-blocking)
        Self::comprehensive_worktree_cleanup_async(
            git_repo_path,
            &worktree_path_owned,
            &worktree_name,
        )
        .await?;

        // Step 2: Ensure parent directory exists (non-blocking)
        if let Some(parent) = worktree_path_owned.parent() {
            let parent_path = parent.to_path_buf();
            tokio::task::spawn_blocking(move || std::fs::create_dir_all(&parent_path))
                .await
                .map_err(|e| WorktreeError::TaskJoin(format!("Task join error: {e}")))?
                .map_err(WorktreeError::Io)?;
        }

        // Step 3: Create the worktree with retry logic for metadata conflicts (non-blocking)
        Self::create_worktree_with_retry(
            git_repo_path,
            &branch_name_owned,
            &worktree_path_owned,
            &worktree_name,
            &path_str,
        )
        .await
    }

    /// Check if a worktree is properly set up (filesystem + git metadata)
    async fn is_worktree_properly_set_up(
        repo_path: &Path,
        worktree_path: &Path,
    ) -> Result<bool, WorktreeError> {
        let repo_path = repo_path.to_path_buf();
        let worktree_path = worktree_path.to_path_buf();

        tokio::task::spawn_blocking(move || -> Result<bool, WorktreeError> {
            // Check 1: Filesystem path must exist
            if !worktree_path.exists() {
                return Ok(false);
            }

            // Check 2: Worktree must be registered in git metadata using find_worktree
            let repo = Repository::open(&repo_path).map_err(WorktreeError::Git)?;
            let worktree_name = worktree_path
                .file_name()
                .and_then(|n| n.to_str())
                .ok_or_else(|| WorktreeError::InvalidPath("Invalid worktree path".to_string()))?;

            // Try to find the worktree - if it exists and is valid, we're good
            match repo.find_worktree(worktree_name) {
                Ok(_) => Ok(true),
                Err(_) => Ok(false),
            }
        })
        .await
        .map_err(|e| WorktreeError::TaskJoin(format!("{e}")))?
    }

    /// Comprehensive cleanup of worktree path and metadata to prevent "path exists" errors (blocking)
    fn comprehensive_worktree_cleanup(
        repo: &Repository,
        worktree_path: &Path,
        worktree_name: &str,
    ) -> Result<(), WorktreeError> {
        debug!("Performing cleanup for worktree: {}", worktree_name);

        let git_repo_path = Self::get_git_repo_path(repo)?;

        // Step 1: Use Git CLI to remove the worktree registration (force) if present
        // The Git CLI is more robust than libgit2 for mutable worktree operations
        let git = GitCli::new();
        if let Err(e) = git.worktree_remove(&git_repo_path, worktree_path, true) {
            debug!("git worktree remove non-fatal error: {}", e);
        }

        // Step 2: Always force cleanup metadata directory (proactive cleanup)
        if let Err(e) = Self::force_cleanup_worktree_metadata(&git_repo_path, worktree_name) {
            debug!("Metadata cleanup failed (non-fatal): {}", e);
        }

        // Step 3: Clean up physical worktree directory if it exists
        if worktree_path.exists() {
            debug!(
                "Removing existing worktree directory: {}",
                worktree_path.display()
            );
            std::fs::remove_dir_all(worktree_path).map_err(WorktreeError::Io)?;
        }

        // Step 4: Good-practice to clean up any other stale admin entries
        if let Err(e) = git.worktree_prune(&git_repo_path) {
            debug!("git worktree prune non-fatal error: {}", e);
        }

        debug!(
            "Comprehensive cleanup completed for worktree: {}",
            worktree_name
        );
        Ok(())
    }

    /// Async version of comprehensive cleanup to avoid blocking the main runtime
    async fn comprehensive_worktree_cleanup_async(
        git_repo_path: &Path,
        worktree_path: &Path,
        worktree_name: &str,
    ) -> Result<(), WorktreeError> {
        let git_repo_path_owned = git_repo_path.to_path_buf();
        let worktree_path_owned = worktree_path.to_path_buf();
        let worktree_name_owned = worktree_name.to_string();

        // First, try to open the repository to see if it exists
        let repo_result = tokio::task::spawn_blocking({
            let git_repo_path = git_repo_path_owned.clone();
            move || Repository::open(&git_repo_path)
        })
        .await;

        match repo_result {
            Ok(Ok(repo)) => {
                // Repository exists, perform comprehensive cleanup
                tokio::task::spawn_blocking(move || {
                    Self::comprehensive_worktree_cleanup(
                        &repo,
                        &worktree_path_owned,
                        &worktree_name_owned,
                    )
                })
                .await
                .map_err(|e| WorktreeError::TaskJoin(format!("Task join error: {e}")))?
            }
            Ok(Err(e)) => {
                // Repository doesn't exist (likely deleted project), fall back to simple cleanup
                debug!(
                    "Failed to open repository at {:?}: {}. Falling back to simple cleanup for worktree at {}",
                    git_repo_path_owned,
                    e,
                    worktree_path_owned.display()
                );
                Self::simple_worktree_cleanup(&worktree_path_owned).await?;
                Ok(())
            }
            Err(e) => Err(WorktreeError::TaskJoin(format!("{e}"))),
        }
    }

    /// Create worktree with retry logic in non-blocking manner
    async fn create_worktree_with_retry(
        git_repo_path: &Path,
        branch_name: &str,
        worktree_path: &Path,
        worktree_name: &str,
        path_str: &str,
    ) -> Result<(), WorktreeError> {
        let git_repo_path = git_repo_path.to_path_buf();
        let branch_name = branch_name.to_string();
        let worktree_path = worktree_path.to_path_buf();
        let worktree_name = worktree_name.to_string();
        let path_str = path_str.to_string();

        tokio::task::spawn_blocking(move || -> Result<(), WorktreeError> {
            // Prefer git CLI for worktree add to inherit sparse-checkout semantics
            let git = GitCli::new();
            match git.worktree_add(&git_repo_path, &worktree_path, &branch_name, false) {
                Ok(()) => {
                    if !worktree_path.exists() {
                        return Err(WorktreeError::Repository(format!(
                            "Worktree creation reported success but path {path_str} does not exist"
                        )));
                    }
                    info!(
                        "Successfully created worktree {} at {} (git CLI)",
                        branch_name, path_str
                    );
                    Ok(())
                }
                Err(e) => {
                    debug!(
                        "git worktree add failed; attempting metadata cleanup and retry: {}",
                        e
                    );
                    // Force cleanup metadata and try one more time
                    Self::force_cleanup_worktree_metadata(&git_repo_path, &worktree_name)
                        .map_err(WorktreeError::Io)?;
                    if let Err(e2) =
                        git.worktree_add(&git_repo_path, &worktree_path, &branch_name, false)
                    {
                        debug!("Retry of git worktree add failed: {}", e2);
                        return Err(WorktreeError::GitCli(e2.to_string()));
                    }
                    if !worktree_path.exists() {
                        return Err(WorktreeError::Repository(format!(
                            "Worktree creation reported success but path {path_str} does not exist"
                        )));
                    }
                    info!(
                        "Successfully created worktree {} at {} after metadata cleanup (git CLI)",
                        branch_name, path_str
                    );
                    Ok(())
                }
            }
        })
        .await
        .map_err(|e| WorktreeError::TaskJoin(format!("{e}")))?
    }

    /// Get the git repository path
    fn get_git_repo_path(repo: &Repository) -> Result<PathBuf, WorktreeError> {
        repo.workdir()
            .ok_or_else(|| {
                WorktreeError::Repository("Repository has no working directory".to_string())
            })?
            .to_str()
            .ok_or_else(|| {
                WorktreeError::InvalidPath("Repository path is not valid UTF-8".to_string())
            })
            .map(PathBuf::from)
    }

    /// Force cleanup worktree metadata directory
    fn force_cleanup_worktree_metadata(
        git_repo_path: &Path,
        worktree_name: &str,
    ) -> Result<(), std::io::Error> {
        let git_worktree_metadata_path = git_repo_path
            .join(".git")
            .join("worktrees")
            .join(worktree_name);

        if git_worktree_metadata_path.exists() {
            debug!(
                "Force removing git worktree metadata: {}",
                git_worktree_metadata_path.display()
            );
            std::fs::remove_dir_all(&git_worktree_metadata_path)?;
        }

        Ok(())
    }

    /// Clean up a worktree path and its git metadata (non-blocking)
    /// If git_repo_path is None, attempts to infer it from the worktree itself
    pub async fn cleanup_worktree(
        worktree_path: &Path,
        git_repo_path: Option<&Path>,
    ) -> Result<(), WorktreeError> {
        let path_str = worktree_path.to_string_lossy().to_string();

        // Get the same lock to ensure we don't interfere with creation
        let lock = {
            let mut locks = WORKTREE_CREATION_LOCKS.lock().unwrap();
            locks
                .entry(path_str.clone())
                .or_insert_with(|| Arc::new(tokio::sync::Mutex::new(())))
                .clone()
        };

        let _guard = lock.lock().await;

        if let Some(worktree_name) = worktree_path.file_name().and_then(|n| n.to_str()) {
            // Try to determine the git repo path if not provided
            let resolved_repo_path = if let Some(repo_path) = git_repo_path {
                Some(repo_path.to_path_buf())
            } else {
                Self::infer_git_repo_path(worktree_path).await
            };

            if let Some(repo_path) = resolved_repo_path {
                Self::comprehensive_worktree_cleanup_async(
                    &repo_path,
                    worktree_path,
                    worktree_name,
                )
                .await?;
            } else {
                // Can't determine repo path, just clean up the worktree directory
                debug!(
                    "Cannot determine git repo path for worktree {}, performing simple cleanup",
                    path_str
                );
                Self::simple_worktree_cleanup(worktree_path).await?;
            }
        } else {
            return Err(WorktreeError::InvalidPath(
                "Invalid worktree path, cannot determine name".to_string(),
            ));
        }

        Ok(())
    }

    /// Try to infer the git repository path from a worktree
    async fn infer_git_repo_path(worktree_path: &Path) -> Option<PathBuf> {
        // Try using git rev-parse --git-common-dir from within the worktree
        let worktree_path_owned = worktree_path.to_path_buf();

        tokio::task::spawn_blocking(move || {
            let (shell_cmd, shell_arg) = get_shell_command();
            let git_command = "git rev-parse --git-common-dir";

            let output = std::process::Command::new(shell_cmd)
                .args([shell_arg, git_command])
                .current_dir(&worktree_path_owned)
                .output()
                .ok()?;

            if output.status.success() {
                let git_common_dir = String::from_utf8(output.stdout).ok()?.trim().to_string();

                // git-common-dir gives us the path to the .git directory
                // We need the working directory (parent of .git)
                let git_dir_path = Path::new(&git_common_dir);
                if git_dir_path.file_name() == Some(std::ffi::OsStr::new(".git")) {
                    git_dir_path.parent()?.to_str().map(PathBuf::from)
                } else {
                    // In case of bare repo or unusual setup, use the git-common-dir as is
                    Some(PathBuf::from(git_common_dir))
                }
            } else {
                None
            }
        })
        .await
        .ok()
        .flatten()
    }

    /// Simple worktree cleanup when we can't determine the main repo
    async fn simple_worktree_cleanup(worktree_path: &Path) -> Result<(), WorktreeError> {
        let worktree_path_owned = worktree_path.to_path_buf();

        tokio::task::spawn_blocking(move || -> Result<(), WorktreeError> {
            if worktree_path_owned.exists() {
                std::fs::remove_dir_all(&worktree_path_owned).map_err(WorktreeError::Io)?;
                info!(
                    "Removed worktree directory: {}",
                    worktree_path_owned.display()
                );
            }
            Ok(())
        })
        .await
        .map_err(|e| WorktreeError::TaskJoin(format!("{e}")))?
    }

    /// Get the base directory for vibe-kanban worktrees
    pub fn get_worktree_base_dir() -> std::path::PathBuf {
        utils::path::get_vibe_kanban_temp_dir().join("worktrees")
    }
}
</file>

<file path="crates/services/src/lib.rs">
pub mod services;
</file>

<file path="crates/services/tests/git_ops_safety.rs">
use std::{
    fs,
    io::Write,
    path::{Path, PathBuf},
};

use git2::{PushOptions, Repository, build::CheckoutBuilder};
use services::services::{
    git::GitService,
    git_cli::{GitCli, GitCliError},
};
use tempfile::TempDir;
// Avoid direct git CLI usage in tests; exercise GitService instead.

fn write_file<P: AsRef<Path>>(base: P, rel: &str, content: &str) {
    let path = base.as_ref().join(rel);
    if let Some(parent) = path.parent() {
        fs::create_dir_all(parent).unwrap();
    }
    let mut f = fs::File::create(&path).unwrap();
    f.write_all(content.as_bytes()).unwrap();
}

fn commit_all(repo: &Repository, message: &str) {
    let mut index = repo.index().unwrap();
    index
        .add_all(["*"].iter(), git2::IndexAddOption::DEFAULT, None)
        .unwrap();
    index.write().unwrap();
    let tree_id = index.write_tree().unwrap();
    let tree = repo.find_tree(tree_id).unwrap();
    let sig = repo.signature().unwrap();
    let parents: Vec<git2::Commit> = match repo.head() {
        Ok(h) => vec![h.peel_to_commit().unwrap()],
        Err(e) if e.code() == git2::ErrorCode::UnbornBranch => vec![],
        Err(e) => panic!("failed to read HEAD: {e}"),
    };
    let parent_refs: Vec<&git2::Commit> = parents.iter().collect();
    let update_ref = if repo.head().is_ok() {
        Some("HEAD")
    } else {
        None
    };
    repo.commit(update_ref, &sig, &sig, message, &tree, &parent_refs)
        .unwrap();
}

fn checkout_branch(repo: &Repository, name: &str) {
    repo.set_head(&format!("refs/heads/{name}")).unwrap();
    let mut co = CheckoutBuilder::new();
    co.force();
    repo.checkout_head(Some(&mut co)).unwrap();
}

fn create_branch_from_head(repo: &Repository, name: &str) {
    let head = repo.head().unwrap().peel_to_commit().unwrap();
    let _ = repo.branch(name, &head, true).unwrap();
}

fn configure_user(repo: &Repository) {
    let mut cfg = repo.config().unwrap();
    cfg.set_str("user.name", "Test User").unwrap();
    cfg.set_str("user.email", "test@example.com").unwrap();
}

fn push_ref(repo: &Repository, local: &str, remote: &str) {
    let mut remote_handle = repo.find_remote("origin").unwrap();
    let mut opts = PushOptions::new();
    let spec = format!("+{local}:{remote}");
    remote_handle
        .push(&[spec.as_str()], Some(&mut opts))
        .unwrap();
}

use services::services::git::DiffTarget;

// Non-conflicting setup used by several tests
fn setup_repo_with_worktree(root: &TempDir) -> (PathBuf, PathBuf) {
    let repo_path = root.path().join("repo");
    let worktree_path = root.path().join("wt-feature");

    let service = GitService::new();
    service
        .initialize_repo_with_main_branch(&repo_path)
        .expect("init repo");

    let repo = Repository::open(&repo_path).unwrap();
    configure_user(&repo);
    checkout_branch(&repo, "main");

    write_file(&repo_path, "common.txt", "base\n");
    commit_all(&repo, "initial main commit");

    create_branch_from_head(&repo, "old-base");
    checkout_branch(&repo, "old-base");
    write_file(&repo_path, "base.txt", "from old-base\n");
    commit_all(&repo, "old-base commit");

    checkout_branch(&repo, "main");
    create_branch_from_head(&repo, "new-base");
    checkout_branch(&repo, "new-base");
    write_file(&repo_path, "base.txt", "from new-base\n");
    commit_all(&repo, "new-base commit");

    checkout_branch(&repo, "old-base");
    create_branch_from_head(&repo, "feature");

    let svc = GitService::new();
    svc.add_worktree(&repo_path, &worktree_path, "feature", false)
        .expect("create worktree");

    write_file(&worktree_path, "feat.txt", "feat change\n");
    let wt_repo = Repository::open(&worktree_path).unwrap();
    commit_all(&wt_repo, "feature commit");

    (repo_path, worktree_path)
}

// Conflicting setup to simulate interactive rebase interruption
fn setup_conflict_repo_with_worktree(root: &TempDir) -> (PathBuf, PathBuf) {
    let repo_path = root.path().join("repo");
    let worktree_path = root.path().join("wt-feature");

    let service = GitService::new();
    service
        .initialize_repo_with_main_branch(&repo_path)
        .expect("init repo");

    let repo = Repository::open(&repo_path).unwrap();
    configure_user(&repo);
    checkout_branch(&repo, "main");

    write_file(&repo_path, "conflict.txt", "base\n");
    commit_all(&repo, "initial main commit");

    // old-base modifies conflict.txt one way
    create_branch_from_head(&repo, "old-base");
    checkout_branch(&repo, "old-base");
    write_file(&repo_path, "conflict.txt", "old-base version\n");
    commit_all(&repo, "old-base change");

    // feature builds on old-base and modifies same lines differently
    create_branch_from_head(&repo, "feature");

    // new-base modifies in a conflicting way
    checkout_branch(&repo, "main");
    create_branch_from_head(&repo, "new-base");
    checkout_branch(&repo, "new-base");
    write_file(&repo_path, "conflict.txt", "new-base version\n");
    commit_all(&repo, "new-base change");

    // add a worktree for feature and create the conflicting commit
    let svc = GitService::new();
    svc.add_worktree(&repo_path, &worktree_path, "feature", false)
        .expect("create worktree");
    let wt_repo = Repository::open(&worktree_path).unwrap();
    write_file(&worktree_path, "conflict.txt", "feature version\n");
    commit_all(&wt_repo, "feature conflicting change");

    (repo_path, worktree_path)
}

// Setup where feature has no unique commits (feature == old-base)
fn setup_no_unique_feature_repo(root: &TempDir) -> (PathBuf, PathBuf) {
    let repo_path = root.path().join("repo");
    let worktree_path = root.path().join("wt-feature");

    let service = GitService::new();
    service
        .initialize_repo_with_main_branch(&repo_path)
        .expect("init repo");

    let repo = Repository::open(&repo_path).unwrap();
    configure_user(&repo);
    checkout_branch(&repo, "main");

    write_file(&repo_path, "base.txt", "main base\n");
    commit_all(&repo, "initial main commit");

    // Create old-base at this point
    create_branch_from_head(&repo, "old-base");
    // Create new-base diverging
    checkout_branch(&repo, "main");
    create_branch_from_head(&repo, "new-base");
    checkout_branch(&repo, "new-base");
    write_file(&repo_path, "advance.txt", "new base\n");
    commit_all(&repo, "advance new-base");

    // Create feature equal to old-base (no unique commits)
    checkout_branch(&repo, "old-base");
    create_branch_from_head(&repo, "feature");
    let svc = GitService::new();
    svc.add_worktree(&repo_path, &worktree_path, "feature", false)
        .expect("create worktree");

    (repo_path, worktree_path)
}

// Simple two-way conflict between main and feature on the same file
fn setup_direct_conflict_repo(root: &TempDir) -> (PathBuf, PathBuf) {
    let repo_path = root.path().join("repo");
    let worktree_path = root.path().join("wt-feature");

    let service = GitService::new();
    service
        .initialize_repo_with_main_branch(&repo_path)
        .expect("init repo");

    let repo = Repository::open(&repo_path).unwrap();
    configure_user(&repo);
    checkout_branch(&repo, "main");

    write_file(&repo_path, "conflict.txt", "base\n");
    commit_all(&repo, "initial main commit");

    // Create feature and commit conflicting change
    create_branch_from_head(&repo, "feature");
    let svc = GitService::new();
    svc.add_worktree(&repo_path, &worktree_path, "feature", false)
        .expect("create worktree");
    let wt_repo = Repository::open(&worktree_path).unwrap();
    write_file(&worktree_path, "conflict.txt", "feature change\n");
    commit_all(&wt_repo, "feature change");

    // Change main in a conflicting way
    checkout_branch(&repo, "main");
    write_file(&repo_path, "conflict.txt", "main change\n");
    commit_all(&repo, "main change");

    (repo_path, worktree_path)
}

#[test]
fn push_with_token_reports_non_fast_forward() {
    let temp_dir = TempDir::new().unwrap();
    let remote_path = temp_dir.path().join("remote.git");
    Repository::init_bare(&remote_path).expect("init bare remote");
    let remote_url = remote_path.to_str().expect("remote path str");

    // Seed the bare repo with an initial main branch commit
    let seed_path = temp_dir.path().join("seed");
    let service = GitService::new();
    service
        .initialize_repo_with_main_branch(&seed_path)
        .expect("init seed repo");
    let seed_repo = Repository::open(&seed_path).expect("open seed repo");
    configure_user(&seed_repo);
    seed_repo.remote("origin", remote_url).expect("add remote");
    push_ref(&seed_repo, "refs/heads/main", "refs/heads/main");
    Repository::open_bare(&remote_path)
        .expect("open bare remote")
        .set_head("refs/heads/main")
        .expect("set remote HEAD");

    // Local clone that will attempt the push later
    let local_path = temp_dir.path().join("local");
    let local_repo = Repository::clone(remote_url, &local_path).expect("clone local");
    configure_user(&local_repo);
    checkout_branch(&local_repo, "main");
    write_file(&local_path, "file.txt", "initial local\n");
    commit_all(&local_repo, "initial local commit");
    push_ref(&local_repo, "refs/heads/main", "refs/heads/main");

    // Separate clone simulates someone else pushing first
    let updater_path = temp_dir.path().join("updater");
    let updater_repo = Repository::clone(remote_url, &updater_path).expect("clone updater");
    configure_user(&updater_repo);
    checkout_branch(&updater_repo, "main");
    write_file(&updater_path, "file.txt", "upstream change\n");
    commit_all(&updater_repo, "upstream commit");
    push_ref(&updater_repo, "refs/heads/main", "refs/heads/main");

    // Local branch diverges but has not fetched the updater's commit
    write_file(&local_path, "file.txt", "local change\n");
    commit_all(&local_repo, "local commit");
    let remote = local_repo.find_remote("origin").expect("origin remote");
    let remote_url_string = remote.url().expect("origin url").to_string();

    let git_cli = GitCli::new();
    let result = git_cli.push_with_token(&local_path, &remote_url_string, "main", "dummy-token");
    match result {
        Err(GitCliError::PushRejected(msg)) => {
            let lower = msg.to_ascii_lowercase();
            assert!(
                lower.contains("failed to push some refs") || lower.contains("fetch first"),
                "unexpected stderr: {msg}"
            );
        }
        Err(other) => panic!("expected push rejected, got {other:?}"),
        Ok(_) => panic!("push unexpectedly succeeded"),
    }
}

#[test]
fn fetch_with_token_missing_ref_returns_error() {
    let temp_dir = TempDir::new().unwrap();
    let remote_path = temp_dir.path().join("remote.git");
    Repository::init_bare(&remote_path).expect("init bare remote");
    let remote_url = remote_path.to_str().expect("remote path str");

    let seed_path = temp_dir.path().join("seed");
    let service = GitService::new();
    service
        .initialize_repo_with_main_branch(&seed_path)
        .expect("init seed repo");
    let seed_repo = Repository::open(&seed_path).expect("open seed repo");
    configure_user(&seed_repo);
    seed_repo.remote("origin", remote_url).expect("add remote");
    push_ref(&seed_repo, "refs/heads/main", "refs/heads/main");
    Repository::open_bare(&remote_path)
        .expect("open bare remote")
        .set_head("refs/heads/main")
        .expect("set remote HEAD");

    let local_path = temp_dir.path().join("local");
    Repository::clone(remote_url, &local_path).expect("clone local");

    let git_cli = GitCli::new();
    let refspec = "+refs/heads/missing:refs/remotes/origin/missing";
    let result =
        git_cli.fetch_with_token_and_refspec(&local_path, remote_url, refspec, "dummy-token");
    match result {
        Err(GitCliError::CommandFailed(msg)) => {
            assert!(
                msg.to_ascii_lowercase()
                    .contains("couldn't find remote ref"),
                "unexpected stderr: {msg}"
            );
        }
        Err(other) => panic!("expected command failed, got {other:?}"),
        Ok(_) => panic!("fetch unexpectedly succeeded"),
    }
}

#[test]
fn push_and_fetch_roundtrip_updates_tracking_branch() {
    let temp_dir = TempDir::new().unwrap();
    let remote_path = temp_dir.path().join("remote.git");
    Repository::init_bare(&remote_path).expect("init bare remote");
    let remote_url = remote_path.to_str().expect("remote path str");

    let seed_path = temp_dir.path().join("seed");
    let service = GitService::new();
    service
        .initialize_repo_with_main_branch(&seed_path)
        .expect("init seed repo");
    let seed_repo = Repository::open(&seed_path).expect("open seed repo");
    configure_user(&seed_repo);
    seed_repo.remote("origin", remote_url).expect("add remote");
    push_ref(&seed_repo, "refs/heads/main", "refs/heads/main");
    Repository::open_bare(&remote_path)
        .expect("open bare remote")
        .set_head("refs/heads/main")
        .expect("set remote HEAD");

    let producer_path = temp_dir.path().join("producer");
    let producer_repo = Repository::clone(remote_url, &producer_path).expect("clone producer");
    configure_user(&producer_repo);
    checkout_branch(&producer_repo, "main");

    let consumer_path = temp_dir.path().join("consumer");
    let consumer_repo = Repository::clone(remote_url, &consumer_path).expect("clone consumer");
    configure_user(&consumer_repo);
    checkout_branch(&consumer_repo, "main");
    let old_oid = consumer_repo
        .find_reference("refs/remotes/origin/main")
        .expect("consumer tracking ref")
        .target()
        .expect("consumer tracking ref");

    write_file(&producer_path, "file.txt", "new work\n");
    commit_all(&producer_repo, "producer commit");

    let remote = producer_repo.find_remote("origin").expect("origin remote");
    let remote_url_string = remote.url().expect("origin url").to_string();

    let git_cli = GitCli::new();
    git_cli
        .push_with_token(&producer_path, &remote_url_string, "main", "dummy-token")
        .expect("push succeeded");

    let new_oid = producer_repo
        .head()
        .expect("producer head")
        .target()
        .expect("producer head oid");
    assert_ne!(old_oid, new_oid, "producer created new commit");

    git_cli
        .fetch_with_token_and_refspec(
            &consumer_path,
            &remote_url_string,
            "+refs/heads/main:refs/remotes/origin/main",
            "dummy-token",
        )
        .expect("fetch succeeded");

    let updated_oid = consumer_repo
        .find_reference("refs/remotes/origin/main")
        .expect("updated tracking ref")
        .target()
        .expect("updated tracking ref");
    assert_eq!(
        updated_oid, new_oid,
        "tracking branch advanced to remote head"
    );
}

#[test]
fn rebase_preserves_untracked_files() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);

    write_file(&worktree_path, "scratch/untracked.txt", "temporary note\n");

    let service = GitService::new();
    let res = service.rebase_branch(
        &repo_path,
        &worktree_path,
        Some("new-base"),
        "old-base",
        None,
    );
    assert!(res.is_ok(), "rebase should succeed: {res:?}");

    let scratch = worktree_path.join("scratch/untracked.txt");
    let content = fs::read_to_string(&scratch).expect("untracked file exists");
    assert_eq!(content, "temporary note\n");
}

#[test]
fn rebase_aborts_on_uncommitted_tracked_changes() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);

    write_file(&worktree_path, "feat.txt", "feat change (edited)\n");

    let service = GitService::new();
    let res = service.rebase_branch(
        &repo_path,
        &worktree_path,
        Some("new-base"),
        "old-base",
        None,
    );
    assert!(res.is_err(), "rebase should fail on dirty worktree");

    let edited = fs::read_to_string(worktree_path.join("feat.txt")).unwrap();
    assert_eq!(edited, "feat change (edited)\n");
}

#[test]
fn rebase_aborts_if_untracked_would_be_overwritten_by_base() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);

    write_file(&worktree_path, "base.txt", "my scratch note\n");

    let service = GitService::new();
    let res = service.rebase_branch(
        &repo_path,
        &worktree_path,
        Some("new-base"),
        "old-base",
        None,
    );
    assert!(
        res.is_err(),
        "rebase should fail due to untracked overwrite risk"
    );

    let content = std::fs::read_to_string(worktree_path.join("base.txt")).unwrap();
    assert_eq!(content, "my scratch note\n");
}

#[test]
fn merge_does_not_overwrite_main_repo_untracked_files() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);

    write_file(&worktree_path, "danger.txt", "tracked from feature\n");
    let wt_repo = Repository::open(&worktree_path).unwrap();
    commit_all(&wt_repo, "add danger.txt in feature");

    write_file(&repo_path, "danger.txt", "my untracked data\n");
    let main_repo = Repository::open(&repo_path).unwrap();
    checkout_branch(&main_repo, "main");

    let service = GitService::new();
    let res = service.merge_changes(
        &repo_path,
        &worktree_path,
        "feature",
        "main",
        "squash merge",
    );
    assert!(
        res.is_err(),
        "merge should refuse due to untracked conflict"
    );

    // Untracked file remains untouched
    let content = std::fs::read_to_string(repo_path.join("danger.txt")).unwrap();
    assert_eq!(content, "my untracked data\n");
}

#[test]
fn merge_does_not_touch_tracked_uncommitted_changes_in_base_worktree() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);

    // Prepare: modify a tracked file in the base worktree (main) without committing
    let _main_repo = Repository::open(&repo_path).unwrap();
    // Base branch commits will be advanced by the merge operation; record before via service
    let g = GitService::new();
    let before_oid = g.get_branch_oid(&repo_path, "main").unwrap();

    // Create a tracked file that will also be added by feature branch to simulate overlap
    write_file(&repo_path, "danger2.txt", "my staged change\n");
    {
        // stage and then unstage to leave WT_MODIFIED? Simpler: just modify an existing tracked file
        // Use common.txt which is tracked
        write_file(&repo_path, "common.txt", "edited locally\n");
    }

    // Feature adds a change and is committed in worktree
    write_file(&worktree_path, "danger2.txt", "feature tracked\n");
    let wt_repo = Repository::open(&worktree_path).unwrap();
    commit_all(&wt_repo, "feature adds danger2.txt");

    // Merge via service (squash into main) should not modify files in the main worktree
    let service = GitService::new();
    let res = service.merge_changes(
        &repo_path,
        &worktree_path,
        "feature",
        "main",
        "squash merge",
    );
    assert!(
        res.is_ok(),
        "merge should succeed without touching worktree"
    );

    // Confirm the local edit to tracked file remains
    let content = std::fs::read_to_string(repo_path.join("common.txt")).unwrap();
    assert_eq!(content, "edited locally\n");

    // Confirm the main branch ref advanced
    let after_oid = g.get_branch_oid(&repo_path, "main").unwrap();
    assert_ne!(before_oid, after_oid, "main ref should be updated by merge");
}

#[test]
fn merge_refuses_with_staged_changes_on_base() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);
    let s = GitService::new();
    // ensure main is checked out
    s.checkout_branch(&repo_path, "main").unwrap();
    // feature adds change and commits
    write_file(&worktree_path, "m.txt", "feature\n");
    let wt_repo = Repository::open(&worktree_path).unwrap();
    commit_all(&wt_repo, "feat change");
    // main has staged change
    write_file(&repo_path, "staged.txt", "staged\n");
    s.add_path(&repo_path, "staged.txt").unwrap();
    let res = s.merge_changes(&repo_path, &worktree_path, "feature", "main", "squash");
    assert!(res.is_err(), "should refuse merge due to staged changes");
    // staged file remains
    let content = std::fs::read_to_string(repo_path.join("staged.txt")).unwrap();
    assert_eq!(content, "staged\n");
}

#[test]
fn merge_preserves_unstaged_changes_on_base() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);
    let s = GitService::new();
    s.checkout_branch(&repo_path, "main").unwrap();
    // modify unstaged
    write_file(&repo_path, "common.txt", "local edited\n");
    // feature modifies a different file
    write_file(&worktree_path, "merged.txt", "merged content\n");
    let wt_repo = Repository::open(&worktree_path).unwrap();
    commit_all(&wt_repo, "feature merged");

    let _sha = s
        .merge_changes(&repo_path, &worktree_path, "feature", "main", "squash")
        .unwrap();
    // local edit preserved
    let loc = std::fs::read_to_string(repo_path.join("common.txt")).unwrap();
    assert_eq!(loc, "local edited\n");
    // merged file updated
    let m = std::fs::read_to_string(repo_path.join("merged.txt")).unwrap();
    assert_eq!(m, "merged content\n");
}

#[test]
fn update_ref_does_not_destroy_feature_worktree_dirty_state() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);
    let s = GitService::new();
    // ensure main is checked out
    s.checkout_branch(&repo_path, "main").unwrap();
    // feature makes an initial change and commits
    write_file(&worktree_path, "f.txt", "feat\n");
    let wt_repo = Repository::open(&worktree_path).unwrap();
    commit_all(&wt_repo, "feat commit");
    // dirty change in feature worktree (uncommitted)
    write_file(&worktree_path, "dirty.txt", "unstaged\n");
    // merge from feature into main (CLI path updates task ref via update-ref)
    let sha = s
        .merge_changes(&repo_path, &worktree_path, "feature", "main", "squash")
        .unwrap();
    // uncommitted change in feature worktree preserved
    let dirty = std::fs::read_to_string(worktree_path.join("dirty.txt")).unwrap();
    assert_eq!(dirty, "unstaged\n");
    // feature branch ref updated to the squash commit in main repo
    let feature_oid = s.get_branch_oid(&repo_path, "feature").unwrap();
    assert_eq!(feature_oid, sha);
    // and the feature worktree HEAD now points to that commit
    let head = s.get_head_info(&worktree_path).unwrap();
    assert_eq!(head.branch, "feature");
    assert_eq!(head.oid, sha);
}

#[test]
fn libgit2_merge_updates_base_ref_in_both_repos() {
    // Ensure we hit the libgit2 path by NOT checking out the base branch in main repo
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);
    let s = GitService::new();

    // Record current main OID from both main repo and worktree repo; they should match pre-merge
    let before_main_repo = s.get_branch_oid(&repo_path, "main").unwrap();
    let before_main_wt = s.get_branch_oid(&worktree_path, "main").unwrap();
    assert_eq!(before_main_repo, before_main_wt);

    // Perform merge (squash) while main repo is NOT on base branch (libgit2 path)
    let sha = s
        .merge_changes(&repo_path, &worktree_path, "feature", "main", "squash")
        .expect("merge should succeed via libgit2 path");

    // Base branch ref advanced in both main and worktree repositories
    let after_main_repo = s.get_branch_oid(&repo_path, "main").unwrap();
    let after_main_wt = s.get_branch_oid(&worktree_path, "main").unwrap();
    assert_eq!(after_main_repo, sha);
    assert_eq!(after_main_wt, sha);
}

#[test]
fn libgit2_merge_updates_task_ref_and_feature_head_preserves_dirty() {
    // Hit libgit2 path (main repo not on base) and verify task ref + HEAD update safely
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);
    let s = GitService::new();

    // Make an uncommitted change in the feature worktree to ensure it's preserved
    write_file(&worktree_path, "dirty2.txt", "keep me\n");

    // Perform merge (squash) from feature into main; this path uses libgit2
    let sha = s
        .merge_changes(&repo_path, &worktree_path, "feature", "main", "squash")
        .expect("merge should succeed via libgit2 path");

    // Dirty file preserved in worktree
    let dirty = std::fs::read_to_string(worktree_path.join("dirty2.txt")).unwrap();
    assert_eq!(dirty, "keep me\n");

    // Task branch (feature) updated to squash commit in both repos
    let feat_main_repo = s.get_branch_oid(&repo_path, "feature").unwrap();
    let feat_worktree = s.get_branch_oid(&worktree_path, "feature").unwrap();
    assert_eq!(feat_main_repo, sha);
    assert_eq!(feat_worktree, sha);

    // Feature worktree HEAD points to the new squash commit
    let head = s.get_head_info(&worktree_path).unwrap();
    assert_eq!(head.branch, "feature");
    assert_eq!(head.oid, sha);
}

#[test]
fn rebase_refuses_to_abort_existing_rebase() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_conflict_repo_with_worktree(&td);

    // Start a rebase via GitService that will pause/conflict
    let svc = GitService::new();
    let _ = svc
        .rebase_branch(
            &repo_path,
            &worktree_path,
            Some("new-base"),
            "old-base",
            None,
        )
        .expect_err("first rebase should error and leave in-progress state");

    // Our service should refuse to proceed and not abort the user's rebase
    let service = GitService::new();
    let res = service.rebase_branch(
        &repo_path,
        &worktree_path,
        Some("new-base"),
        "old-base",
        None,
    );
    assert!(res.is_err(), "should error because rebase is in progress");
    // Note: We do not auto-abort; user should resolve or abort explicitly
}

#[test]
fn rebase_fast_forwards_when_no_unique_commits() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_no_unique_feature_repo(&td);
    let g = GitService::new();
    let before = g.get_head_info(&worktree_path).unwrap().oid;
    let new_base_oid = g.get_branch_oid(&repo_path, "new-base").unwrap();

    let _res = g
        .rebase_branch(
            &repo_path,
            &worktree_path,
            Some("new-base"),
            "old-base",
            None,
        )
        .expect("rebase should succeed");
    let after_oid = g.get_head_info(&worktree_path).unwrap().oid;
    assert_ne!(before, after_oid, "HEAD should move after rebase");
    assert_eq!(after_oid, new_base_oid, "fast-forward onto new-base");
}

#[test]
fn rebase_applies_multiple_commits_onto_ahead_base() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);
    let repo = Repository::open(&repo_path).unwrap();
    // Advance new-base further
    checkout_branch(&repo, "new-base");
    write_file(&repo_path, "base_more.txt", "nb more\n");
    commit_all(&repo, "advance new-base more");

    // Add another commit to feature
    write_file(&worktree_path, "feat2.txt", "second change\n");
    let wt_repo = Repository::open(&worktree_path).unwrap();
    commit_all(&wt_repo, "feature second commit");

    // Rebase feature onto new-base
    let service = GitService::new();
    let _ = service
        .rebase_branch(
            &repo_path,
            &worktree_path,
            Some("new-base"),
            "old-base",
            None,
        )
        .expect("rebase should succeed");

    // Verify both files exist with expected content in the rebased worktree
    let feat = std::fs::read_to_string(worktree_path.join("feat.txt")).unwrap();
    let feat2 = std::fs::read_to_string(worktree_path.join("feat2.txt")).unwrap();
    assert_eq!(feat, "feat change\n");
    assert_eq!(feat2, "second change\n");
}

#[test]
fn merge_when_base_ahead_and_feature_ahead_fails() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);
    let repo = Repository::open(&repo_path).unwrap();
    // Advance base (main) after feature was created
    checkout_branch(&repo, "main");
    write_file(&repo_path, "base_ahead.txt", "base ahead\n");
    commit_all(&repo, "base ahead commit");

    // Feature adds its own file (already has feat.txt from setup) and commit another
    write_file(&worktree_path, "another.txt", "feature ahead\n");
    let wt_repo = Repository::open(&worktree_path).unwrap();
    commit_all(&wt_repo, "feature ahead extra");

    let g = GitService::new();
    let before_main = g.get_branch_oid(&repo_path, "main").unwrap();

    // Attempt to merge (squash) into main - should fail because base is ahead
    let service = GitService::new();
    let res = service.merge_changes(
        &repo_path,
        &worktree_path,
        "feature",
        "main",
        "squash merge",
    );

    assert!(
        res.is_err(),
        "merge should fail when base branch is ahead of task branch"
    );

    // Verify main branch was not modified
    let after_main = g.get_branch_oid(&repo_path, "main").unwrap();
    assert_eq!(
        before_main, after_main,
        "main ref should remain unchanged when merge fails"
    );
}

#[test]
fn merge_conflict_does_not_move_base_ref() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_direct_conflict_repo(&td);

    // Record main ref before
    let _repo = Repository::open(&repo_path).unwrap();
    let g = GitService::new();
    let before = g.get_branch_oid(&repo_path, "main").unwrap();

    let service = GitService::new();
    let res = service.merge_changes(
        &repo_path,
        &worktree_path,
        "feature",
        "main",
        "squash merge",
    );

    assert!(res.is_err(), "conflicting merge should fail");

    let after = g.get_branch_oid(&repo_path, "main").unwrap();
    assert_eq!(before, after, "main ref must remain unchanged on conflict");
}

#[test]
fn merge_delete_vs_modify_conflict_behaves_safely() {
    // main modifies file, feature deletes it -> but now blocked by branch ahead check
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);
    let repo = Repository::open(&repo_path).unwrap();

    // start from main with a file
    checkout_branch(&repo, "main");
    write_file(&repo_path, "conflict_dm.txt", "base\n");
    commit_all(&repo, "add conflict file");

    // feature deletes it and commits
    let wt_repo = Repository::open(&worktree_path).unwrap();
    let path = worktree_path.join("conflict_dm.txt");
    if path.exists() {
        std::fs::remove_file(&path).unwrap();
    }
    commit_all(&wt_repo, "delete in feature");

    // main modifies same file (this puts main ahead of feature)
    write_file(&repo_path, "conflict_dm.txt", "main modify\n");
    commit_all(&repo, "modify in main");

    // Capture main state AFTER all setup commits
    let g = GitService::new();
    let before = g.get_branch_oid(&repo_path, "main").unwrap();

    let service = GitService::new();
    let res = service.merge_changes(
        &repo_path,
        &worktree_path,
        "feature",
        "main",
        "squash merge",
    );

    // Should now fail due to base branch being ahead, not due to merge conflicts
    assert!(res.is_err(), "merge should fail when base branch is ahead");

    // Ensure base ref unchanged on failure
    let after = g.get_branch_oid(&repo_path, "main").unwrap();
    assert_eq!(before, after, "main ref must remain unchanged on failure");
}

#[test]
fn rebase_preserves_rename_changes() {
    // feature renames a file; rebase onto new-base preserves rename
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);

    // feature: rename feat.txt -> feat_renamed.txt
    std::fs::rename(
        worktree_path.join("feat.txt"),
        worktree_path.join("feat_renamed.txt"),
    )
    .unwrap();
    let wt_repo = Repository::open(&worktree_path).unwrap();
    commit_all(&wt_repo, "rename feat");

    // rebase onto new-base
    let service = GitService::new();
    let _ = service
        .rebase_branch(
            &repo_path,
            &worktree_path,
            Some("new-base"),
            "old-base",
            None,
        )
        .expect("rebase should succeed");

    // after rebase, renamed file present; original absent
    assert!(worktree_path.join("feat_renamed.txt").exists());
    assert!(!worktree_path.join("feat.txt").exists());
}

#[test]
fn merge_refreshes_main_worktree_when_on_base() {
    let td = TempDir::new().unwrap();
    // Initialize repo and ensure main is checked out
    let repo_path = td.path().join("repo_refresh");
    let s = GitService::new();
    s.initialize_repo_with_main_branch(&repo_path).unwrap();
    s.configure_user(&repo_path, "Test User", "test@example.com")
        .unwrap();
    s.checkout_branch(&repo_path, "main").unwrap();
    // Baseline file
    write_file(&repo_path, "file.txt", "base\n");
    let _ = s.commit(&repo_path, "add base").unwrap();

    // Create feature branch and worktree
    s.create_branch(&repo_path, "feature").unwrap();
    let wt = td.path().join("wt_refresh");
    s.add_worktree(&repo_path, &wt, "feature", false).unwrap();
    // Modify file in worktree and commit
    write_file(&wt, "file.txt", "feature change\n");
    let _ = s.commit(&wt, "feature change").unwrap();

    // Merge into main (squash) and ensure main worktree is updated since it is on base
    let merge_sha = s
        .merge_changes(&repo_path, &wt, "feature", "main", "squash")
        .unwrap();
    // Since main is on base branch and we use safe CLI merge, both working tree
    // and ref should reflect the merged content.
    let content = std::fs::read_to_string(repo_path.join("file.txt")).unwrap();
    assert_eq!(content, "feature change\n");
    let oid = s.get_branch_oid(&repo_path, "main").unwrap();
    assert_eq!(oid, merge_sha);
}

#[test]
fn sparse_checkout_respected_in_worktree_diffs_and_commit() {
    let td = TempDir::new().unwrap();
    let repo_path = td.path().join("repo_sparse");
    let s = GitService::new();
    s.initialize_repo_with_main_branch(&repo_path).unwrap();
    s.configure_user(&repo_path, "Test User", "test@example.com")
        .unwrap();
    s.checkout_branch(&repo_path, "main").unwrap();
    // baseline content
    write_file(&repo_path, "included/a.txt", "A\n");
    write_file(&repo_path, "excluded/b.txt", "B\n");
    let _ = s.commit(&repo_path, "baseline").unwrap();

    // enable sparse-checkout for 'included' only
    let cli = GitCli::new();
    cli.git(&repo_path, ["sparse-checkout", "init", "--cone"])
        .unwrap();
    cli.git(&repo_path, ["sparse-checkout", "set", "included"])
        .unwrap();

    // create feature branch and worktree
    s.create_branch(&repo_path, "feature").unwrap();
    let wt = td.path().join("wt_sparse");
    s.add_worktree(&repo_path, &wt, "feature", false).unwrap();

    // materialization check: included exists, excluded does not
    assert!(wt.join("included/a.txt").exists());
    assert!(!wt.join("excluded/b.txt").exists());

    // modify included file
    write_file(&wt, "included/a.txt", "A-mod\n");
    // get worktree diffs vs main, ensure excluded/b.txt is NOT reported deleted
    let diffs = s
        .get_diffs(
            DiffTarget::Worktree {
                worktree_path: Path::new(&wt),
                branch_name: "feature",
                base_branch: "main",
            },
            None,
        )
        .unwrap();
    assert!(
        diffs
            .iter()
            .any(|d| d.new_path.as_deref() == Some("included/a.txt"))
    );
    assert!(
        !diffs
            .iter()
            .any(|d| d.old_path.as_deref() == Some("excluded/b.txt")
                || d.new_path.as_deref() == Some("excluded/b.txt"))
    );

    // commit and verify commit diffs also only include included/ changes
    let _ = s.commit(&wt, "modify included").unwrap();
    let head_sha = s.get_head_info(&wt).unwrap().oid;
    let commit_diffs = s
        .get_diffs(
            DiffTarget::Commit {
                repo_path: Path::new(&wt),
                commit_sha: &head_sha,
            },
            None,
        )
        .unwrap();
    assert!(
        commit_diffs
            .iter()
            .any(|d| d.new_path.as_deref() == Some("included/a.txt"))
    );
    assert!(
        commit_diffs
            .iter()
            .all(|d| d.new_path.as_deref() != Some("excluded/b.txt")
                && d.old_path.as_deref() != Some("excluded/b.txt"))
    );
}

// Helper: initialize a repo with main, configure user via service
fn init_repo_only_service(root: &TempDir) -> PathBuf {
    let repo_path = root.path().join("repo_svc");
    let s = GitService::new();
    s.initialize_repo_with_main_branch(&repo_path).unwrap();
    s.configure_user(&repo_path, "Test User", "test@example.com")
        .unwrap();
    s.checkout_branch(&repo_path, "main").unwrap();
    repo_path
}

#[test]
fn merge_binary_conflict_does_not_move_ref() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_only_service(&td);
    let s = GitService::new();
    // seed
    let _ = s.commit(&repo_path, "seed").unwrap();
    // create feature branch and worktree
    s.create_branch(&repo_path, "feature").unwrap();
    let worktree_path = td.path().join("wt_bin");
    s.add_worktree(&repo_path, &worktree_path, "feature", false)
        .unwrap();

    // feature adds/commits binary file
    let mut f = fs::File::create(worktree_path.join("bin.dat")).unwrap();
    f.write_all(&[0, 1, 2, 3]).unwrap();
    let _ = s.commit(&worktree_path, "feature bin").unwrap();

    // main adds conflicting binary content
    let mut f2 = fs::File::create(repo_path.join("bin.dat")).unwrap();
    f2.write_all(&[9, 8, 7, 6]).unwrap();
    let _ = s.commit(&repo_path, "main bin").unwrap();

    let before = s.get_branch_oid(&repo_path, "main").unwrap();
    let res = s.merge_changes(&repo_path, &worktree_path, "feature", "main", "merge bin");
    assert!(res.is_err(), "binary conflict should fail");
    let after = s.get_branch_oid(&repo_path, "main").unwrap();
    assert_eq!(before, after, "main ref unchanged on conflict");
}

#[test]
fn merge_rename_vs_modify_conflict_does_not_move_ref() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_only_service(&td);
    let s = GitService::new();
    // base file
    fs::write(repo_path.join("conflict.txt"), b"base\n").unwrap();
    let _ = s.commit(&repo_path, "base").unwrap();
    s.create_branch(&repo_path, "feature").unwrap();
    let worktree_path = td.path().join("wt_ren");
    s.add_worktree(&repo_path, &worktree_path, "feature", false)
        .unwrap();

    // feature renames file
    std::fs::rename(
        worktree_path.join("conflict.txt"),
        worktree_path.join("conflict_renamed.txt"),
    )
    .unwrap();
    let _ = s.commit(&worktree_path, "rename").unwrap();

    // main modifies original path
    fs::write(repo_path.join("conflict.txt"), b"main change\n").unwrap();
    let _ = s.commit(&repo_path, "modify main").unwrap();

    let before = s.get_branch_oid(&repo_path, "main").unwrap();
    let res = s.merge_changes(
        &repo_path,
        &worktree_path,
        "feature",
        "main",
        "merge rename",
    );
    match res {
        Err(_) => {
            let after = s.get_branch_oid(&repo_path, "main").unwrap();
            assert_eq!(before, after, "main unchanged on conflict");
        }
        Ok(sha) => {
            // ensure main advanced and result contains either renamed or modified content
            let after = s.get_branch_oid(&repo_path, "main").unwrap();
            assert_eq!(after, sha);
            let diffs = s
                .get_diffs(
                    DiffTarget::Commit {
                        repo_path: Path::new(&repo_path),
                        commit_sha: &after,
                    },
                    None,
                )
                .unwrap();
            let has_renamed = diffs
                .iter()
                .any(|d| d.new_path.as_deref() == Some("conflict_renamed.txt"));
            let has_modified = diffs.iter().any(|d| {
                d.new_path.as_deref() == Some("conflict.txt")
                    && d.new_content.as_deref() == Some("main change\n")
            });
            assert!(has_renamed || has_modified);
        }
    }
}

#[test]
fn merge_leaves_no_staged_changes_on_target_branch() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);

    // Ensure main repo is on the base branch (triggers CLI merge path)
    let s = GitService::new();
    s.checkout_branch(&repo_path, "main").unwrap();

    // Feature branch makes some changes
    write_file(&worktree_path, "feature_file.txt", "feature content\n");
    write_file(&worktree_path, "common.txt", "modified by feature\n");
    let wt_repo = Repository::open(&worktree_path).unwrap();
    commit_all(&wt_repo, "feature changes");

    // Perform the merge
    let _merge_sha = s
        .merge_changes(
            &repo_path,
            &worktree_path,
            "feature",
            "main",
            "merge feature",
        )
        .expect("merge should succeed");

    // THE KEY CHECK: Verify no staged changes remain on target branch
    let git_cli = GitCli::new();
    let has_staged = git_cli
        .has_staged_changes(&repo_path)
        .expect("should be able to check staged changes");

    assert!(
        !has_staged,
        "Target branch should have no staged changes after merge"
    );

    // Debug info if test fails
    if has_staged {
        let status_output = git_cli.git(&repo_path, ["status", "--porcelain"]).unwrap();
        panic!("Found staged changes after merge:\n{status_output}");
    }
}

#[test]
fn worktree_to_worktree_merge_leaves_no_staged_changes() {
    let td = TempDir::new().unwrap();
    let repo_path = td.path().join("repo");
    let worktree_a_path = td.path().join("wt-feature-a");
    let worktree_b_path = td.path().join("wt-feature-b");

    // Setup: Initialize repo with main branch
    let service = GitService::new();
    service
        .initialize_repo_with_main_branch(&repo_path)
        .expect("init repo");
    let repo = Repository::open(&repo_path).unwrap();
    configure_user(&repo);
    checkout_branch(&repo, "main");

    write_file(&repo_path, "base.txt", "base content\n");
    commit_all(&repo, "initial commit");

    // Create two feature branches
    create_branch_from_head(&repo, "feature-a");
    create_branch_from_head(&repo, "feature-b");

    // Create worktrees for both feature branches
    service
        .add_worktree(&repo_path, &worktree_a_path, "feature-a", false)
        .expect("create worktree A");
    service
        .add_worktree(&repo_path, &worktree_b_path, "feature-b", false)
        .expect("create worktree B");

    // Make changes in worktree A
    write_file(
        &worktree_a_path,
        "feature_a.txt",
        "content from feature A\n",
    );
    write_file(&worktree_a_path, "base.txt", "modified by feature A\n");
    let wt_a_repo = Repository::open(&worktree_a_path).unwrap();
    commit_all(&wt_a_repo, "feature A changes");

    // Ensure main repo is on different branch (neither feature-a nor feature-b)
    checkout_branch(&repo, "main");

    let _sha = service.merge_changes(
        &repo_path,
        &worktree_a_path,
        "feature-a",
        "feature-b",
        "merge feature-a into feature-b",
    );

    // Verify no staged changes were introduced
    let git_cli = GitCli::new();
    let has_staged_main = git_cli
        .has_staged_changes(&repo_path)
        .expect("should be able to check staged changes in main repo");
    let has_staged_target = git_cli
        .has_staged_changes(&worktree_b_path)
        .expect("should be able to check staged changes in target worktree");

    assert!(
        !has_staged_main,
        "Main repo should have no staged changes after failed merge"
    );
    assert!(
        !has_staged_target,
        "Target worktree should have no staged changes after failed merge"
    );
}

#[test]
fn merge_into_orphaned_branch_uses_libgit2_fallback() {
    let td = TempDir::new().unwrap();
    let (repo_path, worktree_path) = setup_repo_with_worktree(&td);

    // Create an "orphaned" target branch that exists as ref but isn't checked out anywhere
    let service = GitService::new();
    let repo = Repository::open(&repo_path).unwrap();

    // Create orphaned-feature branch from current main HEAD but don't check it out
    let main_commit = repo.head().unwrap().peel_to_commit().unwrap();
    repo.branch("orphaned-feature", &main_commit, false)
        .unwrap();

    // Ensure main repo is on different branch and no worktree has orphaned-feature
    service.checkout_branch(&repo_path, "main").unwrap();

    // Make changes in source worktree
    write_file(
        &worktree_path,
        "feature_content.txt",
        "content from feature\n",
    );
    let wt_repo = Repository::open(&worktree_path).unwrap();
    commit_all(&wt_repo, "feature changes");

    // orphaned-feature is not checked out anywhere, so should trigger libgit2 path

    // Perform merge into orphaned branch (should use libgit2 fallback)
    let merge_sha = service
        .merge_changes(
            &repo_path,
            &worktree_path,
            "feature",
            "orphaned-feature",
            "merge into orphaned branch",
        )
        .expect("libgit2 merge into orphaned branch should succeed");

    // Verify merge worked - orphaned-feature branch should now point to merge commit
    let orphaned_branch_oid = service
        .get_branch_oid(&repo_path, "orphaned-feature")
        .unwrap();
    assert_eq!(
        orphaned_branch_oid, merge_sha,
        "orphaned-feature branch should point to merge commit"
    );

    // Verify no working tree was affected (since branch wasn't checked out anywhere)
    let main_git_cli = GitCli::new();
    let main_has_staged = main_git_cli.has_staged_changes(&repo_path).unwrap();
    let worktree_has_staged = main_git_cli.has_staged_changes(&worktree_path).unwrap();

    assert!(
        !main_has_staged,
        "Main repo should remain clean after libgit2 merge"
    );
    assert!(
        !worktree_has_staged,
        "Source worktree should remain clean after libgit2 merge"
    );
}

#[test]
fn merge_base_ahead_of_task_should_error() {
    let td = TempDir::new().unwrap();
    let repo_path = td.path().join("repo");
    let worktree_path = td.path().join("wt-feature");

    // Setup: Initialize repo with main branch
    let service = GitService::new();
    service
        .initialize_repo_with_main_branch(&repo_path)
        .expect("init repo");
    let repo = Repository::open(&repo_path).unwrap();
    configure_user(&repo);
    checkout_branch(&repo, "main");

    // Initial commit on main
    write_file(&repo_path, "base.txt", "initial content\n");
    commit_all(&repo, "initial commit");

    // Create feature branch from this point
    create_branch_from_head(&repo, "feature");
    service
        .add_worktree(&repo_path, &worktree_path, "feature", false)
        .expect("create worktree");

    // Feature makes a change and commits
    write_file(&worktree_path, "feature.txt", "feature content\n");
    let wt_repo = Repository::open(&worktree_path).unwrap();
    commit_all(&wt_repo, "feature change");

    // Main branch advances ahead of feature (this is the key scenario)
    checkout_branch(&repo, "main");
    write_file(&repo_path, "main_advance.txt", "main advanced\n");
    commit_all(&repo, "main advances ahead");
    write_file(&repo_path, "main_advance2.txt", "main advanced more\n");
    commit_all(&repo, "main advances further");

    // Attempt to merge feature into main when main is ahead
    // This should error because base branch has moved ahead of task branch
    let res = service.merge_changes(
        &repo_path,
        &worktree_path,
        "feature",
        "main",
        "attempt merge when base ahead",
    );

    // TDD: This test will initially fail because merge currently succeeds
    // Later we'll fix the merge logic to detect this scenario and error
    assert!(
        res.is_err(),
        "Merge should error when base branch is ahead of task branch"
    );
}
</file>

<file path="crates/services/tests/git_remote_ops.rs">
use std::{
    net::{TcpStream, ToSocketAddrs},
    path::{Path, PathBuf},
    time::Duration,
};

use git2::Repository;
use services::services::{
    git::GitService,
    git_cli::{GitCli, GitCliError},
};

fn workspace_root() -> PathBuf {
    // CARGO_MANIFEST_DIR for this crate is <workspace>/crates/services
    let manifest_dir = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
    manifest_dir
        .parent()
        .and_then(Path::parent)
        .expect("workspace root")
        .to_path_buf()
}

fn repo_https_remote(repo_path: &Path) -> Option<String> {
    let repo = Repository::open(repo_path).ok()?;
    let remote = repo.find_remote("origin").ok()?;
    let url = remote.url()?;
    Some(GitService::new().convert_to_https_url(url))
}

fn assert_auth_failed(result: Result<(), GitCliError>) {
    match result {
        Err(GitCliError::AuthFailed(_)) => {}
        Err(other) => panic!("expected auth failure, got {other:?}"),
        Ok(_) => panic!("operation unexpectedly succeeded"),
    }
}

fn can_reach_github() -> bool {
    let addr = match ("github.com", 443).to_socket_addrs() {
        Ok(mut addrs) => addrs.next(),
        Err(_) => return false,
    };
    if let Some(addr) = addr {
        TcpStream::connect_timeout(&addr, Duration::from_secs(2)).is_ok()
    } else {
        false
    }
}

#[ignore]
#[test]
fn fetch_with_invalid_token_returns_auth_error() {
    let repo_path = workspace_root();
    let Some(remote_url) = repo_https_remote(&repo_path) else {
        eprintln!("Skipping fetch test: origin remote not configured");
        return;
    };

    if !can_reach_github() {
        eprintln!("Skipping fetch test: cannot reach github.com");
        return;
    }

    let cli = GitCli::new();
    let refspec = "+refs/heads/main:refs/remotes/origin/main";
    let result =
        cli.fetch_with_token_and_refspec(&repo_path, &remote_url, refspec, "invalid-token");
    assert_auth_failed(result);
}

#[ignore]
#[test]
fn push_with_invalid_token_returns_auth_error() {
    let repo_path = workspace_root();
    let Some(remote_url) = repo_https_remote(&repo_path) else {
        eprintln!("Skipping push test: origin remote not configured");
        return;
    };

    if !can_reach_github() {
        eprintln!("Skipping push test: cannot reach github.com");
        return;
    }

    let cli = GitCli::new();
    let result = cli.push_with_token(&repo_path, &remote_url, "main", "invalid-token");
    assert_auth_failed(result);
}
</file>

<file path="crates/services/tests/git_workflow.rs">
use std::{
    fs,
    io::Write,
    path::{Path, PathBuf},
};

use services::services::{
    git::{DiffTarget, GitService},
    github_service::{GitHubRepoInfo, GitHubServiceError},
};
use tempfile::TempDir;
use utils::diff::DiffChangeKind;

fn write_file<P: AsRef<Path>>(base: P, rel: &str, content: &str) {
    let path = base.as_ref().join(rel);
    if let Some(parent) = path.parent() {
        fs::create_dir_all(parent).unwrap();
    }
    let mut f = fs::File::create(&path).unwrap();
    f.write_all(content.as_bytes()).unwrap();
}

fn init_repo_main(root: &TempDir) -> PathBuf {
    let path = root.path().join("repo");
    let s = GitService::new();
    s.initialize_repo_with_main_branch(&path).unwrap();
    s.configure_user(&path, "Test User", "test@example.com")
        .unwrap();
    s.checkout_branch(&path, "main").unwrap();
    path
}

#[test]
fn commit_empty_message_behaviour() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    write_file(&repo_path, "x.txt", "x\n");
    let s = GitService::new();
    let res = s.commit(&repo_path, "");
    // Some environments disallow empty commit messages by default.
    // Accept either success or a clear error.
    if let Err(e) = &res {
        let msg = format!("{e}");
        assert!(msg.contains("empty commit message") || msg.contains("git commit failed"));
    }
}

fn has_global_git_identity() -> bool {
    if let Ok(cfg) = git2::Config::open_default() {
        let has_name = cfg.get_string("user.name").is_ok();
        let has_email = cfg.get_string("user.email").is_ok();
        return has_name && has_email;
    }
    false
}

#[test]
fn initialize_repo_without_user_creates_initial_commit() {
    let td = TempDir::new().unwrap();
    let repo_path = td.path().join("repo_no_user_init");
    let s = GitService::new();
    // No configure_user call; rely on fallback signature for initial commit
    s.initialize_repo_with_main_branch(&repo_path).unwrap();
    let head = s.get_head_info(&repo_path).unwrap();
    assert_eq!(head.branch, "main");
    assert!(!head.oid.is_empty());
    // Verify author is set: either global identity (if configured) or fallback
    let (name, email) = s.get_head_author(&repo_path).unwrap();
    if has_global_git_identity() {
        assert!(name.is_some() && email.is_some());
    } else {
        assert_eq!(name.as_deref(), Some("Vibe Kanban"));
        assert_eq!(email.as_deref(), Some("noreply@vibekanban.com"));
    }
}

#[test]
fn commit_without_user_config_succeeds() {
    let td = TempDir::new().unwrap();
    let repo_path = td.path().join("repo_no_user");
    let s = GitService::new();
    s.initialize_repo_with_main_branch(&repo_path).unwrap();
    write_file(&repo_path, "f.txt", "x\n");
    // No configure_user call here
    let res = s.commit(&repo_path, "no user config");
    assert!(res.is_ok());
}

#[test]
fn commit_fails_when_index_locked() {
    use std::fs::File;
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    write_file(&repo_path, "y.txt", "y\n");
    // Simulate index lock
    let git_dir = repo_path.join(".git");
    let _lock = File::create(git_dir.join("index.lock")).unwrap();
    let s = GitService::new();
    let res = s.commit(&repo_path, "should fail");
    assert!(res.is_err());
}

#[test]
fn staged_but_uncommitted_changes_is_dirty() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    let s = GitService::new();
    // seed tracked file
    write_file(&repo_path, "t1.txt", "a\n");
    let _ = s.commit(&repo_path, "seed").unwrap();
    // modify and stage
    write_file(&repo_path, "t1.txt", "b\n");
    s.add_path(&repo_path, "t1.txt").unwrap();
    assert!(!s.is_worktree_clean(&repo_path).unwrap());
}

#[test]
fn delete_nonexistent_file_creates_noop_commit() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    // baseline commit first so we have HEAD
    write_file(&repo_path, "seed.txt", "s\n");
    let s = GitService::new();
    let _ = s.commit(&repo_path, "seed").unwrap();
    let before = s.get_head_info(&repo_path).unwrap().oid;
    let res = s.delete_file_and_commit(&repo_path, "nope.txt").unwrap();
    let after = s.get_head_info(&repo_path).unwrap().oid;
    assert_ne!(before, after);
    assert_eq!(after, res);
}

#[test]
fn delete_directory_path_errors() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    // create and commit a file so repo has history
    write_file(&repo_path, "dir/file.txt", "z\n");
    let s = GitService::new();
    let _ = s.commit(&repo_path, "add file").unwrap();
    // directory path should cause an error
    let s = GitService::new();
    let res = s.delete_file_and_commit(&repo_path, "dir");
    assert!(res.is_err());
}

#[test]
fn worktree_clean_detects_staged_deleted_and_renamed() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    write_file(&repo_path, "t1.txt", "1\n");
    write_file(&repo_path, "t2.txt", "2\n");
    let s = GitService::new();
    let _ = s.commit(&repo_path, "seed").unwrap();

    // delete tracked file
    std::fs::remove_file(repo_path.join("t2.txt")).unwrap();
    assert!(!s.is_worktree_clean(&repo_path).unwrap());

    // restore and test rename
    write_file(&repo_path, "t2.txt", "2\n");
    let _ = s.commit(&repo_path, "restore t2").unwrap();
    std::fs::rename(repo_path.join("t2.txt"), repo_path.join("t2-renamed.txt")).unwrap();
    assert!(!s.is_worktree_clean(&repo_path).unwrap());
}

#[test]
fn diff_added_binary_file_has_no_content() {
    // ensure binary file content is not loaded (null byte guard)
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    // base
    let s = GitService::new();
    let _ = s.commit(&repo_path, "base").unwrap();
    // branch with binary file
    s.create_branch(&repo_path, "feature").unwrap();
    s.checkout_branch(&repo_path, "feature").unwrap();
    // write binary with null byte
    let mut f = fs::File::create(repo_path.join("bin.dat")).unwrap();
    f.write_all(&[0u8, 1, 2, 3]).unwrap();
    let _ = s.commit(&repo_path, "add binary").unwrap();

    let s = GitService::new();
    let diffs = s
        .get_diffs(
            DiffTarget::Branch {
                repo_path: Path::new(&repo_path),
                branch_name: "feature",
                base_branch: "main",
            },
            None,
        )
        .unwrap();
    let bin = diffs
        .iter()
        .find(|d| d.new_path.as_deref() == Some("bin.dat"))
        .expect("binary diff present");
    assert!(bin.new_content.is_none());
}

#[test]
fn initialize_and_default_branch_and_head_info() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);

    // Default branch should be main
    let s = GitService::new();
    let def = s.get_default_branch_name(&repo_path).unwrap();
    assert_eq!(def, "main");

    // Head info branch should be main
    let head = s.get_head_info(&repo_path).unwrap();
    assert_eq!(head.branch, "main");

    // Repo has an initial commit (OID parsable)
    assert!(!head.oid.is_empty());
}

#[test]
fn commit_and_is_worktree_clean() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    write_file(&repo_path, "foo.txt", "hello\n");

    let s = GitService::new();
    let committed = s.commit(&repo_path, "add foo").unwrap();
    assert!(committed);
    assert!(s.is_worktree_clean(&repo_path).unwrap());

    // Verify commit contains file
    let diffs = s
        .get_diffs(
            DiffTarget::Commit {
                repo_path: Path::new(&repo_path),
                commit_sha: &s.get_head_info(&repo_path).unwrap().oid,
            },
            None,
        )
        .unwrap();
    assert!(
        diffs
            .iter()
            .any(|d| d.new_path.as_deref() == Some("foo.txt"))
    );
}

#[test]
fn commit_in_detached_head_succeeds_via_service() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    // initial parent
    write_file(&repo_path, "a.txt", "a\n");
    let s = GitService::new();
    let _ = s.commit(&repo_path, "add a").unwrap();
    // detach via service
    s.detach_head_current(&repo_path).unwrap();
    // commit while detached
    write_file(&repo_path, "b.txt", "b\n");
    let ok = s.commit(&repo_path, "detached commit").unwrap();
    assert!(ok);
}

#[test]
fn branch_status_ahead_and_behind() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    let s = GitService::new();

    // main: initial commit
    write_file(&repo_path, "base.txt", "base\n");
    let _ = s.commit(&repo_path, "base").unwrap();

    // create feature from main
    s.create_branch(&repo_path, "feature").unwrap();
    // advance feature by 1
    s.checkout_branch(&repo_path, "feature").unwrap();
    write_file(&repo_path, "feature.txt", "f1\n");
    let _ = s.commit(&repo_path, "f1").unwrap();

    // advance main by 1
    s.checkout_branch(&repo_path, "main").unwrap();
    write_file(&repo_path, "main.txt", "m1\n");
    let _ = s.commit(&repo_path, "m1").unwrap();

    let s = GitService::new();
    let (ahead, behind) = s.get_branch_status(&repo_path, "feature", "main").unwrap();
    assert_eq!((ahead, behind), (1, 1));

    // advance feature by one more (ahead 2, behind 1)
    s.checkout_branch(&repo_path, "feature").unwrap();
    write_file(&repo_path, "feature2.txt", "f2\n");
    let _ = s.commit(&repo_path, "f2").unwrap();
    let (ahead2, behind2) = s.get_branch_status(&repo_path, "feature", "main").unwrap();
    assert_eq!((ahead2, behind2), (2, 1));
}

#[test]
fn get_all_branches_lists_current_and_others() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    let s = GitService::new();
    s.create_branch(&repo_path, "feature").unwrap();

    let s = GitService::new();
    let branches = s.get_all_branches(&repo_path).unwrap();
    let names: Vec<_> = branches.iter().map(|b| b.name.as_str()).collect();
    assert!(names.contains(&"main"));
    assert!(names.contains(&"feature"));
    // current should be main
    let main_entry = branches.iter().find(|b| b.name == "main").unwrap();
    assert!(main_entry.is_current);
}

#[test]
fn delete_file_and_commit_creates_new_commit() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    write_file(&repo_path, "to_delete.txt", "bye\n");
    let s = GitService::new();
    let _ = s.commit(&repo_path, "add to_delete").unwrap();
    let before = s.get_head_info(&repo_path).unwrap().oid;

    let new_commit = s
        .delete_file_and_commit(&repo_path, "to_delete.txt")
        .unwrap();
    let after = s.get_head_info(&repo_path).unwrap().oid;
    assert_ne!(before, after);
    assert_eq!(after, new_commit);
    assert!(!repo_path.join("to_delete.txt").exists());
}

#[test]
fn get_github_repo_info_parses_origin() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    let s = GitService::new();
    s.set_remote(&repo_path, "origin", "https://github.com/foo/bar.git")
        .unwrap();
    let info = s.get_github_repo_info(&repo_path).unwrap();
    assert_eq!(info.owner, "foo");
    assert_eq!(info.repo_name, "bar");
}

#[test]
fn get_branch_diffs_between_branches() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    let s = GitService::new();
    // base commit on main
    write_file(&repo_path, "a.txt", "a\n");
    let _ = s.commit(&repo_path, "add a").unwrap();

    // create branch and add new file
    s.create_branch(&repo_path, "feature").unwrap();
    s.checkout_branch(&repo_path, "feature").unwrap();
    write_file(&repo_path, "b.txt", "b\n");
    let _ = s.commit(&repo_path, "add b").unwrap();

    let s = GitService::new();
    let diffs = s
        .get_diffs(
            DiffTarget::Branch {
                repo_path: Path::new(&repo_path),
                branch_name: "feature",
                base_branch: "main",
            },
            None,
        )
        .unwrap();
    assert!(diffs.iter().any(|d| d.new_path.as_deref() == Some("b.txt")));
}

#[test]
fn worktree_diff_respects_path_filter() {
    // Use git CLI status diff under the hood
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);

    // main baseline
    write_file(&repo_path, "src/keep.txt", "k\n");
    write_file(&repo_path, "other/skip.txt", "s\n");
    let s = GitService::new();
    let _ = s.commit(&repo_path, "baseline").unwrap();

    // create feature and work in place (worktree is repo_path)
    s.create_branch(&repo_path, "feature").unwrap();

    // modify files without committing
    write_file(&repo_path, "src/only.txt", "only\n");
    write_file(&repo_path, "other/skip2.txt", "skip\n");

    let s = GitService::new();
    let diffs = s
        .get_diffs(
            DiffTarget::Worktree {
                worktree_path: Path::new(&repo_path),
                branch_name: "feature",
                base_branch: "main",
            },
            Some(&["src"]),
        )
        .unwrap();
    assert!(
        diffs
            .iter()
            .any(|d| d.new_path.as_deref() == Some("src/only.txt"))
    );
    assert!(
        !diffs
            .iter()
            .any(|d| d.new_path.as_deref() == Some("other/skip2.txt"))
    );
}

#[test]
fn get_branch_oid_nonexistent_errors() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    let s = GitService::new();
    let res = s.get_branch_oid(&repo_path, "no-such-branch");
    assert!(res.is_err());
}

#[test]
fn create_unicode_branch_and_list() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    let s = GitService::new();
    // base commit
    write_file(&repo_path, "file.txt", "ok\n");
    let _ = s.commit(&repo_path, "base");
    // unicode/slash branch name (valid ref)
    let bname = "feature/ünicode";
    s.create_branch(&repo_path, bname).unwrap();
    let names: Vec<_> = s
        .get_all_branches(&repo_path)
        .unwrap()
        .into_iter()
        .map(|b| b.name)
        .collect();
    assert!(names.iter().any(|n| n == bname));
}

#[cfg(unix)]
#[test]
fn worktree_diff_permission_only_change() {
    use std::os::unix::fs::PermissionsExt;
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    let s = GitService::new();
    // baseline commit
    write_file(&repo_path, "p.sh", "echo hi\n");
    let _ = s.commit(&repo_path, "add p.sh").unwrap();
    // create a feature branch baseline at HEAD
    s.create_branch(&repo_path, "feature").unwrap();

    // change only the permission (chmod +x)
    let mut perms = std::fs::metadata(repo_path.join("p.sh"))
        .unwrap()
        .permissions();
    perms.set_mode(perms.mode() | 0o111);
    std::fs::set_permissions(repo_path.join("p.sh"), perms).unwrap();

    // Compute worktree diff vs main on feature
    let diffs = s
        .get_diffs(
            DiffTarget::Worktree {
                worktree_path: Path::new(&repo_path),
                branch_name: "feature",
                base_branch: "main",
            },
            None,
        )
        .unwrap();
    let d = diffs
        .into_iter()
        .find(|d| d.new_path.as_deref() == Some("p.sh"))
        .expect("p.sh diff present");
    assert!(matches!(d.change, DiffChangeKind::PermissionChange));
    assert_eq!(d.old_content, d.new_content);
}

#[test]
fn delete_with_uncommitted_changes_succeeds() {
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    let s = GitService::new();
    // baseline file and commit
    write_file(&repo_path, "d.txt", "v1\n");
    let _ = s.commit(&repo_path, "add d").unwrap();
    let before = s.get_head_info(&repo_path).unwrap().oid;
    // uncommitted change
    write_file(&repo_path, "d.txt", "v2\n");
    // delete and commit
    let new_sha = s.delete_file_and_commit(&repo_path, "d.txt").unwrap();
    assert_eq!(s.get_head_info(&repo_path).unwrap().oid, new_sha);
    assert!(!repo_path.join("d.txt").exists());
    assert_ne!(before, new_sha);
}

#[cfg(unix)]
#[test]
fn delete_symlink_and_commit() {
    use std::os::unix::fs::symlink;
    let td = TempDir::new().unwrap();
    let repo_path = init_repo_main(&td);
    let s = GitService::new();
    // Create target and symlink, commit
    write_file(&repo_path, "target.txt", "t\n");
    let _ = s.commit(&repo_path, "add target").unwrap();
    symlink(repo_path.join("target.txt"), repo_path.join("link.txt")).unwrap();
    let _ = s.commit(&repo_path, "add symlink").unwrap();
    let before = s.get_head_info(&repo_path).unwrap().oid;
    // Delete symlink
    let new_sha = s.delete_file_and_commit(&repo_path, "link.txt").unwrap();
    assert_eq!(s.get_head_info(&repo_path).unwrap().oid, new_sha);
    assert!(!repo_path.join("link.txt").exists());
    assert_ne!(before, new_sha);
}

#[test]
fn delete_file_commit_has_author_without_user() {
    // Verify libgit2 path uses fallback author when no config exists
    let td = TempDir::new().unwrap();
    let repo_path = td.path().join("repo_fallback_delete");
    let s = GitService::new();
    // No configure_user call; initial commit uses fallback signature too
    s.initialize_repo_with_main_branch(&repo_path).unwrap();

    // Create then delete an untracked file via service
    write_file(&repo_path, "q.txt", "temp\n");
    let sha = s.delete_file_and_commit(&repo_path, "q.txt").unwrap();

    // Author should be present: either global identity or fallback
    let (name, email) = s.get_commit_author(&repo_path, &sha).unwrap();
    if has_global_git_identity() {
        assert!(name.is_some() && email.is_some());
    } else {
        assert_eq!(name.as_deref(), Some("Vibe Kanban"));
        assert_eq!(email.as_deref(), Some("noreply@vibekanban.com"));
    }
}

#[test]
fn convert_to_https_url_handles_common_git_forms() {
    let svc = GitService::new();

    let ssh_url = "git@github.com:owner/repo.git";
    assert_eq!(
        svc.convert_to_https_url(ssh_url),
        "https://github.com/owner/repo.git"
    );

    let ssh_scheme_url = "ssh://git@github.com/owner/repo";
    assert_eq!(
        svc.convert_to_https_url(ssh_scheme_url),
        "https://github.com/owner/repo.git"
    );

    let https_without_suffix = "https://github.com/owner/repo";
    assert_eq!(
        svc.convert_to_https_url(https_without_suffix),
        "https://github.com/owner/repo.git"
    );

    let converted = svc.convert_to_https_url("https://github.com/owner/repo/");
    assert_eq!(converted, "https://github.com/owner/repo.git");
}

#[test]
fn github_repo_info_parses_https_and_ssh_urls() {
    let info = GitHubRepoInfo::from_remote_url("https://github.com/owner/repo.git").unwrap();
    assert_eq!(info.owner, "owner");
    assert_eq!(info.repo_name, "repo");

    let info = GitHubRepoInfo::from_remote_url("git@github.com:owner/repo.git").unwrap();
    assert_eq!(info.owner, "owner");
    assert_eq!(info.repo_name, "repo");

    let info = GitHubRepoInfo::from_remote_url("https://github.com/owner/repo/pull/123").unwrap();
    assert_eq!(info.owner, "owner");
    assert_eq!(info.repo_name, "repo");

    let err = GitHubRepoInfo::from_remote_url("https://example.com/not/github").unwrap_err();
    match err {
        GitHubServiceError::Repository(msg) => assert!(msg.contains("Invalid GitHub URL")),
        other => panic!("unexpected error variant: {other:?}"),
    }
}

#[test]
fn squash_merge_libgit2_sets_author_without_user() {
    // Verify merge_changes (libgit2 path) uses fallback author when no config exists
    use git2::Repository;

    let td = TempDir::new().unwrap();
    let repo_path = td.path().join("repo_fallback_merge");
    let worktree_path = td.path().join("wt_feature");
    let s = GitService::new();

    // Init repo without user config
    s.initialize_repo_with_main_branch(&repo_path).unwrap();

    // Create feature branch and worktree
    s.create_branch(&repo_path, "feature").unwrap();
    s.add_worktree(&repo_path, &worktree_path, "feature", false)
        .unwrap();

    // Make a feature commit in the worktree via libgit2 using an explicit signature
    write_file(&worktree_path, "f.txt", "feat\n");
    {
        let repo = Repository::open(&worktree_path).unwrap();
        // stage all
        let mut index = repo.index().unwrap();
        index
            .add_all(["*"].iter(), git2::IndexAddOption::DEFAULT, None)
            .unwrap();
        index.write().unwrap();
        let tree_id = index.write_tree().unwrap();
        let tree = repo.find_tree(tree_id).unwrap();
        let sig = git2::Signature::now("Other Author", "other@example.com").unwrap();
        let parent = repo.head().unwrap().peel_to_commit().unwrap();
        let _cid = repo
            .commit(Some("HEAD"), &sig, &sig, "feat", &tree, &[&parent])
            .unwrap();
    }

    // Ensure main repo is NOT on base branch so merge_changes takes libgit2 path
    s.create_branch(&repo_path, "dev").unwrap();
    s.checkout_branch(&repo_path, "dev").unwrap();

    // Merge feature -> main (libgit2 squash)
    let merge_sha = s
        .merge_changes(&repo_path, &worktree_path, "feature", "main", "squash")
        .unwrap();

    // The squash commit author should not be the feature commit's author, and must be present.
    let (name, email) = s.get_commit_author(&repo_path, &merge_sha).unwrap();
    assert_ne!(name.as_deref(), Some("Other Author"));
    assert_ne!(email.as_deref(), Some("other@example.com"));
    if has_global_git_identity() {
        assert!(name.is_some() && email.is_some());
    } else {
        assert_eq!(name.as_deref(), Some("Vibe Kanban"));
        assert_eq!(email.as_deref(), Some("noreply@vibekanban.com"));
    }
}
</file>

<file path="crates/services/Cargo.toml">
[package]
name = "services"
version = "0.0.94"
edition = "2024"

[features]
default = []
cloud = []

[dependencies]
utils = { path = "../utils" }
executors = { path = "../executors" }
db = { path = "../db" }
tokio = { workspace = true }
tokio-util = { version = "0.7", features = ["io"] }
axum = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
anyhow = { workspace = true }
tracing = { workspace = true }
tracing-subscriber = { workspace = true }
sqlx = { version = "0.8.6", features = ["runtime-tokio-rustls", "sqlite", "chrono", "uuid"] }
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1.0", features = ["v4", "serde"] }
ts-rs = { workspace = true }
dirs = "5.0"
xdg = "3.0"
git2 = "0.18"
tempfile = "3.21"
async-trait = "0.1"
libc = "0.2"
rust-embed = "8.2"
directories = "6.0.0"
open = "5.3.2"
ignore = "0.4"
command-group = { version = "5.0", features = ["with-tokio"] }
openssl-sys = { workspace = true }
regex = "1.11.1"
notify-rust = "4.11"
octocrab = "0.44"
os_info = "3.12.0"
sentry = { version = "0.41.0", features = ["anyhow", "backtrace", "panic", "debug-images"] }
sentry-tracing = { version = "0.41.0", features = ["backtrace"] }
reqwest = { version = "0.12", features = ["json"] }
lazy_static = "1.4"
futures-util = "0.3"
json-patch = "2.0"
backon = "1.5.1"
base64 = "0.22"
thiserror = { workspace = true }
futures = "0.3.31"
tokio-stream = "0.1.17"
secrecy = "0.10.3"
strum_macros = "0.27.2"
strum = "0.27.2"
notify = "8.2.0"
notify-debouncer-full = "0.5.0"
dunce = "1.0"
dashmap = "6.1"
once_cell = "1.20"
sha2 = "0.10"
fst = "0.4"
moka = { version = "0.12", features = ["future"] }
</file>

<file path="crates/utils/src/assets.rs">
use directories::ProjectDirs;
use rust_embed::RustEmbed;

const PROJECT_ROOT: &str = env!("CARGO_MANIFEST_DIR");

pub fn asset_dir() -> std::path::PathBuf {
    let path = if cfg!(debug_assertions) {
        std::path::PathBuf::from(PROJECT_ROOT).join("../../dev_assets")
    } else {
        ProjectDirs::from("ai", "bloop", "vibe-kanban")
            .expect("OS didn't give us a home directory")
            .data_dir()
            .to_path_buf()
    };

    // Ensure the directory exists
    if !path.exists() {
        std::fs::create_dir_all(&path).expect("Failed to create asset directory");
    }

    path
    // ✔ macOS → ~/Library/Application Support/MyApp
    // ✔ Linux → ~/.local/share/myapp   (respects XDG_DATA_HOME)
    // ✔ Windows → %APPDATA%\Example\MyApp
}

pub fn config_path() -> std::path::PathBuf {
    asset_dir().join("config.json")
}

pub fn profiles_path() -> std::path::PathBuf {
    asset_dir().join("profiles.json")
}

#[derive(RustEmbed)]
#[folder = "../../assets/sounds"]
pub struct SoundAssets;

#[derive(RustEmbed)]
#[folder = "../../assets/scripts"]
pub struct ScriptAssets;
</file>

<file path="crates/utils/src/browser.rs">
use crate::is_wsl2;

/// Open URL in browser with WSL2 support
pub async fn open_browser(url: &str) -> Result<(), Box<dyn std::error::Error + Send + Sync>> {
    if is_wsl2() {
        // In WSL2, use PowerShell to open the browser
        tokio::process::Command::new("powershell.exe")
            .arg("-Command")
            .arg(format!("Start-Process '{url}'"))
            .spawn()?;
        Ok(())
    } else {
        // Use the standard open crate for other platforms
        open::that(url).map_err(|e| e.into())
    }
}
</file>

<file path="crates/utils/src/diff.rs">
use serde::{Deserialize, Serialize};
use similar::{ChangeTag, TextDiff};
use ts_rs::TS;

// Structs compatable with props: https://github.com/MrWangJustToDo/git-diff-view

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[serde(rename_all = "camelCase")]
pub struct FileDiffDetails {
    pub file_name: Option<String>,
    pub content: Option<String>,
}

// Worktree diffs for the diffs tab: minimal, no hunks, optional full contents
#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[serde(rename_all = "camelCase")]
pub struct Diff {
    pub change: DiffChangeKind,
    pub old_path: Option<String>,
    pub new_path: Option<String>,
    pub old_content: Option<String>,
    pub new_content: Option<String>,
    /// True when file contents are intentionally omitted (e.g., too large)
    pub content_omitted: bool,
    /// Optional precomputed stats for omitted content
    pub additions: Option<usize>,
    pub deletions: Option<usize>,
}

#[derive(Debug, Clone, Serialize, Deserialize, TS)]
#[ts(export)]
#[serde(rename_all = "camelCase")]
pub enum DiffChangeKind {
    Added,
    Deleted,
    Modified,
    Renamed,
    Copied,
    PermissionChange,
}

// ==============================
// Unified diff utility functions
// ==============================

/// Converts a replace diff to a unified diff hunk without the hunk header.
/// The hunk returned will have valid hunk, and diff lines.
pub fn create_unified_diff_hunk(old: &str, new: &str) -> String {
    // normalize ending line feed to optimize diff output
    let mut old = old.to_string();
    let mut new = new.to_string();
    if !old.ends_with('\n') {
        old.push('\n');
    }
    if !new.ends_with('\n') {
        new.push('\n');
    }

    let diff = TextDiff::from_lines(&old, &new);

    let mut out = String::new();

    // We need a valud hunk header. assume lines are 0. but - + count will be correct.

    let old_count = diff.old_slices().len();
    let new_count = diff.new_slices().len();

    out.push_str(&format!("@@ -1,{old_count} +1,{new_count} @@\n"));

    for change in diff.iter_all_changes() {
        let sign = match change.tag() {
            ChangeTag::Equal => ' ',
            ChangeTag::Delete => '-',
            ChangeTag::Insert => '+',
        };
        let val = change.value();
        out.push(sign);
        out.push_str(val);
    }

    out
}

/// Creates a full unified diff with the file path in the header.
pub fn create_unified_diff(file_path: &str, old: &str, new: &str) -> String {
    let mut out = String::new();
    out.push_str(format!("--- a/{file_path}\n+++ b/{file_path}\n").as_str());
    out.push_str(&create_unified_diff_hunk(old, new));
    out
}

/// Extracts unified diff hunks from a string containing a full unified diff.
/// Tolerates non-diff lines and missing `@@`` hunk headers.
pub fn extract_unified_diff_hunks(unified_diff: &str) -> Vec<String> {
    let lines = unified_diff.split_inclusive('\n').collect::<Vec<_>>();

    if !lines.iter().any(|l| l.starts_with("@@")) {
        // No @@ hunk headers: treat as a single hunk
        let hunk = lines
            .iter()
            .copied()
            .filter(|line| line.starts_with([' ', '+', '-']))
            .collect::<String>();

        let old_count = lines
            .iter()
            .filter(|line| line.starts_with(['-', ' ']))
            .count();
        let new_count = lines
            .iter()
            .filter(|line| line.starts_with(['+', ' ']))
            .count();

        return if hunk.is_empty() {
            vec![]
        } else {
            vec![format!("@@ -1,{old_count} +1,{new_count} @@\n{hunk}")]
        };
    }

    let mut hunks = vec![];
    let mut current_hunk: Option<String> = None;

    // Collect hunks starting with @@ headers
    for line in lines {
        if line.starts_with("@@") {
            // new hunk starts
            if let Some(hunk) = current_hunk.take() {
                // flush current hunk
                if !hunk.is_empty() {
                    hunks.push(hunk);
                }
            }
            current_hunk = Some(line.to_string());
        } else if let Some(ref mut hunk) = current_hunk {
            if line.starts_with([' ', '+', '-']) {
                // hunk content
                hunk.push_str(line);
            } else {
                // unkown line, flush current hunk
                if !hunk.is_empty() {
                    hunks.push(hunk.clone());
                }
                current_hunk = None;
            }
        }
    }
    // we have reached the end. flush the last hunk if it exists
    if let Some(hunk) = current_hunk
        && !hunk.is_empty()
    {
        hunks.push(hunk);
    }

    // Fix hunk headers if they are empty @@\n
    hunks = fix_hunk_headers(hunks);

    hunks
}

// Helper function to ensure valid hunk headers
fn fix_hunk_headers(hunks: Vec<String>) -> Vec<String> {
    if hunks.is_empty() {
        return hunks;
    }

    let mut new_hunks = Vec::new();
    // if hunk header is empty @@\n, ten we need to replace it with a valid header
    for hunk in hunks {
        let mut lines = hunk
            .split_inclusive('\n')
            .map(str::to_string)
            .collect::<Vec<_>>();
        if lines.len() < 2 {
            // empty hunk, skip
            continue;
        }

        let header = &lines[0];
        if !header.starts_with("@@") {
            // no header, skip
            continue;
        }

        if header.trim() == "@@" {
            // empty header, replace with a valid one
            lines.remove(0);
            let old_count = lines
                .iter()
                .filter(|line| line.starts_with(['-', ' ']))
                .count();
            let new_count = lines
                .iter()
                .filter(|line| line.starts_with(['+', ' ']))
                .count();
            let new_header = format!("@@ -1,{old_count} +1,{new_count} @@");
            lines.insert(0, new_header);
            new_hunks.push(lines.join(""));
        } else {
            // valid header, keep as is
            new_hunks.push(hunk);
        }
    }

    new_hunks
}

/// Creates a full unified diff with the file path in the header,
pub fn concatenate_diff_hunks(file_path: &str, hunks: &[String]) -> String {
    let mut unified_diff = String::new();

    let header = format!("--- a/{file_path}\n+++ b/{file_path}\n");

    unified_diff.push_str(&header);

    if !hunks.is_empty() {
        let lines = hunks
            .iter()
            .flat_map(|hunk| hunk.lines())
            .filter(|line| line.starts_with("@@ ") || line.starts_with([' ', '+', '-']))
            .collect::<Vec<_>>();
        unified_diff.push_str(lines.join("\n").as_str());
        if !unified_diff.ends_with('\n') {
            unified_diff.push('\n');
        }
    }

    unified_diff
}
</file>

<file path="crates/utils/src/lib.rs">
use std::{env, sync::OnceLock};

use directories::ProjectDirs;

pub mod assets;
pub mod browser;
pub mod diff;
pub mod log_msg;
pub mod msg_store;
pub mod path;
pub mod port_file;
pub mod response;
pub mod sentry;
pub mod shell;
pub mod stream_ext;
pub mod stream_lines;
pub mod text;
pub mod version;

/// Cache for WSL2 detection result
static WSL2_CACHE: OnceLock<bool> = OnceLock::new();

/// Check if running in WSL2 (cached)
pub fn is_wsl2() -> bool {
    *WSL2_CACHE.get_or_init(|| {
        // Check for WSL environment variables
        if std::env::var("WSL_DISTRO_NAME").is_ok() || std::env::var("WSLENV").is_ok() {
            tracing::debug!("WSL2 detected via environment variables");
            return true;
        }

        // Check /proc/version for WSL2 signature
        if let Ok(version) = std::fs::read_to_string("/proc/version")
            && (version.contains("WSL2") || version.contains("microsoft"))
        {
            tracing::debug!("WSL2 detected via /proc/version");
            return true;
        }

        tracing::debug!("WSL2 not detected");
        false
    })
}

pub fn cache_dir() -> std::path::PathBuf {
    let proj = if cfg!(debug_assertions) {
        ProjectDirs::from("ai", "bloop-dev", env!("CARGO_PKG_NAME"))
            .expect("OS didn't give us a home directory")
    } else {
        ProjectDirs::from("ai", "bloop", env!("CARGO_PKG_NAME"))
            .expect("OS didn't give us a home directory")
    };

    // ✔ macOS → ~/Library/Caches/MyApp
    // ✔ Linux → ~/.cache/myapp (respects XDG_CACHE_HOME)
    // ✔ Windows → %LOCALAPPDATA%\Example\MyApp
    proj.cache_dir().to_path_buf()
}

// Get or create cached PowerShell script file
pub async fn get_powershell_script()
-> Result<std::path::PathBuf, Box<dyn std::error::Error + Send + Sync>> {
    use std::io::Write;

    let cache_dir = cache_dir();
    let script_path = cache_dir.join("toast-notification.ps1");

    // Check if cached file already exists and is valid
    if script_path.exists() {
        // Verify file has content (basic validation)
        if let Ok(metadata) = std::fs::metadata(&script_path)
            && metadata.len() > 0
        {
            return Ok(script_path);
        }
    }

    // File doesn't exist or is invalid, create it
    let script_content = assets::ScriptAssets::get("toast-notification.ps1")
        .ok_or("Embedded PowerShell script not found: toast-notification.ps1")?
        .data;

    // Ensure cache directory exists
    std::fs::create_dir_all(&cache_dir)
        .map_err(|e| format!("Failed to create cache directory: {e}"))?;

    let mut file = std::fs::File::create(&script_path)
        .map_err(|e| format!("Failed to create PowerShell script file: {e}"))?;

    file.write_all(&script_content)
        .map_err(|e| format!("Failed to write PowerShell script data: {e}"))?;

    drop(file); // Ensure file is closed

    Ok(script_path)
}
</file>

<file path="crates/utils/src/log_msg.rs">
use axum::{extract::ws::Message, response::sse::Event};
use json_patch::Patch;
use serde::{Deserialize, Serialize};

pub const EV_STDOUT: &str = "stdout";
pub const EV_STDERR: &str = "stderr";
pub const EV_JSON_PATCH: &str = "json_patch";
pub const EV_SESSION_ID: &str = "session_id";
pub const EV_FINISHED: &str = "finished";

#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum LogMsg {
    Stdout(String),
    Stderr(String),
    JsonPatch(Patch),
    SessionId(String),
    Finished,
}

impl LogMsg {
    pub fn name(&self) -> &'static str {
        match self {
            LogMsg::Stdout(_) => EV_STDOUT,
            LogMsg::Stderr(_) => EV_STDERR,
            LogMsg::JsonPatch(_) => EV_JSON_PATCH,
            LogMsg::SessionId(_) => EV_SESSION_ID,
            LogMsg::Finished => EV_FINISHED,
        }
    }

    pub fn to_sse_event(&self) -> Event {
        match self {
            LogMsg::Stdout(s) => Event::default().event(EV_STDOUT).data(s.clone()),
            LogMsg::Stderr(s) => Event::default().event(EV_STDERR).data(s.clone()),
            LogMsg::JsonPatch(patch) => {
                let data = serde_json::to_string(patch).unwrap_or_else(|_| "[]".to_string());
                Event::default().event(EV_JSON_PATCH).data(data)
            }
            LogMsg::SessionId(s) => Event::default().event(EV_SESSION_ID).data(s.clone()),
            LogMsg::Finished => Event::default().event(EV_FINISHED).data(""),
        }
    }

    /// Convert LogMsg to WebSocket message with proper error handling
    pub fn to_ws_message(&self) -> Result<Message, serde_json::Error> {
        let json = serde_json::to_string(self)?;
        Ok(Message::Text(json.into()))
    }

    /// Convert LogMsg to WebSocket message with fallback error handling
    ///
    /// This method mirrors the behavior of the original logmsg_to_ws function
    /// but with better error handling than unwrap().
    pub fn to_ws_message_unchecked(&self) -> Message {
        // Finished becomes JSON {finished: true}
        let json = match self {
            LogMsg::Finished => r#"{"finished":true}"#.to_string(),
            _ => serde_json::to_string(self)
                .unwrap_or_else(|_| r#"{"error":"serialization_failed"}"#.to_string()),
        };

        Message::Text(json.into())
    }

    /// Rough size accounting for your byte‑budgeted history.
    pub fn approx_bytes(&self) -> usize {
        const OVERHEAD: usize = 8;
        match self {
            LogMsg::Stdout(s) => EV_STDOUT.len() + s.len() + OVERHEAD,
            LogMsg::Stderr(s) => EV_STDERR.len() + s.len() + OVERHEAD,
            LogMsg::JsonPatch(patch) => {
                let json_len = serde_json::to_string(patch).map(|s| s.len()).unwrap_or(2);
                EV_JSON_PATCH.len() + json_len + OVERHEAD
            }
            LogMsg::SessionId(s) => EV_SESSION_ID.len() + s.len() + OVERHEAD,
            LogMsg::Finished => EV_FINISHED.len() + OVERHEAD,
        }
    }
}
</file>

<file path="crates/utils/src/msg_store.rs">
use std::{
    collections::VecDeque,
    sync::{Arc, RwLock},
};

use axum::response::sse::Event;
use futures::{StreamExt, TryStreamExt, future};
use tokio::{sync::broadcast, task::JoinHandle};
use tokio_stream::wrappers::BroadcastStream;

use crate::{log_msg::LogMsg, stream_lines::LinesStreamExt};

// 100 MB Limit
const HISTORY_BYTES: usize = 100000 * 1024;

#[derive(Clone)]
struct StoredMsg {
    msg: LogMsg,
    bytes: usize,
}

struct Inner {
    history: VecDeque<StoredMsg>,
    total_bytes: usize,
}

pub struct MsgStore {
    inner: RwLock<Inner>,
    sender: broadcast::Sender<LogMsg>,
}

impl Default for MsgStore {
    fn default() -> Self {
        Self::new()
    }
}

impl MsgStore {
    pub fn new() -> Self {
        let (sender, _) = broadcast::channel(10000);
        Self {
            inner: RwLock::new(Inner {
                history: VecDeque::with_capacity(32),
                total_bytes: 0,
            }),
            sender,
        }
    }

    pub fn push(&self, msg: LogMsg) {
        let _ = self.sender.send(msg.clone()); // live listeners
        let bytes = msg.approx_bytes();

        let mut inner = self.inner.write().unwrap();
        while inner.total_bytes.saturating_add(bytes) > HISTORY_BYTES {
            if let Some(front) = inner.history.pop_front() {
                inner.total_bytes = inner.total_bytes.saturating_sub(front.bytes);
            } else {
                break;
            }
        }
        inner.history.push_back(StoredMsg { msg, bytes });
        inner.total_bytes = inner.total_bytes.saturating_add(bytes);
    }

    // Convenience
    pub fn push_stdout<S: Into<String>>(&self, s: S) {
        self.push(LogMsg::Stdout(s.into()));
    }
    pub fn push_stderr<S: Into<String>>(&self, s: S) {
        self.push(LogMsg::Stderr(s.into()));
    }
    pub fn push_patch(&self, patch: json_patch::Patch) {
        self.push(LogMsg::JsonPatch(patch));
    }

    pub fn push_session_id(&self, session_id: String) {
        self.push(LogMsg::SessionId(session_id));
    }

    pub fn push_finished(&self) {
        self.push(LogMsg::Finished);
    }

    pub fn get_receiver(&self) -> broadcast::Receiver<LogMsg> {
        self.sender.subscribe()
    }
    pub fn get_history(&self) -> Vec<LogMsg> {
        self.inner
            .read()
            .unwrap()
            .history
            .iter()
            .map(|s| s.msg.clone())
            .collect()
    }

    /// History then live, as `LogMsg`.
    pub fn history_plus_stream(
        &self,
    ) -> futures::stream::BoxStream<'static, Result<LogMsg, std::io::Error>> {
        let (history, rx) = (self.get_history(), self.get_receiver());

        let hist = futures::stream::iter(history.into_iter().map(Ok::<_, std::io::Error>));
        let live = BroadcastStream::new(rx)
            .filter_map(|res| async move { res.ok().map(Ok::<_, std::io::Error>) });

        Box::pin(hist.chain(live))
    }

    pub fn stdout_chunked_stream(
        &self,
    ) -> futures::stream::BoxStream<'static, Result<String, std::io::Error>> {
        self.history_plus_stream()
            .take_while(|res| future::ready(!matches!(res, Ok(LogMsg::Finished))))
            .filter_map(|res| async move {
                match res {
                    Ok(LogMsg::Stdout(s)) => Some(Ok(s)),
                    _ => None,
                }
            })
            .boxed()
    }

    pub fn stdout_lines_stream(
        &self,
    ) -> futures::stream::BoxStream<'static, std::io::Result<String>> {
        self.stdout_chunked_stream().lines()
    }

    pub fn stderr_chunked_stream(
        &self,
    ) -> futures::stream::BoxStream<'static, Result<String, std::io::Error>> {
        self.history_plus_stream()
            .take_while(|res| future::ready(!matches!(res, Ok(LogMsg::Finished))))
            .filter_map(|res| async move {
                match res {
                    Ok(LogMsg::Stderr(s)) => Some(Ok(s)),
                    _ => None,
                }
            })
            .boxed()
    }

    pub fn stderr_lines_stream(
        &self,
    ) -> futures::stream::BoxStream<'static, std::io::Result<String>> {
        self.stderr_chunked_stream().lines()
    }

    /// Same stream but mapped to `Event` for SSE handlers.
    pub fn sse_stream(&self) -> futures::stream::BoxStream<'static, Result<Event, std::io::Error>> {
        self.history_plus_stream()
            .map_ok(|m| m.to_sse_event())
            .boxed()
    }

    /// Forward a stream of typed log messages into this store.
    pub fn spawn_forwarder<S, E>(self: Arc<Self>, stream: S) -> JoinHandle<()>
    where
        S: futures::Stream<Item = Result<LogMsg, E>> + Send + 'static,
        E: std::fmt::Display + Send + 'static,
    {
        tokio::spawn(async move {
            tokio::pin!(stream);

            while let Some(next) = stream.next().await {
                match next {
                    Ok(msg) => self.push(msg),
                    Err(e) => self.push(LogMsg::Stderr(format!("stream error: {e}"))),
                }
            }
        })
    }
}
</file>

<file path="crates/utils/src/path.rs">
use std::path::{Path, PathBuf};

/// Directory name for storing images in worktrees
pub const VIBE_IMAGES_DIR: &str = ".vibe-images";

/// Convert absolute paths to relative paths based on worktree path
/// This is a robust implementation that handles symlinks and edge cases
pub fn make_path_relative(path: &str, worktree_path: &str) -> String {
    tracing::debug!("Making path relative: {} -> {}", path, worktree_path);

    let path_obj = normalize_macos_private_alias(Path::new(&path));
    let worktree_path_obj = normalize_macos_private_alias(Path::new(worktree_path));

    // If path is already relative, return as is
    if path_obj.is_relative() {
        return path.to_string();
    }

    if let Ok(relative_path) = path_obj.strip_prefix(&worktree_path_obj) {
        let result = relative_path.to_string_lossy().to_string();
        tracing::debug!("Successfully made relative: '{}' -> '{}'", path, result);
        if result.is_empty() {
            return ".".to_string();
        }
        return result;
    }

    if !path_obj.exists() || !worktree_path_obj.exists() {
        return path.to_string();
    }

    // canonicalize may fail if paths don't exist
    let canonical_path = std::fs::canonicalize(&path_obj);
    let canonical_worktree = std::fs::canonicalize(&worktree_path_obj);

    match (canonical_path, canonical_worktree) {
        (Ok(canon_path), Ok(canon_worktree)) => {
            tracing::debug!(
                "Trying canonical path resolution: '{}' -> '{}', '{}' -> '{}'",
                path,
                canon_path.display(),
                worktree_path,
                canon_worktree.display()
            );

            match canon_path.strip_prefix(&canon_worktree) {
                Ok(relative_path) => {
                    let result = relative_path.to_string_lossy().to_string();
                    tracing::debug!(
                        "Successfully made relative with canonical paths: '{}' -> '{}'",
                        path,
                        result
                    );
                    if result.is_empty() {
                        return ".".to_string();
                    }
                    result
                }
                Err(e) => {
                    tracing::warn!(
                        "Failed to make canonical path relative: '{}' relative to '{}', error: {}, returning original",
                        canon_path.display(),
                        canon_worktree.display(),
                        e
                    );
                    path.to_string()
                }
            }
        }
        _ => {
            tracing::debug!(
                "Could not canonicalize paths (paths may not exist): '{}', '{}', returning original",
                path,
                worktree_path
            );
            path.to_string()
        }
    }
}

/// Normalize macOS prefix /private/var/ and /private/tmp/ to their public aliases without resolving paths.
/// This allows prefix normalization to work when the full paths don't exist.
fn normalize_macos_private_alias<P: AsRef<Path>>(p: P) -> PathBuf {
    let p = p.as_ref();
    if cfg!(target_os = "macos")
        && let Some(s) = p.to_str()
    {
        if s == "/private/var" {
            return PathBuf::from("/var");
        }
        if let Some(rest) = s.strip_prefix("/private/var/") {
            return PathBuf::from(format!("/var/{rest}"));
        }
        if s == "/private/tmp" {
            return PathBuf::from("/tmp");
        }
        if let Some(rest) = s.strip_prefix("/private/tmp/") {
            return PathBuf::from(format!("/tmp/{rest}"));
        }
    }
    p.to_path_buf()
}

pub fn get_vibe_kanban_temp_dir() -> std::path::PathBuf {
    let dir_name = if cfg!(debug_assertions) {
        "vibe-kanban-dev"
    } else {
        "vibe-kanban"
    };

    if cfg!(target_os = "macos") {
        // macOS already uses /var/folders/... which is persistent storage
        std::env::temp_dir().join(dir_name)
    } else if cfg!(target_os = "linux") {
        // Linux: use /var/tmp instead of /tmp to avoid RAM usage
        std::path::PathBuf::from("/var/tmp").join(dir_name)
    } else {
        // Windows and other platforms: use temp dir with vibe-kanban subdirectory
        std::env::temp_dir().join(dir_name)
    }
}

/// Expand leading ~ to user's home directory.
pub fn expand_tilde(path_str: &str) -> std::path::PathBuf {
    shellexpand::tilde(path_str).as_ref().into()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_make_path_relative() {
        // Test with relative path (should remain unchanged)
        assert_eq!(
            make_path_relative("src/main.rs", "/tmp/test-worktree"),
            "src/main.rs"
        );

        // Test with absolute path (should become relative if possible)
        let test_worktree = "/tmp/test-worktree";
        let absolute_path = format!("{test_worktree}/src/main.rs");
        let result = make_path_relative(&absolute_path, test_worktree);
        assert_eq!(result, "src/main.rs");

        // Test with path outside worktree (should return original)
        assert_eq!(
            make_path_relative("/other/path/file.js", "/tmp/test-worktree"),
            "/other/path/file.js"
        );
    }

    #[cfg(target_os = "macos")]
    #[test]
    fn test_make_path_relative_macos_private_alias() {
        // Simulate a worktree under /var with a path reported under /private/var
        let worktree = "/var/folders/zz/abc123/T/vibe-kanban-dev/worktrees/vk-test";
        let path_under_private = format!(
            "/private/var{}/hello-world.txt",
            worktree.strip_prefix("/var").unwrap()
        );
        assert_eq!(
            make_path_relative(&path_under_private, worktree),
            "hello-world.txt"
        );

        // Also handle the inverse: worktree under /private and path under /var
        let worktree_private = format!("/private{worktree}");
        let path_under_var = format!("{worktree}/hello-world.txt");
        assert_eq!(
            make_path_relative(&path_under_var, &worktree_private),
            "hello-world.txt"
        );
    }
}
</file>

<file path="crates/utils/src/port_file.rs">
use std::{env, path::PathBuf};

use tokio::fs;

pub async fn write_port_file(port: u16) -> std::io::Result<PathBuf> {
    let dir = env::temp_dir().join("vibe-kanban");
    let path = dir.join("vibe-kanban.port");
    tracing::debug!("Writing port {} to {:?}", port, path);
    fs::create_dir_all(&dir).await?;
    fs::write(&path, port.to_string()).await?;
    Ok(path)
}
</file>

<file path="crates/utils/src/response.rs">
use serde::Serialize;
use ts_rs::TS;

#[derive(Debug, Serialize, TS)]
pub struct ApiResponse<T, E = T> {
    success: bool,
    data: Option<T>,
    error_data: Option<E>,
    message: Option<String>,
}

impl<T, E> ApiResponse<T, E> {
    /// Creates a successful response, with `data` and no message.
    pub fn success(data: T) -> Self {
        ApiResponse {
            success: true,
            data: Some(data),
            message: None,
            error_data: None,
        }
    }

    /// Creates an error response, with `message` and no data.
    pub fn error(message: &str) -> Self {
        ApiResponse {
            success: false,
            data: None,
            message: Some(message.to_string()),
            error_data: None,
        }
    }
    /// Creates an error response, with no `data`, no `message`, but with arbitrary `error_data`.
    pub fn error_with_data(data: E) -> Self {
        ApiResponse {
            success: false,
            data: None,
            error_data: Some(data),
            message: None,
        }
    }
}
</file>

<file path="crates/utils/src/sentry.rs">
use sentry_tracing::{EventFilter, SentryLayer};
use tracing::Level;

pub fn sentry_layer<S>() -> SentryLayer<S>
where
    S: tracing::Subscriber,
    S: for<'a> tracing_subscriber::registry::LookupSpan<'a>,
{
    SentryLayer::default()
        .span_filter(|meta| {
            matches!(
                *meta.level(),
                Level::DEBUG | Level::INFO | Level::WARN | Level::ERROR
            )
        })
        .event_filter(|meta| match *meta.level() {
            Level::ERROR => EventFilter::Event,
            Level::DEBUG | Level::INFO | Level::WARN => EventFilter::Breadcrumb,
            Level::TRACE => EventFilter::Ignore,
        })
}
</file>

<file path="crates/utils/src/shell.rs">
//! Cross-platform shell command utilities

/// Returns the appropriate shell command and argument for the current platform.
///
/// Returns (shell_program, shell_arg) where:
/// - Windows: ("cmd", "/C")
/// - Unix-like: ("sh", "-c") or ("bash", "-c") if available
pub fn get_shell_command() -> (&'static str, &'static str) {
    if cfg!(windows) {
        ("cmd", "/C")
    } else {
        // Prefer bash if available, fallback to sh
        if std::path::Path::new("/bin/bash").exists() {
            ("bash", "-c")
        } else {
            ("sh", "-c")
        }
    }
}

/// Resolves the full path of an executable using the system's PATH environment variable.
/// Note: On Windows, resolving the executable path can be necessary before passing
/// it to `std::process::Command::new`, as the latter has been deficient in finding executables.
pub fn resolve_executable_path(executable: &str) -> Option<String> {
    which::which(executable)
        .ok()
        .map(|p| p.to_string_lossy().to_string())
}
</file>

<file path="crates/utils/src/stream_ext.rs">
use std::io;

use futures::{Stream, StreamExt};
use tokio::time::{Duration, Instant, sleep_until};

use crate::log_msg::LogMsg;

const WINDOW_MS: u64 = 10;
const WINDOW_LIMIT: usize = 100 * 1024; // 100 KiB per window
// To avoid unbounded growth within a window, cap accumulation.
// We allow collecting more than WINDOW_LIMIT to preserve both head and tail,
// then apply middle truncation on flush.
const COLLECT_LIMIT: usize = WINDOW_LIMIT * 2;

const TRUNC_MARKER: &str = " [truncated] ";

fn middle_truncate_bytes(bytes: &[u8], limit: usize, marker: &str) -> String {
    if bytes.len() <= limit {
        return String::from_utf8_lossy(bytes).into_owned();
    }
    let m = marker.as_bytes();
    let mlen = m.len();
    if limit <= mlen {
        // Degenerate case: not enough room; return a cut marker
        return String::from_utf8_lossy(&m[..limit]).into_owned();
    }
    let keep_prefix = (limit - mlen) / 2;
    let keep_suffix = limit - mlen - keep_prefix;

    let mut out = Vec::with_capacity(limit);
    out.extend_from_slice(&bytes[..keep_prefix]);
    out.extend_from_slice(m);
    out.extend_from_slice(&bytes[bytes.len() - keep_suffix..]);
    String::from_utf8_lossy(&out).into_owned()
}

fn shrink_middle(buf: &mut Vec<u8>, target_len: usize) {
    if buf.len() <= target_len {
        return;
    }
    let extra = buf.len() - target_len;
    let mid = buf.len() / 2;
    let start = mid.saturating_sub(extra / 2);
    let end = start + extra;
    buf.drain(start..end);
}

// Helper that flushes buffer, inserting a middle [truncated] marker when needed
fn flush_buf(
    buf: &mut Vec<u8>,
    kind: Option<bool>,
    truncated_in_window: &mut bool,
) -> Option<LogMsg> {
    if buf.is_empty() && !*truncated_in_window {
        return None;
    }

    let needs_marker = *truncated_in_window || buf.len() > WINDOW_LIMIT;
    let out = if needs_marker {
        middle_truncate_bytes(buf, WINDOW_LIMIT, TRUNC_MARKER)
    } else {
        String::from_utf8_lossy(buf).into_owned()
    };

    buf.clear();
    *truncated_in_window = false;

    match kind {
        Some(true) => Some(LogMsg::Stdout(out)),
        Some(false) => Some(LogMsg::Stderr(out)),
        None => None,
    }
}

pub fn debounce_logs<S>(input: S) -> impl Stream<Item = Result<LogMsg, io::Error>>
where
    S: Stream<Item = Result<LogMsg, io::Error>> + Unpin,
{
    async_stream::stream! {
        // Single accumulation buffer per window; we trim from the middle when exceeding COLLECT_LIMIT
        let mut buf: Vec<u8> = Vec::with_capacity(WINDOW_LIMIT);
        let mut current_stream_type: Option<bool> = None; // Some(true)=stdout, Some(false)=stderr
        let mut timer = Instant::now() + Duration::from_millis(WINDOW_MS);

        // per-window accounting
        let mut truncated_in_window: bool = false;

        tokio::pin!(input);

        loop {
            tokio::select! {
                maybe = input.next() => {
                    let msg = match maybe {
                        Some(Ok(v)) => v,
                        Some(Err(e)) => { yield Err(e); continue; }
                        None => break,
                    };

                    match &msg {
                        LogMsg::Stdout(s) | LogMsg::Stderr(s) => {
                            let is_stdout = matches!(msg, LogMsg::Stdout(_));

                            // Flush if switching stream kind
                            if current_stream_type != Some(is_stdout) {
                                if let Some(flushed) = flush_buf(&mut buf, current_stream_type, &mut truncated_in_window) {
                                    yield Ok(flushed);
                                }
                                current_stream_type = Some(is_stdout);
                                buf.clear();
                                truncated_in_window = false;
                            }

                            let bytes = s.as_bytes();
                            buf.extend_from_slice(bytes);
                            if buf.len() > COLLECT_LIMIT {
                                truncated_in_window = true;
                                shrink_middle(&mut buf, COLLECT_LIMIT);
                            }
                        }

                        _ => {
                            // Flush accumulated stdout/stderr before passing through other messages
                            if let Some(flushed) = flush_buf(&mut buf, current_stream_type, &mut truncated_in_window) {
                                yield Ok(flushed);
                            }
                            current_stream_type = None;
                            yield Ok(msg);
                        }
                    }
                }

                _ = sleep_until(timer) => {
                    if let Some(flushed) = {
                        let kind = current_stream_type;
                        flush_buf(&mut buf, kind, &mut truncated_in_window)
                    } {
                        yield Ok(flushed);
                    }
                    // Start a fresh time window
                    timer = Instant::now() + Duration::from_millis(WINDOW_MS);
                    buf.clear();
                    truncated_in_window = false;
                }
            }
        }

        // Final flush on stream end
        if let Some(flushed) = {
            let kind = current_stream_type;
            flush_buf(&mut buf, kind, &mut truncated_in_window)
        } {
            yield Ok(flushed);
        }
    }
}
</file>

<file path="crates/utils/src/stream_lines.rs">
use bytes::Bytes;
use futures::{Stream, StreamExt, TryStreamExt};
use tokio_util::{
    codec::{FramedRead, LinesCodec},
    io::StreamReader,
};

/// Extension trait for converting chunked string streams to line streams.
pub trait LinesStreamExt: Stream<Item = Result<String, std::io::Error>> + Sized {
    /// Convert a chunked string stream to a line stream.
    fn lines(self) -> futures::stream::BoxStream<'static, std::io::Result<String>>
    where
        Self: Send + 'static,
    {
        let reader = StreamReader::new(self.map(|result| result.map(Bytes::from)));
        FramedRead::new(reader, LinesCodec::new())
            .map_err(|e| std::io::Error::new(std::io::ErrorKind::InvalidData, e))
            .boxed()
    }
}

impl<S> LinesStreamExt for S where S: Stream<Item = Result<String, std::io::Error>> {}
</file>

<file path="crates/utils/src/text.rs">
use regex::Regex;
use uuid::Uuid;

pub fn git_branch_id(input: &str) -> String {
    // 1. lowercase
    let lower = input.to_lowercase();

    // 2. replace non-alphanumerics with hyphens
    let re = Regex::new(r"[^a-z0-9]+").unwrap();
    let slug = re.replace_all(&lower, "-");

    // 3. trim extra hyphens
    let trimmed = slug.trim_matches('-');

    // 4. take up to 10 chars, then trim trailing hyphens again
    let cut: String = trimmed.chars().take(10).collect();
    cut.trim_end_matches('-').to_string()
}

pub fn short_uuid(u: &Uuid) -> String {
    // to_simple() gives you a 32-char hex string with no hyphens
    let full = u.simple().to_string();
    full.chars().take(4).collect() // grab the first 4 chars
}
</file>

<file path="crates/utils/src/version.rs">
/// The current application version from Cargo.toml
pub const APP_VERSION: &str = env!("CARGO_PKG_VERSION");
</file>

<file path="crates/utils/Cargo.toml">
[package]
name = "utils"
version = "0.0.94"
edition = "2024"

[dependencies]
tokio-util = { version = "0.7", features = ["io", "codec"] }
bytes = "1.0"
axum = { workspace = true, features = ["ws"] }
serde = { workspace = true }
serde_json = { workspace = true }
tracing = { workspace = true }
tracing-subscriber = { workspace = true }
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1.0", features = ["v4", "serde"] }
ts-rs = { workspace = true }
libc = "0.2"
rust-embed = "8.2"
directories = "6.0.0"
open = "5.3.2"
regex = "1.11.1"
sentry-tracing = { version = "0.41.0", features = ["backtrace"] }
lazy_static = "1.4"
futures-util = "0.3"
json-patch = "2.0"
base64 = "0.22"
tokio = { workspace = true }
futures = "0.3.31"
tokio-stream = { version = "0.1.17", features = ["sync"] }
async-stream = "0.3"
shellexpand = "3.1.1"
which = "8.0.0"
similar = "2"
</file>

<file path="dev_assets_seed/config.json">
{
  "theme": "light",
  "executor": {
    "type": "claude"
  },
  "disclaimer_acknowledged": true,
  "onboarding_acknowledged": true,
  "sound_alerts": true,
  "sound_file": "abstract-sound4",
  "push_notifications": true,
  "editor": {
    "editor_type": "VS_CODE",
    "custom_command": null
  },
  "github": {
    "token": "",
    "default_pr_base": "main"
  }
}
</file>

<file path="docs/configuration-customisation/agent-configurations.mdx">
---
title: "Agent Profiles & Configuration"
description: "Configure and customise coding agent variants with different settings for planning, models, and sandbox permissions"
---

Agent profiles let you define multiple named variants for each supported coding agent. Variants capture configuration differences like planning mode, model choice, and sandbox permissions that you can quickly select when creating attempts.

<Info>
Agent profiles are used throughout Vibe Kanban wherever agents run: onboarding, default settings, attempt creation, and follow-ups.
</Info>

## Configuration Access

You can configure agent profiles in two ways through Settings → Agents:

<Tabs>
<Tab title="Form Editor">
  Use the guided interface with form fields for each agent setting.

  <Frame>
  <img src="/images/coding-agent-configurations.png" alt="Agent configuration form editor interface" />
  </Frame>
</Tab>

<Tab title="JSON Editor">
  Edit the underlying `profiles.json` file directly for advanced configurations.

  <Frame>
  <img src="/images/coding-agent-configurations-json.png" alt="JSON editor for agent configurations" />
  </Frame>
</Tab>
</Tabs>

<Note>
The configuration page displays the exact file path where your settings are stored. Vibe Kanban saves only your overrides whilst preserving built-in defaults.
</Note>

## Configuration Structure

The profiles configuration uses a JSON structure with an `executors` object containing agent variants:

```json profiles.json
{
  "executors": {
    "CLAUDE_CODE": {
      "DEFAULT": { "CLAUDE_CODE": { "dangerously_skip_permissions": true } },
      "PLAN":    { "CLAUDE_CODE": { "plan": true } },
      "ROUTER":  { "CLAUDE_CODE": { "claude_code_router": true, "dangerously_skip_permissions": true } }
    },
    "GEMINI": {
      "DEFAULT": { "GEMINI": { "model": "default", "yolo": true } },
      "FLASH":   { "GEMINI": { "model": "flash",   "yolo": true } }
    },
    "CODEX": {
      "DEFAULT": { "CODEX": { "sandbox": "danger-full-access" } },
      "HIGH":    { "CODEX": { "sandbox": "danger-full-access", "model_reasoning_effort": "high" } }
    }
  }
}
```

<AccordionGroup>
<Accordion title="Structure Rules">
  - **Variant names**: Case-insensitive and normalised to SCREAMING_SNAKE_CASE
  - **DEFAULT variant**: Reserved and always present for each agent
  - **Custom variants**: Add new variants like `PLAN`, `FLASH`, `HIGH` as needed
  - **Built-in protection**: Cannot remove built-in executors, but can override values
</Accordion>

<Accordion title="Configuration Inheritance">
  - Your custom settings override built-in defaults
  - Built-in configurations remain available as fallbacks
  - Each variant contains a complete configuration object for its agent
</Accordion>
</AccordionGroup>

## Agent Configuration Options

<Tabs>
<Tab title="CLAUDE_CODE">
  <ParamField path="plan" type="boolean">
  Enable planning mode for complex tasks
  </ParamField>

  <ParamField path="claude_code_router" type="boolean">
  Route requests across multiple Claude Code instances
  </ParamField>

  <ParamField path="dangerously_skip_permissions" type="boolean">
  Skip permission prompts (use with caution)
  </ParamField>

  [View full CLI reference →](https://docs.anthropic.com/en/docs/claude-code/cli-reference#cli-flags)
</Tab>

<Tab title="GEMINI">
  <ParamField path="model" type="string">
  Choose model variant: `"default"` or `"flash"`
  </ParamField>

  <ParamField path="yolo" type="boolean">
  Run without confirmations
  </ParamField>

  [View full CLI reference →](https://google-gemini.github.io/gemini-cli/)
</Tab>

<Tab title="AMP">
  <ParamField path="dangerously_allow_all" type="boolean">
  Allow all actions without restrictions (unsafe)
  </ParamField>

  [View full documentation →](https://ampcode.com/manual#cli)
</Tab>

<Tab title="CODEX">
  <ParamField path="sandbox" type="string">
  Execution environment: `"read-only"`, `"workspace-write"`, or `"danger-full-access"`
  </ParamField>

  <ParamField path="approval" type="string">
  Approval level: `"untrusted"`, `"on-failure"`, `"on-request"`, or `"never"`
  </ParamField>

  <ParamField path="model_reasoning_effort" type="string">
  Reasoning depth: `"low"`, `"medium"`, or `"high"`
  </ParamField>

  <ParamField path="model_reasoning_summary" type="string">
  Summary style: `"auto"`, `"concise"`, `"detailed"`, or `"none"`
  </ParamField>

  [View full documentation →](https://github.com/openai/codex)
</Tab>

<Tab title="CURSOR">
  <ParamField path="force" type="boolean">
  Force execution without confirmation
  </ParamField>

  <ParamField path="model" type="string">
  Specify model to use
  </ParamField>

  [View full CLI reference →](https://docs.cursor.com/en/cli/reference/parameters)
</Tab>

<Tab title="OPENCODE">
  <ParamField path="model" type="string">
  Specify model to use
  </ParamField>

  <ParamField path="agent" type="string">
  Choose agent type
  </ParamField>

  [View full documentation →](https://opencode.ai/docs/cli/#flags-1)
</Tab>

<Tab title="QWEN_CODE">
  <ParamField path="yolo" type="boolean">
  Run without confirmations
  </ParamField>

  [View full documentation →](https://qwenlm.github.io/qwen-code-docs/en/cli/index)
</Tab>
</Tabs>

### Universal Options

These options work across multiple agent types:

<ParamField path="append_prompt" type="string | null">
Text appended to the system prompt
</ParamField>

<ParamField path="base_command_override" type="string | null">
Override the underlying CLI command
</ParamField>

<ParamField path="additional_params" type="string[] | null">
Additional CLI arguments to pass
</ParamField>

<Warning>
Options prefixed with "dangerously_" bypass safety confirmations and can perform destructive actions. Use with extreme caution.
</Warning>

## Using Agent Configurations

<CardGroup cols={2}>
<Card title="Default Configuration" icon="gear">
  Set your default agent and variant in **Settings → General → Default Agent Configuration** for consistent behaviour across all attempts.
</Card>

<Card title="Per-Attempt Selection" icon="rocket">
  Override defaults when creating attempts by selecting different agent/variant combinations in the attempt dialogue.
</Card>
</CardGroup>

## Related Configuration

<Note>
MCP (Model Context Protocol) servers are configured separately under **Settings → MCP Servers** but work alongside agent profiles to extend functionality.
</Note>

<CardGroup cols={2}>
<Card title="MCP Server Configuration" icon="server" href="/integrations/mcp-server-configuration">
  Configure MCP servers within Vibe Kanban for your coding agents
</Card>

<Card title="Vibe Kanban MCP Server" icon="plug" href="/integrations/vibe-kanban-mcp-server">
  Connect external MCP clients to Vibe Kanban's MCP server
</Card>
</CardGroup>
</file>

<file path="docs/configuration-customisation/creating-task-templates.mdx">
---
title: "Creating Task Templates"
description: "Create reusable task formats that speed up task creation. You can create templates globally (across all projects) or for specific projects."
---

## Managing global task templates

Access global templates from **Global Settings → Task Templates**. These templates are available across every project in your workspace.

<Frame>
<img src="/images/screenshot-global-task-templates.png" alt="Global task templates interface showing the template list and management options" />
</Frame>

<Steps>
<Step title="Create a new template">
  Click **Add Template** to create a new global template.
</Step>

<Step title="Edit existing templates">
  Use the edit icon (✏️) to modify template content.
</Step>

<Step title="Remove unwanted templates">
  Click the delete icon (🗑️) to remove templates you no longer need.
</Step>
</Steps>

## Managing project task templates

Create templates scoped to individual projects for project-specific workflows.

<Frame>
<img src="/images/screenshot-project-task-templates.png" alt="Project task templates interface within the template library" />
</Frame>

<Steps>
<Step title="Access project templates">
  1. Open your project
  2. Click the library icon in the project header
  3. Select **Manage Templates**
</Step>

<Step title="Manage project templates">
  Use **Add Template**, **Edit** (✏️), and **Remove** (🗑️) controls to manage project-specific templates.
</Step>
</Steps>

## Using templates

Apply templates when creating tasks to prefill content and maintain consistency.

<Steps>
<Step title="Select a template">
  When creating a new task, click the **Use a template** dropdown.
</Step>

<Step title="Choose from available templates">
  Select from your global or project-specific templates.

  <Check>
  The template automatically prefills the task title and description.
  </Check>
</Step>
</Steps>

<Tip>
Global templates appear in all projects, whilst project templates only appear in their specific project. Use global templates for common task types and project templates for specialised workflows.
</Tip>
</file>

<file path="docs/configuration-customisation/global-settings.mdx">
---
title: "Global Settings"
description: "Configure application-wide settings including themes, agents, GitHub integration, and more"
sidebarTitle: "Settings"
---

You can configure application-wide settings via the **Settings** page. To access it, click the ⚙️ icon in the sidebar or select "Settings" from the top-right menu.

<Frame>
<img src="/images/vk-settings.png" alt="Vibe Kanban global settings page showing theme options, agent configuration, and GitHub integration settings" />
</Frame>

## Themes

Switch between light and dark themes to suit your preference.

## Default Agent Configuration

Choose the default agent and variant for new task attempts. This profile is pre-selected when creating new task attempts and follow-ups.

1. **Select an agent** (e.g., Claude Code, Gemini CLI, Codex)
2. **Choose a variant** if available (e.g., Default, Plan, Router)

<Tip>
You can override the default agent configuration per attempt in the create attempt dialog.
</Tip>

## Editor Integration

Configure integration with your preferred code editor (e.g., VS Code) for a seamless development workflow.

## GitHub Integration

Link your GitHub account by providing a Personal Access Token (PAT) to enable:

- Automatic branch management
- Pull request creation and tracking
- Status synchronisation directly from Vibe Kanban

## Notifications

Toggle sound effects and push notifications to stay informed about task status changes.

## Telemetry

Enable or disable telemetry data collection to help improve Vibe Kanban.

## Task Templates

Manage global task templates to accelerate task creation across all projects. Templates allow you to define reusable titles and descriptions for common tasks.

<Card title="Learn more about task templates" icon="clone" href="/configuration-customisation/creating-task-templates">
  Complete guide to creating and managing task templates
</Card>

## Agent Settings (Profiles & Variants)

Define and customise agent variants under **Settings → Agents**. Variants let you maintain multiple configurations for the same agent (for example, a Claude Code "PLAN" variant).

<Card title="Agent Profiles & Variants" icon="robot" href="/configuration-customisation/agent-configurations">
  Detailed guide with examples for configuring agent variants
</Card>

## Safety & Disclaimers

Manage acknowledgments and reset options for onboarding, safety disclaimers, and telemetry notices.

- **Onboarding**: Reset the onboarding process to rerun the initial setup.
- **Safety Disclaimer**: Reset or review the safety disclaimer prompt.
- **Telemetry Notice**: Reset or review the telemetry data collection acknowledgment.
</file>

<file path="docs/configuration-customisation/keyboard-shortcuts.mdx">
---
title: "Keyboard Shortcuts"
description: "Complete guide to keyboard shortcuts for efficient navigation and interaction in Vibe Kanban"
---

Vibe Kanban provides comprehensive keyboard shortcuts to help you navigate and interact with the application efficiently. These shortcuts are organised by scope and context.

## Platform Keys

- **⌘** = Mac Command key
- **Ctrl** = Windows/Linux Control key

## Global Shortcuts

These shortcuts work anywhere in the application (unless you're typing in an input field):

| Shortcut | Action | Description |
|----------|--------|-------------|
| `C` | Create Task | Opens the Task Form dialog to create a new task |
| `⌘/Ctrl + S` | Focus Search | Puts keyboard focus into the global search input |

## Board Navigation

These shortcuts work when a task card is highlighted with the blue focus ring:

| Shortcut | Action | Description |
|----------|--------|-------------|
| `↑` / `↓` | Vertical Navigation | Move focus to previous/next task in the current column |
| `←` / `→` | Horizontal Navigation | Move focus to first task in previous/next non-empty column |
| `Enter` / `Space` | Open Task | Open the Task Details panel for the focused task |
| `Backspace` | Delete Task | Delete the currently focused task |

## Form Submission

These shortcuts work in various forms and dialogs:

| Shortcut | Action | Context |
|----------|--------|---------|
| `⌘/Ctrl + Enter` | Submit | Create & Start (Task Form), Save (Template Editor) |
| `⌘/Ctrl + Enter` | Send | Send follow-up message in Task Details |
| `⌘/Ctrl + Enter` | Save Comment | Add or save comment in Diff Review |
| `Escape` | Cancel/Clear | Clear draft in follow-up, cancel comment editing |

## Search

| Shortcut | Action | Description |
|----------|--------|-------------|
| `⌘/Ctrl + S` | Focus Search | Move cursor to the search input field |
| `Escape` | Clear Search | Clear search text and remove focus (when in search field) |

*Note: The search field displays a "⌘S" badge to help you discover this shortcut.*

## Tips

- **Focus Indicators**: Look for blue focus rings around task cards to know when board navigation shortcuts are active
- **Input Fields**: Global shortcuts are disabled when typing in text fields to avoid conflicts
- **Visual Cues**: Some shortcuts display badges or hints in the interface (like the "⌘S" next to search)
- **Modifier Keys**: Global shortcuts are disabled when holding Ctrl/⌘/Alt unless the shortcut specifically uses those modifiers
</file>

<file path="docs/core-features/creating-projects.mdx">
---
title: "Creating Projects"
description: "Learn how to create and configure projects in Vibe Kanban"
---

Before you can create tasks and execute coding agents, you must create a project.

<Frame>
<img src="/images/vk-create-project.png" alt="Create project dialog showing options to create from existing git repository or blank project" />
</Frame>

## Creating Your Project
Click the **Create Project** button to choose from two options:
- **From existing git repository**: Browse your file system and select from a list of git repositories sorted by recent activity
- **Create blank project**: Generate a new git repository from scratch
Each project represents a git repository. After creation, you can configure it with setup scripts, dev server scripts, and other settings.


<Note>
After creating a project, you need to press the settings button in the top right to configure project scripts and settings.
</Note>

## Project Settings

Once you've created a project, you can access the project settings by clicking the settings button in the top right corner. From here, you can configure various aspects of your project.

### Setup Scripts

Setup scripts will be run before the coding agent is executed. This is useful for installing dependencies, for example you might run `npm install` or `cargo build`. This will save you time as your agent won't need to figure out that these commands haven't already been run.

<Note>
Each time a coding agent is executed it runs in a [git worktree](https://git-scm.com/docs/git-worktree) which is unlikely to contain your dependencies, configs, .env etc.
</Note>

### Dev Server Scripts

The dev server script is run whenever you press the "run dev server" button. It's useful for quickly reviewing work after a coding agent has run.

### Copy Files

Comma-separated list of files to copy from the original project directory to the worktree. These files will be copied after the worktree is created but before the setup script runs. Useful for environment-specific files like `.env`, configuration files, and local settings.

<Warning>
Make sure these files are gitignored or they could get committed!
</Warning>

### Project Task Templates

From project settings, you can also configure project-specific task templates. For more details about this feature, see the [project task templates section](/configuration-customisation/creating-task-templates#project-task-templates).

### Cleanup Scripts

Cleanup scripts run after a coding agent finishes. You can use these to tidy up the workspace, remove temporary files, or perform any post-execution cleanup. For example, you might run:

```bash
npm run clean
```
</file>

<file path="docs/core-features/creating-task-attempts.mdx">
---
title: "Creating Task Attempts"
description: "Understand when and why to create multiple task attempts for fresh restarts with different configurations."
sidebarTitle: "Creating Task Attempts"
---

A task attempt represents a single execution of a coding agent against a task. Most tasks only need one attempt, but you may need additional attempts for fresh restarts.

## When to Create New Task Attempts

Create a new task attempt when you want to:

- **Start from scratch** with a different approach after an unsuccessful attempt
- **Try a different coding agent** (e.g., switching from Claude to Codex)
- **Use a different agent profile or variant** for specialised behaviour
- **Work from a different base branch** to incorporate recent changes
- **Reset the conversation context** for a completely fresh start

<Tip>
Most users will only need one attempt per task. Only create additional attempts if the first approach didn't work as expected.
</Tip>

## Creating Additional Attempts

To create a new task attempt for an existing task:

<Steps>
<Step title="Navigate to the task">
  Open the task that needs a fresh attempt.
</Step>

<Step title="Click New Attempt">
  In the task interface, click the **New Attempt** button.
</Step>

<Step title="Configure the attempt">
  Choose your agent profile, variant, and base branch. These can be different from previous attempts.
</Step>

<Step title="Start execution">
  Click **Create Attempt** to begin a fresh execution with the new configuration.
</Step>
</Steps>

## Impact on Subtasks

<Warning>
Creating new task attempts affects subtasks. Subtasks are linked to specific task attempts, not tasks themselves.
</Warning>

When you create a new task attempt:

- **Existing subtasks** remain linked to their original parent attempt
- **New subtasks** created from the new attempt will use the new attempt's branch as their base
- This allows you to maintain different subtask workflows for different approaches

<Info>
For more details about how subtasks work with task attempts, see [Creating Subtasks](/core-features/subtasks).
</Info>

## What Happens Next

After creating a task attempt:

1. **Setup scripts run** automatically (if configured in project settings)
2. **Agent executes** using your task title and description
3. **Real-time monitoring** shows progress through streaming logs
4. **Follow-up questions** can be asked to refine results

For detailed task management after execution, including development server controls, git operations, and monitoring features, see [Task Details Full Screen Mode](/core-features/task-details-full-screen).

## Git Worktrees

Vibe Kanban uses Git worktrees to create isolated environments for each task attempt. These environments are ephemeral and automatically cleaned up after execution completes.

<Info>
Worktrees ensure task attempts don't interfere with each other or your main working directory.
</Info>
</file>

<file path="docs/core-features/creating-tasks.mdx">
---
title: "Creating Tasks"
description: "Learn how to create and manage tasks on your kanban board, including using templates, starting coding agents, and understanding task states"
---

<Frame>
<img src="/images/screenshot-create-task.png" alt="Task creation interface showing Add Task button and form fields" />
</Frame>

After creating a project, add tasks by clicking the **plus (+) icon** in the top right of your project kanban page, or by using the keyboard shortcut **`c`**. Creating a task adds it to your kanban board without automatically starting a coding agent.

## Using Templates

<Frame>
<img src="/images/screenshot-create-task-template-dropdown.png" alt="Template dropdown menu showing available global and project-specific templates" />
</Frame>

When adding a task, you can select from your saved templates:

1. Click the **Use a template** dropdown
2. Choose from available global or project templates
3. The template automatically populates the task title and description

<Note>
Templates save time by reusing common task structures. Learn more about creating templates in the [Task Templates](/configuration-customisation/creating-task-templates) guide.
</Note>

## Starting Your First Task Attempt

<Frame>
<img src="/images/screenshot-create-task-attempt.png" alt="Task attempt creation dialog showing agent profile and variant selection options" />
</Frame>

To activate a coding agent on your task, create a task attempt:

<Steps>
<Step title="Select your agent profile">
  Choose from available agents (e.g., CLAUDE_CODE, GEMINI, CODEX). Your default configuration from Settings is pre-selected.
</Step>

<Step title="Choose a variant">
  If your selected agent has variants, pick the appropriate one (e.g., DEFAULT, PLAN).
</Step>

<Step title="Set the base branch">
  Specify which branch the agent should work from. Your current branch is selected by default.
</Step>
</Steps>

<Tip>
Use **Create & Start** to add the task and immediately create a task attempt with your default settings in one action.
</Tip>

### What Happens During Execution

1. **Setup Scripts Run**: Any setup script defined in your project settings runs automatically
2. **Agent Execution**: The coding agent processes your task using the title and description  
3. **Real-time Monitoring**: Watch progress through streaming logs in the task interface
4. **Follow-up Questions**: Continue the conversation after execution to refine results

Most tasks only need a single attempt. You'll only need additional attempts if you want to restart from scratch with different settings.

<Note>
For detailed task management after execution, see [Task Details Full Screen Mode](/core-features/task-details-full-screen). To understand when you might need multiple attempts, see [Creating Task Attempts](/core-features/creating-task-attempts).
</Note>

## Creating Tasks via MCP Clients

<Warning>
This is not the typical method for creating tasks but can be valuable for bulk task creation or migrating from other systems.
</Warning>

Tasks can also be created programmatically using coding agents or MCP (Model Context Protocol) clients such as Claude Desktop or Raycast. This approach is particularly useful for:

- **Bulk task creation** based on existing data or project specifications
- **Migration from other systems** like Linear, GitHub Issues, or Jira
- **Automated task generation** from project plans or requirements documents

<Tip>
For detailed setup instructions and examples, see the [Vibe Kanban MCP Server](/integrations/vibe-kanban-mcp-server) documentation.
</Tip>

### Example MCP Task Creation

Once configured with an MCP client, you can create multiple tasks from a project description:

```
I need to implement user authentication with:
- Email/password registration
- Login with session management  
- Password reset functionality
- Email verification
- Protected route middleware

Please create individual tasks for each component.
```

The MCP client will automatically generate structured tasks in your Vibe Kanban project based on this description.

## Understanding Task Columns

Tasks begin in the "To do" column and move automatically based on their progress:

| Action | Column |
|--------|---------|
| Task created | To do |
| Task attempt started | In Progress |
| Task attempt completed (success or failure) | In Review |
| Task attempt merged | Done |
| PR merged on GitHub | Done |

<Info>
You can manually drag tasks between columns, but this won't trigger any functionality. Task movement is primarily driven by coding agent actions and GitHub integration (which polls every 60 seconds).
</Info>
</file>

<file path="docs/core-features/resolving-rebase-conflicts.mdx">
---
title: "Resolving Rebase Conflicts"
description: "Learn how to handle rebase conflicts when your base branch has advanced, using either manual resolution or automatic conflict resolution with coding agents."
sidebarTitle: "Resolving Rebase Conflicts"
---

## When You See "Rebase Conflicts"

After clicking the rebase button, if your changes conflict with the base branch, your task status changes to "Rebase conflicts" and a conflict resolution banner appears.

<Frame>
<img src="/images/vk-rebase-conflicts-actions-pane.png" alt="Task showing rebase conflicts status with conflict resolution options" />
</Frame>

The conflict banner provides three options to resolve the situation:

<Frame>
<img src="/images/vk-rebase-conflicts-banner.png" alt="Conflict resolution banner showing the three available options" />
</Frame>

- **Open in Editor** - Manually edit conflicted files
- **Insert Resolve-Conflicts Instructions** - Auto-generate resolution instructions for the coding agent
- **Abort Rebase** - Cancel and return to previous state

## Resolving Conflicts Automatically

The simplest solution is to let the coding agent resolve conflicts automatically:

1. Click **Insert Resolve-Conflicts Instructions** from the conflict banner to generate specific instructions tailored to your conflict situation and insert them into the follow-up message area.

2. Review the generated instructions and click **Send** to have the agent analyse the conflicted files and complete the rebase automatically.

<Frame>
<img src="/images/vk-rebase-conflicts-prompt-zoom.png" alt="Conflict resolution banner with auto-generated instructions in the follow-up field" />
</Frame>

Once the agent completes the resolution, your task status will show *n* commits ahead and the **Merge** button becomes available again.

## Manual Resolution (Alternative)

If you prefer to resolve conflicts manually, you have two options:

**For single files:** Use **Open in Editor** from the conflict banner to edit one conflicted file at a time. After resolving and refreshing the page, you can press the button again for the next file.

**For multiple files (recommended):** Use the **Open in [Your IDE]** button from the Actions pane to open all worktree files in your chosen IDE, where you can resolve all conflicts at once.

<Steps>
<Step title="Open your IDE">
  Click **Open in [Your IDE]** from the Actions pane to access all worktree files, or use **Open in Editor** from the banner for individual files.
</Step>

<Step title="Edit conflicted files">
  Resolve merge markers in each file:
  
  ```diff
  <<<<<<< HEAD (your changes)
  function newFeature() {
    return "new implementation";
  }
  =======
  function oldFeature() {
    return "existing implementation";
  }
  >>>>>>> main (base branch changes)
  ```
</Step>

<Step title="Continue the rebase">
  After editing all conflicts, stage and continue:
  
  ```bash
  git add .
  git rebase --continue
  ```
</Step>
</Steps>

## Aborting a Rebase

If you need to cancel the rebase entirely, click **Abort Rebase** to return to the "Rebase needed" state. You can then try rebasing again or create a new task attempt from the updated base branch.

<Tip>
Automatic resolution works best for most conflicts. Use manual resolution only when you need precise control over the merge decisions.
</Tip>
</file>

<file path="docs/core-features/reviewing-code-changes.mdx">
---
title: "Reviewing Code Changes"
description: "Learn how to review and provide feedback on code changes made by coding agents"
---

When a coding agent completes a task, it automatically moves to the **In Review** column. This is where you can examine the changes, provide feedback, and ensure the implementation meets your requirements.

<video
  controls
  className="w-full aspect-video rounded-xl"
  src="https://vkcdn.britannio.dev/vk-code-review-2.mp4"
></video>

## Opening the Code Review Interface

<Steps>
<Step title="Access the task">
  Click on any task in the **In Review** column to open it in full screen mode.
</Step>

<Step title="Navigate to the Diffs tab">
  Once the task is open, click on the **Diffs** tab to view all the code changes made by the agent.
</Step>
</Steps>

## Adding Review Comments

### Line-Specific Comments

To provide feedback on specific lines of code:

<Steps>
<Step title="Locate the line">
  Find the line you want to comment on in the diffs view.
</Step>

<Step title="Add a comment">
  Click the **plus icon** (+) at the beginning of the line to create a review comment.

  <Frame>
  <img src="/images/add-line-comment.png" alt="Plus icon for adding line comments" />
  </Frame>
</Step>

<Step title="Write your feedback">
  Enter your comment in the text field that appears. You can provide suggestions, ask questions, or request changes.
</Step>
</Steps>

### Multiple Comments Across Files

You can create several review comments across different files in the same review:

- Add comments to multiple lines within a single file
- Switch between different changed files and add comments to each
- All comments will be collected and submitted together as part of your review

<Note>
Review comments are not submitted individually. They are collected and sent as a complete review when you submit your feedback.
</Note>

## General Review Comments

In addition to line-specific feedback, you can provide general comments about the entire implementation:

<Steps>
<Step title="Use the task chat">
  Navigate to the task chat field at the bottom of the review interface.
</Step>

<Step title="Add general feedback">
  Enter broader comments about the implementation, such as:
  - Overall code quality assessment
  - Architectural concerns
  - Performance considerations
  - Testing requirements
</Step>
</Steps>

## Submitting Your Review

<Steps>
<Step title="Review all comments">
  Before submitting, review all the line-specific and general comments you've added.
</Step>

<Step title="Submit the review">
  Click the **Send** button to send all your feedback to the coding agent.

  <Info>
  All comments (line-specific and general) are combined into a single message for the coding agent to address.
  </Info>
</Step>

<Step title="Task moves back to In Progress">
  Once submitted, the task returns to the **In Progress** column where the agent will address your feedback and implement the requested changes.
</Step>
</Steps>
</file>

<file path="docs/core-features/subtasks.mdx">
---
title: "Creating Subtasks"
description: "Learn how to create and manage subtasks to break down complex work into smaller, manageable pieces"
---

Subtasks allow you to break down complex tasks into smaller, more manageable pieces. Each subtask is linked to a specific task attempt and inherits the same project and branch context.

## Creating Subtasks

<Frame>
<img src="/images/screenshot-create-subtask-button.png" alt="Current attempt toolbar showing the Create Subtask button with GitFork icon" />
</Frame>

To create a subtask from an existing task attempt:

<Steps>
<Step title="Navigate to the task attempt">
  Open the task you want to create subtasks for.
</Step>

<Step title="Click Create Subtask">
  In the actions toolbar, click the **Create Subtask** button (marked with a fork icon).
</Step>

<Step title="Fill in subtask details">
  The task creation dialog opens with the parent task attempt and base branch automatically set. Add your subtask title and description.
</Step>

<Step title="Save the subtask">
  Click **Save** to create the subtask. It will appear as a new task on your kanban board.
</Step>
</Steps>

<Note>
When you create a subtask, it automatically inherits the base branch from its parent task attempt, ensuring consistency in your development workflow.
</Note>

## Viewing Tasks with Subtasks

<Frame>
<img src="/images/screenshot-task-with-subtasks.png" alt="Task view showing a parent task with its associated subtasks listed in the Task Relationships panel" />
</Frame>

When viewing a parent task, you can see its subtasks in the **Task Relationships** panel. This collapsible section shows:

- **Child Tasks** with a count (e.g., "CHILD TASKS (1)")
- Individual subtask titles with links to view them
- Easy navigation between parent and child tasks

This helps you track progress across all related work items and understand the task hierarchy at a glance.

## Viewing Subtask Details

<Frame>
<img src="/images/screenshot-subtask-parent-info.png" alt="Subtask detail view showing parent task information in the Task Relationships panel" />
</Frame>

When viewing a subtask, the **Task Relationships** panel displays:

- **Parent Task** section showing the parent task title
- Direct link to navigate to the parent task
- Clear visual indication that this is a child task
- Context about the parent-child relationship

The subtask also shows its own **Create Subtask** button, allowing you to create nested subtasks if needed.

## How Subtasks Work

Subtasks in Vibe Kanban follow these key principles:

### Git Branching Workflow

Subtasks create their own feature branches that can work independently while maintaining connection to the parent task:

```mermaid
gitGraph
    commit id: "main"
    branch feature/parent-task
    checkout feature/parent-task
    commit id: "Parent Task Start"
    commit id: "Initial work"

    branch feature/subtask-1
    checkout feature/subtask-1
    commit id: "Subtask 1: Backend API"
    commit id: "API implementation"
    commit id: "API tests"

    checkout feature/parent-task
    branch feature/subtask-2
    checkout feature/subtask-2
    commit id: "Subtask 2: Frontend UI"
    commit id: "Component creation"
    commit id: "UI styling"

    checkout feature/parent-task
    branch feature/subtask-3
    checkout feature/subtask-3
    commit id: "Subtask 3: Integration"
    commit id: "Connect API to UI"

    checkout feature/parent-task
    merge feature/subtask-1
    merge feature/subtask-2
    merge feature/subtask-3
    commit id: "Parent Task Complete"

    checkout main
    merge feature/parent-task
```

### Parent-Child Relationships

- Subtasks are linked to specific **task attempts**, not just tasks
- Each subtask knows which attempt created it
- Multiple subtasks can be created from the same parent attempt

### Branch Inheritance

- Subtasks automatically inherit the base branch from their parent attempt
- This ensures subtasks work within the same development context
- You can modify the branch when creating the subtask if needed

### Independent Task Lifecycle

- Subtasks appear as regular tasks on your kanban board
- Each subtask has its own lifecycle (To do → In Progress → In Review → Done)
- Subtasks can have their own task attempts and coding agents
</file>

<file path="docs/core-features/task-details-full-screen.mdx">
---
title: "Task Details Full Screen Mode"
sidebarTitle: "Full Screen Mode"
description: "Complete guide to using the task details full screen mode for development workflows, including dev server, git operations, logs, diffs, and process monitoring"
---

## Accessing Full Screen Mode

Click the **Maximise** button (⛶) in the task details header to expand the view to occupy the full right side of your screen.

<Frame>
<img src="/images/vk-task-detail.jpeg" alt="Task details in full screen mode showing the complete interface with sidebar and tabs" />
</Frame>

## Interface Layout

Full screen mode uses a two-column layout:

### Left Sidebar
- **Task Information**: Title, description, and metadata
- **Development Toolbar**: Primary development actions
- **TODOs**: Task checklist and progress tracking  
- **Relationships**: Subtasks and parent task links

### Right Panel
- **Tab Navigation**: Switch between Logs, Diffs, and Processes
- **Content Area**: Active tab content with real-time updates
- **Follow-up Section**: Additional context and actions

## Core Features

### Development Server

Start and manage your project's development environment directly from the task interface.

**Controls:**
- **Dev** (▶️): Start the development server using your project's configured dev script
- **Stop Dev** (⏹️): Terminate the running development server
- **View Logs** (📄): Jump to process logs to monitor server output

The development server runs in a git worktree environment and streams all output to the Processes tab. Click the logs button next to the dev server control to jump directly to the process logs for real-time monitoring.

### Git Operations

#### Rebase
Keep your task branch up-to-date with the latest changes from the base branch.

- **Automatic Detection**: The rebase button appears when your branch is behind the base branch

For detailed guidance on handling rebase conflicts, see [Resolving Rebase Conflicts](/core-features/resolving-rebase-conflicts).

#### Merge
Integrate your completed work back into the base branch.

- **Availability**: Disabled when an open pull request exists
- **Success Indication**: A "Merged!" notification confirms successful integration

#### Push & Pull Request Management
- **Create PR**: Opens pull request creation dialog for new branches
- **Push to PR**: Updates existing pull request with latest changes
- **Auto-detection**: Automatically detects existing PRs and adjusts button behavior

### Task Management

#### History & Attempts
View and manage all task attempts from the full screen interface.

- **History**: View all previous attempts for this task, review execution status and outcomes, and switch between different attempts to compare results
- **New Attempt**: Create additional attempts when previous approaches didn't work as expected or when testing different implementation strategies

<Info>
For detailed information about when and why to create new attempts, see [Creating Task Attempts](/core-features/creating-task-attempts).
</Info>

#### Subtasks
Break down complex tasks into manageable pieces.

- **Create Subtask** (🔀): Opens task creation form with:
  - Parent task automatically linked
  - Initial base branch set to the current attempt's branch
  - Subtasks link to specific task attempts, not tasks themselves

<Warning>
Creating new task attempts affects existing subtasks, as they remain linked to their original parent attempt.
</Warning>

Learn more in [Creating Subtasks](/core-features/subtasks).

### Monitoring & Analysis

#### Logs Tab
The Logs tab provides real-time streaming of coding agent activities and reasoning. You can see the agent's thought process, actions being taken, and responses as tasks are executed. Process events from development servers, builds, and script execution are also logged here.

<Frame>
<img src="/images/vk-logs.png" alt="Logs tab showing real-time streaming of agent output and process events" />
</Frame>

#### Diffs Tab  
The Diffs tab provides a visual representation of all code changes made during task execution. It shows a git diff between your branch and the base branch, with syntax highlighting and language-aware code formatting. You can see a summary of added, modified, and deleted files with precise line-by-line change tracking and context.

<Frame>
<img src="/images/vk-diffs.png" alt="Diffs tab displaying code changes with syntax highlighting and line-by-line comparison" />
</Frame>

Learn more in [Reviewing Code Changes](/core-features/reviewing-code-changes).

#### Processes Tab
The Processes tab displays a timeline view of all running and completed processes. You can track the status of running, completed, failed, and stopped processes with their complete execution timeline. Click any process to view its specific output logs, and use the interactive controls to stop running processes directly from the interface.

<Frame>
<img src="/images/vk-processes.png" alt="Processes tab showing timeline of running and completed processes with status tracking" />
</Frame>

## Navigation & Controls

### Keyboard Shortcuts
- **Escape**: Exit full screen mode and return to sidebar view
- **n**: Create a new task attempt



## Exiting Full Screen Mode

- Click the **Minimise** button in the header
- Press **Escape** key
- Click the **×** button to close the task panel entirely
- Navigate to a different task or project

The interface gracefully transitions back to the compact sidebar view while preserving your work context.

## Related Documentation

- [Creating Task Attempts](/core-features/creating-task-attempts) - Learn about task attempt lifecycle
- [Reviewing Code Changes](/core-features/reviewing-code-changes) - Deep dive into diff analysis  
- [Resolving Rebase Conflicts](/core-features/resolving-rebase-conflicts) - Handle rebase conflicts and merge issues
- [Creating Tasks](/core-features/creating-tasks) - Task creation and management
- [Agent Configurations](/configuration-customisation/agent-configurations) - Customise AI agent behaviour
</file>

<file path="docs/integrations/github-integration.mdx">
---
title: "GitHub Integration"
description: "Connect to GitHub to unlock seamless branch management and pull request workflows directly from your kanban board"
---

Connect your GitHub account to push commits and create pull requests directly from task attempts.

## See It In Action

### Pull Request Creation Dialog

<Frame>
<img src="/images/vk-create-pr.png" alt="Pull request creation dialog with title, description, and base branch selection" />
</Frame>

When you're ready to create a PR, get a streamlined dialog that:
- Pre-fills title and description from your task details
- Lets you select the target branch (defaults to main/master)
- Validates that your branch is ready to go
- Provides direct links to your new pull request

### Smart Push/PR Button

<Frame>
<img src="/images/task-attempt-push-button.png" alt="Task attempt interface showing dynamic push/PR button" />
</Frame>

Each task attempt gets a smart button that adapts to your current state:
- **"Create PR"** when ready to share your work
- **"Push X commits"** when you have changes to upload  
- **"Push to PR"** when updating an existing pull request
- **"Pushed!"** confirmation after successful operations

<Note>
Each task attempt automatically creates an isolated git worktree, so you can work on multiple features simultaneously without conflicts.
</Note>

## Getting Started

### Connect Your GitHub Account

<Frame>
<img src="/images/github-connect-settings.png" alt="Settings page with Connect GitHub Account button" />
</Frame>

Click **Connect GitHub Account** in General Settings to authenticate with GitHub using device flow authentication.

<Frame>
<img src="/images/github-device-code.png" alt="GitHub device authentication dialog with verification code" />
</Frame>

The authentication process is simple:
1. Click **Sign in with GitHub** 
2. Visit the provided GitHub URL in your browser
3. Enter the device code shown in the dialog
4. Authorize Vibe Kanban in your browser
5. Return to complete the setup

**Required Permissions**: Vibe Kanban needs access to your repositories (`repo`), basic profile info (`read:user`), and email address (`user:email`) for Git commits.

<Note>
If you encounter permission issues when creating pull requests, you'll be prompted to provide a Personal Access Token as a fallback authentication method.
</Note>

### Start Working with GitHub

<Steps>
<Step title="Create or Import Projects">
[Create projects](/core-features/creating-projects) from existing Git repositories to enable GitHub features.
</Step>

<Step title="Work on Tasks">
[Create task attempts](/core-features/creating-task-attempts) to start developing in isolated branches.
</Step>

<Step title="Push and Create PRs">
Use the dynamic push/PR button in task attempts to share your work on GitHub.
</Step>
</Steps>

## Next Steps

With GitHub connected, you can:
- [Learn about creating projects](/core-features/creating-projects)
- [Set up task attempts for feature development](/core-features/creating-task-attempts) 
- [Review code changes before committing](/core-features/reviewing-code-changes)
</file>

<file path="docs/integrations/mcp-server-configuration.mdx">
---
title: "Connecting MCP Servers"
description: "Configure MCP (Model Context Protocol) servers to enhance your coding agents within Vibe Kanban with additional tools and capabilities."
---


<Note>
This page covers configuring MCP servers **within** Vibe Kanban for your coding agents. For connecting external MCP clients to Vibe Kanban's MCP server, see the [Vibe Kanban MCP Server](/integrations/vibe-kanban-mcp-server) guide.
</Note>

## Overview

MCP servers provide additional functionality to coding agents through standardized protocols. You can configure different MCP servers for each coding agent in Vibe Kanban, giving them access to specialized tools like browser automation, access to remote logs, error tracking via Sentry, or documentation from Notion.

## Accessing MCP Server Configuration

1. Navigate to **Settings** in the Vibe Kanban interface
2. Click on **MCP Servers** in the settings menu
3. Select the coding agent you want to configure MCP servers for

<Frame>
<img src="/images/vk-mcp-server-config.jpeg" alt="MCP Server configuration page showing available servers and one-click installation options" />
</Frame>

## Popular MCP Servers

Vibe Kanban provides one-click installation for popular MCP servers.

<Frame>
<img src="/images/vk-popular-mcp-servers.png" alt="Popular MCP servers available for one-click installation" />
</Frame>

## Adding Custom MCP Servers

You can also add your own MCP servers by configuring them manually:

<Steps>
<Step title="Select Coding Agent">
Choose the coding agent you want to configure MCP servers for from the MCP Servers settings page.
</Step>

<Step title="Update Server Configuration JSON">
In the Server Configuration JSON field displayed in the UI, add your custom MCP server configuration. The JSON will show the current configuration for the selected agent, and you can modify it to include additional servers.

Example addition:
```json
{
  "mcpServers": {
    "existing_server": {
      "command": "npx",
      "args": ["-y", "some-existing-server"]
    },
    "my_custom_server": {
      "command": "node",
      "args": ["/path/to/my-server.js"]
    }
  }
}
```
</Step>

<Step title="Save and Test">
After updating the JSON configuration:

1. Click "Save Settings" to apply changes
2. Test the configuration by using the agent with MCP functionality
3. Check agent logs for any connection issues

<Check>
These changes update the global configuration file of the coding agent and will persist even if you stop using Vibe Kanban.
</Check>
</Step>
</Steps>

## Best Practices

<Tip>
**Server Selection**: Choose MCP servers that complement your coding agent's primary function. For example, use Playwright for agents focused on web development.
</Tip>

<Tip>
**Limit MCP Servers**: Avoid adding too many MCP servers to a single coding agent. Too many servers and tools will degrade the effectiveness of coding agents by overwhelming them with options.
</Tip>

## Next Steps

- Explore the [Agent Configurations](/configuration-customisation/agent-configurations) guide for advanced agent setup
- Learn about [Creating Tasks](/core-features/creating-tasks) with enhanced MCP capabilities
- Check out [Supported Coding Agents](/supported-coding-agents) for agent-specific features
</file>

<file path="docs/integrations/vibe-kanban-mcp-server.mdx">
---
title: "Vibe Kanban MCP Server"
description: "Configure the Vibe Kanban MCP server"
---

Vibe Kanban exposes a local MCP (Model Context Protocol) server, allowing you to manage projects and tasks from external MCP clients like Claude Desktop, Raycast, or even coding agents running within Vibe Kanban itself.

<Note>
This page covers connecting **external MCP clients** to Vibe Kanban's MCP server. For configuring MCP servers **within** Vibe Kanban for your coding agents, see the [MCP Server Configuration](/integrations/mcp-server-configuration) guide.
</Note>

<Info>
Vibe Kanban's MCP server is **local-only** - it runs on your computer and can only be accessed by applications installed locally. It cannot be accessed via publicly accessible URLs.
</Info>

<video
  controls
  className="w-full aspect-video rounded-xl"
  src="https://vkcdn.britannio.dev/vk-mcp-server.mp4"
></video>

## Setting Up MCP Integration

### Option 1: Using the Web Interface

This works if you're adding the Vibe Kanban MCP server to any [supported coding agent](/supported-coding-agents) **within** Vibe Kanban.

1. In Vibe Kanban Settings, navigate to the "MCP Servers" page
2. In the "Popular servers" section, click on the Vibe Kanban card
3. Click the `Save Settings` button

<Frame>
<img src="/images/vk-mcp-server-config.jpeg" alt="MCP Servers configuration page showing how to add Vibe Kanban MCP server" />
</Frame>

### Option 2: Manual Configuration

You can manually add the MCP server to your coding agent's configuration. Add the following configuration to your agent's MCP servers configuration:

```json
{
  "mcpServers": {
    "vibe_kanban": {
      "command": "npx",
      "args": ["-y", "vibe-kanban", "--mcp"]
    }
  }
}
```

## Using MCP with Coding Agents

Once you have the MCP server configured, you can leverage it to streamline your project planning workflow:

1. **Plan Your Work**: Describe a large feature or project to your coding agent
2. **Request Task Creation**: At the end of your task description, simply add "then turn this plan into tasks"
3. **Automatic Task Generation**: Your coding agent will use the Vibe Kanban MCP server to automatically create structured tasks in your project

## Example Usage

### External Coding Agents

```
I need to build a user authentication system with the following features:
- User registration with email validation
- Login/logout functionality
- Password reset capability
- Session management
- Protected routes

Then turn this plan into tasks.
```

The coding agent will break this down into individual tasks and add them to your Vibe Kanban project automatically.

### Internal Coding Agents (Within Vibe Kanban)

A powerful workflow involves using coding agents within Vibe Kanban that are also connected to the Vibe Kanban MCP server:

1. **Create a Planning Task**: Create a task in "plan mode" within Vibe Kanban
2. **Explore and Plan**: The coding agent explores the codebase and develops a comprehensive plan
3. **Generate Sub-Tasks**: Ask the coding agent to "create a series of individual tasks for this plan"
4. **Automatic Population**: The agent uses the MCP server to populate individual tasks directly in the Vibe Kanban interface

This creates a seamless workflow where high-level planning automatically generates actionable tasks in your project board.

## Installation Instructions for MCP Clients

### Raycast Example

Raycast is a popular MCP client that can connect to Vibe Kanban's MCP server. Here's how to configure it:

For complete Raycast MCP configuration details, see the [official Raycast MCP documentation](https://manual.raycast.com/model-context-protocol).

<Tabs>
<Tab title="Step 1: Open MCP Server Installer">
<Frame>
<img src="/images/vk-raycast-mcp-part-1.png" alt="Raycast MCP configuration - adding Vibe Kanban server" />
</Frame>

Configure the Vibe Kanban MCP server in Raycast by adding the server details.
</Tab>

<Tab title="Step 2: Supply Command">
<Frame>
<img src="/images/vk-raycast-mcp-part-2.png" alt="Raycast MCP configuration - server successfully added" />
</Frame>

Once configured, you'll see the Vibe Kanban MCP server listed and ready to use in Raycast.
</Tab>
</Tabs>

<Note>
Similar configuration steps apply to other MCP clients like Claude Desktop, VS Code with MCP extensions, or any custom MCP client implementations.
</Note>
</file>

<file path="docs/integrations/vscode-extension.mdx">
---
title: "VSCode Extension Integration"
description: "Complete guide to using the Vibe Kanban extension with VSCode, Cursor, and Windsurf"
---

The Vibe Kanban VSCode extension brings task management directly into your IDE, providing seamless integration with logs, diffs, and process monitoring. This extension works with VSCode and popular forks including Cursor and Windsurf.

<video
  controls
  className="w-full aspect-video rounded-xl"
  src="https://vkcdn.britannio.dev/vk-ide-extension-light.mp4"
></video>

## Installation

<Tabs>
<Tab title="VSCode">
Install directly from the Visual Studio Code Marketplace:

<Card title="VSCode Marketplace" icon="download" href="https://marketplace.visualstudio.com/items?itemName=bloop.vibe-kanban">
  Install the official Vibe Kanban extension for VSCode
</Card>

Alternatively, search for the extension ID in VSCode:
1. Open VSCode
2. Press `Ctrl+Shift+X` (Windows/Linux) or `Cmd+Shift+X` (macOS) to open Extensions
3. Search for `@id:bloop.vibe-kanban`
4. Click **Install**
</Tab>

<Tab title="Cursor">
For Cursor, use the Open VSX Registry:

<Card title="Open VSX Registry" icon="download" href="https://open-vsx.org/extension/bloop/vibe-kanban">
  Install from Open VSX for Cursor compatibility
</Card>

<Tip>
Since deeplinking from Open VSX doesn't work reliably in Cursor, the easiest installation method is searching by extension ID within the IDE.
</Tip>

**Installation Steps**:
1. Open Cursor
2. Open the Extensions panel
3. Search for `@id:bloop.vibe-kanban`
4. Install the extension
</Tab>

<Tab title="Windsurf">
For Windsurf, use the Open VSX Registry:

<Card title="Open VSX Registry" icon="download" href="https://open-vsx.org/extension/bloop/vibe-kanban">
  Install from Open VSX for Windsurf compatibility
</Card>

<Tip>
Since deeplinking from Open VSX doesn't work reliably in Windsurf, the easiest installation method is searching by extension ID within the IDE.
</Tip>

**Installation Steps**:
1. Open Windsurf
2. Open the Extensions panel
3. Search for `@id:bloop.vibe-kanban`
4. Install the extension
</Tab>
</Tabs>

## Features

The extension provides an integrated workspace view with three main components:

### Logs View
- List of task attempts for the current task
- Agent steps performed in each task attempt

### Diffs View  
- Side-by-side comparison of code changes
- Inline commenting and review capabilities

### Processes View
- Monitor running task processes
- Process status indicators (running, completed, failed)

### Task Management
- **Task Iteration**: Continue iterating on the current ongoing task
- **Status Updates**: Real-time task status synchronization

## Usage Workflow

<Steps>
<Step title="Start a task in Vibe Kanban">
  Navigate to your project's kanban board and create or select an existing task.
</Step>

<Step title="Open task in full screen">
  Click on the task to open its detailed view, then switch to full screen mode for better visibility.
</Step>

<Step title="Launch IDE integration">
  Click the **"Open in VSCode"**, **"Open in Cursor"**, or **"Open in Windsurf"** button depending on your preferred IDE.
  
  <Note>
  The button text will reflect your editor choice configured in Vibe Kanban settings.
  </Note>
</Step>

<Step title="Work in your IDE">
  Your IDE will open in the task's dedicated worktree with the extension UI populated with:
  - Current task context
  - Real-time logs
  - Code diffs
  - Process monitoring
</Step>
</Steps>

## Troubleshooting

### Empty Extension UI

**Problem**: The extension UI appears empty or shows no task information.

**Solution**: Ensure you're working within a worktree created by a Vibe Kanban task.

<Warning>
The extension can only display task information when VSCode is opened in a directory that corresponds to an active Vibe Kanban task worktree.
</Warning>

**Steps to resolve**:
1. Verify you opened VSCode through the "Open in [IDE]" button from a Vibe Kanban task
2. Check that you're in the correct directory/worktree
3. If the issue persists, restart your IDE and try the workflow again

### Extension Not Loading

**Problem**: The Vibe Kanban extension doesn't appear in your IDE.

**Solutions**:
- Verify the extension is installed by searching `@id:bloop.vibe-kanban` in Extensions
- Restart your IDE after installation
- Check that you're using a compatible IDE version
- For Cursor/Windsurf users, ensure you installed from Open VSX Registry

## Best Practices

<Tip>
**Workflow Optimization**: Always start tasks from the Vibe Kanban web interface before opening in your IDE to ensure proper context and worktree setup.
</Tip>

<Tip>
**Performance**: Close unused IDE windows to reduce resource usage.
</Tip>

## Supported IDEs

| IDE | Status | Installation Method |
|-----|--------|-------------------|
| **VSCode** | ✅ Fully Supported | VSCode Marketplace |
| **Cursor** | ✅ Fully Supported | Open VSX Registry |  
| **Windsurf** | ✅ Fully Supported | Open VSX Registry |

<Info>
The extension is designed to work with any VSCode-compatible editor. If you're using a different VSCode fork, try installing from Open VSX Registry using the extension ID `bloop.vibe-kanban`.
</Info>
</file>

<file path="docs/logo/dark.svg">
<svg width="604" height="74" viewBox="0 0 604 74" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M0 13.6035V0.00976562H7.20117V13.6035H0ZM7.20703 13.6035V0.00976562H14.4082V13.6035H7.20703ZM18.5215 13.6035V6.42578H14.4141V5.56445H19.3828V13.6035H18.5215ZM16.6465 8.30078H14.4141V7.43945H17.5078V13.6035H16.6465V8.30078ZM43.2422 13.6035V0.00976562H50.4434V13.6035H43.2422ZM50.4492 13.6035V0.00976562H57.6504V13.6035H50.4492ZM61.7637 13.6035V6.42578H57.6562V5.56445H62.625V13.6035H61.7637ZM59.8887 8.30078H57.6562V7.43945H60.75V13.6035H59.8887V8.30078ZM64.8633 13.6035V0.00976562H72.0645V13.6035H64.8633ZM72.0703 13.6035V0.00976562H79.2715V13.6035H72.0703ZM83.3848 13.6035V6.42578H79.2773V5.56445H84.2461V13.6035H83.3848ZM81.5098 8.30078H79.2773V7.43945H82.3711V13.6035H81.5098V8.30078ZM86.4844 13.6035V0.00976562H93.6855V13.6035H86.4844ZM93.6914 13.6035V0.00976562H100.893V13.6035H93.6914ZM100.898 13.6035V0.00976562H108.1V13.6035H100.898ZM108.105 13.6035V0.00976562H115.307V13.6035H108.105ZM115.312 13.6035V0.00976562H122.514V13.6035H115.312ZM122.52 13.6035V0.00976562H129.721V13.6035H122.52ZM133.834 13.6035V6.42578H129.727V5.56445H134.695V13.6035H133.834ZM131.959 8.30078H129.727V7.43945H132.82V13.6035H131.959V8.30078ZM144.141 13.6035V0.00976562H151.342V13.6035H144.141ZM151.348 13.6035V0.00976562H158.549V13.6035H151.348ZM158.555 13.6035V0.00976562H165.756V13.6035H158.555ZM165.762 13.6035V0.00976562H172.963V13.6035H165.762ZM172.969 13.6035V0.00976562H180.17V13.6035H172.969ZM180.176 13.6035V0.00976562H187.377V13.6035H180.176ZM187.383 13.6035V0.00976562H194.584V13.6035H187.383ZM198.697 13.6035V6.42578H194.59V5.56445H199.559V13.6035H198.697ZM196.822 8.30078H194.59V7.43945H197.684V13.6035H196.822V8.30078ZM230.625 13.6035V0.00976562H237.826V13.6035H230.625ZM237.832 13.6035V0.00976562H245.033V13.6035H237.832ZM249.146 13.6035V6.42578H245.039V5.56445H250.008V13.6035H249.146ZM247.271 8.30078H245.039V7.43945H248.133V13.6035H247.271V8.30078ZM266.66 13.6035V0.00976562H273.861V13.6035H266.66ZM273.867 13.6035V0.00976562H281.068V13.6035H273.867ZM285.182 13.6035V6.42578H281.074V5.56445H286.043V13.6035H285.182ZM283.307 8.30078H281.074V7.43945H284.168V13.6035H283.307V8.30078ZM295.488 13.6035V0.00976562H302.689V13.6035H295.488ZM302.695 13.6035V0.00976562H309.896V13.6035H302.695ZM309.902 13.6035V0.00976562H317.104V13.6035H309.902ZM317.109 13.6035V0.00976562H324.311V13.6035H317.109ZM324.316 13.6035V0.00976562H331.518V13.6035H324.316ZM335.631 13.6035V6.42578H331.523V5.56445H336.492V13.6035H335.631ZM333.756 8.30078H331.523V7.43945H334.617V13.6035H333.756V8.30078ZM345.938 13.6035V0.00976562H353.139V13.6035H345.938ZM353.145 13.6035V0.00976562H360.346V13.6035H353.145ZM360.352 13.6035V0.00976562H367.553V13.6035H360.352ZM371.666 13.6035V6.42578H367.559V5.56445H372.527V13.6035H371.666ZM369.791 8.30078H367.559V7.43945H370.652V13.6035H369.791V8.30078ZM396.387 13.6035V0.00976562H403.588V13.6035H396.387ZM403.594 13.6035V0.00976562H410.795V13.6035H403.594ZM414.908 13.6035V6.42578H410.801V5.56445H415.77V13.6035H414.908ZM413.033 8.30078H410.801V7.43945H413.895V13.6035H413.033V8.30078ZM418.008 13.6035V0.00976562H425.209V13.6035H418.008ZM425.215 13.6035V0.00976562H432.416V13.6035H425.215ZM432.422 13.6035V0.00976562H439.623V13.6035H432.422ZM439.629 13.6035V0.00976562H446.83V13.6035H439.629ZM446.836 13.6035V0.00976562H454.037V13.6035H446.836ZM454.043 13.6035V0.00976562H461.244V13.6035H454.043ZM465.357 13.6035V6.42578H461.25V5.56445H466.219V13.6035H465.357ZM463.482 8.30078H461.25V7.43945H464.344V13.6035H463.482V8.30078ZM482.871 13.6035V0.00976562H490.072V13.6035H482.871ZM490.078 13.6035V0.00976562H497.279V13.6035H490.078ZM497.285 13.6035V0.00976562H504.486V13.6035H497.285ZM504.492 13.6035V0.00976562H511.693V13.6035H504.492ZM511.699 13.6035V0.00976562H518.9V13.6035H511.699ZM523.014 13.6035V6.42578H518.906V5.56445H523.875V13.6035H523.014ZM521.139 8.30078H518.906V7.43945H522V13.6035H521.139V8.30078ZM533.32 13.6035V0.00976562H540.521V13.6035H533.32ZM540.527 13.6035V0.00976562H547.729V13.6035H540.527ZM547.734 13.6035V0.00976562H554.936V13.6035H547.734ZM559.049 13.6035V6.42578H554.941V5.56445H559.91V13.6035H559.049ZM557.174 8.30078H554.941V7.43945H558.035V13.6035H557.174V8.30078ZM583.77 13.6035V0.00976562H590.971V13.6035H583.77ZM590.977 13.6035V0.00976562H598.178V13.6035H590.977ZM602.291 13.6035V6.42578H598.184V5.56445H603.152V13.6035H602.291ZM600.416 8.30078H598.184V7.43945H601.277V13.6035H600.416V8.30078ZM0 26.6035V13.0098H7.20117V26.6035H0ZM7.20703 26.6035V13.0098H14.4082V26.6035H7.20703ZM18.5215 26.6035V13.0098H19.3828V26.6035H18.5215ZM16.6465 26.6035V13.0098H17.5078V26.6035H16.6465ZM43.2422 26.6035V13.0098H50.4434V26.6035H43.2422ZM50.4492 26.6035V13.0098H57.6504V26.6035H50.4492ZM61.7637 26.6035V13.0098H62.625V26.6035H61.7637ZM59.8887 26.6035V13.0098H60.75V26.6035H59.8887ZM64.8633 26.6035V13.0098H72.0645V26.6035H64.8633ZM72.0703 26.6035V13.0098H79.2715V26.6035H72.0703ZM83.3848 26.6035V13.0098H84.2461V26.6035H83.3848ZM81.5098 26.6035V13.0098H82.3711V26.6035H81.5098ZM86.4844 26.6035V13.0098H93.6855V26.6035H86.4844ZM93.6914 26.6035V13.0098H100.893V26.6035H93.6914ZM103.992 26.6035H103.131V18.5645H108.1V19.4258H103.992V26.6035ZM105.867 21.3008V26.6035H105.006V20.4395H108.1V21.3008H105.867ZM115.307 19.4258H108.105V18.5645H115.307V19.4258ZM115.307 21.3008H108.105V20.4395H115.307V21.3008ZM122.514 19.4258H115.312V18.5645H122.514V19.4258ZM122.514 21.3008H115.312V20.4395H122.514V21.3008ZM122.52 26.6035V13.0098H129.721V26.6035H122.52ZM129.727 26.6035V13.0098H136.928V26.6035H129.727ZM141.041 26.6035V19.4258H136.934V18.5645H141.902V26.6035H141.041ZM139.166 21.3008H136.934V20.4395H140.027V26.6035H139.166V21.3008ZM144.141 26.6035V13.0098H151.342V26.6035H144.141ZM151.348 26.6035V13.0098H158.549V26.6035H151.348ZM161.648 26.6035H160.787V18.5645H165.756V19.4258H161.648V26.6035ZM163.523 21.3008V26.6035H162.662V20.4395H165.756V21.3008H163.523ZM172.963 19.4258H165.762V18.5645H172.963V19.4258ZM172.963 21.3008H165.762V20.4395H172.963V21.3008ZM180.17 19.4258H172.969V18.5645H180.17V19.4258ZM180.17 21.3008H172.969V20.4395H180.17V21.3008ZM187.377 19.4258H180.176V18.5645H187.377V19.4258ZM187.377 21.3008H180.176V20.4395H187.377V21.3008ZM194.584 19.4258H187.383V18.5645H194.584V19.4258ZM194.584 21.3008H187.383V20.4395H194.584V21.3008ZM198.697 13.0098H199.559V21.3008H194.59V20.4395H198.697V13.0098ZM196.822 18.5645V13.0098H197.684V19.4258H194.59V18.5645H196.822ZM230.625 26.6035V13.0098H237.826V26.6035H230.625ZM237.832 26.6035V13.0098H245.033V26.6035H237.832ZM249.146 26.6035V13.0098H250.008V26.6035H249.146ZM247.271 26.6035V13.0098H248.133V26.6035H247.271ZM259.453 26.6035V13.0098H266.654V26.6035H259.453ZM266.66 26.6035V13.0098H273.861V26.6035H266.66ZM276.961 26.6035H276.1V18.5645H281.068V19.4258H276.961V26.6035ZM278.836 21.3008V26.6035H277.975V20.4395H281.068V21.3008H278.836ZM285.182 13.0098H286.043V21.3008H281.074V20.4395H285.182V13.0098ZM283.307 18.5645V13.0098H284.168V19.4258H281.074V18.5645H283.307ZM288.281 26.6035V13.0098H295.482V26.6035H288.281ZM295.488 26.6035V13.0098H302.689V26.6035H295.488ZM305.789 26.6035H304.928V18.5645H309.896V19.4258H305.789V26.6035ZM307.664 21.3008V26.6035H306.803V20.4395H309.896V21.3008H307.664ZM317.104 19.4258H309.902V18.5645H317.104V19.4258ZM317.104 21.3008H309.902V20.4395H317.104V21.3008ZM324.311 19.4258H317.109V18.5645H324.311V19.4258ZM324.311 21.3008H317.109V20.4395H324.311V21.3008ZM324.316 26.6035V13.0098H331.518V26.6035H324.316ZM331.523 26.6035V13.0098H338.725V26.6035H331.523ZM342.838 26.6035V19.4258H338.73V18.5645H343.699V26.6035H342.838ZM340.963 21.3008H338.73V20.4395H341.824V26.6035H340.963V21.3008ZM345.938 26.6035V13.0098H353.139V26.6035H345.938ZM353.145 26.6035V13.0098H360.346V26.6035H353.145ZM360.352 26.6035V13.0098H367.553V26.6035H360.352ZM367.559 26.6035V13.0098H374.76V26.6035H367.559ZM378.873 26.6035V19.4258H374.766V18.5645H379.734V26.6035H378.873ZM376.998 21.3008H374.766V20.4395H377.859V26.6035H376.998V21.3008ZM396.387 26.6035V13.0098H403.588V26.6035H396.387ZM403.594 26.6035V13.0098H410.795V26.6035H403.594ZM414.908 26.6035V13.0098H415.77V26.6035H414.908ZM413.033 26.6035V13.0098H413.895V26.6035H413.033ZM418.008 26.6035V13.0098H425.209V26.6035H418.008ZM425.215 26.6035V13.0098H432.416V26.6035H425.215ZM435.516 26.6035H434.654V18.5645H439.623V19.4258H435.516V26.6035ZM437.391 21.3008V26.6035H436.529V20.4395H439.623V21.3008H437.391ZM446.83 19.4258H439.629V18.5645H446.83V19.4258ZM446.83 21.3008H439.629V20.4395H446.83V21.3008ZM454.037 19.4258H446.836V18.5645H454.037V19.4258ZM454.037 21.3008H446.836V20.4395H454.037V21.3008ZM454.043 26.6035V13.0098H461.244V26.6035H454.043ZM461.25 26.6035V13.0098H468.451V26.6035H461.25ZM472.564 26.6035V19.4258H468.457V18.5645H473.426V26.6035H472.564ZM470.689 21.3008H468.457V20.4395H471.551V26.6035H470.689V21.3008ZM475.664 26.6035V13.0098H482.865V26.6035H475.664ZM482.871 26.6035V13.0098H490.072V26.6035H482.871ZM493.172 26.6035H492.311V18.5645H497.279V19.4258H493.172V26.6035ZM495.047 21.3008V26.6035H494.186V20.4395H497.279V21.3008H495.047ZM504.486 19.4258H497.285V18.5645H504.486V19.4258ZM504.486 21.3008H497.285V20.4395H504.486V21.3008ZM511.693 19.4258H504.492V18.5645H511.693V19.4258ZM511.693 21.3008H504.492V20.4395H511.693V21.3008ZM511.699 26.6035V13.0098H518.9V26.6035H511.699ZM518.906 26.6035V13.0098H526.107V26.6035H518.906ZM530.221 26.6035V19.4258H526.113V18.5645H531.082V26.6035H530.221ZM528.346 21.3008H526.113V20.4395H529.207V26.6035H528.346V21.3008ZM533.32 26.6035V13.0098H540.521V26.6035H533.32ZM540.527 26.6035V13.0098H547.729V26.6035H540.527ZM547.734 26.6035V13.0098H554.936V26.6035H547.734ZM554.941 26.6035V13.0098H562.143V26.6035H554.941ZM566.256 26.6035V19.4258H562.148V18.5645H567.117V26.6035H566.256ZM564.381 21.3008H562.148V20.4395H565.242V26.6035H564.381V21.3008ZM583.77 26.6035V13.0098H590.971V26.6035H583.77ZM590.977 26.6035V13.0098H598.178V26.6035H590.977ZM602.291 26.6035V13.0098H603.152V26.6035H602.291ZM600.416 26.6035V13.0098H601.277V26.6035H600.416ZM0 39.6035V26.0098H7.20117V39.6035H0ZM7.20703 39.6035V26.0098H14.4082V39.6035H7.20703ZM18.5215 39.6035V26.0098H19.3828V39.6035H18.5215ZM16.6465 39.6035V26.0098H17.5078V39.6035H16.6465ZM43.2422 39.6035V26.0098H50.4434V39.6035H43.2422ZM50.4492 39.6035V26.0098H57.6504V39.6035H50.4492ZM61.7637 39.6035V26.0098H62.625V39.6035H61.7637ZM59.8887 39.6035V26.0098H60.75V39.6035H59.8887ZM64.8633 39.6035V26.0098H72.0645V39.6035H64.8633ZM72.0703 39.6035V26.0098H79.2715V39.6035H72.0703ZM83.3848 39.6035V26.0098H84.2461V39.6035H83.3848ZM81.5098 39.6035V26.0098H82.3711V39.6035H81.5098ZM86.4844 39.6035V26.0098H93.6855V39.6035H86.4844ZM93.6914 39.6035V26.0098H100.893V39.6035H93.6914ZM100.898 39.6035V26.0098H108.1V39.6035H100.898ZM108.105 39.6035V26.0098H115.307V39.6035H108.105ZM115.312 39.6035V26.0098H122.514V39.6035H115.312ZM122.52 39.6035V26.0098H129.721V39.6035H122.52ZM132.82 39.6035H131.959V31.5645H136.928V32.4258H132.82V39.6035ZM134.695 34.3008V39.6035H133.834V33.4395H136.928V34.3008H134.695ZM141.041 26.0098H141.902V34.3008H136.934V33.4395H141.041V26.0098ZM139.166 31.5645V26.0098H140.027V32.4258H136.934V31.5645H139.166ZM144.141 39.6035V26.0098H151.342V39.6035H144.141ZM151.348 39.6035V26.0098H158.549V39.6035H151.348ZM158.555 39.6035V26.0098H165.756V39.6035H158.555ZM165.762 39.6035V26.0098H172.963V39.6035H165.762ZM172.969 39.6035V26.0098H180.17V39.6035H172.969ZM184.283 39.6035V32.4258H180.176V31.5645H185.145V39.6035H184.283ZM182.408 34.3008H180.176V33.4395H183.27V39.6035H182.408V34.3008ZM187.383 39.6035V26.0098H194.584V39.6035H187.383ZM194.59 39.6035V26.0098H201.791V39.6035H194.59ZM201.797 39.6035V26.0098H208.998V39.6035H201.797ZM209.004 39.6035V26.0098H216.205V39.6035H209.004ZM216.211 39.6035V26.0098H223.412V39.6035H216.211ZM227.525 39.6035V32.4258H223.418V31.5645H228.387V39.6035H227.525ZM225.65 34.3008H223.418V33.4395H226.512V39.6035H225.65V34.3008ZM230.625 39.6035V26.0098H237.826V39.6035H230.625ZM237.832 39.6035V26.0098H245.033V39.6035H237.832ZM245.039 39.6035V26.0098H252.24V39.6035H245.039ZM252.246 39.6035V26.0098H259.447V39.6035H252.246ZM259.453 39.6035V26.0098H266.654V39.6035H259.453ZM269.754 39.6035H268.893V31.5645H273.861V32.4258H269.754V39.6035ZM271.629 34.3008V39.6035H270.768V33.4395H273.861V34.3008H271.629ZM277.975 26.0098H278.836V34.3008H273.867V33.4395H277.975V26.0098ZM276.1 31.5645V26.0098H276.961V32.4258H273.867V31.5645H276.1ZM288.281 39.6035V26.0098H295.482V39.6035H288.281ZM295.488 39.6035V26.0098H302.689V39.6035H295.488ZM302.695 39.6035V26.0098H309.896V39.6035H302.695ZM309.902 39.6035V26.0098H317.104V39.6035H309.902ZM317.109 39.6035V26.0098H324.311V39.6035H317.109ZM324.316 39.6035V26.0098H331.518V39.6035H324.316ZM331.523 39.6035V26.0098H338.725V39.6035H331.523ZM342.838 39.6035V26.0098H343.699V39.6035H342.838ZM340.963 39.6035V26.0098H341.824V39.6035H340.963ZM345.938 39.6035V26.0098H353.139V39.6035H345.938ZM353.145 39.6035V26.0098H360.346V39.6035H353.145ZM363.445 39.6035H362.584V31.5645H367.553V32.4258H363.445V39.6035ZM365.32 34.3008V39.6035H364.459V33.4395H367.553V34.3008H365.32ZM367.559 39.6035V26.0098H374.76V39.6035H367.559ZM374.766 39.6035V26.0098H381.967V39.6035H374.766ZM386.08 39.6035V32.4258H381.973V31.5645H386.941V39.6035H386.08ZM384.205 34.3008H381.973V33.4395H385.066V39.6035H384.205V34.3008ZM396.387 39.6035V26.0098H403.588V39.6035H396.387ZM403.594 39.6035V26.0098H410.795V39.6035H403.594ZM414.908 39.6035V26.0098H415.77V39.6035H414.908ZM413.033 39.6035V26.0098H413.895V39.6035H413.033ZM418.008 39.6035V26.0098H425.209V39.6035H418.008ZM425.215 39.6035V26.0098H432.416V39.6035H425.215ZM432.422 39.6035V26.0098H439.623V39.6035H432.422ZM439.629 39.6035V26.0098H446.83V39.6035H439.629ZM446.836 39.6035V26.0098H454.037V39.6035H446.836ZM454.043 39.6035V26.0098H461.244V39.6035H454.043ZM464.344 39.6035H463.482V31.5645H468.451V32.4258H464.344V39.6035ZM466.219 34.3008V39.6035H465.357V33.4395H468.451V34.3008H466.219ZM472.564 26.0098H473.426V34.3008H468.457V33.4395H472.564V26.0098ZM470.689 31.5645V26.0098H471.551V32.4258H468.457V31.5645H470.689ZM475.664 39.6035V26.0098H482.865V39.6035H475.664ZM482.871 39.6035V26.0098H490.072V39.6035H482.871ZM490.078 39.6035V26.0098H497.279V39.6035H490.078ZM497.285 39.6035V26.0098H504.486V39.6035H497.285ZM504.492 39.6035V26.0098H511.693V39.6035H504.492ZM511.699 39.6035V26.0098H518.9V39.6035H511.699ZM518.906 39.6035V26.0098H526.107V39.6035H518.906ZM530.221 39.6035V26.0098H531.082V39.6035H530.221ZM528.346 39.6035V26.0098H529.207V39.6035H528.346ZM533.32 39.6035V26.0098H540.521V39.6035H533.32ZM540.527 39.6035V26.0098H547.729V39.6035H540.527ZM550.828 39.6035H549.967V31.5645H554.936V32.4258H550.828V39.6035ZM552.703 34.3008V39.6035H551.842V33.4395H554.936V34.3008H552.703ZM554.941 39.6035V26.0098H562.143V39.6035H554.941ZM562.148 39.6035V26.0098H569.35V39.6035H562.148ZM573.463 39.6035V32.4258H569.355V31.5645H574.324V39.6035H573.463ZM571.588 34.3008H569.355V33.4395H572.449V39.6035H571.588V34.3008ZM583.77 39.6035V26.0098H590.971V39.6035H583.77ZM590.977 39.6035V26.0098H598.178V39.6035H590.977ZM602.291 39.6035V26.0098H603.152V39.6035H602.291ZM600.416 39.6035V26.0098H601.277V39.6035H600.416ZM3.09375 39.0098V46.4395H7.20117V47.3008H2.23242V39.0098H3.09375ZM4.96875 44.5645H7.20117V45.4258H4.10742V39.0098H4.96875V44.5645ZM7.20703 52.6035V39.0098H14.4082V52.6035H7.20703ZM14.4141 52.6035V39.0098H21.6152V52.6035H14.4141ZM25.7285 52.6035V45.4258H21.6211V44.5645H26.5898V52.6035H25.7285ZM23.8535 47.3008H21.6211V46.4395H24.7148V52.6035H23.8535V47.3008ZM36.0352 52.6035V39.0098H43.2363V52.6035H36.0352ZM43.2422 52.6035V39.0098H50.4434V52.6035H43.2422ZM53.543 52.6035H52.6816V44.5645H57.6504V45.4258H53.543V52.6035ZM55.418 47.3008V52.6035H54.5566V46.4395H57.6504V47.3008H55.418ZM61.7637 39.0098H62.625V47.3008H57.6562V46.4395H61.7637V39.0098ZM59.8887 44.5645V39.0098H60.75V45.4258H57.6562V44.5645H59.8887ZM64.8633 52.6035V39.0098H72.0645V52.6035H64.8633ZM72.0703 52.6035V39.0098H79.2715V52.6035H72.0703ZM83.3848 52.6035V39.0098H84.2461V52.6035H83.3848ZM81.5098 52.6035V39.0098H82.3711V52.6035H81.5098ZM86.4844 52.6035V39.0098H93.6855V52.6035H86.4844ZM93.6914 52.6035V39.0098H100.893V52.6035H93.6914ZM103.992 52.6035H103.131V44.5645H108.1V45.4258H103.992V52.6035ZM105.867 47.3008V52.6035H105.006V46.4395H108.1V47.3008H105.867ZM115.307 45.4258H108.105V44.5645H115.307V45.4258ZM115.307 47.3008H108.105V46.4395H115.307V47.3008ZM122.514 45.4258H115.312V44.5645H122.514V45.4258ZM122.514 47.3008H115.312V46.4395H122.514V47.3008ZM122.52 52.6035V39.0098H129.721V52.6035H122.52ZM129.727 52.6035V39.0098H136.928V52.6035H129.727ZM141.041 52.6035V45.4258H136.934V44.5645H141.902V52.6035H141.041ZM139.166 47.3008H136.934V46.4395H140.027V52.6035H139.166V47.3008ZM144.141 52.6035V39.0098H151.342V52.6035H144.141ZM151.348 52.6035V39.0098H158.549V52.6035H151.348ZM161.648 52.6035H160.787V44.5645H165.756V45.4258H161.648V52.6035ZM163.523 47.3008V52.6035H162.662V46.4395H165.756V47.3008H163.523ZM172.963 45.4258H165.762V44.5645H172.963V45.4258ZM172.963 47.3008H165.762V46.4395H172.963V47.3008ZM180.17 45.4258H172.969V44.5645H180.17V45.4258ZM180.17 47.3008H172.969V46.4395H180.17V47.3008ZM184.283 39.0098H185.145V47.3008H180.176V46.4395H184.283V39.0098ZM182.408 44.5645V39.0098H183.27V45.4258H180.176V44.5645H182.408ZM190.477 39.0098V46.4395H194.584V47.3008H189.615V39.0098H190.477ZM192.352 44.5645H194.584V45.4258H191.49V39.0098H192.352V44.5645ZM201.791 45.4258H194.59V44.5645H201.791V45.4258ZM201.791 47.3008H194.59V46.4395H201.791V47.3008ZM208.998 45.4258H201.797V44.5645H208.998V45.4258ZM208.998 47.3008H201.797V46.4395H208.998V47.3008ZM216.205 45.4258H209.004V44.5645H216.205V45.4258ZM216.205 47.3008H209.004V46.4395H216.205V47.3008ZM223.412 45.4258H216.211V44.5645H223.412V45.4258ZM223.412 47.3008H216.211V46.4395H223.412V47.3008ZM227.525 39.0098H228.387V47.3008H223.418V46.4395H227.525V39.0098ZM225.65 44.5645V39.0098H226.512V45.4258H223.418V44.5645H225.65ZM230.625 52.6035V39.0098H237.826V52.6035H230.625ZM237.832 52.6035V39.0098H245.033V52.6035H237.832ZM248.133 52.6035H247.271V44.5645H252.24V45.4258H248.133V52.6035ZM250.008 47.3008V52.6035H249.146V46.4395H252.24V47.3008H250.008ZM259.447 45.4258H252.246V44.5645H259.447V45.4258ZM259.447 47.3008H252.246V46.4395H259.447V47.3008ZM259.453 52.6035V39.0098H266.654V52.6035H259.453ZM266.66 52.6035V39.0098H273.861V52.6035H266.66ZM277.975 52.6035V45.4258H273.867V44.5645H278.836V52.6035H277.975ZM276.1 47.3008H273.867V46.4395H276.961V52.6035H276.1V47.3008ZM288.281 52.6035V39.0098H295.482V52.6035H288.281ZM295.488 52.6035V39.0098H302.689V52.6035H295.488ZM305.789 52.6035H304.928V44.5645H309.896V45.4258H305.789V52.6035ZM307.664 47.3008V52.6035H306.803V46.4395H309.896V47.3008H307.664ZM317.104 45.4258H309.902V44.5645H317.104V45.4258ZM317.104 47.3008H309.902V46.4395H317.104V47.3008ZM324.311 45.4258H317.109V44.5645H324.311V45.4258ZM324.311 47.3008H317.109V46.4395H324.311V47.3008ZM324.316 52.6035V39.0098H331.518V52.6035H324.316ZM331.523 52.6035V39.0098H338.725V52.6035H331.523ZM342.838 52.6035V39.0098H343.699V52.6035H342.838ZM340.963 52.6035V39.0098H341.824V52.6035H340.963ZM345.938 52.6035V39.0098H353.139V52.6035H345.938ZM353.145 52.6035V39.0098H360.346V52.6035H353.145ZM364.459 52.6035V39.0098H365.32V52.6035H364.459ZM362.584 52.6035V39.0098H363.445V52.6035H362.584ZM370.652 39.0098V46.4395H374.76V47.3008H369.791V39.0098H370.652ZM372.527 44.5645H374.76V45.4258H371.666V39.0098H372.527V44.5645ZM374.766 52.6035V39.0098H381.967V52.6035H374.766ZM381.973 52.6035V39.0098H389.174V52.6035H381.973ZM393.287 52.6035V45.4258H389.18V44.5645H394.148V52.6035H393.287ZM391.412 47.3008H389.18V46.4395H392.273V52.6035H391.412V47.3008ZM396.387 52.6035V39.0098H403.588V52.6035H396.387ZM403.594 52.6035V39.0098H410.795V52.6035H403.594ZM414.908 52.6035V39.0098H415.77V52.6035H414.908ZM413.033 52.6035V39.0098H413.895V52.6035H413.033ZM418.008 52.6035V39.0098H425.209V52.6035H418.008ZM425.215 52.6035V39.0098H432.416V52.6035H425.215ZM435.516 52.6035H434.654V44.5645H439.623V45.4258H435.516V52.6035ZM437.391 47.3008V52.6035H436.529V46.4395H439.623V47.3008H437.391ZM446.83 45.4258H439.629V44.5645H446.83V45.4258ZM446.83 47.3008H439.629V46.4395H446.83V47.3008ZM454.037 45.4258H446.836V44.5645H454.037V45.4258ZM454.037 47.3008H446.836V46.4395H454.037V47.3008ZM454.043 52.6035V39.0098H461.244V52.6035H454.043ZM461.25 52.6035V39.0098H468.451V52.6035H461.25ZM472.564 52.6035V45.4258H468.457V44.5645H473.426V52.6035H472.564ZM470.689 47.3008H468.457V46.4395H471.551V52.6035H470.689V47.3008ZM475.664 52.6035V39.0098H482.865V52.6035H475.664ZM482.871 52.6035V39.0098H490.072V52.6035H482.871ZM493.172 52.6035H492.311V44.5645H497.279V45.4258H493.172V52.6035ZM495.047 47.3008V52.6035H494.186V46.4395H497.279V47.3008H495.047ZM504.486 45.4258H497.285V44.5645H504.486V45.4258ZM504.486 47.3008H497.285V46.4395H504.486V47.3008ZM511.693 45.4258H504.492V44.5645H511.693V45.4258ZM511.693 47.3008H504.492V46.4395H511.693V47.3008ZM511.699 52.6035V39.0098H518.9V52.6035H511.699ZM518.906 52.6035V39.0098H526.107V52.6035H518.906ZM530.221 52.6035V39.0098H531.082V52.6035H530.221ZM528.346 52.6035V39.0098H529.207V52.6035H528.346ZM533.32 52.6035V39.0098H540.521V52.6035H533.32ZM540.527 52.6035V39.0098H547.729V52.6035H540.527ZM551.842 52.6035V39.0098H552.703V52.6035H551.842ZM549.967 52.6035V39.0098H550.828V52.6035H549.967ZM558.035 39.0098V46.4395H562.143V47.3008H557.174V39.0098H558.035ZM559.91 44.5645H562.143V45.4258H559.049V39.0098H559.91V44.5645ZM562.148 52.6035V39.0098H569.35V52.6035H562.148ZM569.355 52.6035V39.0098H576.557V52.6035H569.355ZM580.67 52.6035V45.4258H576.562V44.5645H581.531V52.6035H580.67ZM578.795 47.3008H576.562V46.4395H579.656V52.6035H578.795V47.3008ZM583.77 52.6035V39.0098H590.971V52.6035H583.77ZM590.977 52.6035V39.0098H598.178V52.6035H590.977ZM602.291 52.6035V39.0098H603.152V52.6035H602.291ZM600.416 52.6035V39.0098H601.277V52.6035H600.416ZM10.3008 52.0098V59.4395H14.4082V60.3008H9.43945V52.0098H10.3008ZM12.1758 57.5645H14.4082V58.4258H11.3145V52.0098H12.1758V57.5645ZM14.4141 65.6035V52.0098H21.6152V65.6035H14.4141ZM21.6211 65.6035V52.0098H28.8223V65.6035H21.6211ZM28.8281 65.6035V52.0098H36.0293V65.6035H28.8281ZM36.0352 65.6035V52.0098H43.2363V65.6035H36.0352ZM46.3359 65.6035H45.4746V57.5645H50.4434V58.4258H46.3359V65.6035ZM48.2109 60.3008V65.6035H47.3496V59.4395H50.4434V60.3008H48.2109ZM54.5566 52.0098H55.418V60.3008H50.4492V59.4395H54.5566V52.0098ZM52.6816 57.5645V52.0098H53.543V58.4258H50.4492V57.5645H52.6816ZM64.8633 65.6035V52.0098H72.0645V65.6035H64.8633ZM72.0703 65.6035V52.0098H79.2715V65.6035H72.0703ZM83.3848 65.6035V52.0098H84.2461V65.6035H83.3848ZM81.5098 65.6035V52.0098H82.3711V65.6035H81.5098ZM86.4844 65.6035V52.0098H93.6855V65.6035H86.4844ZM93.6914 65.6035V52.0098H100.893V65.6035H93.6914ZM100.898 65.6035V52.0098H108.1V65.6035H100.898ZM108.105 65.6035V52.0098H115.307V65.6035H108.105ZM115.312 65.6035V52.0098H122.514V65.6035H115.312ZM122.52 65.6035V52.0098H129.721V65.6035H122.52ZM132.82 65.6035H131.959V57.5645H136.928V58.4258H132.82V65.6035ZM134.695 60.3008V65.6035H133.834V59.4395H136.928V60.3008H134.695ZM141.041 52.0098H141.902V60.3008H136.934V59.4395H141.041V52.0098ZM139.166 57.5645V52.0098H140.027V58.4258H136.934V57.5645H139.166ZM144.141 65.6035V52.0098H151.342V65.6035H144.141ZM151.348 65.6035V52.0098H158.549V65.6035H151.348ZM158.555 65.6035V52.0098H165.756V65.6035H158.555ZM165.762 65.6035V52.0098H172.963V65.6035H165.762ZM172.969 65.6035V52.0098H180.17V65.6035H172.969ZM180.176 65.6035V52.0098H187.377V65.6035H180.176ZM187.383 65.6035V52.0098H194.584V65.6035H187.383ZM198.697 65.6035V58.4258H194.59V57.5645H199.559V65.6035H198.697ZM196.822 60.3008H194.59V59.4395H197.684V65.6035H196.822V60.3008ZM230.625 65.6035V52.0098H237.826V65.6035H230.625ZM237.832 65.6035V52.0098H245.033V65.6035H237.832ZM249.146 65.6035V52.0098H250.008V65.6035H249.146ZM247.271 65.6035V52.0098H248.133V65.6035H247.271ZM266.66 65.6035V52.0098H273.861V65.6035H266.66ZM273.867 65.6035V52.0098H281.068V65.6035H273.867ZM285.182 65.6035V58.4258H281.074V57.5645H286.043V65.6035H285.182ZM283.307 60.3008H281.074V59.4395H284.168V65.6035H283.307V60.3008ZM288.281 65.6035V52.0098H295.482V65.6035H288.281ZM295.488 65.6035V52.0098H302.689V65.6035H295.488ZM306.803 65.6035V52.0098H307.664V65.6035H306.803ZM304.928 65.6035V52.0098H305.789V65.6035H304.928ZM324.316 65.6035V52.0098H331.518V65.6035H324.316ZM331.523 65.6035V52.0098H338.725V65.6035H331.523ZM342.838 65.6035V52.0098H343.699V65.6035H342.838ZM340.963 65.6035V52.0098H341.824V65.6035H340.963ZM345.938 65.6035V52.0098H353.139V65.6035H345.938ZM353.145 65.6035V52.0098H360.346V65.6035H353.145ZM364.459 65.6035V52.0098H365.32V65.6035H364.459ZM362.584 65.6035V52.0098H363.445V65.6035H362.584ZM377.859 52.0098V59.4395H381.967V60.3008H376.998V52.0098H377.859ZM379.734 57.5645H381.967V58.4258H378.873V52.0098H379.734V57.5645ZM381.973 65.6035V52.0098H389.174V65.6035H381.973ZM389.18 65.6035V52.0098H396.381V65.6035H389.18ZM396.387 65.6035V52.0098H403.588V65.6035H396.387ZM403.594 65.6035V52.0098H410.795V65.6035H403.594ZM414.908 65.6035V52.0098H415.77V65.6035H414.908ZM413.033 65.6035V52.0098H413.895V65.6035H413.033ZM418.008 65.6035V52.0098H425.209V65.6035H418.008ZM425.215 65.6035V52.0098H432.416V65.6035H425.215ZM432.422 65.6035V52.0098H439.623V65.6035H432.422ZM439.629 65.6035V52.0098H446.83V65.6035H439.629ZM446.836 65.6035V52.0098H454.037V65.6035H446.836ZM454.043 65.6035V52.0098H461.244V65.6035H454.043ZM464.344 65.6035H463.482V57.5645H468.451V58.4258H464.344V65.6035ZM466.219 60.3008V65.6035H465.357V59.4395H468.451V60.3008H466.219ZM472.564 52.0098H473.426V60.3008H468.457V59.4395H472.564V52.0098ZM470.689 57.5645V52.0098H471.551V58.4258H468.457V57.5645H470.689ZM475.664 65.6035V52.0098H482.865V65.6035H475.664ZM482.871 65.6035V52.0098H490.072V65.6035H482.871ZM494.186 65.6035V52.0098H495.047V65.6035H494.186ZM492.311 65.6035V52.0098H493.172V65.6035H492.311ZM511.699 65.6035V52.0098H518.9V65.6035H511.699ZM518.906 65.6035V52.0098H526.107V65.6035H518.906ZM530.221 65.6035V52.0098H531.082V65.6035H530.221ZM528.346 65.6035V52.0098H529.207V65.6035H528.346ZM533.32 65.6035V52.0098H540.521V65.6035H533.32ZM540.527 65.6035V52.0098H547.729V65.6035H540.527ZM551.842 65.6035V52.0098H552.703V65.6035H551.842ZM549.967 65.6035V52.0098H550.828V65.6035H549.967ZM565.242 52.0098V59.4395H569.35V60.3008H564.381V52.0098H565.242ZM567.117 57.5645H569.35V58.4258H566.256V52.0098H567.117V57.5645ZM569.355 65.6035V52.0098H576.557V65.6035H569.355ZM576.562 65.6035V52.0098H583.764V65.6035H576.562ZM583.77 65.6035V52.0098H590.971V65.6035H583.77ZM590.977 65.6035V52.0098H598.178V65.6035H590.977ZM602.291 65.6035V52.0098H603.152V65.6035H602.291ZM600.416 65.6035V52.0098H601.277V65.6035H600.416ZM17.5078 65.0098V72.4395H21.6152V73.3008H16.6465V65.0098H17.5078ZM19.3828 70.5645H21.6152V71.4258H18.5215V65.0098H19.3828V70.5645ZM28.8223 71.4258H21.6211V70.5645H28.8223V71.4258ZM28.8223 73.3008H21.6211V72.4395H28.8223V73.3008ZM36.0293 71.4258H28.8281V70.5645H36.0293V71.4258ZM36.0293 73.3008H28.8281V72.4395H36.0293V73.3008ZM43.2363 71.4258H36.0352V70.5645H43.2363V71.4258ZM43.2363 73.3008H36.0352V72.4395H43.2363V73.3008ZM47.3496 65.0098H48.2109V73.3008H43.2422V72.4395H47.3496V65.0098ZM45.4746 70.5645V65.0098H46.3359V71.4258H43.2422V70.5645H45.4746ZM67.957 65.0098V72.4395H72.0645V73.3008H67.0957V65.0098H67.957ZM69.832 70.5645H72.0645V71.4258H68.9707V65.0098H69.832V70.5645ZM79.2715 71.4258H72.0703V70.5645H79.2715V71.4258ZM79.2715 73.3008H72.0703V72.4395H79.2715V73.3008ZM83.3848 65.0098H84.2461V73.3008H79.2773V72.4395H83.3848V65.0098ZM81.5098 70.5645V65.0098H82.3711V71.4258H79.2773V70.5645H81.5098ZM89.5781 65.0098V72.4395H93.6855V73.3008H88.7168V65.0098H89.5781ZM91.4531 70.5645H93.6855V71.4258H90.5918V65.0098H91.4531V70.5645ZM100.893 71.4258H93.6914V70.5645H100.893V71.4258ZM100.893 73.3008H93.6914V72.4395H100.893V73.3008ZM108.1 71.4258H100.898V70.5645H108.1V71.4258ZM108.1 73.3008H100.898V72.4395H108.1V73.3008ZM115.307 71.4258H108.105V70.5645H115.307V71.4258ZM115.307 73.3008H108.105V72.4395H115.307V73.3008ZM122.514 71.4258H115.312V70.5645H122.514V71.4258ZM122.514 73.3008H115.312V72.4395H122.514V73.3008ZM129.721 71.4258H122.52V70.5645H129.721V71.4258ZM129.721 73.3008H122.52V72.4395H129.721V73.3008ZM133.834 65.0098H134.695V73.3008H129.727V72.4395H133.834V65.0098ZM131.959 70.5645V65.0098H132.82V71.4258H129.727V70.5645H131.959ZM147.234 65.0098V72.4395H151.342V73.3008H146.373V65.0098H147.234ZM149.109 70.5645H151.342V71.4258H148.248V65.0098H149.109V70.5645ZM158.549 71.4258H151.348V70.5645H158.549V71.4258ZM158.549 73.3008H151.348V72.4395H158.549V73.3008ZM165.756 71.4258H158.555V70.5645H165.756V71.4258ZM165.756 73.3008H158.555V72.4395H165.756V73.3008ZM172.963 71.4258H165.762V70.5645H172.963V71.4258ZM172.963 73.3008H165.762V72.4395H172.963V73.3008ZM180.17 71.4258H172.969V70.5645H180.17V71.4258ZM180.17 73.3008H172.969V72.4395H180.17V73.3008ZM187.377 71.4258H180.176V70.5645H187.377V71.4258ZM187.377 73.3008H180.176V72.4395H187.377V73.3008ZM194.584 71.4258H187.383V70.5645H194.584V71.4258ZM194.584 73.3008H187.383V72.4395H194.584V73.3008ZM198.697 65.0098H199.559V73.3008H194.59V72.4395H198.697V65.0098ZM196.822 70.5645V65.0098H197.684V71.4258H194.59V70.5645H196.822ZM233.719 65.0098V72.4395H237.826V73.3008H232.857V65.0098H233.719ZM235.594 70.5645H237.826V71.4258H234.732V65.0098H235.594V70.5645ZM245.033 71.4258H237.832V70.5645H245.033V71.4258ZM245.033 73.3008H237.832V72.4395H245.033V73.3008ZM249.146 65.0098H250.008V73.3008H245.039V72.4395H249.146V65.0098ZM247.271 70.5645V65.0098H248.133V71.4258H245.039V70.5645H247.271ZM269.754 65.0098V72.4395H273.861V73.3008H268.893V65.0098H269.754ZM271.629 70.5645H273.861V71.4258H270.768V65.0098H271.629V70.5645ZM281.068 71.4258H273.867V70.5645H281.068V71.4258ZM281.068 73.3008H273.867V72.4395H281.068V73.3008ZM285.182 65.0098H286.043V73.3008H281.074V72.4395H285.182V65.0098ZM283.307 70.5645V65.0098H284.168V71.4258H281.074V70.5645H283.307ZM291.375 65.0098V72.4395H295.482V73.3008H290.514V65.0098H291.375ZM293.25 70.5645H295.482V71.4258H292.389V65.0098H293.25V70.5645ZM302.689 71.4258H295.488V70.5645H302.689V71.4258ZM302.689 73.3008H295.488V72.4395H302.689V73.3008ZM306.803 65.0098H307.664V73.3008H302.695V72.4395H306.803V65.0098ZM304.928 70.5645V65.0098H305.789V71.4258H302.695V70.5645H304.928ZM327.41 65.0098V72.4395H331.518V73.3008H326.549V65.0098H327.41ZM329.285 70.5645H331.518V71.4258H328.424V65.0098H329.285V70.5645ZM338.725 71.4258H331.523V70.5645H338.725V71.4258ZM338.725 73.3008H331.523V72.4395H338.725V73.3008ZM342.838 65.0098H343.699V73.3008H338.73V72.4395H342.838V65.0098ZM340.963 70.5645V65.0098H341.824V71.4258H338.73V70.5645H340.963ZM349.031 65.0098V72.4395H353.139V73.3008H348.17V65.0098H349.031ZM350.906 70.5645H353.139V71.4258H350.045V65.0098H350.906V70.5645ZM360.346 71.4258H353.145V70.5645H360.346V71.4258ZM360.346 73.3008H353.145V72.4395H360.346V73.3008ZM364.459 65.0098H365.32V73.3008H360.352V72.4395H364.459V65.0098ZM362.584 70.5645V65.0098H363.445V71.4258H360.352V70.5645H362.584ZM385.066 65.0098V72.4395H389.174V73.3008H384.205V65.0098H385.066ZM386.941 70.5645H389.174V71.4258H386.08V65.0098H386.941V70.5645ZM396.381 71.4258H389.18V70.5645H396.381V71.4258ZM396.381 73.3008H389.18V72.4395H396.381V73.3008ZM403.588 71.4258H396.387V70.5645H403.588V71.4258ZM403.588 73.3008H396.387V72.4395H403.588V73.3008ZM410.795 71.4258H403.594V70.5645H410.795V71.4258ZM410.795 73.3008H403.594V72.4395H410.795V73.3008ZM414.908 65.0098H415.77V73.3008H410.801V72.4395H414.908V65.0098ZM413.033 70.5645V65.0098H413.895V71.4258H410.801V70.5645H413.033ZM421.102 65.0098V72.4395H425.209V73.3008H420.24V65.0098H421.102ZM422.977 70.5645H425.209V71.4258H422.115V65.0098H422.977V70.5645ZM432.416 71.4258H425.215V70.5645H432.416V71.4258ZM432.416 73.3008H425.215V72.4395H432.416V73.3008ZM439.623 71.4258H432.422V70.5645H439.623V71.4258ZM439.623 73.3008H432.422V72.4395H439.623V73.3008ZM446.83 71.4258H439.629V70.5645H446.83V71.4258ZM446.83 73.3008H439.629V72.4395H446.83V73.3008ZM454.037 71.4258H446.836V70.5645H454.037V71.4258ZM454.037 73.3008H446.836V72.4395H454.037V73.3008ZM461.244 71.4258H454.043V70.5645H461.244V71.4258ZM461.244 73.3008H454.043V72.4395H461.244V73.3008ZM465.357 65.0098H466.219V73.3008H461.25V72.4395H465.357V65.0098ZM463.482 70.5645V65.0098H464.344V71.4258H461.25V70.5645H463.482ZM478.758 65.0098V72.4395H482.865V73.3008H477.896V65.0098H478.758ZM480.633 70.5645H482.865V71.4258H479.771V65.0098H480.633V70.5645ZM490.072 71.4258H482.871V70.5645H490.072V71.4258ZM490.072 73.3008H482.871V72.4395H490.072V73.3008ZM494.186 65.0098H495.047V73.3008H490.078V72.4395H494.186V65.0098ZM492.311 70.5645V65.0098H493.172V71.4258H490.078V70.5645H492.311ZM514.793 65.0098V72.4395H518.9V73.3008H513.932V65.0098H514.793ZM516.668 70.5645H518.9V71.4258H515.807V65.0098H516.668V70.5645ZM526.107 71.4258H518.906V70.5645H526.107V71.4258ZM526.107 73.3008H518.906V72.4395H526.107V73.3008ZM530.221 65.0098H531.082V73.3008H526.113V72.4395H530.221V65.0098ZM528.346 70.5645V65.0098H529.207V71.4258H526.113V70.5645H528.346ZM536.414 65.0098V72.4395H540.521V73.3008H535.553V65.0098H536.414ZM538.289 70.5645H540.521V71.4258H537.428V65.0098H538.289V70.5645ZM547.729 71.4258H540.527V70.5645H547.729V71.4258ZM547.729 73.3008H540.527V72.4395H547.729V73.3008ZM551.842 65.0098H552.703V73.3008H547.734V72.4395H551.842V65.0098ZM549.967 70.5645V65.0098H550.828V71.4258H547.734V70.5645H549.967ZM572.449 65.0098V72.4395H576.557V73.3008H571.588V65.0098H572.449ZM574.324 70.5645H576.557V71.4258H573.463V65.0098H574.324V70.5645ZM583.764 71.4258H576.562V70.5645H583.764V71.4258ZM583.764 73.3008H576.562V72.4395H583.764V73.3008ZM590.971 71.4258H583.77V70.5645H590.971V71.4258ZM590.971 73.3008H583.77V72.4395H590.971V73.3008ZM598.178 71.4258H590.977V70.5645H598.178V71.4258ZM598.178 73.3008H590.977V72.4395H598.178V73.3008ZM602.291 65.0098H603.152V73.3008H598.184V72.4395H602.291V65.0098ZM600.416 70.5645V65.0098H601.277V71.4258H598.184V70.5645H600.416Z" fill="white"/>
</svg>
</file>

<file path="docs/logo/light.svg">
<?xml version="1.0" encoding="UTF-8"?>
<svg id="Layer_1" data-name="Layer 1" xmlns="http://www.w3.org/2000/svg" width="212.8mm" height="25.88mm" viewBox="0 0 603.21 73.35">
  <polygon points="165.79 21.32 165.79 21.32 165.79 21.33 173 21.33 172.99 20.49 172.99 20.46 172.99 20.46 172.99 20.46 166.09 20.46 165.79 20.46 165.79 20.46 165.79 20.46 165.79 21.27 165.79 21.32"/>
  <polygon points="172.99 72.46 172.99 72.46 172.99 72.46 166.09 72.46 165.79 72.46 165.79 72.46 165.79 72.46 165.79 73.27 165.79 73.32 165.79 73.32 165.79 73.33 173 73.33 172.99 72.49 172.99 72.46"/>
  <polygon points="165.79 39.62 165.79 39.62 165.79 39.63 173 39.63 172.99 26.59 172.99 26.03 172.99 26.03 172.99 26.03 166.09 26.03 165.79 26.03 165.79 26.03 165.79 26.03 165.79 38.78 165.79 39.62"/>
  <polygon points="165.79 19.45 165.79 19.45 165.79 19.45 173 19.45 172.99 18.62 172.99 18.59 172.99 18.59 172.99 18.58 166.09 18.59 165.79 18.59 165.79 18.59 165.79 18.59 165.79 19.4 165.79 19.45"/>
  <polygon points="165.79 45.45 165.79 45.45 165.79 45.45 173 45.45 172.99 44.62 172.99 44.59 172.99 44.59 172.99 44.58 166.09 44.59 165.79 44.59 165.79 44.59 165.79 44.59 165.79 45.4 165.79 45.45"/>
  <polygon points="165.79 71.45 165.79 71.45 165.79 71.45 173 71.45 172.99 70.62 172.99 70.59 172.99 70.59 172.99 70.58 166.09 70.59 165.79 70.59 165.79 70.59 165.79 70.59 165.79 71.4 165.79 71.45"/>
  <polygon points="165.79 65.62 165.79 65.62 165.79 65.63 173 65.63 172.99 52.59 172.99 52.03 172.99 52.03 172.99 52.03 166.09 52.03 165.79 52.03 165.79 52.03 165.79 52.03 165.79 64.78 165.79 65.62"/>
  <polygon points="165.79 47.32 165.79 47.32 165.79 47.33 173 47.33 172.99 46.49 172.99 46.46 172.99 46.46 172.99 46.46 166.09 46.46 165.79 46.46 165.79 46.46 165.79 46.46 165.79 47.27 165.79 47.32"/>
  <polygon points="165.79 12.78 165.79 13.62 165.79 13.62 165.79 13.63 173 13.63 172.99 .59 172.99 .03 172.99 .03 172.99 .03 166.24 .03 165.79 .03 165.79 .03 165.79 .03 165.79 12.78"/>
  <polygon points="173 21.33 180.2 21.33 180.2 20.48 180.2 20.46 180.2 20.46 180.2 20.46 173.3 20.46 173 20.46 173 20.46 173 20.46 173 21.33"/>
  <polygon points="173 39.63 180.2 39.63 180.2 26.31 180.2 26.03 180.2 26.03 180.2 26.03 173.3 26.03 173 26.03 173 26.03 173 26.03 173 39.63"/>
  <polygon points="180.2 72.46 180.2 72.46 180.2 72.46 173.3 72.46 173 72.46 173 72.46 173 72.46 173 73.33 180.2 73.33 180.2 72.48 180.2 72.46"/>
  <polygon points="173 45.45 180.2 45.45 180.2 44.6 180.2 44.59 180.2 44.59 180.2 44.58 173.3 44.59 173 44.59 173 44.59 173 44.59 173 45.45"/>
  <polygon points="180.2 18.59 180.2 18.58 173.3 18.59 173 18.59 173 18.59 173 18.59 173 19.45 180.2 19.45 180.2 18.6 180.2 18.59 180.2 18.59"/>
  <polygon points="180.2 70.59 180.2 70.58 173.3 70.59 173 70.59 173 70.59 173 70.59 173 71.45 180.2 71.45 180.2 70.6 180.2 70.59 180.2 70.59"/>
  <polygon points="173 47.33 180.2 47.33 180.2 46.48 180.2 46.46 180.2 46.46 180.2 46.46 173.3 46.46 173 46.46 173 46.46 173 46.46 173 47.33"/>
  <polygon points="180.2 52.31 180.2 52.03 180.2 52.03 180.2 52.03 173.3 52.03 173 52.03 173 52.03 173 52.03 173 65.63 180.2 65.63 180.2 52.31"/>
  <polygon points="129.76 71.45 129.75 70.58 122.85 70.59 122.55 70.59 122.55 70.59 122.55 70.59 122.55 71.45 129.76 71.45"/>
  <polygon points="141.93 18.59 141.93 18.59 141.93 18.58 137.17 18.59 136.97 18.59 136.97 18.59 136.96 18.59 136.97 19.43 136.97 19.45 136.97 19.45 136.97 19.45 141.07 19.45 141.07 33.46 137.13 33.46 136.97 33.46 136.97 33.46 136.96 33.46 136.97 34.31 136.97 34.32 136.97 34.32 136.97 34.33 141.94 34.33 141.93 19.24 141.93 18.59"/>
  <polygon points="165.79 .03 165.79 .03 165.79 .03 159.03 .03 158.59 .03 158.59 .03 158.58 .03 158.59 12.78 158.59 13.62 158.59 13.62 158.59 13.63 165.79 13.63 165.79 .59 165.79 .03"/>
  <polygon points="136.96 57.59 136.96 57.59 136.96 57.58 132.19 57.59 131.99 57.59 131.99 57.59 131.99 57.59 131.99 70.58 129.85 70.59 129.76 70.59 129.76 70.59 129.76 70.59 129.76 71.43 129.76 71.45 129.76 71.45 129.76 71.45 132.85 71.45 132.85 59.26 132.85 58.45 136.96 58.45 136.96 57.66 136.96 57.59"/>
  <polygon points="141.93 44.59 141.93 44.59 141.93 44.58 137.17 44.59 136.97 44.59 136.97 44.59 136.96 44.59 136.97 45.43 136.97 45.45 136.97 45.45 136.97 45.45 141.07 45.45 141.07 59.46 137.13 59.46 136.97 59.46 136.97 59.46 136.96 59.46 136.97 60.31 136.97 60.32 136.97 60.32 136.97 60.33 141.94 60.33 141.93 45.24 141.93 44.59"/>
  <polygon points="136.96 13.03 136.96 13.03 136.96 13.03 134.82 13.03 134.73 13.03 134.73 6.05 134.73 5.59 134.73 5.59 134.73 5.58 130.07 5.59 129.76 5.59 129.76 5.59 129.76 5.59 129.76 6.43 129.76 6.45 129.76 6.45 129.76 6.45 133.86 6.45 133.87 13.03 132.9 13.03 132.85 13.03 132.85 7.81 132.85 7.46 132.85 7.46 132.85 7.46 129.95 7.46 129.76 7.46 129.76 7.46 129.76 7.46 129.76 8.31 129.76 8.32 129.76 8.32 129.76 8.33 131.99 8.32 131.99 13.03 129.85 13.03 129.76 13.03 129.76 13.03 129.76 13.03 129.76 26.34 129.76 26.62 129.76 26.62 129.76 26.63 136.96 26.63 136.96 14.16 136.96 13.03"/>
  <polygon points="136.96 60.32 136.96 59.53 136.96 59.46 136.96 59.46 136.96 59.46 133.99 59.46 133.87 59.46 133.87 59.46 133.86 59.46 133.87 72.46 129.93 72.46 129.76 72.46 129.76 72.46 129.76 72.46 129.76 73.31 129.76 73.32 129.76 73.32 129.76 73.33 134.73 73.33 134.73 61.14 134.73 60.33 136.96 60.32"/>
  <polygon points="122.85 72.46 122.55 72.46 122.55 72.46 122.55 72.46 122.55 73.33 129.76 73.33 129.75 72.46 122.85 72.46"/>
  <polygon points="122.55 65.63 129.76 65.63 129.75 .03 123 .03 122.55 .03 122.55 .03 122.55 .03 122.55 65.63"/>
  <polygon points="132.85 39.03 132.85 32.86 132.85 32.45 136.96 32.45 136.96 31.66 136.96 31.59 136.96 31.59 136.96 31.58 132.19 31.59 131.99 31.59 131.99 31.59 131.99 31.59 131.99 39.03 129.85 39.03 129.76 39.03 129.76 39.03 129.76 39.03 129.76 52.34 129.76 52.62 129.76 52.62 129.76 52.63 136.96 52.63 136.96 40.16 136.96 39.03 136.96 39.03 136.96 39.03 134.82 39.03 134.73 39.03 134.73 34.62 134.73 34.33 136.96 34.32 136.96 33.53 136.96 33.46 136.96 33.46 136.96 33.46 133.99 33.46 133.87 33.46 133.87 33.46 133.86 33.46 133.87 39.03 132.9 39.03 132.85 39.03"/>
  <polygon points="165.79 72.46 165.79 72.46 165.79 72.46 158.88 72.46 158.59 72.46 158.59 72.46 158.58 72.46 158.59 73.27 158.59 73.32 158.59 73.32 158.59 73.33 165.79 73.33 165.79 72.49 165.79 72.46"/>
  <polygon points="165.79 70.59 165.79 70.59 165.79 70.58 158.88 70.59 158.59 70.59 158.59 70.59 158.58 70.59 158.59 71.4 158.59 71.45 158.59 71.45 158.59 71.45 165.79 71.45 165.79 70.62 165.79 70.59"/>
  <polygon points="149.14 65.83 149.14 65.63 151.38 65.62 151.37 4.13 151.37 .03 151.37 .03 151.37 .03 144.62 .03 144.17 .03 144.17 .03 144.17 .03 144.17 62.89 144.17 65.62 144.17 65.62 144.17 65.63 146.4 65.62 146.4 73.16 146.4 73.32 146.4 73.32 146.4 73.33 151.38 73.33 151.37 72.51 151.37 72.46 151.37 72.46 151.37 72.46 147.44 72.46 147.27 72.46 147.27 65.91 147.27 65.63 148.28 65.62 148.28 71.33 148.28 71.45 148.28 71.45 148.28 71.45 151.38 71.45 151.37 70.64 151.37 70.59 151.37 70.59 151.37 70.58 149.24 70.59 149.14 70.59 149.14 65.83"/>
  <polygon points="161.72 52.03 161.68 52.03 161.68 45.59 161.68 45.45 165.79 45.45 165.79 44.62 165.79 44.59 165.79 44.59 165.79 44.58 161.02 44.59 160.82 44.59 160.82 44.59 160.82 44.59 160.82 51.72 160.82 52.03 158.68 52.03 158.59 52.03 158.59 52.03 158.58 52.03 158.59 64.78 158.59 65.62 158.59 65.62 158.59 65.63 165.79 65.63 165.79 52.59 165.79 52.03 165.79 52.03 165.79 52.03 163.65 52.03 163.56 52.03 163.55 47.42 163.55 47.33 165.79 47.32 165.79 46.49 165.79 46.46 165.79 46.46 165.79 46.46 162.82 46.46 162.69 46.46 162.69 46.46 162.69 46.46 162.69 51.8 162.69 52.03 161.72 52.03"/>
  <polygon points="158.58 72.46 158.58 72.46 158.58 72.46 151.68 72.46 151.38 72.46 151.38 72.46 151.38 72.46 151.38 73.29 151.38 73.32 151.38 73.32 151.38 73.33 158.58 73.33 158.58 72.51 158.58 72.46"/>
  <polygon points="158.58 .03 158.58 .03 158.58 .03 151.83 .03 151.38 .03 151.38 .03 151.38 .03 151.38 62.89 151.38 65.62 151.38 65.62 151.38 65.63 158.58 65.63 158.58 4.13 158.58 .03"/>
  <polygon points="158.58 70.59 158.58 70.59 158.58 70.58 151.68 70.59 151.38 70.59 151.38 70.59 151.38 70.59 151.38 71.41 151.38 71.45 151.38 71.45 151.38 71.45 158.58 71.45 158.58 70.64 158.58 70.59"/>
  <polygon points="173 13.63 180.2 13.63 180.2 .31 180.2 .03 180.2 .03 180.2 .03 173.45 .03 173 .03 173 .03 173 .03 173 13.63"/>
  <polygon points="209.04 39.63 216.24 39.63 216.24 26.03 209.33 26.03 209.04 26.03 209.04 26.03 209.03 26.03 209.04 39.63"/>
  <polygon points="216.54 46.46 216.24 46.46 216.24 46.46 216.24 46.46 216.24 47.33 223.45 47.33 223.44 46.46 216.54 46.46"/>
  <polygon points="209.04 44.59 209.04 44.59 209.03 44.59 209.04 45.45 216.24 45.45 216.24 44.58 209.33 44.59 209.04 44.59"/>
  <polygon points="216.24 44.59 216.24 44.59 216.24 44.59 216.24 45.45 223.45 45.45 223.44 44.58 216.54 44.59 216.24 44.59"/>
  <polygon points="201.83 39.63 209.03 39.63 209.03 26.03 202.13 26.03 201.83 26.03 201.83 26.03 201.83 26.03 201.83 39.63"/>
  <polygon points="201.83 44.59 201.83 44.59 201.83 44.59 201.83 45.45 209.03 45.45 209.03 44.58 202.13 44.59 201.83 44.59"/>
  <polygon points="216.24 39.63 223.45 39.63 223.44 26.03 216.54 26.03 216.24 26.03 216.24 26.03 216.24 26.03 216.24 39.63"/>
  <polygon points="209.33 46.46 209.04 46.46 209.04 46.46 209.03 46.46 209.04 47.33 216.24 47.33 216.24 46.46 209.33 46.46"/>
  <polygon points="233.92 72.46 233.75 72.46 233.75 66.2 233.75 65.63 234.76 65.62 234.76 71.45 237.86 71.45 237.86 70.58 235.72 70.59 235.63 70.59 235.63 66.04 235.63 65.63 237.86 65.62 237.86 .03 231.1 .03 230.66 .03 230.66 .03 230.65 .03 230.66 65.63 232.89 65.62 232.89 73.33 237.86 73.33 237.86 72.46 233.92 72.46"/>
  <polygon points="252.27 45.45 252.27 44.58 247.51 44.59 247.3 44.59 247.3 44.59 247.3 44.59 247.3 70.58 245.16 70.59 245.07 70.59 245.07 70.59 245.07 70.59 245.07 71.43 245.07 71.45 245.07 71.45 245.07 71.45 248.17 71.45 248.16 47.08 248.16 45.45 252.27 45.45"/>
  <polygon points="252.27 47.32 252.27 46.46 249.3 46.46 249.18 46.46 249.18 46.46 249.17 46.46 249.18 72.46 245.24 72.46 245.07 72.46 245.07 72.46 245.07 72.46 245.07 73.31 245.07 73.32 245.07 73.32 245.07 73.33 250.04 73.33 250.04 48.95 250.04 47.33 252.27 47.32"/>
  <polygon points="245.07 26.03 245.07 26.03 245.07 26.03 245.07 39.34 245.07 39.62 245.07 39.62 245.07 39.63 252.27 39.63 252.27 26.03 250.13 26.03 250.04 26.03 250.04 6.86 250.04 5.59 250.04 5.59 250.04 5.58 245.38 5.59 245.07 5.59 245.07 5.59 245.07 5.59 245.07 6.43 245.07 6.45 245.07 6.45 245.07 6.45 249.17 6.45 249.18 26.03 248.21 26.03 248.17 26.03 248.16 8.62 248.16 7.46 248.16 7.46 248.16 7.46 245.26 7.46 245.07 7.46 245.07 7.46 245.07 7.46 245.07 8.31 245.07 8.32 245.07 8.32 245.07 8.33 247.3 8.32 247.3 26.03 245.16 26.03 245.07 26.03"/>
  <polygon points="223.65 31.59 223.45 31.59 223.45 31.59 223.45 31.59 223.45 32.45 227.55 32.45 227.56 46.46 223.62 46.46 223.45 46.46 223.45 46.46 223.45 46.46 223.45 47.33 228.42 47.33 228.42 31.58 223.65 31.59"/>
  <polygon points="237.86 65.63 245.07 65.63 245.06 .03 238.31 .03 237.86 .03 237.86 .03 237.86 .03 237.86 65.63"/>
  <polygon points="202.13 46.46 201.83 46.46 201.83 46.46 201.83 46.46 201.83 47.33 209.03 47.33 209.03 46.46 202.13 46.46"/>
  <polygon points="238.16 72.46 237.86 72.46 237.86 72.46 237.86 72.46 237.86 73.33 245.07 73.33 245.06 72.46 238.16 72.46"/>
  <polygon points="237.86 70.59 237.86 70.59 237.86 70.59 237.86 71.45 245.07 71.45 245.06 70.58 238.16 70.59 237.86 70.59"/>
  <polygon points="180.21 13.63 187.41 13.63 187.41 .31 187.41 .03 187.41 .03 187.41 .03 180.65 .03 180.21 .03 180.21 .03 180.2 .03 180.21 13.63"/>
  <polygon points="187.71 72.46 187.41 72.46 187.41 72.46 187.41 72.46 187.41 73.33 194.62 73.33 194.62 72.46 187.71 72.46"/>
  <polygon points="187.41 72.46 187.41 72.46 187.41 72.46 180.5 72.46 180.21 72.46 180.21 72.46 180.2 72.46 180.21 73.33 187.41 73.33 187.41 72.48 187.41 72.46"/>
  <polygon points="194.62 71.45 194.62 70.58 187.71 70.59 187.41 70.59 187.41 70.59 187.41 70.59 187.41 71.45 194.62 71.45"/>
  <polygon points="187.41 18.59 187.41 18.58 180.5 18.59 180.21 18.59 180.21 18.59 180.2 18.59 180.21 19.45 187.41 19.45 187.41 18.6 187.41 18.59 187.41 18.59"/>
  <polygon points="187.41 52.31 187.41 52.03 187.41 52.03 187.41 52.03 180.5 52.03 180.21 52.03 180.21 52.03 180.2 52.03 180.21 65.63 187.41 65.63 187.41 52.31"/>
  <polygon points="180.21 21.33 187.41 21.33 187.41 20.48 187.41 20.46 187.41 20.46 187.41 20.46 180.5 20.46 180.21 20.46 180.21 20.46 180.2 20.46 180.21 21.33"/>
  <polygon points="187.41 71.45 187.41 70.6 187.41 70.59 187.41 70.59 187.41 70.58 180.5 70.59 180.21 70.59 180.21 70.59 180.2 70.59 180.21 71.45 187.41 71.45"/>
  <polygon points="194.62 52.03 187.71 52.03 187.41 52.03 187.41 52.03 187.41 52.03 187.41 65.63 194.62 65.63 194.62 52.03"/>
  <polygon points="187.41 21.33 194.62 21.33 194.62 20.46 187.71 20.46 187.41 20.46 187.41 20.46 187.41 20.46 187.41 21.33"/>
  <polygon points="194.92 46.46 194.62 46.46 194.62 46.46 194.62 46.46 194.62 47.33 201.83 47.33 201.82 46.46 194.92 46.46"/>
  <polygon points="201.83 45.45 201.82 44.58 194.92 44.59 194.62 44.59 194.62 44.59 194.62 44.59 194.62 45.45 201.83 45.45"/>
  <polygon points="194.83 57.59 194.62 57.59 194.62 57.59 194.62 57.59 194.62 58.45 198.73 58.45 198.73 71.29 198.73 72.46 194.79 72.46 194.62 72.46 194.62 72.46 194.62 72.46 194.62 73.33 199.59 73.33 199.59 57.58 194.83 57.59"/>
  <polygon points="187.41 13.63 194.62 13.63 194.62 .03 187.86 .03 187.41 .03 187.41 .03 187.41 .03 187.41 13.63"/>
  <polygon points="194.62 26.03 194.62 26.03 194.62 26.03 194.62 39.63 201.83 39.63 201.82 26.03 194.92 26.03 194.62 26.03"/>
  <polygon points="192.38 39.63 194.62 39.62 194.62 26.03 187.71 26.03 187.41 26.03 187.41 26.03 187.41 26.03 187.41 39.63 189.64 39.62 189.65 46.68 189.65 47.32 189.65 47.32 189.65 47.33 194.62 47.33 194.62 46.46 190.68 46.46 190.51 46.46 190.51 39.63 191.52 39.62 191.52 44.96 191.52 45.45 191.52 45.45 191.52 45.45 194.62 45.45 194.62 44.58 192.48 44.59 192.39 44.59 192.38 39.63"/>
  <polygon points="194.62 18.58 187.71 18.59 187.41 18.59 187.41 18.59 187.41 18.59 187.41 19.45 194.62 19.45 194.62 18.58"/>
  <polygon points="338.76 71.45 338.76 71.45 341.86 71.45 341.86 20.46 338.89 20.46 338.76 20.46 338.76 20.46 338.76 20.46 338.76 21.2 338.76 21.32 338.76 21.32 338.76 21.33 340.99 21.32 340.99 70.58 338.85 70.59 338.76 70.59 338.76 70.59 338.76 70.59 338.76 71.32 338.76 71.45 338.76 71.45"/>
  <polygon points="17.71 72.46 17.54 72.46 17.54 65.63 18.55 65.62 18.55 71.45 21.65 71.45 21.65 70.58 19.51 70.59 19.42 70.59 19.41 65.63 21.65 65.62 21.65 39.03 19.51 39.03 19.42 39.03 19.41 5.58 14.75 5.59 14.45 5.59 14.45 5.59 14.44 5.59 14.45 6.45 18.55 6.45 18.55 39.03 17.58 39.03 17.54 39.03 17.54 7.46 14.64 7.46 14.45 7.46 14.45 7.46 14.44 7.46 14.45 8.33 16.67 8.32 16.68 39.03 14.54 39.03 14.45 39.03 14.45 39.03 14.44 39.03 14.45 65.63 16.67 65.62 16.68 73.33 21.65 73.33 21.65 72.46 17.71 72.46"/>
  <polygon points="79.31 71.45 79.31 71.45 82.41 71.45 82.4 7.46 79.5 7.46 79.31 7.46 79.31 7.46 79.31 7.46 79.31 8.31 79.31 8.32 79.31 8.32 79.31 8.33 81.54 8.32 81.54 70.58 79.4 70.59 79.31 70.59 79.31 70.59 79.31 70.59 79.31 71.43 79.31 71.45 79.31 71.45"/>
  <polygon points="28.85 70.59 28.85 70.58 21.95 70.59 21.65 70.59 21.65 70.59 21.65 70.59 21.65 71.45 28.86 71.45 28.85 70.6 28.85 70.59 28.85 70.59"/>
  <polygon points="21.65 52.03 21.65 52.03 21.65 52.03 21.65 65.63 28.86 65.63 28.85 52.31 28.85 52.03 28.85 52.03 28.85 52.03 26.72 52.03 26.62 52.03 26.62 44.58 21.86 44.59 21.65 44.59 21.65 44.59 21.65 44.59 21.65 45.45 25.76 45.45 25.76 52.03 24.79 52.03 24.75 52.03 24.75 46.46 21.78 46.46 21.65 46.46 21.65 46.46 21.65 46.46 21.65 47.33 23.88 47.32 23.88 52.03 21.74 52.03 21.65 52.03"/>
  <polygon points="57.68 47.32 57.68 46.46 54.71 46.46 54.59 46.46 54.59 46.46 54.58 46.46 54.59 59.19 54.59 59.46 50.65 59.46 50.48 59.46 50.48 59.46 50.48 59.46 50.48 60.33 55.45 60.33 55.45 47.6 55.45 47.33 57.68 47.32"/>
  <polygon points="29.16 72.46 28.86 72.46 28.86 72.46 28.86 72.46 28.86 73.33 36.06 73.33 36.06 72.46 29.16 72.46"/>
  <polygon points="5 39.63 7.24 39.62 7.23 .03 .48 .03 .03 .03 .03 .03 .03 .03 .03 39.63 2.26 39.62 2.26 47.33 7.24 47.33 7.23 46.46 3.3 46.46 3.13 46.46 3.12 39.63 4.14 39.62 4.14 45.45 7.24 45.45 7.23 44.58 5.1 44.59 5 44.59 5 39.63"/>
  <polygon points="12.21 52.63 14.44 52.62 14.44 .03 7.69 .03 7.24 .03 7.24 .03 7.24 .03 7.24 52.63 9.47 52.62 9.47 60.33 14.44 60.33 14.44 59.46 10.51 59.46 10.33 59.46 10.33 52.63 11.34 52.62 11.35 58.45 14.44 58.45 14.44 57.58 12.3 57.59 12.21 57.59 12.21 52.63"/>
  <polygon points="28.85 72.46 28.85 72.46 28.85 72.46 21.95 72.46 21.65 72.46 21.65 72.46 21.65 72.46 21.65 73.33 28.86 73.33 28.85 72.48 28.85 72.46"/>
  <polygon points="28.86 70.59 28.86 70.59 28.86 70.59 28.86 71.45 36.06 71.45 36.06 70.58 29.16 70.59 28.86 70.59"/>
  <polygon points="50.47 57.6 50.47 57.59 50.47 57.59 50.47 57.58 45.71 57.59 45.51 57.59 45.51 57.59 45.5 57.59 45.51 70.58 43.36 70.59 43.27 70.59 43.27 70.59 43.27 70.59 43.27 71.45 46.37 71.45 46.37 58.45 50.48 58.45 50.47 57.6"/>
  <polygon points="36.07 65.63 43.27 65.63 43.27 39.03 36.36 39.03 36.07 39.03 36.07 39.03 36.06 39.03 36.07 65.63"/>
  <polygon points="43.27 52.63 50.48 52.63 50.47 1.12 50.47 .03 50.47 .03 50.47 .03 43.72 .03 43.27 .03 43.27 .03 43.27 .03 43.27 52.63"/>
  <polygon points="50.47 59.48 50.47 59.46 50.47 59.46 50.47 59.46 47.51 59.46 47.38 59.46 47.38 59.46 47.38 59.46 47.38 72.46 43.44 72.46 43.27 72.46 43.27 72.46 43.27 72.46 43.27 73.33 48.25 73.33 48.24 60.33 50.48 60.32 50.47 59.48"/>
  <polygon points="28.86 65.63 36.06 65.63 36.06 52.03 29.16 52.03 28.86 52.03 28.86 52.03 28.86 52.03 28.86 65.63"/>
  <polygon points="36.36 72.46 36.07 72.46 36.07 72.46 36.06 72.46 36.07 73.33 43.27 73.33 43.27 72.46 36.36 72.46"/>
  <polygon points="36.07 70.59 36.07 70.59 36.06 70.59 36.07 71.45 43.27 71.45 43.27 70.58 36.36 70.59 36.07 70.59"/>
  <polygon points="57.69 45.45 57.69 45.45 60.78 45.45 60.78 7.46 57.88 7.46 57.69 7.46 57.69 7.46 57.68 7.46 57.69 8.31 57.69 8.32 57.69 8.32 57.69 8.33 59.92 8.32 59.92 44.58 57.78 44.59 57.69 44.59 57.69 44.59 57.68 44.59 57.69 45.43 57.69 45.45 57.69 45.45"/>
  <polygon points="471.58 46.46 471.58 46.46 468.61 46.46 468.49 46.46 468.49 46.46 468.49 46.46 468.49 47.33 470.72 47.32 470.72 56.09 470.72 57.58 468.58 57.59 468.49 57.59 468.49 57.59 468.49 57.59 468.49 58.45 471.58 58.45 471.58 47.46 471.58 46.46 471.58 46.46"/>
  <polygon points="252.57 46.46 252.28 46.46 252.28 46.46 252.27 46.46 252.28 47.31 252.28 47.32 252.28 47.32 252.28 47.33 259.48 47.33 259.48 46.46 252.57 46.46"/>
  <polygon points="413.93 13.03 413.93 7.46 413.93 7.46 413.93 7.46 411.02 7.46 410.83 7.46 410.83 7.46 410.83 7.46 410.83 8.33 413.06 8.32 413.06 66.69 413.06 70.58 410.92 70.59 410.83 70.59 410.83 70.59 410.83 70.59 410.83 71.45 413.93 71.45 413.93 18.12 413.93 13.62 413.93 13.03"/>
  <polygon points="136.97 57.59 136.97 57.59 136.96 57.59 136.97 58.43 136.97 58.45 136.97 58.45 136.97 58.45 140.06 58.45 140.06 46.96 140.06 46.46 140.06 46.46 140.06 46.46 137.09 46.46 136.97 46.46 136.97 46.46 136.96 46.46 136.97 47.31 136.97 47.32 136.97 47.32 136.97 47.33 139.19 47.32 139.2 57.58 137.06 57.59 136.97 57.59"/>
  <polygon points="281.2 18.59 281.11 18.59 281.11 18.59 281.1 18.59 281.11 19.4 281.11 19.45 281.11 19.45 281.11 19.45 284.2 19.45 284.2 7.71 284.2 7.46 284.2 7.46 284.2 7.46 281.3 7.46 281.11 7.46 281.11 7.46 281.1 7.46 281.11 8.27 281.11 8.32 281.11 8.32 281.11 8.33 283.34 8.32 283.34 18.16 283.34 18.58 281.2 18.59"/>
  <polygon points="598.22 70.59 598.22 70.59 598.21 70.59 598.21 71.45 601.31 71.45 601.31 7.46 598.41 7.46 598.22 7.46 598.22 7.46 598.21 7.46 598.21 8.33 600.44 8.32 600.45 70.58 598.31 70.59 598.22 70.59"/>
  <polygon points="526.14 71.45 526.14 71.45 529.24 71.45 529.24 20.46 526.27 20.46 526.14 20.46 526.14 20.46 526.14 20.46 526.14 21.24 526.14 21.32 526.14 21.32 526.14 21.33 528.37 21.32 528.38 70.58 526.23 70.59 526.14 70.59 526.14 70.59 526.14 70.59 526.14 71.36 526.14 71.45 526.14 71.45"/>
  <polygon points="471.58 20.46 471.58 20.46 468.61 20.46 468.49 20.46 468.49 20.46 468.49 20.46 468.49 21.33 470.72 21.32 470.72 30.09 470.72 31.58 468.58 31.59 468.49 31.59 468.49 31.59 468.49 31.59 468.49 32.45 471.58 32.45 471.58 21.46 471.58 20.46 471.58 20.46"/>
  <polygon points="281.2 70.59 281.11 70.59 281.11 70.59 281.1 70.59 281.11 71.4 281.11 71.45 281.11 71.45 281.11 71.45 284.2 71.45 284.2 59.71 284.2 59.46 284.2 59.46 284.2 59.46 281.23 59.46 281.11 59.46 281.11 59.46 281.1 59.46 281.11 60.27 281.11 60.32 281.11 60.32 281.11 60.33 283.34 60.32 283.34 70.16 283.34 70.58 281.2 70.59"/>
  <polygon points="180.21 47.33 185.18 47.33 185.18 31.58 180.41 31.59 180.21 31.59 180.21 31.59 180.2 31.59 180.21 32.45 184.31 32.45 184.31 45.58 184.31 46.46 180.38 46.46 180.21 46.46 180.21 46.46 180.2 46.46 180.21 47.33"/>
  <polygon points="180.3 44.59 180.21 44.59 180.21 44.59 180.2 44.59 180.21 45.45 183.3 45.45 183.3 33.46 180.33 33.46 180.21 33.46 180.21 33.46 180.2 33.46 180.21 34.33 182.44 34.32 182.44 43.94 182.44 44.58 180.3 44.59"/>
  <polygon points="136.97 31.59 136.97 31.59 136.96 31.59 136.97 32.43 136.97 32.45 136.97 32.45 136.97 32.45 140.06 32.45 140.06 20.96 140.06 20.46 140.06 20.46 140.06 20.46 137.09 20.46 136.97 20.46 136.97 20.46 136.96 20.46 136.97 21.31 136.97 21.32 136.97 21.32 136.97 21.33 139.19 21.32 139.2 31.58 137.06 31.59 136.97 31.59"/>
  <polygon points="194.62 6.45 198.73 6.45 198.73 19.29 198.73 20.46 194.79 20.46 194.62 20.46 194.62 20.46 194.62 20.46 194.62 21.33 199.59 21.33 199.59 5.58 194.93 5.59 194.62 5.59 194.62 5.59 194.62 5.59 194.62 6.45"/>
  <polygon points="196.85 8.32 196.85 17.73 196.85 18.58 194.71 18.59 194.62 18.59 194.62 18.59 194.62 18.59 194.62 19.45 197.72 19.45 197.72 7.46 194.81 7.46 194.62 7.46 194.62 7.46 194.62 7.46 194.62 8.33 196.85 8.32"/>
  <polygon points="197.72 71.45 197.72 59.46 194.75 59.46 194.62 59.46 194.62 59.46 194.62 59.46 194.62 60.33 196.85 60.32 196.85 69.73 196.85 70.58 194.71 70.59 194.62 70.59 194.62 70.59 194.62 70.59 194.62 71.45 197.72 71.45"/>
  <polygon points="223.45 44.59 223.45 44.59 223.45 44.59 223.45 45.45 226.55 45.45 226.54 33.46 223.58 33.46 223.45 33.46 223.45 33.46 223.45 33.46 223.45 34.33 225.68 34.32 225.68 44.58 223.54 44.59 223.45 44.59"/>
  <polygon points="108.13 44.59 108.14 45.45 115.34 45.45 115.34 44.58 108.43 44.59 108.14 44.59 108.14 44.59 108.13 44.59"/>
  <polygon points="108.43 72.46 108.14 72.46 108.14 72.46 108.13 72.46 108.14 73.33 115.34 73.33 115.34 72.46 108.43 72.46"/>
  <polygon points="108.13 26.03 108.14 39.63 115.34 39.63 115.34 26.03 108.43 26.03 108.14 26.03 108.14 26.03 108.13 26.03"/>
  <polygon points="115.34 13.63 122.55 13.63 122.55 .03 115.79 .03 115.34 .03 115.34 .03 115.34 .03 115.34 13.63"/>
  <polygon points="108.43 18.59 108.14 18.59 108.14 18.59 108.13 18.59 108.14 19.45 115.34 19.45 115.34 18.58 108.43 18.59"/>
  <polygon points="108.13 20.46 108.14 21.33 115.34 21.33 115.34 20.46 108.43 20.46 108.14 20.46 108.14 20.46 108.13 20.46"/>
  <polygon points="115.34 71.45 115.34 70.58 108.43 70.59 108.14 70.59 108.14 70.59 108.13 70.59 108.14 71.45 115.34 71.45"/>
  <polygon points="108.13 52.03 108.14 65.63 115.34 65.63 115.34 52.03 108.43 52.03 108.14 52.03 108.14 52.03 108.13 52.03"/>
  <polygon points="108.13 46.46 108.14 47.33 115.34 47.33 115.34 46.46 108.43 46.46 108.14 46.46 108.14 46.46 108.13 46.46"/>
  <polygon points="108.14 13.63 115.34 13.63 115.34 .03 108.58 .03 108.14 .03 108.14 .03 108.13 .03 108.14 13.63"/>
  <polygon points="122.55 44.58 115.64 44.59 115.34 44.59 115.34 44.59 115.34 44.59 115.34 45.45 122.55 45.45 122.55 44.58"/>
  <polygon points="122.55 26.03 115.64 26.03 115.34 26.03 115.34 26.03 115.34 26.03 115.34 39.63 122.55 39.63 122.55 26.03"/>
  <polygon points="122.55 20.46 115.64 20.46 115.34 20.46 115.34 20.46 115.34 20.46 115.34 21.33 122.55 21.33 122.55 20.46"/>
  <polygon points="122.55 18.58 115.64 18.59 115.34 18.59 115.34 18.59 115.34 18.59 115.34 19.45 122.55 19.45 122.55 18.58"/>
  <polygon points="115.64 72.46 115.34 72.46 115.34 72.46 115.34 72.46 115.34 73.33 122.55 73.33 122.55 72.46 115.64 72.46"/>
  <polygon points="122.55 71.45 122.55 70.58 115.64 70.59 115.34 70.59 115.34 70.59 115.34 70.59 115.34 71.45 122.55 71.45"/>
  <polygon points="122.55 52.03 115.64 52.03 115.34 52.03 115.34 52.03 115.34 52.03 115.34 65.63 122.55 65.63 122.55 52.03"/>
  <polygon points="115.34 46.46 115.34 46.46 115.34 46.46 115.34 47.33 122.55 47.33 122.55 46.46 115.64 46.46 115.34 46.46"/>
  <polygon points="108.13 71.45 108.13 70.58 101.23 70.59 100.93 70.59 100.93 70.59 100.93 70.59 100.93 71.45 108.13 71.45"/>
  <polygon points="72.1 65.63 79.31 65.63 79.3 .03 72.55 .03 72.1 .03 72.1 .03 72.1 .03 72.1 65.63"/>
  <polygon points="72.1 70.59 72.1 70.59 72.1 70.59 72.1 71.45 79.31 71.45 79.3 70.58 72.4 70.59 72.1 70.59"/>
  <polygon points="100.93 13.63 108.13 13.63 108.13 .03 101.38 .03 100.93 .03 100.93 .03 100.93 .03 100.93 13.63"/>
  <polygon points="72.4 72.46 72.1 72.46 72.1 72.46 72.1 72.46 72.1 73.33 79.31 73.33 79.3 72.46 72.4 72.46"/>
  <polygon points="79.62 5.59 79.31 5.59 79.31 5.59 79.31 5.59 79.31 6.43 79.31 6.45 79.31 6.45 79.31 6.45 83.41 6.45 83.42 72.46 79.48 72.46 79.31 72.46 79.31 72.46 79.31 72.46 79.31 73.31 79.31 73.32 79.31 73.32 79.31 73.33 84.28 73.33 84.28 5.58 79.62 5.59"/>
  <polygon points="50.48 39.63 57.68 39.63 57.68 .03 50.93 .03 50.48 .03 50.48 .03 50.48 .03 50.48 39.63"/>
  <polygon points="72.1 72.46 72.1 72.46 72.1 72.46 68.16 72.46 67.99 72.46 67.99 65.63 69 65.62 69 71.45 72.1 71.45 72.1 70.6 72.1 70.59 72.1 70.59 72.1 70.58 69.96 70.59 69.87 70.59 69.86 65.63 72.1 65.62 72.1 1.39 72.1 .03 72.1 .03 72.1 .03 65.34 .03 64.89 .03 64.89 .03 64.89 .03 64.89 65.63 67.12 65.62 67.13 73.33 72.1 73.33 72.1 72.48 72.1 72.46"/>
  <polygon points="58 5.59 57.69 5.59 57.69 5.59 57.68 5.59 57.69 6.43 57.69 6.45 57.69 6.45 57.69 6.45 61.79 6.45 61.79 46.46 57.86 46.46 57.69 46.46 57.69 46.46 57.68 46.46 57.69 47.31 57.69 47.32 57.69 47.32 57.69 47.33 62.66 47.33 62.66 5.58 58 5.59"/>
  <polygon points="89.78 72.46 89.61 72.46 89.61 65.63 90.62 65.62 90.62 71.45 93.72 71.45 93.72 70.58 91.58 70.59 91.49 70.59 91.48 65.63 93.72 65.62 93.72 .03 86.96 .03 86.52 .03 86.52 .03 86.51 .03 86.52 65.63 88.75 65.62 88.75 73.33 93.72 73.33 93.72 72.46 89.78 72.46"/>
  <polygon points="57.68 45.45 57.68 44.58 52.92 44.59 52.71 44.59 52.71 44.59 52.71 44.59 52.71 57.31 52.71 57.58 50.57 57.59 50.48 57.59 50.48 57.59 50.48 57.59 50.48 58.45 53.58 58.45 53.57 45.72 53.57 45.45 57.68 45.45"/>
  <polygon points="108.13 39.63 108.13 26.03 105.99 26.03 105.9 26.03 105.9 21.33 108.13 21.32 108.13 20.46 105.16 20.46 105.04 20.46 105.04 20.46 105.03 20.46 105.04 26.03 104.07 26.03 104.03 26.03 104.02 19.45 108.13 19.45 108.13 18.58 103.37 18.59 103.16 18.59 103.16 18.59 103.16 18.59 103.16 26.03 101.02 26.03 100.93 26.03 100.93 26.03 100.93 26.03 100.93 39.63 108.13 39.63"/>
  <polygon points="94.02 70.59 93.72 70.59 93.72 70.59 93.72 70.59 93.72 71.45 100.93 71.45 100.92 70.58 94.02 70.59"/>
  <polygon points="94.02 72.46 93.72 72.46 93.72 72.46 93.72 72.46 93.72 73.33 100.93 73.33 100.92 72.46 94.02 72.46"/>
  <polygon points="101.23 72.46 100.93 72.46 100.93 72.46 100.93 72.46 100.93 73.33 108.13 73.33 108.13 72.46 101.23 72.46"/>
  <polygon points="93.72 65.63 100.93 65.63 100.93 54.19 100.93 65.63 108.13 65.63 108.13 52.03 105.99 52.03 105.9 52.03 105.9 47.33 108.13 47.32 108.13 46.46 105.16 46.46 105.04 46.46 105.04 46.46 105.03 46.46 105.04 52.03 104.07 52.03 104.03 52.03 104.02 45.45 108.13 45.45 108.13 44.58 103.37 44.59 103.16 44.59 103.16 44.59 103.16 44.59 103.16 52.03 101.02 52.03 100.93 52.03 100.93 52.03 100.93 52.03 100.92 .03 94.17 .03 93.72 .03 93.72 .03 93.72 .03 93.72 65.63"/>
  <polygon points="161.72 26.03 161.68 26.03 161.68 19.59 161.68 19.45 165.79 19.45 165.79 18.62 165.79 18.59 165.79 18.59 165.79 18.58 161.02 18.59 160.82 18.59 160.82 18.59 160.82 18.59 160.82 25.72 160.82 26.03 158.68 26.03 158.59 26.03 158.59 26.03 158.58 26.03 158.59 38.78 158.59 39.62 158.59 39.62 158.59 39.63 165.79 39.63 165.79 26.59 165.79 26.03 165.79 26.03 165.79 26.03 163.65 26.03 163.56 26.03 163.55 21.42 163.55 21.33 165.79 21.32 165.79 20.49 165.79 20.46 165.79 20.46 165.79 20.46 162.82 20.46 162.69 20.46 162.69 20.46 162.69 20.46 162.69 25.8 162.69 26.03 161.72 26.03"/>
  <polygon points="473.46 18.59 473.46 18.59 473.46 18.58 468.69 18.59 468.49 18.59 468.49 18.59 468.49 18.59 468.49 19.45 472.59 19.45 472.6 31.41 472.6 33.46 468.66 33.46 468.49 33.46 468.49 33.46 468.49 33.46 468.49 34.33 473.46 34.33 473.46 19.89 473.46 18.59"/>
  <polygon points="439.66 47.33 446.86 47.33 446.86 46.46 439.96 46.46 439.66 46.46 439.66 46.46 439.66 46.46 439.66 47.33"/>
  <polygon points="473.46 44.59 473.46 44.59 473.46 44.58 468.69 44.59 468.49 44.59 468.49 44.59 468.49 44.59 468.49 45.45 472.59 45.45 472.6 57.41 472.6 59.46 468.66 59.46 468.49 59.46 468.49 59.46 468.49 59.46 468.49 60.33 473.46 60.33 473.46 45.89 473.46 44.59"/>
  <polygon points="478.96 72.46 478.79 72.46 478.79 66.05 478.79 65.63 479.8 65.62 479.8 70.48 479.8 71.45 479.8 71.45 479.8 71.45 482.9 71.45 482.9 70.58 480.76 70.59 480.67 70.59 480.66 65.94 480.66 65.63 482.9 65.62 482.9 13.03 475.99 13.03 475.7 13.03 475.7 13.03 475.69 13.03 475.7 64.53 475.7 65.62 475.7 65.62 475.7 65.63 477.92 65.62 477.93 72.04 477.93 73.32 477.93 73.32 477.93 73.33 482.9 73.33 482.9 72.46 478.96 72.46"/>
  <polygon points="252.57 44.59 252.28 44.59 252.28 44.59 252.27 44.59 252.28 45.43 252.28 45.45 252.28 45.45 252.28 45.45 259.48 45.45 259.48 44.58 252.57 44.59"/>
  <polygon points="468.49 60.32 468.48 59.46 465.51 59.46 465.39 59.46 465.39 59.46 465.39 59.46 465.39 70.56 465.39 72.46 461.45 72.46 461.28 72.46 461.28 72.46 461.28 72.46 461.28 73.33 466.25 73.33 466.25 61.41 466.25 60.33 468.49 60.32"/>
  <polygon points="468.49 58.45 468.48 57.58 463.72 57.59 463.51 57.59 463.51 57.59 463.51 57.59 463.51 68.69 463.51 70.58 461.37 70.59 461.28 70.59 461.28 70.59 461.28 70.59 461.28 71.45 464.38 71.45 464.38 59.53 464.38 58.45 468.49 58.45"/>
  <polygon points="468.49 52.63 468.48 39.03 466.35 39.03 466.25 39.03 466.25 34.72 466.25 34.33 468.49 34.32 468.48 33.46 465.51 33.46 465.39 33.46 465.39 33.46 465.39 33.46 465.39 38.22 465.39 39.03 464.42 39.03 464.38 39.03 464.38 33 464.38 32.45 468.49 32.45 468.48 31.58 463.72 31.59 463.51 31.59 463.51 31.59 463.51 31.59 463.51 37.94 463.51 39.03 461.37 39.03 461.28 39.03 461.28 39.03 461.28 39.03 461.28 52.63 468.49 52.63"/>
  <polygon points="482.9 64.26 482.9 65.62 482.9 65.62 482.9 65.63 490.11 65.63 490.1 .03 483.35 .03 482.9 .03 482.9 .03 482.9 .03 482.9 64.26"/>
  <polygon points="490.11 13.06 490.11 13.62 490.11 13.62 490.11 13.63 497.31 13.63 497.31 .03 490.56 .03 490.11 .03 490.11 .03 490.11 .03 490.11 13.06"/>
  <polygon points="497.61 46.46 497.32 46.46 497.32 46.46 497.31 46.46 497.32 47.29 497.32 47.32 497.32 47.32 497.32 47.33 504.52 47.33 504.52 46.46 497.61 46.46"/>
  <polygon points="497.32 45.45 497.32 45.45 504.52 45.45 504.52 44.58 497.61 44.59 497.32 44.59 497.32 44.59 497.31 44.59 497.32 45.41 497.32 45.45 497.32 45.45"/>
  <polygon points="490.11 39.62 490.11 39.62 490.11 39.63 497.31 39.63 497.31 26.03 495.17 26.03 495.08 26.03 495.08 21.52 495.08 21.33 497.31 21.32 497.31 20.46 494.34 20.46 494.22 20.46 494.22 20.46 494.21 20.46 494.22 26.03 493.25 26.03 493.21 26.03 493.2 19.72 493.2 19.45 497.31 19.45 497.31 18.58 492.55 18.59 492.34 18.59 492.34 18.59 492.34 18.59 492.34 26.03 490.2 26.03 490.11 26.03 490.11 26.03 490.11 26.03 490.11 39.06 490.11 39.62"/>
  <polygon points="504.52 26.03 497.61 26.03 497.32 26.03 497.32 26.03 497.31 26.03 497.32 39.06 497.32 39.62 497.32 39.62 497.32 39.63 504.52 39.63 504.52 26.03"/>
  <polygon points="482.9 71.45 482.9 71.45 490.11 71.45 490.1 70.58 483.2 70.59 482.9 70.59 482.9 70.59 482.9 70.59 482.9 71.43 482.9 71.45 482.9 71.45"/>
  <polygon points="497.31 45.45 497.31 44.58 492.55 44.59 492.34 44.59 492.34 44.59 492.34 44.59 492.34 70.58 490.2 70.59 490.11 70.59 490.11 70.59 490.11 70.59 490.11 71.41 490.11 71.45 490.11 71.45 490.11 71.45 493.21 71.45 493.2 46.53 493.2 45.45 497.31 45.45"/>
  <polygon points="497.31 47.32 497.31 46.46 494.34 46.46 494.22 46.46 494.22 46.46 494.21 46.46 494.22 72.46 490.28 72.46 490.11 72.46 490.11 72.46 490.11 72.46 490.11 73.29 490.11 73.32 490.11 73.32 490.11 73.33 495.08 73.33 495.08 48.41 495.08 47.33 497.31 47.32"/>
  <polygon points="454.07 65.63 461.28 65.63 461.28 .03 454.52 .03 454.07 .03 454.07 .03 454.07 .03 454.07 65.63"/>
  <polygon points="439.96 18.59 439.66 18.59 439.66 18.59 439.66 18.59 439.66 19.45 446.86 19.45 446.86 18.58 439.96 18.59"/>
  <polygon points="447.16 72.46 446.87 72.46 446.87 72.46 446.86 72.46 446.87 73.33 454.07 73.33 454.07 72.46 447.16 72.46"/>
  <polygon points="454.07 52.03 447.16 52.03 446.87 52.03 446.87 52.03 446.86 52.03 446.87 65.63 454.07 65.63 454.07 52.03"/>
  <polygon points="454.07 71.45 454.07 70.58 447.16 70.59 446.87 70.59 446.87 70.59 446.86 70.59 446.87 71.45 454.07 71.45"/>
  <polygon points="439.66 13.63 446.86 13.63 446.86 .03 440.11 .03 439.66 .03 439.66 .03 439.66 .03 439.66 13.63"/>
  <polygon points="439.66 45.45 446.86 45.45 446.86 44.58 439.96 44.59 439.66 44.59 439.66 44.59 439.66 44.59 439.66 45.45"/>
  <polygon points="439.66 21.33 446.86 21.33 446.86 20.46 439.96 20.46 439.66 20.46 439.66 20.46 439.66 20.46 439.66 21.33"/>
  <polygon points="439.66 39.63 446.86 39.63 446.86 26.03 439.96 26.03 439.66 26.03 439.66 26.03 439.66 26.03 439.66 39.63"/>
  <polygon points="446.87 46.46 446.87 46.46 446.86 46.46 446.87 47.33 454.07 47.33 454.07 46.46 447.16 46.46 446.87 46.46"/>
  <polygon points="446.87 13.63 454.07 13.63 454.07 .03 447.31 .03 446.87 .03 446.87 .03 446.86 .03 446.87 13.63"/>
  <polygon points="461.28 71.45 461.28 70.58 454.37 70.59 454.07 70.59 454.07 70.59 454.07 70.59 454.07 71.45 461.28 71.45"/>
  <polygon points="454.07 18.58 447.16 18.59 446.87 18.59 446.87 18.59 446.86 18.59 446.87 19.45 454.07 19.45 454.07 18.58"/>
  <polygon points="454.37 72.46 454.07 72.46 454.07 72.46 454.07 72.46 454.07 73.33 461.28 73.33 461.28 72.46 454.37 72.46"/>
  <polygon points="446.87 21.33 454.07 21.33 454.07 20.46 447.16 20.46 446.87 20.46 446.87 20.46 446.86 20.46 446.87 21.33"/>
  <polygon points="454.07 44.58 447.16 44.59 446.87 44.59 446.87 44.59 446.86 44.59 446.87 45.45 454.07 45.45 454.07 44.58"/>
  <polygon points="454.07 26.03 447.16 26.03 446.87 26.03 446.87 26.03 446.86 26.03 446.87 39.63 454.07 39.63 454.07 26.03"/>
  <polygon points="483.2 72.46 482.9 72.46 482.9 72.46 482.9 72.46 482.9 73.31 482.9 73.32 482.9 73.32 482.9 73.33 490.11 73.33 490.1 72.46 483.2 72.46"/>
  <polygon points="576.59 71.45 576.59 71.45 583.8 71.45 583.8 70.67 583.8 70.59 583.79 70.59 583.79 70.58 576.89 70.59 576.59 70.59 576.59 70.59 576.59 70.59 576.59 71.31 576.59 71.45 576.59 71.45"/>
  <polygon points="559.94 39.63 562.18 39.62 562.17 16.91 562.17 13.03 562.17 13.03 562.17 13.03 560.04 13.03 559.94 13.03 559.94 5.58 555.28 5.59 554.97 5.59 554.97 5.59 554.97 5.59 554.97 6.32 554.97 6.45 554.97 6.45 554.97 6.45 559.08 6.45 559.08 13.03 558.11 13.03 558.07 13.03 558.07 7.46 555.16 7.46 554.97 7.46 554.97 7.46 554.97 7.46 554.97 8.2 554.97 8.32 554.97 8.32 554.97 8.33 557.2 8.32 557.21 13.03 555.06 13.03 554.97 13.03 554.97 13.03 554.97 13.03 554.97 35.75 554.97 39.62 554.97 39.62 554.97 39.63 557.2 39.62 557.21 47.33 562.18 47.33 562.17 46.58 562.17 46.46 562.17 46.46 562.17 46.46 558.24 46.46 558.07 46.46 558.07 39.63 559.08 39.62 559.08 45.45 562.18 45.45 562.17 44.71 562.17 44.59 562.17 44.59 562.17 44.58 560.04 44.59 559.94 44.59 559.94 39.63"/>
  <polygon points="540.56 57.43 540.56 65.62 540.56 65.62 540.56 65.63 547.76 65.63 547.76 10.96 547.76 .03 547.76 .03 547.76 .03 541.01 .03 540.56 .03 540.56 .03 540.56 .03 540.56 57.43"/>
  <polygon points="583.8 72.46 583.79 72.46 583.79 72.46 576.89 72.46 576.59 72.46 576.59 72.46 576.59 72.46 576.59 73.18 576.59 73.32 576.59 73.32 576.59 73.33 583.8 73.33 583.8 72.55 583.8 72.46"/>
  <polygon points="567.15 52.63 569.38 52.62 569.38 29.35 569.38 26.03 569.38 26.03 569.38 26.03 567.24 26.03 567.15 26.03 567.15 18.58 562.38 18.59 562.18 18.59 562.18 18.59 562.18 18.59 562.18 19.32 562.18 19.45 562.18 19.45 562.18 19.45 566.28 19.45 566.29 26.03 565.32 26.03 565.28 26.03 565.27 20.46 562.31 20.46 562.18 20.46 562.18 20.46 562.18 20.46 562.18 21.2 562.18 21.32 562.18 21.32 562.18 21.33 564.41 21.32 564.41 26.03 562.27 26.03 562.18 26.03 562.18 26.03 562.18 26.03 562.18 48.75 562.18 52.62 562.18 52.62 562.18 52.63 564.41 52.62 564.41 60.33 569.38 60.33 569.38 59.57 569.38 59.46 569.38 59.46 569.38 59.46 565.45 59.46 565.28 59.46 565.27 52.63 566.28 52.62 566.29 58.45 569.38 58.45 569.38 57.69 569.38 57.59 569.38 57.59 569.38 57.58 567.24 57.59 567.15 57.59 567.15 52.63"/>
  <polygon points="554.97 .03 554.97 .03 554.97 .03 548.21 .03 547.77 .03 547.77 .03 547.76 .03 547.77 23.3 547.77 26.62 547.77 26.62 547.77 26.63 554.97 26.63 554.97 3.91 554.97 .03"/>
  <polygon points="554.97 33.58 554.97 33.46 554.97 33.46 554.97 33.46 552 33.46 551.87 33.46 551.87 33.46 551.87 33.46 551.87 72.46 547.93 72.46 547.77 72.46 547.77 72.46 547.76 72.46 547.77 73.22 547.77 73.32 547.77 73.32 547.77 73.33 552.74 73.33 552.73 34.33 554.97 34.32 554.97 33.58"/>
  <polygon points="554.97 31.59 554.97 31.59 554.97 31.58 550.2 31.59 550 31.59 550 31.59 550 31.59 550 70.58 547.86 70.59 547.77 70.59 547.77 70.59 547.76 70.59 547.77 71.34 547.77 71.45 547.77 71.45 547.77 71.45 550.86 71.45 550.86 32.45 554.97 32.45 554.97 31.71 554.97 31.59"/>
  <polygon points="576.59 72.46 576.59 72.46 576.59 72.46 572.65 72.46 572.48 72.46 572.48 65.63 573.49 65.62 573.49 71.45 576.59 71.45 576.59 70.69 576.59 70.59 576.59 70.59 576.59 70.58 574.45 70.59 574.36 70.59 574.36 65.63 576.59 65.62 576.59 42.35 576.59 39.03 576.59 39.03 576.59 39.03 574.45 39.03 574.36 39.03 574.36 31.58 569.59 31.59 569.39 31.59 569.39 31.59 569.38 31.59 569.39 32.31 569.39 32.45 569.39 32.45 569.39 32.45 573.49 32.45 573.49 39.03 572.53 39.03 572.48 39.03 572.48 33.46 569.51 33.46 569.39 33.46 569.39 33.46 569.38 33.46 569.39 34.18 569.39 34.32 569.39 34.32 569.39 34.33 571.62 34.32 571.62 39.03 569.48 39.03 569.39 39.03 569.39 39.03 569.38 39.03 569.39 61.2 569.39 65.62 569.39 65.62 569.39 65.63 571.62 65.62 571.62 73.33 576.59 73.33 576.59 72.57 576.59 72.46"/>
  <polygon points="591.01 65.63 598.21 65.63 598.21 5.49 598.21 .03 598.21 .03 598.21 .03 591.46 .03 591.01 .03 591.01 .03 591 .03 591.01 65.63"/>
  <polygon points="598.21 72.46 598.21 72.46 598.21 72.46 591.31 72.46 591.01 72.46 591.01 72.46 591 72.46 591.01 73.33 598.21 73.33 598.21 72.53 598.21 72.46"/>
  <polygon points="598.21 70.59 598.21 70.58 591.31 70.59 591.01 70.59 591.01 70.59 591 70.59 591.01 71.45 598.21 71.45 598.21 70.66 598.21 70.59 598.21 70.59"/>
  <polygon points="603.18 5.58 598.52 5.59 598.22 5.59 598.22 5.59 598.21 5.59 598.21 6.45 602.32 6.45 602.32 72.46 598.38 72.46 598.22 72.46 598.22 72.46 598.21 72.46 598.21 73.33 603.19 73.33 603.18 5.58"/>
  <polygon points="591 72.46 591 72.46 591 72.46 584.1 72.46 583.8 72.46 583.8 72.46 583.8 72.46 583.8 73.33 591 73.33 591 72.55 591 72.46"/>
  <polygon points="583.8 65.63 591 65.63 591 6.86 591 .03 591 .03 591 .03 584.25 .03 583.8 .03 583.8 .03 583.8 .03 583.8 65.63"/>
  <polygon points="504.52 20.46 497.61 20.46 497.32 20.46 497.32 20.46 497.31 20.46 497.32 21.29 497.32 21.32 497.32 21.32 497.32 21.33 504.52 21.33 504.52 20.46"/>
  <polygon points="591 70.59 591 70.58 584.1 70.59 583.8 70.59 583.8 70.59 583.8 70.59 583.8 71.45 591 71.45 591 70.67 591 70.59 591 70.59"/>
  <polygon points="576.59 52.03 576.59 52.03 576.59 52.03 576.59 63.36 576.59 65.62 576.59 65.62 576.59 65.63 583.8 65.63 583.8 53.44 583.8 52.03 583.79 52.03 583.79 52.03 581.66 52.03 581.57 52.03 581.56 44.58 576.8 44.59 576.59 44.59 576.59 44.59 576.59 44.59 576.59 45.31 576.59 45.45 576.59 45.45 576.59 45.45 580.7 45.45 580.7 52.03 579.73 52.03 579.69 52.03 579.69 46.46 576.72 46.46 576.59 46.46 576.59 46.46 576.59 46.46 576.59 47.18 576.59 47.32 576.59 47.32 576.59 47.33 578.82 47.32 578.83 52.03 576.68 52.03 576.59 52.03"/>
  <polygon points="540.56 71.45 540.56 71.45 547.76 71.45 547.76 70.73 547.76 70.59 547.76 70.59 547.76 70.58 540.86 70.59 540.56 70.59 540.56 70.59 540.56 70.59 540.56 71.34 540.56 71.45 540.56 71.45"/>
  <polygon points="504.82 26.03 504.52 26.03 504.52 26.03 504.52 26.03 504.52 38.78 504.52 39.62 504.52 39.62 504.52 39.63 511.73 39.63 511.72 26.03 504.82 26.03"/>
  <polygon points="504.82 18.59 504.52 18.59 504.52 18.59 504.52 18.59 504.52 19.4 504.52 19.45 504.52 19.45 504.52 19.45 511.73 19.45 511.72 18.58 504.82 18.59"/>
  <polygon points="504.82 44.59 504.52 44.59 504.52 44.59 504.52 44.59 504.52 45.4 504.52 45.45 504.52 45.45 504.52 45.45 511.73 45.45 511.72 44.58 504.82 44.59"/>
  <polygon points="497.32 13.06 497.32 13.62 497.32 13.62 497.32 13.63 504.52 13.63 504.52 .03 497.76 .03 497.32 .03 497.32 .03 497.31 .03 497.32 13.06"/>
  <polygon points="504.52 18.58 497.61 18.59 497.32 18.59 497.32 18.59 497.31 18.59 497.32 19.41 497.32 19.45 497.32 19.45 497.32 19.45 504.52 19.45 504.52 18.58"/>
  <polygon points="504.82 46.46 504.52 46.46 504.52 46.46 504.52 46.46 504.52 47.27 504.52 47.32 504.52 47.32 504.52 47.33 511.73 47.33 511.72 46.46 504.82 46.46"/>
  <polygon points="504.97 .03 504.52 .03 504.52 .03 504.52 .03 504.52 12.78 504.52 13.62 504.52 13.62 504.52 13.63 511.73 13.63 511.72 .03 504.97 .03"/>
  <polygon points="504.82 20.46 504.52 20.46 504.52 20.46 504.52 20.46 504.52 21.27 504.52 21.32 504.52 21.32 504.52 21.33 511.73 21.33 511.72 20.46 504.82 20.46"/>
  <polygon points="536.62 72.46 536.45 72.46 536.45 65.63 537.46 65.62 537.46 71.45 540.56 71.45 540.55 70.58 538.42 70.59 538.32 70.59 538.32 65.63 540.56 65.62 540.55 .03 533.8 .03 533.35 .03 533.35 .03 533.35 .03 533.35 58.79 533.35 65.62 533.35 65.62 533.35 65.63 535.58 65.62 535.58 73.33 540.56 73.33 540.55 72.46 536.62 72.46"/>
  <polygon points="547.76 72.46 547.76 72.46 547.76 72.46 540.86 72.46 540.56 72.46 540.56 72.46 540.56 72.46 540.56 73.22 540.56 73.32 540.56 73.32 540.56 73.33 547.76 73.33 547.76 72.6 547.76 72.46"/>
  <polygon points="526.35 18.59 526.14 18.59 526.14 18.59 526.14 18.59 526.14 19.36 526.14 19.45 526.14 19.45 526.14 19.45 530.25 19.45 530.25 72.46 526.31 72.46 526.14 72.46 526.14 72.46 526.14 72.46 526.14 73.24 526.14 73.32 526.14 73.32 526.14 73.33 531.12 73.33 531.11 18.58 526.35 18.59"/>
  <polygon points="518.94 71.45 518.94 71.45 526.14 71.45 526.14 70.58 519.23 70.59 518.94 70.59 518.94 70.59 518.93 70.59 518.94 71.38 518.94 71.45 518.94 71.45"/>
  <polygon points="519.23 72.46 518.94 72.46 518.94 72.46 518.93 72.46 518.94 73.25 518.94 73.32 518.94 73.32 518.94 73.33 526.14 73.33 526.14 72.46 519.23 72.46"/>
  <polygon points="518.94 13.03 518.94 13.03 518.93 13.03 518.94 61.24 518.94 65.62 518.94 65.62 518.94 65.63 526.14 65.63 526.14 13.03 524 13.03 523.91 13.03 523.91 5.58 519.25 5.59 518.94 5.59 518.94 5.59 518.93 5.59 518.94 6.38 518.94 6.45 518.94 6.45 518.94 6.45 523.04 6.45 523.04 13.03 522.08 13.03 522.03 13.03 522.03 7.46 519.13 7.46 518.94 7.46 518.94 7.46 518.93 7.46 518.94 8.25 518.94 8.32 518.94 8.32 518.94 8.33 521.17 8.32 521.17 13.03 519.03 13.03 518.94 13.03"/>
  <polygon points="515 72.46 514.83 72.46 514.82 65.77 514.82 65.63 515.83 65.62 515.84 71.45 518.93 71.45 518.93 70.58 516.8 70.59 516.7 70.59 516.7 65.73 516.7 65.63 518.93 65.62 518.93 .03 512.18 .03 511.73 .03 511.73 .03 511.73 .03 511.73 61.53 511.73 65.62 511.73 65.62 511.73 65.63 513.96 65.62 513.96 73.33 518.93 73.33 518.93 72.46 515 72.46"/>
  <polygon points="461.37 13.03 461.28 13.03 461.28 13.03 461.28 13.03 461.28 26.63 468.49 26.63 468.48 13.03 466.35 13.03 466.25 13.03 466.25 6.2 466.25 5.59 466.25 5.59 466.25 5.58 461.59 5.59 461.28 5.59 461.28 5.59 461.28 5.59 461.28 6.45 465.39 6.45 465.39 12.07 465.39 13.03 464.42 13.03 464.38 13.03 464.38 7.92 464.38 7.46 464.38 7.46 464.38 7.46 461.47 7.46 461.28 7.46 461.28 7.46 461.28 7.46 461.28 8.33 463.51 8.32 463.51 12.34 463.51 13.03 461.37 13.03"/>
  <polygon points="309.93 45.45 309.93 45.45 317.14 45.45 317.14 44.73 317.14 44.59 317.13 44.59 317.13 44.58 310.23 44.59 309.93 44.59 309.93 44.59 309.93 44.59 309.93 45.36 309.93 45.45 309.93 45.45"/>
  <polygon points="317.14 28.29 317.14 26.03 317.13 26.03 317.13 26.03 310.23 26.03 309.93 26.03 309.93 26.03 309.93 26.03 309.93 38.21 309.93 39.62 309.93 39.62 309.93 39.63 317.14 39.63 317.14 28.29"/>
  <polygon points="317.14 46.46 317.13 46.46 317.13 46.46 310.23 46.46 309.93 46.46 309.93 46.46 309.93 46.46 309.93 47.24 309.93 47.32 309.93 47.32 309.93 47.33 317.14 47.33 317.14 46.6 317.14 46.46"/>
  <polygon points="317.14 18.73 317.14 18.59 317.13 18.59 317.13 18.58 310.23 18.59 309.93 18.59 309.93 18.59 309.93 18.59 309.93 19.36 309.93 19.45 309.93 19.45 309.93 19.45 317.14 19.45 317.14 18.73"/>
  <polygon points="317.14 20.6 317.14 20.46 317.13 20.46 317.13 20.46 310.23 20.46 309.93 20.46 309.93 20.46 309.93 20.46 309.93 21.24 309.93 21.32 309.93 21.32 309.93 21.33 317.14 21.33 317.14 20.6"/>
  <polygon points="309.93 44.58 305.16 44.59 304.96 44.59 304.96 44.59 304.96 44.59 304.96 70.58 302.82 70.59 302.73 70.59 302.73 70.59 302.72 70.59 302.73 71.45 305.82 71.45 305.82 45.45 309.93 45.45 309.93 44.58"/>
  <polygon points="309.93 46.46 306.96 46.46 306.83 46.46 306.83 46.46 306.83 46.46 306.83 72.46 302.89 72.46 302.73 72.46 302.73 72.46 302.72 72.46 302.73 73.33 307.7 73.33 307.7 47.33 309.93 47.32 309.93 46.46"/>
  <polygon points="309.93 39.63 309.93 26.03 307.79 26.03 307.7 26.03 307.7 21.33 309.93 21.32 309.93 20.46 306.96 20.46 306.83 20.46 306.83 20.46 306.83 20.46 306.83 26.03 305.87 26.03 305.82 26.03 305.82 19.45 309.93 19.45 309.93 18.58 305.16 18.59 304.96 18.59 304.96 18.59 304.96 18.59 304.96 26.03 302.82 26.03 302.73 26.03 302.73 26.03 302.72 26.03 302.73 39.63 309.93 39.63"/>
  <polygon points="302.73 13.63 309.93 13.63 309.93 .03 303.17 .03 302.73 .03 302.73 .03 302.72 .03 302.73 13.63"/>
  <polygon points="324.34 18.59 324.34 18.59 324.34 18.58 317.44 18.59 317.14 18.59 317.14 18.59 317.14 18.59 317.14 19.34 317.14 19.45 317.14 19.45 317.14 19.45 324.34 19.45 324.34 18.73 324.34 18.59"/>
  <polygon points="324.34 .03 324.34 .03 324.34 .03 317.59 .03 317.14 .03 317.14 .03 317.14 .03 317.14 11.93 317.14 13.62 317.14 13.62 317.14 13.63 324.34 13.63 324.34 2.29 324.34 .03"/>
  <polygon points="324.34 20.46 324.34 20.46 324.34 20.46 317.44 20.46 317.14 20.46 317.14 20.46 317.14 20.46 317.14 21.22 317.14 21.32 317.14 21.32 317.14 21.33 324.34 21.33 324.34 20.6 324.34 20.46"/>
  <polygon points="331.55 72.46 331.55 72.46 331.55 72.46 327.62 72.46 327.44 72.46 327.44 65.63 328.45 65.62 328.46 71.45 331.55 71.45 331.55 70.71 331.55 70.59 331.55 70.59 331.55 70.58 329.41 70.59 329.32 70.59 329.32 65.63 331.55 65.62 331.55 9.59 331.55 .03 331.55 .03 331.55 .03 324.79 .03 324.35 .03 324.35 .03 324.34 .03 324.35 57.43 324.35 65.62 324.35 65.62 324.35 65.63 326.58 65.62 326.58 73.33 331.55 73.33 331.55 72.58 331.55 72.46"/>
  <polygon points="324.34 44.59 324.34 44.59 324.34 44.58 317.44 44.59 317.14 44.59 317.14 44.59 317.14 44.59 317.14 45.34 317.14 45.45 317.14 45.45 317.14 45.45 324.34 45.45 324.34 44.73 324.34 44.59"/>
  <polygon points="446.86 52.03 439.96 52.03 439.66 52.03 439.66 52.03 439.66 52.03 439.66 65.63 446.86 65.63 446.86 52.03"/>
  <polygon points="324.34 46.46 324.34 46.46 324.34 46.46 317.44 46.46 317.14 46.46 317.14 46.46 317.14 46.46 317.14 47.22 317.14 47.32 317.14 47.32 317.14 47.33 324.34 47.33 324.34 46.6 324.34 46.46"/>
  <polygon points="309.93 12.21 309.93 13.62 309.93 13.62 309.93 13.63 317.14 13.63 317.14 2.29 317.14 .03 317.13 .03 317.13 .03 310.38 .03 309.93 .03 309.93 .03 309.93 .03 309.93 12.21"/>
  <polygon points="281.1 72.46 281.1 72.46 281.1 72.46 274.2 72.46 273.9 72.46 273.9 72.46 273.9 72.46 273.9 73.27 273.9 73.32 273.9 73.32 273.9 73.33 281.1 73.33 281.1 72.49 281.1 72.46"/>
  <polygon points="281.1 70.59 281.1 70.59 281.1 70.58 274.2 70.59 273.9 70.59 273.9 70.59 273.9 70.59 273.9 71.4 273.9 71.45 273.9 71.45 273.9 71.45 281.1 71.45 281.1 70.62 281.1 70.59"/>
  <polygon points="338.76 72.46 338.76 72.46 338.76 72.46 331.85 72.46 331.55 72.46 331.55 72.46 331.55 72.46 331.55 73.2 331.55 73.32 331.55 73.32 331.55 73.33 338.76 73.33 338.76 72.58 338.76 72.46"/>
  <polygon points="281.1 52.03 281.1 52.03 281.1 52.03 278.96 52.03 278.87 52.03 278.87 44.74 278.87 44.59 278.87 44.59 278.87 44.58 274.1 44.59 273.9 44.59 273.9 44.59 273.9 44.59 273.9 45.4 273.9 45.45 273.9 45.45 273.9 45.45 278 45.45 278.01 51.89 278.01 52.03 277.04 52.03 277 52.03 276.99 46.57 276.99 46.46 276.99 46.46 276.99 46.46 274.02 46.46 273.9 46.46 273.9 46.46 273.9 46.46 273.9 47.27 273.9 47.32 273.9 47.32 273.9 47.33 276.13 47.32 276.13 51.93 276.13 52.03 273.99 52.03 273.9 52.03 273.9 52.03 273.9 52.03 273.9 64.78 273.9 65.62 273.9 65.62 273.9 65.63 281.1 65.63 281.1 52.59 281.1 52.03"/>
  <polygon points="281.1 20.46 281.1 20.46 281.1 20.46 278.13 20.46 278.01 20.46 278.01 20.46 278 20.46 278.01 33.19 278.01 33.46 274.07 33.46 273.9 33.46 273.9 33.46 273.9 33.46 273.9 34.27 273.9 34.32 273.9 34.32 273.9 34.33 278.87 34.33 278.87 21.6 278.87 21.33 281.1 21.32 281.1 20.49 281.1 20.46"/>
  <polygon points="273.89 .03 273.89 .03 273.89 .03 267.14 .03 266.69 .03 266.69 .03 266.69 .03 266.69 25.52 266.69 26.62 266.69 26.62 266.69 26.63 273.9 26.63 273.89 1.69 273.89 .03"/>
  <polygon points="252.57 26.03 252.28 26.03 252.28 26.03 252.27 26.03 252.28 39.34 252.28 39.62 252.28 39.62 252.28 39.63 259.48 39.63 259.48 26.03 252.57 26.03"/>
  <polygon points="295.52 60.16 295.52 65.62 295.52 65.62 295.52 65.63 302.72 65.63 302.72 1.39 302.72 .03 302.72 .03 302.72 .03 295.97 .03 295.52 .03 295.52 .03 295.52 .03 295.52 60.16"/>
  <polygon points="266.69 13.03 266.69 13.03 266.69 13.03 259.78 13.03 259.48 13.03 259.48 13.03 259.48 13.03 259.48 50.98 259.48 52.62 259.48 52.62 259.48 52.63 266.69 52.63 266.69 15.5 266.69 13.03"/>
  <polygon points="269.83 39.03 269.79 39.03 269.79 32.72 269.79 32.45 273.9 32.45 273.89 31.64 273.89 31.59 273.89 31.59 273.89 31.58 269.13 31.59 268.92 31.59 268.92 31.59 268.92 31.59 268.92 38.87 268.92 39.03 266.78 39.03 266.69 39.03 266.69 39.03 266.69 39.03 266.69 64.52 266.69 65.62 266.69 65.62 266.69 65.63 268.92 65.62 268.92 73.16 268.92 73.32 268.92 73.32 268.92 73.33 273.9 73.33 273.89 72.51 273.89 72.46 273.89 72.46 273.89 72.46 269.96 72.46 269.79 72.46 269.79 65.91 269.79 65.63 270.8 65.62 270.8 71.33 270.8 71.45 270.8 71.45 270.8 71.45 273.9 71.45 273.89 70.64 273.89 70.59 273.89 70.59 273.89 70.58 271.76 70.59 271.66 70.59 271.66 65.83 271.66 65.63 273.9 65.62 273.89 40.69 273.89 39.03 273.89 39.03 273.89 39.03 271.76 39.03 271.66 39.03 271.66 34.52 271.66 34.33 273.9 34.32 273.89 33.51 273.89 33.46 273.89 33.46 273.89 33.46 270.93 33.46 270.8 33.46 270.8 33.46 270.8 33.46 270.8 38.91 270.8 39.03 269.83 39.03"/>
  <polygon points="302.72 72.46 302.72 72.46 302.72 72.46 295.82 72.46 295.52 72.46 295.52 72.46 295.52 72.46 295.52 73.25 295.52 73.32 295.52 73.32 295.52 73.33 302.72 73.33 302.72 72.48 302.72 72.46"/>
  <polygon points="281.1 18.59 281.1 18.59 281.1 18.58 276.34 18.59 276.13 18.59 276.13 18.59 276.13 18.59 276.13 31.31 276.13 31.58 273.99 31.59 273.9 31.59 273.9 31.59 273.9 31.59 273.9 32.4 273.9 32.45 273.9 32.45 273.9 32.45 277 32.45 276.99 19.72 276.99 19.45 281.1 19.45 281.1 18.62 281.1 18.59"/>
  <polygon points="295.52 71.45 295.52 71.45 295.52 71.45 302.72 71.45 302.72 70.6 302.72 70.59 302.72 70.59 302.72 70.58 295.82 70.59 295.52 70.59 295.52 70.59 295.52 70.59 295.52 71.38 295.52 71.45"/>
  <polygon points="293.28 65.63 295.52 65.62 295.51 14.12 295.51 13.03 295.51 13.03 295.51 13.03 288.61 13.03 288.31 13.03 288.31 13.03 288.31 13.03 288.31 61.24 288.31 65.62 288.31 65.62 288.31 65.63 290.54 65.62 290.55 73 290.55 73.32 290.55 73.32 290.55 73.33 295.52 73.33 295.51 72.48 295.51 72.46 295.51 72.46 295.51 72.46 291.58 72.46 291.41 72.46 291.41 65.63 292.42 65.62 292.42 71.21 292.42 71.45 292.42 71.45 292.42 71.45 295.52 71.45 295.51 70.6 295.51 70.59 295.51 70.59 295.51 70.58 293.38 70.59 293.28 70.59 293.28 65.63"/>
  <polygon points="281.1 .03 281.1 .03 281.1 .03 274.35 .03 273.9 .03 273.9 .03 273.9 .03 273.9 12.78 273.9 13.62 273.9 13.62 273.9 13.63 281.1 13.63 281.1 .59 281.1 .03"/>
  <polygon points="286.07 5.59 286.07 5.59 286.07 5.58 281.41 5.59 281.11 5.59 281.11 5.59 281.1 5.59 281.11 6.4 281.11 6.45 281.11 6.45 281.11 6.45 285.21 6.45 285.21 19.87 285.21 20.46 281.27 20.46 281.11 20.46 281.11 20.46 281.1 20.46 281.11 21.27 281.11 21.32 281.11 21.32 281.11 21.33 286.08 21.33 286.07 5.91 286.07 5.59"/>
  <polygon points="286.07 57.59 286.07 57.59 286.07 57.58 281.31 57.59 281.11 57.59 281.11 57.59 281.1 57.59 281.11 58.4 281.11 58.45 281.11 58.45 281.11 58.45 285.21 58.45 285.21 71.87 285.21 72.46 281.27 72.46 281.11 72.46 281.11 72.46 281.1 72.46 281.11 73.27 281.11 73.32 281.11 73.32 281.11 73.33 286.08 73.33 286.07 57.91 286.07 57.59"/>
  <polygon points="324.34 26.03 324.34 26.03 324.34 26.03 317.44 26.03 317.14 26.03 317.14 26.03 317.14 26.03 317.14 37.93 317.14 39.62 317.14 39.62 317.14 39.63 324.34 39.63 324.34 28.29 324.34 26.03"/>
  <polygon points="415.8 13.62 415.8 13.03 415.8 5.59 415.8 5.59 415.8 5.58 411.14 5.59 410.83 5.59 410.83 5.59 410.83 5.59 410.83 6.45 414.94 6.45 414.94 68.33 414.94 72.46 411 72.46 410.83 72.46 410.83 72.46 410.83 72.46 410.83 73.33 415.8 73.33 415.8 16.87 415.8 13.62"/>
  <polygon points="403.62 65.63 410.83 65.63 410.83 2.76 410.83 .03 410.83 .03 410.83 .03 404.07 .03 403.63 .03 403.63 .03 403.62 .03 403.62 65.63"/>
  <polygon points="403.62 70.59 403.62 70.58 396.72 70.59 396.42 70.59 396.42 70.59 396.42 70.59 396.42 71.45 403.62 71.45 403.62 70.62 403.62 70.59 403.62 70.59"/>
  <polygon points="425.25 70.59 425.25 70.59 425.24 70.59 425.25 71.45 432.45 71.45 432.45 70.58 425.54 70.59 425.25 70.59"/>
  <polygon points="425.54 72.46 425.25 72.46 425.25 72.46 425.24 72.46 425.25 73.33 432.45 73.33 432.45 72.46 425.54 72.46"/>
  <polygon points="396.42 65.63 403.62 65.63 403.62 2.76 403.62 .03 403.62 .03 403.62 .03 396.87 .03 396.42 .03 396.42 .03 396.42 .03 396.42 65.63"/>
  <polygon points="410.83 70.59 410.83 70.58 403.92 70.59 403.63 70.59 403.63 70.59 403.62 70.59 403.62 71.45 410.83 71.45 410.83 70.62 410.83 70.59 410.83 70.59"/>
  <polygon points="410.83 72.46 410.83 72.46 410.83 72.46 403.92 72.46 403.63 72.46 403.63 72.46 403.62 72.46 403.62 73.33 410.83 73.33 410.83 72.49 410.83 72.46"/>
  <polygon points="425.24 72.46 425.24 72.46 425.24 72.46 421.31 72.46 421.14 72.46 421.13 66.62 421.13 65.63 422.14 65.62 422.15 70.96 422.15 71.45 422.15 71.45 422.15 71.45 425.24 71.45 425.24 70.6 425.24 70.59 425.24 70.59 425.24 70.58 423.1 70.59 423.01 70.59 423.01 66.35 423.01 65.63 425.24 65.62 425.24 1.39 425.24 .03 425.24 .03 425.24 .03 418.49 .03 418.04 .03 418.04 .03 418.04 .03 418.04 65.63 420.27 65.62 420.27 72.68 420.27 73.32 420.27 73.32 420.27 73.33 425.24 73.33 425.24 72.48 425.24 72.46"/>
  <polygon points="439.66 39.63 439.65 26.03 437.52 26.03 437.42 26.03 437.42 21.91 437.42 21.33 439.66 21.32 439.65 20.46 436.69 20.46 436.56 20.46 436.56 20.46 436.56 20.46 436.56 25.45 436.56 26.03 435.59 26.03 435.55 26.03 435.55 20.27 435.55 19.45 439.66 19.45 439.65 18.58 434.89 18.59 434.69 18.59 434.69 18.59 434.68 18.59 434.69 25.25 434.69 26.03 432.54 26.03 432.45 26.03 432.45 26.03 432.45 26.03 432.45 39.63 439.66 39.63"/>
  <polygon points="432.45 13.63 439.66 13.63 439.65 .03 432.9 .03 432.45 .03 432.45 .03 432.45 .03 432.45 13.63"/>
  <polygon points="439.96 72.46 439.66 72.46 439.66 72.46 439.66 72.46 439.66 73.33 446.86 73.33 446.86 72.46 439.96 72.46"/>
  <polygon points="439.66 70.59 439.66 70.59 439.66 70.59 439.66 71.45 446.86 71.45 446.86 70.58 439.96 70.59 439.66 70.59"/>
  <polygon points="432.75 72.46 432.45 72.46 432.45 72.46 432.45 72.46 432.45 73.33 439.66 73.33 439.65 72.46 432.75 72.46"/>
  <polygon points="425.25 65.63 432.45 65.63 432.45 .03 425.69 .03 425.25 .03 425.25 .03 425.24 .03 425.25 65.63"/>
  <polygon points="331.55 71.45 331.55 71.45 338.76 71.45 338.76 70.71 338.76 70.59 338.76 70.59 338.76 70.58 331.85 70.59 331.55 70.59 331.55 70.59 331.55 70.59 331.55 71.32 331.55 71.45 331.55 71.45"/>
  <polygon points="439.66 65.63 439.65 52.03 437.52 52.03 437.42 52.03 437.42 47.91 437.42 47.33 439.66 47.32 439.65 46.46 436.69 46.46 436.56 46.46 436.56 46.46 436.56 46.46 436.56 51.45 436.56 52.03 435.59 52.03 435.55 52.03 435.55 46.27 435.55 45.45 439.66 45.45 439.65 44.58 434.89 44.59 434.69 44.59 434.69 44.59 434.68 44.59 434.69 51.25 434.69 52.03 432.54 52.03 432.45 52.03 432.45 52.03 432.45 52.03 432.45 65.63 439.66 65.63"/>
  <polygon points="432.45 70.59 432.45 70.59 432.45 70.59 432.45 71.45 439.66 71.45 439.65 70.58 432.75 70.59 432.45 70.59"/>
  <polygon points="353.18 65.63 360.38 65.63 360.38 6.86 360.38 .03 360.38 .03 360.38 .03 353.62 .03 353.18 .03 353.18 .03 353.17 .03 353.18 65.63"/>
  <polygon points="367.58 31.59 367.58 31.59 367.58 31.58 362.82 31.59 362.62 31.59 362.62 31.59 362.61 31.59 362.62 70.58 360.47 70.59 360.38 70.59 360.38 70.59 360.38 70.59 360.38 71.45 363.48 71.45 363.48 32.45 367.59 32.45 367.58 31.67 367.58 31.59"/>
  <polygon points="367.58 33.55 367.58 33.46 367.58 33.46 367.58 33.46 364.62 33.46 364.49 33.46 364.49 33.46 364.49 33.46 364.49 72.46 360.55 72.46 360.38 72.46 360.38 72.46 360.38 72.46 360.38 73.33 365.35 73.33 365.35 34.33 367.59 34.32 367.58 33.55"/>
  <polygon points="360.38 72.46 360.38 72.46 360.38 72.46 353.47 72.46 353.18 72.46 353.18 72.46 353.17 72.46 353.18 73.33 360.38 73.33 360.38 72.55 360.38 72.46"/>
  <polygon points="360.38 70.59 360.38 70.58 353.47 70.59 353.18 70.59 353.18 70.59 353.17 70.59 353.18 71.45 360.38 71.45 360.38 70.67 360.38 70.59 360.38 70.59"/>
  <polygon points="353.17 72.46 353.17 72.46 353.17 72.46 349.24 72.46 349.07 72.46 349.06 65.63 350.07 65.62 350.08 71.45 353.17 71.45 353.17 70.69 353.17 70.59 353.17 70.59 353.17 70.58 351.03 70.59 350.94 70.59 350.94 65.63 353.17 65.62 353.17 8.23 353.17 .03 353.17 .03 353.17 .03 346.42 .03 345.97 .03 345.97 .03 345.97 .03 345.97 65.63 348.2 65.62 348.2 73.33 353.17 73.33 353.17 72.57 353.17 72.46"/>
  <polygon points="331.55 13.03 331.55 13.03 331.55 13.03 331.55 57.96 331.55 65.62 331.55 65.62 331.55 65.63 338.76 65.63 338.76 20.7 338.76 13.03 338.76 13.03 338.76 13.03 336.62 13.03 336.53 13.03 336.52 5.58 331.86 5.59 331.55 5.59 331.55 5.59 331.55 5.59 331.55 6.32 331.55 6.45 331.55 6.45 331.55 6.45 335.66 6.45 335.66 13.03 334.69 13.03 334.65 13.03 334.65 7.46 331.75 7.46 331.55 7.46 331.55 7.46 331.55 7.46 331.55 8.2 331.55 8.32 331.55 8.32 331.55 8.33 333.78 8.32 333.79 13.03 331.64 13.03 331.55 13.03"/>
  <polygon points="338.97 18.59 338.76 18.59 338.76 18.59 338.76 18.59 338.76 19.32 338.76 19.45 338.76 19.45 338.76 19.45 342.87 19.45 342.87 72.46 338.93 72.46 338.76 72.46 338.76 72.46 338.76 72.46 338.76 73.2 338.76 73.32 338.76 73.32 338.76 73.33 343.73 73.33 343.73 18.58 338.97 18.59"/>
  <polygon points="389.21 72.46 389.21 72.46 389.21 72.46 385.27 72.46 385.1 72.46 385.1 65.63 386.11 65.62 386.11 71.33 386.11 71.45 386.11 71.45 386.11 71.45 389.21 71.45 389.21 70.64 389.21 70.59 389.21 70.59 389.21 70.58 387.07 70.59 386.98 70.59 386.97 65.63 389.21 65.62 389.21 40.69 389.21 39.03 389.21 39.03 389.21 39.03 387.07 39.03 386.98 39.03 386.97 31.58 382.21 31.59 382 31.59 382 31.59 382 31.59 382 32.45 386.11 32.45 386.11 38.89 386.11 39.03 385.14 39.03 385.1 39.03 385.1 33.46 382.13 33.46 382 33.46 382 33.46 382 33.46 382 34.33 384.23 34.32 384.24 38.93 384.24 39.03 382.09 39.03 382 39.03 382 39.03 382 39.03 382 65.63 384.23 65.62 384.24 73.16 384.24 73.32 384.24 73.32 384.24 73.33 389.21 73.33 389.21 72.51 389.21 72.46"/>
  <polygon points="396.41 72.46 396.41 72.46 396.41 72.46 389.51 72.46 389.21 72.46 389.21 72.46 389.21 72.46 389.21 73.33 396.42 73.33 396.41 72.51 396.41 72.46"/>
  <polygon points="396.41 70.59 396.41 70.58 389.51 70.59 389.21 70.59 389.21 70.59 389.21 70.59 389.21 71.45 396.42 71.45 396.41 70.64 396.41 70.59 396.41 70.59"/>
  <polygon points="389.3 52.03 389.21 52.03 389.21 52.03 389.21 52.03 389.21 65.63 396.42 65.63 396.41 52.88 396.41 52.03 396.41 52.03 396.41 52.03 394.28 52.03 394.18 52.03 394.18 44.58 389.42 44.59 389.21 44.59 389.21 44.59 389.21 44.59 389.21 45.45 393.32 45.45 393.32 51.75 393.32 52.03 392.35 52.03 392.31 52.03 392.3 46.46 389.34 46.46 389.21 46.46 389.21 46.46 389.21 46.46 389.21 47.33 391.44 47.32 391.44 51.83 391.44 52.03 389.3 52.03"/>
  <polygon points="379.77 52.63 382 52.62 382 28.24 382 26.03 382 26.03 382 26.03 379.86 26.03 379.77 26.03 379.77 18.58 375 18.59 374.8 18.59 374.8 18.59 374.79 18.59 374.8 19.45 378.9 19.45 378.9 25.89 378.9 26.03 377.94 26.03 377.89 26.03 377.89 20.46 374.92 20.46 374.8 20.46 374.8 20.46 374.79 20.46 374.8 21.33 377.03 21.32 377.03 25.93 377.03 26.03 374.89 26.03 374.8 26.03 374.8 26.03 374.79 26.03 374.8 52.63 377.03 52.62 377.03 60.16 377.03 60.32 377.03 60.32 377.03 60.33 382 60.33 382 59.53 382 59.46 382 59.46 382 59.46 378.06 59.46 377.89 59.46 377.89 52.63 378.9 52.62 378.9 58.33 378.9 58.45 378.9 58.45 378.9 58.45 382 58.45 382 57.66 382 57.59 382 57.59 382 57.58 379.86 57.59 379.77 57.59 379.77 52.63"/>
  <polygon points="372.56 39.63 374.79 39.62 374.79 15.24 374.79 13.03 374.79 13.03 374.79 13.03 372.65 13.03 372.56 13.03 372.56 5.58 367.9 5.59 367.59 5.59 367.59 5.59 367.59 5.59 367.59 6.45 371.69 6.45 371.7 13.03 370.73 13.03 370.69 13.03 370.68 7.46 367.78 7.46 367.59 7.46 367.59 7.46 367.59 7.46 367.59 8.33 369.82 8.32 369.82 13.03 367.68 13.03 367.59 13.03 367.59 13.03 367.59 13.03 367.59 39.63 369.82 39.62 369.82 47.33 374.79 47.33 374.79 46.53 374.79 46.46 374.79 46.46 374.79 46.46 370.86 46.46 370.69 46.46 370.68 39.63 371.69 39.62 371.7 45.45 374.79 45.45 374.79 44.66 374.79 44.59 374.79 44.59 374.79 44.58 372.65 44.59 372.56 44.59 372.56 39.63"/>
  <polygon points="367.58 .03 367.58 .03 367.58 .03 360.83 .03 360.38 .03 360.38 .03 360.38 .03 360.38 26.63 367.59 26.63 367.58 2.8 367.58 .03"/>
  <g>
    <path d="M136.97,34.35h5v-15.11s-.03-.68-.03-.68h-4.76s-.18.02-.18.02v-4.42s-.03-1.16-.03-1.16l-2.2.03v-6.98s-.03-.49-.03-.49h-4.66s-.31.03-.31.03V0s-6.76,0-6.76,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03h-6.75s-.45.03-.45.03v-.03h-6.75s-.48.03-.48.03l.03,65.62,2.2-.03.03,7.73h46.01v-12.22s-.03-.78-.03-.78l2.24-.03v.03h5v-15.11s-.03-.68-.03-.68h-4.76s-.18.02-.18.02v-4.42s-.03-1.16-.03-1.16l-2.2.03v-4.41s-.03-.27-.03-.27l2.24-.03v.03ZM136.99,26.66v-5.3s2.18-.03,2.18-.03l.03,10.23h-2.14s-.09.02-.09.02v-.02s-4.77,0-4.77,0l-.24.03.03,7.41h-2.14s-.08.02-.08.02v-12.36s7.22,0,7.22,0ZM129.76,13.02v-4.72s0,.05,0,.05l2.2-.03.03,4.68h-2.14s-.09.02-.09.02ZM129.76,6.48l4.08-.03.03,6.55-.98.03v-5.23s-.03-.38-.03-.38h-2.9s-.19.03-.19.03v-1.06s0,.08,0,.08ZM122.54,52h-6.9s-.3.03-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03l-.03-4.68,2.24-.03v.03h14.41v4.65ZM122.54,46.43h-6.9s-.3.03-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.97,0-2.97,0l-.16.03.03,5.54-.98.03-.03-6.55,4.11-.03v.03h14.41v.95ZM122.54,44.55h-6.9s-.3.03-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-4.77,0-4.77,0l-.24.03.03,7.41-2.21.03v-12.38s7.18,0,7.18,0h14.4v4.9ZM122.53,26h-6.89s-.3.03-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03l-.03-4.68,2.24-.03v.03h14.4v4.65ZM122.53,20.43h-6.89s-.3.03-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.97,0-2.97,0l-.16.03.03,5.54-.98.03-.03-6.55,4.11-.03v.03h14.39v.95ZM108.14,13.66h14.39v4.9s-6.89,0-6.89,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-4.77,0-4.77,0l-.24.03.03,7.41h-2.14s-.08.02-.08.02v-12.37s7.2,0,7.2,0ZM141.04,45.45l.03,13.98h-3.94s-.17.03-.17.03v-.03s-2.98,0-2.98,0l-.16.03.03,12.97h-3.94s-.17.03-.17.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-3.93,0-3.93,0l-.14.03-.03-6.8.98-.03.03,5.85,3.1-.03v.03h39.16v-12.22s-.03-.78-.03-.78l4.11-.03v.03h3.13v-11.52s-.03-.53-.03-.53h-2.97s-.1.02-.1.02v-.97s4.05-.03,4.05-.03ZM136.99,52.66v-5.3s2.18-.03,2.18-.03l.03,10.23h-2.14s-.09.02-.09.02v-.02s-4.77,0-4.77,0l-.24.03.03,12.97h-2.14s-.1.03-.1.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.2.03-2.2.03l-.03-4.93,2.24-.03v.03l7.21-.03v.03h28.85v-13s7.21,0,7.21,0ZM133.99,33.43l-.16.03.03,5.54-.98.03v-6.17s-.03-.38-.03-.38l4.11-.03v.03h3.13v-11.52s-.03-.53-.03-.53h-2.97s-.1.02-.1.02v-.97s4.05-.03,4.05-.03l.03,13.98h-3.94s-.17.03-.17.03v-.03s-2.98,0-2.98,0Z"/>
    <path d="M223.65,31.56l-.2.02v-5.58s-6.92,0-6.92,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.33.03.03,13.62,2.2-.03v7.06s.03.67.03.67h38.8l-.03-15.8h-4.77ZM194.62,39.63v.03h28.85v-5.3s2.18-.03,2.18-.03l.03,10.23h-2.14s-.1.03-.1.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03l-.03-4.93,2.24-.03ZM223.62,46.43l-.17.02v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-3.93,0-3.93,0l-.14.03-.03-6.8.98-.03v5.34s.03.51.03.51l3.1-.03v.03h31.95l-.03-12.05h-2.97s-.11.02-.11.02v-.97s4.06-.03,4.06-.03l.03,13.98h-3.94Z"/>
    <path d="M194.83,57.56l-.2.02v-5.58s-6.92,0-6.92,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03l-.03-4.68,2.24-.03v.03h19.42l-.03-15.8h-4.77s-.18.02-.18.02v-5.27s-.03-.31-.03-.31h-6.9s-.3.03-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03l-.03-4.68,2.24-.03v.03h33.83l-.03-15.8h-4.66s-.3.03-.3.03V0s-6.77,0-6.77,0l-.45.03v-.03s-6.76,0-6.76,0l-.45.03v-.03s-6.76,0-6.76,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.48.03v62.86s.03,2.76.03,2.76l2.2-.03v7.54s.03.19.03.19h53.22l-.03-15.8h-4.77ZM184.28,32.45v13.13s.03.85.03.85h-3.94s-.17.03-.17.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.97,0-2.97,0l-.16.03v5.33s.03.2.03.2l-.98.03v-6.44s-.03-.11-.03-.11l4.11-.03v.03h17.54l-.03-12.05h-2.97s-.1.02-.1.02v-.97s4.05-.03,4.05-.03ZM165.79,39.66h14.44v-5.3s2.18-.03,2.18-.03v9.62s.03.61.03.61h-2.14s-.1.03-.1.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-4.77,0-4.77,0l-.24.03v7.13s.03.28.03.28h-2.14s-.07.02-.07.02v-12.36s7.18,0,7.18,0ZM198.7,6.45v12.84s.03,1.14.03,1.14h-3.94s-.17.03-.17.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.97,0-2.97,0l-.16.03v5.34s.03.2.03.2l-.98.03v-6.44s-.03-.11-.03-.11l4.11-.03v.03h31.95l-.03-12.05h-2.9s-.18.03-.18.03v-.98s4.06-.03,4.06-.03ZM165.79,13.66h28.85v-5.3s2.18-.03,2.18-.03v9.41s.03.83.03.83h-2.14s-.1.03-.1.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-4.77,0-4.77,0l-.24.03v7.13s.03.28.03.28h-2.14s-.07.02-.07.02v-12.36s7.18,0,7.18,0ZM151.38,65.63v.03h43.27v-5.3s2.18-.03,2.18-.03v9.41s.03.83.03.83h-2.14s-.1.03-.1.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03v-4.75s-.03-.18-.03-.18l2.24-.03ZM194.79,72.43l-.17.02v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-3.94,0-3.94,0l-.14.03v-6.55s-.03-.26-.03-.26l.98-.03v5.7s.03.15.03.15l3.1-.03v.03h46.37l-.03-12.05h-2.97s-.11.02-.11.02v-.97s4.07-.03,4.07-.03v12.84s.03,1.14.03,1.14h-3.94Z"/>
    <path d="M79.62,5.56l-.31.03V0s-6.76,0-6.76,0l-.45.03v-.03s-6.75,0-6.75,0l-.48.03.03,65.62,2.2-.03.03,7.73h17.18l-.03-67.8h-4.66ZM72.1,65.63v.03h7.23l-.03-57.36v.05s2.2-.03,2.2-.03l.03,62.23h-2.14s-.1.03-.1.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03l-.03-4.93,2.24-.03ZM79.48,72.43l-.17.02v-.03s-6.9,0-6.9,0l-.3.03v-.03s-3.93,0-3.93,0l-.14.03-.03-6.8.98-.03.03,5.85,3.1-.03v.03h10.33l-.03-64.05h-2.9s-.19.03-.19.03v-1.06s0,.08,0,.08l4.08-.03.03,65.98h-3.94Z"/>
    <path d="M58,5.56l-.31.03V0s-6.76,0-6.76,0l-.45.03v-.03s-6.75,0-6.75,0l-.48.03.02,38.97h-6.9s-.33.03-.33.03l.02,12.97h-6.89s-.3.03-.3.03v-.03s-2.2.03-2.2.03l-.03-7.48h-4.76s-.2.03-.2.03v-5.58s-2.21.03-2.21.03l-.03-33.48h-4.66s-.31.03-.31.03V0s-6.76,0-6.76,0l-.45.03v-.03H.48s-.48.03-.48.03l.03,39.62,2.2-.03.03,7.73h4.97v5.3s2.2-.03,2.2-.03l.03,7.73h4.97v5.3s2.21-.03,2.21-.03l.03,7.73h31.6l-.03-13,2.24-.03v.03h5v-12.76s-.03-.24-.03-.24l2.24-.03v.03h5l-.03-41.8h-4.66ZM7.23,39.63v4.96s0-.03,0-.03l-2.2.03-.03-4.93,2.23-.03ZM7.23,46.43h-3.93s-.14.03-.14.03l-.03-6.8.98-.03.03,5.85,3.1-.03v1.05s0-.07,0-.07ZM25.73,45.45l.03,6.55-.98.03-.03-5.6h-2.97s-.12.03-.12.03v-.98s4.07-.03,4.07-.03ZM21.66,52.02v-4.67s2.19-.03,2.19-.03l.03,4.68h-2.14s-.08.02-.08.02ZM21.65,65.63v.03h21.65l-.02-13h7.22v-13s7.21,0,7.21,0l-.02-31.3,2.2-.03.03,36.23h-2.14s-.1.03-.1.03v-.03s-4.77,0-4.77,0l-.24.03v12.73s.03.24.03.24h-2.14s-.1.03-.1.03v-.03s-4.77,0-4.77,0l-.24.03.03,12.97h-2.14s-.1.03-.1.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03l-.03-4.93,2.24-.03ZM14.45,6.48l4.08-.03.03,32.55-.98.03-.03-31.6h-2.9s-.19.03-.19.03v-1.04s0,.06,0,.06ZM14.43,52.63v4.93s-2.19.03-2.19.03l-.03-4.93,2.22-.03ZM10.51,59.43l-.14.03-.03-6.8.98-.03.03,5.85,3.09-.03v.98s-3.93,0-3.93,0ZM14.54,39l-.07.02-.02-30.69v.03s2.2-.03,2.2-.03l.03,30.68h-2.14ZM57.86,46.43l-.17.02v-.03s-2.97,0-2.97,0l-.16.03v12.73s.03.24.03.24h-3.94s-.17.03-.17.03v-.03s-2.97,0-2.97,0l-.16.03.03,12.97h-3.94s-.17.03-.17.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-3.93,0-3.93,0l-.14.03-.03-6.8.98-.03.03,5.85,3.1-.03v.03h24.75l-.03-13,4.11-.03v.03h3.13v-12.76s-.03-.24-.03-.24l4.11-.03v.03h3.13l-.03-38.05h-2.9s-.19.03-.19.03v-1s0,.02,0,.02l4.08-.03.03,39.98h-3.94Z"/>
    <path d="M468.49,34.35h5v-14.46s-.03-1.34-.03-1.34h-4.76s-.2.03-.2.03v-5.58s-2.21.03-2.21.03v-6.83s-.03-.65-.03-.65h-4.66s-.31.03-.31.03V0s-6.76,0-6.76,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03h-6.75s-.45.03-.45.03v-.03s-6.75,0-6.75,0l-.48.03.03,65.62,2.2-.03v7.06s.03.67.03.67h46.01v-11.95s-.03-1.05-.03-1.05l2.24-.03v.03h5v-14.46s-.03-1.34-.03-1.34h-4.76s-.2.03-.2.03v-5.58s-2.21.03-2.21.03v-4.31s-.03-.36-.03-.36l2.24-.03v.03ZM468.51,26.66v-5.3s2.18-.03,2.18-.03v8.76s.03,1.47.03,1.47h-2.14s-.09.03-.09.03v-.03s-4.77,0-4.77,0l-.24.03v6.36s.03,1.06.03,1.06h-2.14s-.08.02-.08.02v-12.36s7.22,0,7.22,0ZM461.28,13.02v-4.72s0,.05,0,.05l2.2-.03v4.02s.03.66.03.66h-2.14s-.09.02-.09.02ZM461.28,6.48l4.08-.03v5.62s.03.93.03.93l-.98.03v-5.11s-.03-.49-.03-.49h-2.9s-.19.03-.19.03v-1.06s0,.08,0,.08ZM454.07,52h-6.9s-.3.03-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03v-4.12s-.03-.56-.03-.56l2.24-.03v.03h14.41v4.65ZM454.07,46.43h-6.9s-.3.03-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.97,0-2.97,0l-.16.03v4.99s.03.55.03.55l-.98.03v-5.76s-.03-.79-.03-.79l4.11-.03v.03h14.41v.95ZM454.06,44.55h-6.9s-.3.03-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-4.77,0-4.77,0l-.24.03v6.67s.03.75.03.75h-2.14s-.07.02-.07.02v-12.36s7.19,0,7.19,0h14.4v4.9ZM454.06,26h-6.89s-.3.03-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03v-4.12s-.03-.56-.03-.56l2.24-.03v.03h14.39v4.65ZM454.05,20.43h-6.89s-.3.03-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.97,0-2.97,0l-.16.03v4.99s.03.55.03.55l-.98.03v-5.76s-.03-.79-.03-.79l4.11-.03v.03h14.39v.95ZM439.66,13.66h14.39v4.9s-6.89,0-6.89,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-4.77,0-4.77,0l-.24.03v6.67s.03.75.03.75h-2.14s-.08.02-.08.02v-12.37s7.2,0,7.2,0ZM472.56,45.45v11.97s.03,2.01.03,2.01h-3.94s-.17.03-.17.03v-.03s-2.97,0-2.97,0l-.16.03v11.1s.03,1.87.03,1.87h-3.94s-.17.03-.17.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-3.93,0-3.93,0l-.14.03v-5.84s-.03-.97-.03-.97l.98-.03v5.34s.03.51.03.51l3.1-.03v.03h39.16v-11.95s-.03-1.05-.03-1.05l4.11-.03v.03h3.13v-11.02s-.03-1.03-.03-1.03h-2.97s-.11.02-.11.02v-.97s4.07-.03,4.07-.03ZM468.51,52.66v-5.3s2.18-.03,2.18-.03v8.76s.03,1.47.03,1.47h-2.14s-.09.03-.09.03v-.03s-4.77,0-4.77,0l-.24.03v11.1s.03,1.87.03,1.87h-2.14s-.09.03-.09.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.2.03-2.2.03v-4.24s-.03-.69-.03-.69l2.24-.03v.03h36.06v-13s7.21,0,7.21,0ZM465.51,33.43l-.16.03v4.76s.03.78.03.78l-.98.03v-6.03s-.03-.52-.03-.52l4.11-.03v.03h3.13v-11.02s-.03-1.03-.03-1.03h-2.97s-.11.02-.11.02v-.97s4.07-.03,4.07-.03v11.97s.03,2.01.03,2.01h-3.94s-.17.03-.17.03v-.03s-2.97,0-2.97,0Z"/>
    <path d="M603.18,5.55h-4.66s-.29.03-.29.03v-.09l-.03-5.5h-6.75s-.45.03-.45.03v-.03s-6.75,0-6.75,0l-.48.03.02,51.97-2.2.03-.03-7.48h-4.76s-.18.02-.18.02v-2.23s-.03-3.35-.03-3.35l-2.2.03-.03-7.48h-4.76s-.18.02-.18.02v-2.23s-.03-3.35-.03-3.35l-2.2.03-.03-7.48h-4.76s-.18.02-.18.02v-1.67s-.03-3.91-.03-3.91l-2.2.03-.03-7.48h-4.66s-.28.03-.28.03v-1.67s-.03-3.91-.03-3.91h-6.75s-.45.03-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03h-6.75s-.48.03-.48.03v58.76s.03,6.86.03,6.86l2.2-.03.03,7.73h17.18l-.03-39,2.21-.03v1.43s.03,3.91.03,3.91l2.2-.03.03,7.73h4.95v1.4s.03,3.91.03,3.91l2.2-.03.03,7.73h4.95v.84s.03,4.46.03,4.46l2.2-.03.03,7.73h31.6l-.03-67.8ZM580.67,45.45l.03,6.55-.98.03-.03-5.6h-2.97s-.1.02-.1.02v-.97s4.05-.03,4.05-.03ZM576.62,52.02v-4.67s2.18-.03,2.18-.03l.03,4.68h-2.14s-.07.02-.07.02ZM576.59,65.63v.03h21.65V8.35s2.18-.03,2.18-.03l.03,62.23h-2.14s-.09.02-.09.02v-.02s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.21.03-2.21.03l-.03-4.93,2.24-.03ZM573.46,32.45l.03,6.55-.98.03-.03-5.6h-2.97s-.1.02-.1.02v-.97s4.05-.03,4.05-.03ZM569.36,52.63v4.93s-2.18.03-2.18.03l-.03-4.93,2.21-.03ZM566.26,19.45l.03,6.55-.98.03-.03-5.6h-2.97s-.1.02-.1.02v-.97s4.05-.03,4.05-.03ZM562.15,39.63v4.93s-2.18.03-2.18.03l-.03-4.93,2.21-.03ZM559.05,6.45l.03,6.55-.98.03-.03-5.6h-2.9s-.17.02-.17.02v-.98s4.05-.03,4.05-.03ZM540.56,65.63v.03h7.23V26.66s7.15,0,7.15,0v4.9s-4.74,0-4.74,0l-.24.03.03,38.97h-2.14s-.09.02-.09.02v-.02s-6.91,0-6.91,0l-.3.03v-.03s-2.2.03-2.2.03l-.03-4.93,2.24-.03ZM552,33.43l-.16.03.03,38.97h-3.94s-.17.03-.17.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-3.93,0-3.93,0l-.14.03-.03-6.8.98-.03.03,5.85,3.09-.03v.03h10.34l-.03-39,4.08-.03v.98s-2.94,0-2.94,0ZM555.06,13l-.07.02v-4.67s2.18-.03,2.18-.03l.03,4.68h-2.14ZM558.24,46.43l-.14.03-.03-6.8.98-.03.03,5.85,3.07-.03v.98s-3.91,0-3.91,0ZM562.27,26l-.07.02v-4.67s2.18-.03,2.18-.03l.03,4.68h-2.14ZM565.45,59.43l-.14.03-.03-6.8.98-.03.03,5.85,3.07-.03v.98s-3.91,0-3.91,0ZM569.48,39l-.07.02v-4.67s2.18-.03,2.18-.03l.03,4.68h-2.14ZM598.38,72.43l-.17.02v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-3.94,0-3.94,0l-.14.03-.03-6.8.98-.03.03,5.85,3.09-.03v.03h24.75l-.03-64.05h-2.9s-.17.02-.17.02v-.98s4.05-.03,4.05-.03l.03,65.98h-3.94Z"/>
    <path d="M526.35,18.56l-.21.02v-5.58s-2.2.03-2.2.03l-.03-7.48h-4.66s-.31.03-.31.03V0s-6.76,0-6.76,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03h-6.75s-.48.03-.48.03v12.97s-6.88,0-6.88,0l-.33.03v51.5s.03,1.12.03,1.12l2.2-.03v6.42s.03,1.31.03,1.31h17.18v-24.95s-.03-1.05-.03-1.05l2.24-.03v.03h14.39v14.17s.03,4.13.03,4.13l2.2-.03.03,7.73h17.18l-.03-54.8h-4.76ZM497.32,13.66h14.38v4.9s-6.88,0-6.88,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-4.77,0-4.77,0l-.24.03.03,7.41h-2.14s-.08.02-.08.02v-12.37s7.2,0,7.2,0ZM511.7,26h-6.88s-.3.03-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03v-4.51s-.03-.17-.03-.17l2.24-.03v.03h14.38v4.65ZM511.7,20.43h-6.88s-.3.03-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.97,0-2.97,0l-.16.03.03,5.54-.98.03v-6.31s-.03-.25-.03-.25l4.11-.03v.03h14.38v.95ZM482.9,65.63v.03h7.23v-26s7.18,0,7.18,0h14.38v4.9s-6.88,0-6.88,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-4.77,0-4.77,0l-.24.03.03,25.97h-2.14s-.09.03-.09.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.2.03-2.2.03v-4.65s-.03-.28-.03-.28l2.24-.03ZM504.82,46.43l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.97,0-2.97,0l-.16.03.03,25.97h-3.94s-.17.03-.17.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-3.93,0-3.93,0l-.14.03v-6.41s-.03-.4-.03-.4l.98-.03v4.85s.03,1,.03,1l3.1-.03v.03h10.33v-24.95s-.03-1.05-.03-1.05l4.11-.03v.03h14.39v.95s-6.88,0-6.88,0ZM518.94,6.48l4.08-.03.03,6.55-.98.03-.03-5.6h-2.9s-.19.03-.19.03v-.99s0,0,0,0ZM518.94,13.02v-4.68s0,0,0,0l2.2-.03.03,4.68h-2.14s-.09.02-.09.02ZM518.94,65.63v.03h7.23l-.03-44.31h0s2.2-.03,2.2-.03l.03,49.23h-2.14s-.09.03-.09.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.2.03-2.2.03v-4.85s-.03-.08-.03-.08l2.24-.03ZM526.31,72.43l-.17.02v-.03s-6.91,0-6.91,0l-.3.03v-.03s-3.93,0-3.93,0l-.14.03v-6.69s-.03-.11-.03-.11l.98-.03.03,5.85,3.09-.03v.03h10.34l-.03-51.05h-2.97s-.13.03-.13.03v-.99s0,0,0,0l4.08-.03.03,52.98h-3.94Z"/>
    <path d="M281.11,21.35h5V5.91s-.03-.36-.03-.36h-4.66s-.28.03-.28.03V.59s-.03-.59-.03-.59h-6.75s-.45.03-.45.03v-.03s-6.75,0-6.75,0l-.48.03v12.97s-6.88,0-6.88,0l-.33.03v12.97s-6.88,0-6.88,0l-.3.03v-.03s-2.2.03-2.2.03V6.86s-.03-1.31-.03-1.31h-4.66s-.31.03-.31.03V0s-6.76,0-6.76,0l-.45.03v-.03h-6.75s-.48.03-.48.03l.03,65.62,2.2-.03.03,7.73h17.18v-24.4s-.03-1.6-.03-1.6l2.24-.03v.03h7.18v3.62s.03,1.68.03,1.68h7.18v11.86s.03,1.14.03,1.14l2.2-.03v7.54s.03.19.03.19h17.18v-15.44s-.03-.36-.03-.36h-4.76s-.18.02-.18.02v-4.98s-.03-.59-.03-.59l-2.2.03v-7.29s-.03-.18-.03-.18h-4.76s-.18.02-.18.02v-3.89s-.03-1.69-.03-1.69l-2.2.03v-4.51s-.03-.17-.03-.17l2.24-.03v.03h5v-12.76s-.03-.24-.03-.24l2.24-.03v.03ZM245.07,6.48l4.08-.03.03,19.55-.98.03V8.62s-.03-1.19-.03-1.19h-2.9s-.19.03-.19.03v-1.06s0,.08,0,.08ZM247.3,26h-2.14s-.08.02-.08.02V8.3s0,.05,0,.05l2.2-.03.03,17.68ZM237.86,65.63v.03h7.23v-26s7.18,0,7.18,0h7.18v4.9s-6.88,0-6.88,0l-.3.03v-.03s-4.77,0-4.77,0l-.24.03.03,25.97h-2.14s-.1.03-.1.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03v-4.54s-.03-.38-.03-.38l2.24-.03ZM252.57,46.43l-.3.03v-.03s-2.97,0-2.97,0l-.16.03.03,25.97h-3.94s-.17.03-.17.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-3.93,0-3.93,0l-.14.03v-6.26s-.03-.54-.03-.54l.98-.03.03,5.85,3.1-.03v.03h10.33v-24.4s-.03-1.6-.03-1.6l4.11-.03v.03h7.18v.95s-6.88,0-6.88,0ZM268.9,38.87l.03.13h-2.14s-.07.02-.07.02v-12.36s7.21,0,7.21,0v-13s7.21,0,7.21,0v-5.3s2.18-.03,2.18-.03v9.83s.03.4.03.4h-2.14s-.1.03-.1.03v-.03s-4.76,0-4.76,0l-.24.03v12.73s.03.24.03.24h-2.14s-.1.03-.1.03v-.03s-4.76,0-4.76,0l-.24.03v7.29ZM285.18,58.45v13.43s.03.56.03.56h-3.94s-.17.03-.17.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-3.93,0-3.93,0l-.14.03v-6.55s-.03-.26-.03-.26l.98-.03v5.7s.03.15.03.15l3.1-.03v.03h10.33v-11.77s-.03-.28-.03-.28h-2.97s-.1.02-.1.02v-.97s4.05-.03,4.05-.03ZM277.98,45.45v6.44s.03.11.03.11l-.98.03v-5.46s-.03-.14-.03-.14h-2.97s-.1.02-.1.02v-.97s4.05-.03,4.05-.03ZM273.92,52.02v-4.67s2.18-.03,2.18-.03l.03,4.68h-2.14s-.07.02-.07.02ZM273.9,65.63v.03h7.23v-5.3s2.18-.03,2.18-.03v9.83s.03.4.03.4h-2.14s-.1.03-.1.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03v-4.75s-.03-.18-.03-.18l2.24-.03ZM278.13,20.43l-.16.03v12.73s.03.24.03.24h-3.94s-.17.03-.17.03v-.03s-2.97,0-2.97,0l-.16.03v5.45s.03.09.03.09l-.98.03v-6.31s-.03-.25-.03-.25l4.11-.03v.03h3.13v-12.76s-.03-.24-.03-.24l4.11-.03v.03h3.13V7.71s-.03-.28-.03-.28h-2.9s-.17.02-.17.02v-.98s4.05-.03,4.05-.03v13.43s.03.56.03.56h-3.94s-.17.03-.17.03v-.03s-2.97,0-2.97,0Z"/>
    <path d="M415.8,5.55h-4.66s-.29.03-.29.03v-2.82s-.03-2.76-.03-2.76h-6.75s-.45.03-.45.03v-.03s-6.75,0-6.75,0l-.48.03.02,51.97-2.2.03-.03-7.48h-4.76s-.18.02-.18.02v-3.89s-.03-1.69-.03-1.69l-2.2.03-.03-7.48h-4.76s-.18.02-.18.02v-3.33s-.03-2.24-.03-2.24l-2.2.03-.03-7.48h-4.76s-.18.02-.18.02v-3.33s-.03-2.24-.03-2.24l-2.2.03-.03-7.48h-4.66s-.28.03-.28.03v-2.78s-.03-2.8-.03-2.8h-6.75s-.45.03-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.48.03.03,65.62,2.2-.03.03,7.73h17.18l-.03-39,2.23-.03v5.33s2.21-.03,2.21-.03l.03,7.73h4.97v5.3s2.21-.03,2.21-.03v7.54s.03.19.03.19h4.97v5.3s2.21-.03,2.21-.03v7.54s.03.19.03.19h31.6V13.62s-.03-8.07-.03-8.07ZM393.29,45.45v6.31s.03.25.03.25l-.98.03-.03-5.6h-2.97s-.1.02-.1.02v-.97s4.05-.03,4.05-.03ZM389.23,52.02v-4.67s2.18-.03,2.18-.03v4.51s.03.17.03.17h-2.14s-.07.02-.07.02ZM389.21,65.63v.03h21.65V8.35s2.18-.03,2.18-.03v58.37s.03,3.86.03,3.86h-2.14s-.09.03-.09.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.2.03-2.2.03l-.03-4.93,2.24-.03ZM386.08,32.45v6.44s.03.11.03.11l-.98.03-.03-5.6h-2.97s-.1.02-.1.02v-.97s4.05-.03,4.05-.03ZM381.99,52.63v4.93s-2.19.03-2.19.03l-.03-4.93,2.22-.03ZM378.87,19.45v6.44s.03.11.03.11l-.98.03-.03-5.6h-2.97s-.1.02-.1.02v-.97s4.05-.03,4.05-.03ZM374.78,39.63v4.93s-2.19.03-2.19.03l-.03-4.93,2.22-.03ZM371.67,6.45l.03,6.55-.98.03-.03-5.6h-2.9s-.17.02-.17.02v-.98s4.05-.03,4.05-.03ZM353.18,65.63v.03h7.23V26.66s7.17,0,7.17,0v4.9s-4.76,0-4.76,0l-.24.03.03,38.97h-2.14s-.09.02-.09.02v-.02s-6.91,0-6.91,0l-.3.03v-.03s-2.21.03-2.21.03l-.03-4.93,2.24-.03ZM364.62,33.43l-.16.03.03,38.97h-3.94s-.17.03-.17.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-3.94,0-3.94,0l-.14.03-.03-6.8.98-.03.03,5.85,3.1-.03v.03h10.33l-.03-39,4.1-.03v.98s-2.97,0-2.97,0ZM367.68,13l-.07.02v-4.67s2.18-.03,2.18-.03l.03,4.68h-2.14ZM370.86,46.43l-.14.03-.03-6.8.98-.03.03,5.85,3.09-.03v.98s-3.93,0-3.93,0ZM374.89,26l-.07.02v-4.67s2.18-.03,2.18-.03l.03,4.68h-2.14ZM378.06,59.43l-.14.03-.03-6.8.98-.03v5.7s.03.15.03.15l3.09-.03v.98s-3.93,0-3.93,0ZM382.09,39l-.07.02v-4.67s2.18-.03,2.18-.03l.03,4.68h-2.14ZM411,72.43l-.17.02v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-3.93,0-3.93,0l-.14.03-.03-6.8.98-.03v5.7s.03.15.03.15l3.1-.03v.03h24.75V13.62s-.03-6.2-.03-6.2h-2.9s-.17.02-.17.02v-.98s4.05-.03,4.05-.03v61.88s.03,4.1.03,4.1h-3.94Z"/>
    <path d="M338.97,18.56l-.19.02-.02-5.58-2.2.03-.03-7.48h-4.66s-.3.03-.3.03l-.02-5.58h-6.75s-.45.03-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.45.03v-.03s-6.75,0-6.75,0l-.48.03v12.97s-6.88,0-6.88,0l-.33.03v48.21s.03,4.41.03,4.41l2.2-.03v7.38s.03.35.03.35h17.18l-.03-26,2.23-.03v.03h14.39v10.08s.03,8.23.03,8.23l2.2-.03.03,7.73h17.18l-.03-54.8h-4.76ZM324.32,26h-6.88s-.3.03-.3.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03l-.03-4.68,2.23-.03v.03h14.39v4.65ZM324.32,20.43h-6.88s-.3.03-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.97,0-2.97,0l-.16.03.03,5.54-.98.03-.03-6.55,4.11-.03v.03h14.39v.95ZM309.93,13.66h14.38v4.9s-6.88,0-6.88,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-4.77,0-4.77,0l-.24.03.03,7.41h-2.14s-.07.02-.07.02v-12.36s7.18,0,7.18,0ZM295.52,65.63v.03h7.23v-26s7.18,0,7.18,0h14.38v4.9s-6.88,0-6.88,0l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-4.77,0-4.77,0l-.24.03.03,25.97h-2.14s-.1.03-.1.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-2.2.03-2.2.03l-.03-4.93,2.24-.03ZM317.44,46.43l-.3.03v-.03s-6.91,0-6.91,0l-.3.03v-.03s-2.97,0-2.97,0l-.16.03.03,25.97h-3.94s-.17.03-.17.03v-.03s-6.9,0-6.9,0l-.3.03v-.03s-3.93,0-3.93,0l-.14.03-.03-6.8.98-.03v5.58s.03.27.03.27l3.09-.03v.03h10.34l-.03-26,4.11-.03v.03h14.39v.95s-6.88,0-6.88,0ZM335.63,6.45l.03,6.55-.98.03-.03-5.6h-2.9s-.17.03-.17.03v-.98s4.06-.03,4.06-.03ZM331.58,13.02v-3.42s0-1.24,0-1.24l2.18-.03.03,4.68h-2.14s-.07.02-.07.02ZM331.55,65.63v.03h7.23V21.35s2.18-.03,2.18-.03l.03,49.23h-2.14s-.09.02-.09.02v-.02s-6.91,0-6.91,0l-.3.03v-.03s-2.21.03-2.21.03l-.03-4.93,2.24-.03ZM338.93,72.43l-.17.02v-.03s-6.91,0-6.91,0l-.3.03v-.03s-3.94,0-3.94,0l-.14.03-.03-6.8.98-.03.03,5.85,3.09-.03v.03h10.34l-.03-51.05h-2.97s-.1.02-.1.02v-.97s4.05-.03,4.05-.03l.03,52.98h-3.94Z"/>
  </g>
  <polygon points="403.62 72.46 403.62 72.46 403.62 72.46 396.72 72.46 396.42 72.46 396.42 72.46 396.42 72.46 396.42 73.33 403.62 73.33 403.62 72.49 403.62 72.46"/>
</svg>
</file>

<file path="docs/AGENTS.md">
# Mintlify technical writing rule

You are an AI writing assistant specialised in creating exceptional technical documentation using Mintlify components and following industry-leading technical writing practices.

## Working relationship
- You can push back on ideas-this can lead to better documentation. Cite sources and explain your reasoning when you do so
- ALWAYS ask for clarification rather than making assumptions
- NEVER lie, guess, or make up information

## Project context
- Format: MDX files with YAML frontmatter
- Config: docs.json for navigation, theme, settings
- Components: Mintlify components

## Core writing principles

### Language and style requirements

- Use clear, direct language appropriate for technical audiences
- Write in second person ("you") for instructions and procedures
- Use active voice over passive voice
- Employ present tense for current states, future tense for outcomes
- Avoid jargon unless necessary and define terms when first used
- Maintain consistent terminology throughout all documentation
- Keep sentences concise whilst providing necessary context
- Use parallel structure in lists, headings, and procedures
- Use British English spelling and grammar

### Content organisation standards

- Lead with the most important information (inverted pyramid structure)
- Use progressive disclosure: basic concepts before advanced ones
- Break complex procedures into numbered steps
- Include prerequisites and context before instructions
- Provide expected outcomes for each major step
- Use descriptive, keyword-rich headings for navigation and SEO
- Group related information logically with clear section breaks
- Make content evergreen when possible
- Search for existing information before adding new content. Avoid duplication unless it is done for a strategic reason
- Check existing patterns for consistency

### User-centred approach

- Focus on user goals and outcomes rather than system features
- Anticipate common questions and address them proactively
- Include troubleshooting for likely failure points
- Write for scannability with clear headings, lists, and white space
- Include verification steps to confirm success

### Frontmatter requirements for pages
- title: Clear, descriptive page title
- description: Concise summary for SEO/navigation

### Do not
- Skip frontmatter on any MDX file
- Use absolute URLs for internal links
- Include untested code examples
- Make assumptions - always ask for clarification

## Mintlify component reference

### docs.json

- Refer to the [docs.json schema](https://mintlify.com/docs.json) when building the docs.json file and site navigation

### Callout components

#### Note - Additional helpful information

<Note>
Supplementary information that supports the main content without interrupting flow
</Note>

#### Tip - Best practices and pro tips

<Tip>
Expert advice, shortcuts, or best practices that enhance user success
</Tip>

#### Warning - Important cautions

<Warning>
Critical information about potential issues, breaking changes, or destructive actions
</Warning>

#### Info - Neutral contextual information

<Info>
Background information, context, or neutral announcements
</Info>

#### Check - Success confirmations

<Check>
Positive confirmations, successful completions, or achievement indicators
</Check>

### Code components

#### Single code block

Example of a single code block:

```javascript config.js
const apiConfig = {
  baseURL: 'https://api.example.com',
  timeout: 5000,
  headers: {
    'Authorisation': `Bearer ${process.env.API_TOKEN}`
  }
};
```

#### Code group with multiple languages

Example of a code group:

<CodeGroup>
```javascript Node.js
const response = await fetch('/api/endpoint', {
  headers: { Authorisation: `Bearer ${apiKey}` }
});
```

```python Python
import requests
response = requests.get('/api/endpoint',
  headers={'Authorisation': f'Bearer {api_key}'})
```

```curl cURL
curl -X GET '/api/endpoint' \
  -H 'Authorisation: Bearer YOUR_API_KEY'
```
</CodeGroup>

#### Request/response examples

Example of request/response documentation:

<RequestExample>
```bash cURL
curl -X POST 'https://api.example.com/users' \
  -H 'Content-Type: application/json' \
  -d '{"name": "John Doe", "email": "john@example.com"}'
```
</RequestExample>

<ResponseExample>
```json Success
{
  "id": "user_123",
  "name": "John Doe",
  "email": "john@example.com",
  "created_at": "2024-01-15T10:30:00Z"
}
```
</ResponseExample>

### Structural components

#### Steps for procedures

Example of step-by-step instructions:

<Steps>
<Step title="Install dependencies">
  Run `npm install` to install required packages.

  <Check>
  Verify installation by running `npm list`.
  </Check>
</Step>

<Step title="Configure environment">
  Create a `.env` file with your API credentials.

  ```bash
  API_KEY=your_api_key_here
  ```

  <Warning>
  Never commit API keys to version control.
  </Warning>
</Step>
</Steps>

#### Tabs for alternative content

Example of tabbed content:

<Tabs>
<Tab title="macOS">
  ```bash
  brew install node
  npm install -g package-name
  ```
</Tab>

<Tab title="Windows">
  ```powershell
  choco install nodejs
  npm install -g package-name
  ```
</Tab>

<Tab title="Linux">
  ```bash
  sudo apt install nodejs npm
  npm install -g package-name
  ```
</Tab>
</Tabs>

#### Accordions for collapsible content

Example of accordion groups:

<AccordionGroup>
<Accordion title="Troubleshooting connection issues">
  - **Firewall blocking**: Ensure ports 80 and 443 are open
  - **Proxy configuration**: Set HTTP_PROXY environment variable
  - **DNS resolution**: Try using 8.8.8.8 as DNS server
</Accordion>

<Accordion title="Advanced configuration">
  ```javascript
  const config = {
    performance: { cache: true, timeout: 30000 },
    security: { encryption: 'AES-256' }
  };
  ```
</Accordion>
</AccordionGroup>

### Cards and columns for emphasising information

Example of cards and card groups:

<Card title="Getting started guide" icon="rocket" href="/quickstart">
Complete walkthrough from installation to your first API call in under 10 minutes.
</Card>

<CardGroup cols={2}>
<Card title="Authentication" icon="key" href="/auth">
  Learn how to authenticate requests using API keys or JWT tokens.
</Card>

<Card title="Rate limiting" icon="clock" href="/rate-limits">
  Understand rate limits and best practices for high-volume usage.
</Card>
</CardGroup>

### API documentation components

#### Parameter fields

Example of parameter documentation:

<ParamField path="user_id" type="string" required>
Unique identifier for the user. Must be a valid UUID v4 format.
</ParamField>

<ParamField body="email" type="string" required>
User's email address. Must be valid and unique within the system.
</ParamField>

<ParamField query="limit" type="integer" default="10">
Maximum number of results to return. Range: 1-100.
</ParamField>

<ParamField header="Authorisation" type="string" required>
Bearer token for API authentication. Format: `Bearer YOUR_API_KEY`
</ParamField>

#### Response fields

Example of response field documentation:

<ResponseField name="user_id" type="string" required>
Unique identifier assigned to the newly created user.
</ResponseField>

<ResponseField name="created_at" type="timestamp">
ISO 8601 formatted timestamp of when the user was created.
</ResponseField>

<ResponseField name="permissions" type="array">
List of permission strings assigned to this user.
</ResponseField>

#### Expandable nested fields

Example of nested field documentation:

<ResponseField name="user" type="object">
Complete user object with all associated data.

<Expandable title="User properties">
  <ResponseField name="profile" type="object">
  User profile information including personal details.

  <Expandable title="Profile details">
    <ResponseField name="first_name" type="string">
    User's first name as entered during registration.
    </ResponseField>

    <ResponseField name="avatar_url" type="string | null">
    URL to user's profile picture. Returns null if no avatar is set.
    </ResponseField>
  </Expandable>
  </ResponseField>
</Expandable>
</ResponseField>

### Media and advanced components

#### Frames for images

Wrap all images in frames:

<Frame>
<img src="/images/dashboard.png" alt="Main dashboard showing analytics overview" />
</Frame>

<Frame caption="The analytics dashboard provides real-time insights">
<img src="/images/analytics.png" alt="Analytics dashboard with charts" />
</Frame>

#### Videos

Use the HTML video element for self-hosted video content:

<video
  controls
  className="w-full aspect-video rounded-xl"
  src="link-to-your-video.com"
></video>

Embed YouTube videos using iframe elements:

<iframe
  className="w-full aspect-video rounded-xl"
  src="https://www.youtube.com/embed/4KzFe50RQkQ"
  title="YouTube video player"
  frameBorder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
></iframe>

#### Tooltips

Example of tooltip usage:

<Tooltip tip="Application Programming Interface - protocols for building software">
API
</Tooltip>

#### Updates

Use updates for changelogs:

<Update label="Version 2.1.0" description="Released March 15, 2024">
## New features
- Added bulk user import functionality
- Improved error messages with actionable suggestions

## Bug fixes
- Fixed pagination issue with large datasets
- Resolved authentication timeout problems
</Update>

## Required page structure

Every documentation page must begin with YAML frontmatter:

```yaml
---
title: "Clear, specific, keyword-rich title"
description: "Concise description explaining page purpose and value"
---
```

## Content quality standards

### Code examples requirements

- Always include complete, runnable examples that users can copy and execute
- Show proper error handling and edge case management
- Use realistic data instead of placeholder values
- Include expected outputs and results for verification
- Test all code examples thoroughly before publishing
- Specify language and include filename when relevant
- Add explanatory comments for complex logic
- Never include real API keys or secrets in code examples


### Accessibility requirements

- Include descriptive alt text for all images and diagrams
- Use specific, actionable link text instead of "click here"
- Ensure proper heading hierarchy starting with H2
- Provide keyboard navigation considerations
- Use sufficient colour contrast in examples and visuals
- Structure content for easy scanning with headers and lists

## Component selection logic

- Use **Steps** for procedures and sequential instructions
- Use **Tabs** for platform-specific content or alternative approaches
- Use **CodeGroup** when showing the same concept in multiple programming languages
- Use **Accordions** for progressive disclosure of information
- Use **RequestExample/ResponseExample** specifically for API endpoint documentation
- Use **ParamField** for API parameters, **ResponseField** for API responses
- Use **Expandable** for nested object properties or hierarchical information
</file>

<file path="docs/CLAUDE.md">
AGENTS.md
</file>

<file path="docs/docs.json">
{
  "$schema": "https://mintlify.com/docs.json",
  "theme": "mint",
  "name": "Vibe Kanban",
  "description": "A kanban board for developers to track coding tasks with AI coding agents",
  "banner": {
    "content": "🎉 **Ultimate Agents Hackathon** - London, September 20th | £10k in prizes! [Register now](https://london.aitinkerers.org/p/ultimate-agents-hackathon-10k-total-prizes)",
    "dismissible": true
  },
  "colors": {
    "primary": "#000000",
    "light": "#fefefe",
    "dark": "#121212"
  },
  "background": {
    "color": {
      "light": "#FAF9F5",
      "dark": "#2F2F2D"
    }
  },
  "favicon": "/logo/v-192.png",
  "navigation": {
    "groups": [
      {
        "group": "Getting started",
        "pages": [
          "index",
          "getting-started",
          "supported-coding-agents"
        ]
      },
      {
        "group": "Core Features",
        "pages": [
          "core-features/creating-projects",
          "core-features/creating-tasks",
          "core-features/subtasks",
          "core-features/creating-task-attempts",
          "core-features/reviewing-code-changes",
          "core-features/resolving-rebase-conflicts",
          "core-features/task-details-full-screen"
        ]
      },
      {
        "group": "Configuration & Customisation",
        "pages": [
          "configuration-customisation/global-settings",
          "configuration-customisation/agent-configurations",
          "configuration-customisation/creating-task-templates",
          "configuration-customisation/keyboard-shortcuts"
        ]
      },
      {
        "group": "Integrations",
        "pages": [
          "integrations/github-integration",
          "integrations/vscode-extension",
          "integrations/mcp-server-configuration",
          "integrations/vibe-kanban-mcp-server"
        ]
      }
    ]
  },
  "logo": {
    "light": "/logo/light.svg",
    "dark": "/logo/dark.svg"
  },
  "navbar": {
    "links": [
      {
        "label": "GitHub",
        "href": "https://github.com/BloopAI/vibe-kanban"
      }
    ],
    "primary": {
      "type": "button",
      "label": "Get Started",
      "href": "https://vibekanban.com/docs"
    }
  },
  "contextual": {
    "options": [
      "copy",
      "view",
      "chatgpt",
      "claude",
      "perplexity",
      "mcp",
      "cursor",
      "vscode"
    ]
  },
  "integrations": {
    "posthog": {
      "apiKey": "phc_V5XpxUvgOfEWk38iGHr0Kve2oZTmWFjDe3mIwTCXzx0",
      "apiHost": "https://eu.i.posthog.com"
    }
  },
  "footer": {
    "socials": {
      "github": "https://github.com/BloopAI/vibe-kanban"
    }
  }
}
</file>

<file path="docs/getting-started.mdx">
---
title: "Install & Run Vibe Kanban"
description: "Complete installation guide for Vibe Kanban on macOS, Linux, and Windows"
sidebarTitle: "Installation"
---

## Supported Systems

We build and test Vibe Kanban on the following systems:

- macOS (Intel and Apple Silicon)
- Linux
- Windows

## Prerequisites

Before installing Vibe Kanban, ensure you have:

- **Node.js**: Latest LTS version recommended
- **Coding agent authentication**: Authenticate with your preferred coding agents outside of Vibe Kanban

<Info>
All supported coding agents can be used via npx without separate installation. You can find authentication instructions for each coding agent on their respective [websites](./supported-coding-agents).
</Info>

## Installation & Setup

<Steps>
<Step title="Install and launch Vibe Kanban">
  Open a terminal and run:

  ```bash
  npx vibe-kanban
  ```

  <Check>
  The application will bind to a random free port, print the URL in the terminal, and automatically open in your default browser.
  </Check>
</Step>

<Step title="Complete initial setup">
  Complete the setup dialogs to configure your coding agent and editor preferences. GitHub authentication is optional and can be configured later.
</Step>

<Step title="Create your first project">
  You'll land on an empty Projects page. Click "Create your first project" to set up your initial project.
</Step>

<Step title="Add tasks">
  Start tracking your work by [creating tasks](/core-features/creating-tasks) within your project.
</Step>

<Step title="Optional: Configure GitHub integration">
  Connect to GitHub in [Settings](/configuration-customisation/global-settings) to enable branch management and pull request creation.
</Step>

<Step title="Optional: Set up MCP integration">
  Streamline task creation with coding agents by [setting up MCP integration](/integrations/vibe-kanban-mcp-server).
</Step>
</Steps>

<Tip>
To use a fixed port, specify the `PORT` environment variable: `PORT=8080 npx vibe-kanban`
</Tip>
</file>

<file path="docs/index.mdx">
---
title: "Vibe Kanban Documentation"
description: "A specialised kanban board designed for developers who want to track and manage their coding tasks while working with AI coding agents. It provides seamless integration with various coding agents and helps you organise your development workflow efficiently."
sidebarTitle: "Home"
---

## What is Vibe Kanban?

Vibe Kanban transforms how developers work with AI coding agents by providing:

- **Task Management**: Create, organise, and track coding tasks on an intuitive kanban board
- **AI Agent Integration**: Works with popular coding agents like Amp, Claude Code, Cursor CLI, and more
- **Git Integration**: Automatic branch management and pull request tracking
- **Workspace Management**: Organise multiple projects with custom setup scripts
- **Real-time Progress Tracking**: Monitor task attempts and coding agent execution

## Key Features

<CardGroup cols={2}>
  <Card title="Multi-Agent Support" icon="robot" href="/supported-coding-agents">
    Integrate with 8+ popular AI coding agents via npx - no separate installation required
  </Card>

  <Card title="Project Management" icon="folder" href="/core-features/creating-projects">
    Organise your repositories with custom setup scripts and development workflows
  </Card>

  <Card title="Task Tracking" icon="list-check" href="/core-features/creating-tasks">
    Create and manage coding tasks with templates, descriptions, and progress tracking
  </Card>

  <Card title="Git Integration" icon="code-branch" href="/integrations/github-integration">
    Automatic branch creation, PR management, and GitHub status synchronisation
  </Card>
</CardGroup>

## Quick Start

Ready to get started? Follow our getting started guide to install and configure Vibe Kanban:

<Card title="Get Started" icon="rocket" href="/getting-started">
  Install Vibe Kanban and complete the setup process in minutes
</Card>

## Need Help?

<CardGroup cols={2}>
  <Card title="User Guide" icon="book" href="/core-features/creating-projects">
    Complete guides for creating projects, tasks, and working with coding agents
  </Card>

  <Card title="Settings" icon="gear" href="/configuration-customisation/global-settings">
    Configure themes, default agents, GitHub integration, and more
  </Card>
</CardGroup>
</file>

<file path="docs/README.md">
# Mintlify Starter Kit

**[Mintlify Quickstart Guide](https://starter.mintlify.com/quickstart)**

## Development

Install the [Mintlify CLI](https://www.npmjs.com/package/mint) to preview your documentation changes locally. To install, use the following command:

```
npm i -g mint
```

Run the following command at the root of your documentation, where your `docs.json` is located:

```
mint dev
```

View your local preview at `http://localhost:3000`.

## Publishing changes

Install our GitHub app from your [dashboard](https://dashboard.mintlify.com/settings/organization/github-app) to propagate changes from your repo to your deployment. Changes are deployed to production automatically after pushing to the default branch.

## Need help?

### Troubleshooting

- If your dev environment isn't running: Run `mint update` to ensure you have the most recent version of the CLI.
- If a page loads as a 404: Make sure you are running in a folder with a valid `docs.json`.

### Resources
- [Mintlify documentation](https://mintlify.com/docs)
- [Mintlify community](https://mintlify.com/community)
</file>

<file path="docs/supported-coding-agents.mdx">
---
title: "Supported Coding Agents"
description: "Complete guide to all coding agents supported by Vibe Kanban, including installation and authentication instructions"
---

Vibe Kanban integrates with a variety of coding agents. These agents are available via `npx` and can be selected when creating task attempts.

<AccordionGroup>

<Accordion title="Amp" icon="code">
**Amp code completion agent**

## Installation
Amp is now available via npx - no separate installation required!

## Authentication
You need to authenticate with Amp outside of Vibe Kanban before using it. Please follow the authentication instructions on the [Amp Owner's Manual](https://ampcode.com/manual#install).
</Accordion>

<Accordion title="Claude Code" icon="code">
**Anthropic's Claude Code**

## Installation
Claude Code is now available via npx - no separate installation required!

## Authentication
You need to authenticate with Claude Code outside of Vibe Kanban before using it. Please follow the authentication instructions on the [Claude Code GitHub page](https://github.com/anthropics/claude-code).
</Accordion>

<Accordion title="Cursor CLI" icon="code">
**Cursor's command-line agent**

## Installation
Install Cursor CLI using the official guide: https://docs.cursor.com/en/cli/installation. On most systems you can run `curl https://cursor.com/install -fsS | bash`, then verify with `cursor-agent --version`.

## Authentication
Sign in with `cursor-agent login` (opens a browser). You can also set the `CURSOR_API_KEY=...` environment variable. Full instructions: https://docs.cursor.com/en/cli/reference/authentication
</Accordion>

<Accordion title="CCR (Claude Code Router)" icon="code">
**Orchestrate multiple Claude Code models**

CCR (Claude Code Router) lets you route coding prompts across different LLM providers and models, and select specialised models for specific tasks like long context, background work, or image understanding.

## Installation
CCR is available via `npx` – no separate installation required.

```bash
npx -y @musistudio/claude-code-router ui
```

This launches the CCR local UI where you configure providers and models.

## Authentication
Authenticate and configure CCR outside of Vibe Kanban. Follow the instructions on the CCR GitHub repo:

- GitHub: https://github.com/musistudio/claude-code-router

You'll add providers, set API keys, and register model names in the CCR UI or via CCR's JSON configuration (see the CCR repo for the schema and file location).

## Configure CCR (Providers and Models)

Configure CCR either via the UI or JSON config.

In the CCR UI (`npx -y @musistudio/claude-code-router ui`):

1) Add providers
- Choose a provider (e.g., `openrouter`, `deepseek`, etc.).
- Enter the required API key(s) and settings for that provider.

2) Add models
- For each provider, register the model identifier (e.g., `moonshotai/kimi-k2-0905`, `deepseek-chat`).
- CCR supports configuring different models for specific cases:
  - `default`: general coding
  - `background`: lightweight/background operations
  - `think`: models that support "thinking" modes
  - `longContext`: very long inputs/files
  - `webSearch`: models that support web/tool use
  - `image`: models with vision capabilities

Note: not all models support web search, thinking, or images. Choose models accordingly in the CCR UI.

### Configure via JSON (optional)
CCR can also be configured via its JSON configuration file. Refer to the CCR GitHub documentation for the exact schema, keys, and file location. Define providers (with API keys) and map the model cases (`default`, `background`, `think`, `longContext`, `webSearch`, `image`) to specific provider/model pairs.

### Example: OpenRouter provider configured in CCR UI

<Frame>
<img src="/images/ccr-openrouter-ui.png" alt="OpenRouter configured in CCR UI" />
</Frame>

### Example: CCR model mapping (default/background/think/etc.)

<Frame>
<img src="/images/ccr-config-example.png" alt="CCR models configuration example" />
</Frame>

## Configure Vibe Kanban

Vibe Kanban does not ship a default configuration for CCR. Add configurations to the existing Claude Code agent:

1) Open the "Coding Agent Configurations" page.
2) Add a new configuration for the Claude Code agent (or edit an existing one).
3) Enable the `claude_code_router` checkbox.
4) Optionally set a model string to target a specific CCR provider/model.

See the [Agent Profiles & Variants](/configuration-customisation/agent-configurations) guide for managing agent configurations.

Model string format: `<provider>,<model-name>`

Examples:
```text
openrouter,moonshotai/kimi-k2-0905
deepseek,deepseek-chat
```

Tips:
- Create multiple configurations if you want easy switching between different models.
- Leave the model string empty if you want CCR to use its own routing based on your CCR UI configuration (e.g., its `default`/`longContext`/etc. mappings).

### Example: Claude Code agent configuration in Vibe Kanban

<Frame>
<img src="/images/vk-ccr-agent-config.png" alt="Claude Code agent configuration in Vibe Kanban" />
</Frame>

## Using CCR in Vibe Kanban

When creating a Task Attempt, select the coding agent and configuration: choose the Claude Code agent, then pick one of your CCR-enabled configurations.

## Troubleshooting

- Authentication errors: verify your API keys/provider settings in CCR (via UI or JSON config).
- Model not found: confirm the model identifier is correct for the chosen provider.
- Missing features (webSearch/think/image): switch to a model that supports the capability and update your CCR mapping (via UI or JSON config).
</Accordion>

<Accordion title="Codex" icon="code">
**OpenAI Codex integration**

## Installation
OpenAI Codex is now available via `npx` - no separate installation required!

## Authentication
You need to configure your OpenAI API credentials before using Codex. Please set the `OPENAI_API_KEY` environment variable. For more details, see the [OpenAI documentation](https://platform.openai.com/docs/guides/code).
</Accordion>

<Accordion title="Gemini CLI" icon="code">
**Google Gemini CLI**

## Installation
Gemini CLI is now available via npx - no separate installation required!

## Authentication
You need to authenticate with Gemini CLI outside of Vibe Kanban before using it. Please follow the authentication instructions on the [Gemini CLI GitHub page](https://github.com/google-gemini/gemini-cli).
</Accordion>

<Accordion title="SST OpenCode" icon="code">
**SST's OpenCode**

## Installation
SST OpenCode is now available via `npx` - no separate installation required!

## Authentication
You need to authenticate with SST OpenCode outside of Vibe Kanban before using it. Please follow the authentication instructions on the [SST OpenCode GitHub page](https://github.com/sst/opencode).
</Accordion>

<Accordion title="Qwen Code" icon="code">
**Qwen code-focused assistant**

## Installation
Qwen Code is available via npx - no separate installation required!

## Authentication
You need to authenticate with Qwen outside of Vibe Kanban before using it. Please follow the authentication instructions in the official Qwen documentation: https://github.com/QwenLM/qwen-code
</Accordion>

</AccordionGroup>
</file>

<file path="frontend/public/mcp/playwright_logo_icon.svg">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" id="Playwright--Streamline-Svg-Logos" height="24" width="24">
  <desc>
    Playwright Streamline Icon: https://streamlinehq.com
  </desc>
  <path fill="#2d4552" d="M7.99585 13.141725c-0.87725 0.248975 -1.452775 0.685475 -1.8319 1.12165 0.363125 -0.317775 0.849525 -0.609425 1.505675 -0.795425 0.6711 -0.1902 1.243625 -0.188825 1.7167 -0.09755v-0.369925c-0.40355 -0.0369 -0.866225 -0.0075 -1.390475 0.14125Zm-1.872 -3.109775 -3.25795 0.858325s0.059375 0.083875 0.1693 0.195775l2.76235 -0.727875s-0.03915 0.5044 -0.379075 0.9556c0.643 -0.486475 0.705375 -1.281825 0.705375 -1.281825Zm2.727125 7.65675C4.26615 18.923575 1.8404825 13.61025 1.1060875 10.852425c-0.3393 -1.273 -0.487415 -2.2371 -0.5268925 -2.859275 -0.0042425 -0.0646 -0.0022825 -0.11905 0.002285 -0.16895 -0.237835 0.01435 -0.3517015 0.137975 -0.328535 0.49525 0.0394775 0.621825 0.187595 1.585875 0.526895 2.859275C1.5139075 13.936125 3.9399 19.24945 8.52475 18.0146c0.99795 -0.26885 1.747675 -0.758525 2.310475 -1.383625 -0.51875 0.468525 -1.168 0.8375 -1.98425 1.057725Zm0.861575 -10.90855v0.3263h1.79835c-0.0369 -0.115525 -0.074075 -0.219625 -0.110975 -0.3263h-1.687375Z" stroke-width="0.25"></path>
  <path fill="#2d4552" d="M11.9129 9.46735c0.80875 0.229675 1.2365 0.7967 1.462575 1.2985l0.90175 0.2561s-0.123 -1.756175 -1.711525 -2.2074c-1.486075 -0.422225 -2.400575 0.8257 -2.5118 0.9872 0.4323 -0.308 1.063575 -0.56015 1.859 -0.3344Zm7.178175 1.3066c-1.487425 -0.424125 -2.401575 0.8264 -2.511175 0.985625 0.432625 -0.307625 1.063575 -0.559875 1.85865 -0.3331 0.80745 0.23005 1.23485 0.796375 1.461625 1.298525l0.90305 0.25705s-0.125 -1.756525 -1.71215 -2.2081Zm-0.8959 4.6305 -7.501475 -2.097125s0.0812 0.411725 0.3928 0.94485l6.3159 1.765675c0.519975 -0.30085 0.792775 -0.6134 0.792775 -0.6134ZM12.994375 19.918475C7.054675 18.326 7.77275 10.758025 8.733875 7.171825c0.395725 -1.4779 0.802575 -2.576375 1.13995 -3.312725 -0.2013 -0.041425 -0.368025 0.0646 -0.532775 0.39965 -0.358225 0.726575 -0.8163 1.90955 -1.259625 3.5656 -0.96085 3.586125 -1.67895 11.15385 4.2605 12.746325 2.79955 0.75 4.980475 -0.3899 6.60625 -2.18005 -1.543175 1.3977 -3.513425 2.181325 -5.9538 1.52785Z" stroke-width="0.25"></path>
  <path fill="#e2574c" d="M9.7126 15.915175V14.388l-4.243175 1.2032s0.313525 -1.82175 2.526475 -2.4495c0.6711 -0.1902 1.2437 -0.1889 1.7167 -0.09755V6.780125h2.124575c-0.231325 -0.714825 -0.4551 -1.26515 -0.64305 -1.64755 -0.310925 -0.632925 -0.62965 -0.21335 -1.35325 0.39185 -0.50965 0.425775 -1.797675 1.33405 -3.7359 1.85635 -1.938275 0.522625 -3.50525 0.384025 -4.15906 0.2708 -0.9268825 -0.1599 -1.4116925 -0.36345 -1.3663375 0.34155 0.03947 0.621825 0.187595 1.58595 0.5268925 2.859275C1.8405325 13.609875 4.266525 18.9232 8.85135 17.68835c1.197625 -0.3227 2.04295 -0.960525 2.6289 -1.7735h-1.76765v0.000325ZM2.865625 10.89025l3.258275 -0.858325s-0.094975 1.25345 -1.31645 1.57545c-1.2218 0.321675 -1.941825 -0.717125 -1.941825 -0.717125Z" stroke-width="0.25"></path>
  <path fill="#2ead33" d="M21.975075 6.8525c-0.84695 0.148475 -2.878875 0.33345 -5.389975 -0.339625 -2.5118 -0.672675 -4.17835 -1.849175 -4.838625 -2.402175 -0.936 -0.783975 -1.347725 -1.328825 -1.752925 -0.5047 -0.358225 0.726875 -0.816325 1.909875 -1.259725 3.565925 -0.960775 3.586125 -1.67885 11.15385 4.260525 12.7463 5.938125 1.591125 9.09945 -5.322175 10.0603 -8.908625 0.4434 -1.655725 0.637825 -2.9095 0.691325 -3.717925 0.061 -0.915775 -0.568025 -0.64995 -1.7709 -0.439175ZM10.0418 9.81945s0.936 -1.45575 2.523525 -1.00455c1.588525 0.451225 1.711525 2.207425 1.711525 2.207425l-4.23505 -1.202875ZM13.917 16.352c-2.79235 -0.817975 -3.223 -3.04465 -3.223 -3.04465l7.501125 2.0972c0 -0.00035 -1.5141 1.755175 -4.278125 0.94745Zm2.6521 -4.57605s0.9347 -1.45475 2.521925 -1.00225c1.587175 0.4519 1.71215 2.2081 1.71215 2.2081l-4.234075 -1.20585Z" stroke-width="0.25"></path>
  <path fill="#d65348" d="M8.2299 14.808525 5.4695 15.590875s0.29985 -1.708225 2.33335 -2.385175l-1.563075 -5.865975 -0.135075 0.04105c-1.93825 0.5227 -3.505225 0.384025 -4.15903 0.2708 -0.9268775 -0.159825 -1.411685 -0.36345 -1.3663375 0.341625 0.0394775 0.621825 0.187595 1.585875 0.5268925 2.85925 0.7340675 2.757425 3.160075 8.07075 7.744875 6.8359l0.135075 -0.042425 -0.756275 -2.8374ZM2.8657 10.8903l3.258275 -0.858375s-0.094975 1.25345 -1.316425 1.57545c-1.221825 0.321675 -1.94185 -0.717075 -1.94185 -0.717075Z" stroke-width="0.25"></path>
  <path fill="#1d8d22" d="m14.04295 16.382625 -0.1263 -0.0307c-2.792325 -0.8179 -3.223 -3.044575 -3.223 -3.044575l3.86805 1.0812 2.047825 -7.86915 -0.024775 -0.006525c-2.5118 -0.672675 -4.17825 -1.849175 -4.838625 -2.402175 -0.936 -0.783975 -1.347725 -1.328825 -1.752925 -0.5047 -0.357875 0.726875 -0.815975 1.909875 -1.259375 3.565925 -0.960775 3.586125 -1.67885 11.15385 4.260525 12.74625l0.121725 0.027425 0.926875 -3.562975ZM10.0418 9.819475s0.936 -1.455775 2.523525 -1.004575c1.588525 0.451225 1.711525 2.207425 1.711525 2.207425l-4.23505 -1.20285Z" stroke-width="0.25"></path>
  <path fill="#c04b41" d="m8.37055 14.7683 -0.740275 0.2101c0.174875 0.9859 0.483125 1.93205 0.966975 2.7679 0.0842 -0.0186 0.167725 -0.034575 0.2535 -0.058075 0.2248 -0.06065 0.43325 -0.13575 0.63395 -0.21765 -0.5406 -0.802225 -0.898225 -1.726175 -1.11415 -2.702275Zm-0.289075 -6.9439c-0.3804 1.419825 -0.720725 3.46345 -0.62705 5.51325 0.167675 -0.072775 0.3448 -0.140575 0.54155 -0.1964l0.13705 -0.030625c-0.167075 -2.189525 0.194075 -4.4207 0.600925 -5.93875 0.103125 -0.384025 0.206525 -0.741225 0.3096 -1.07435 -0.166025 0.105675 -0.3448 0.213975 -0.548425 0.32555 -0.137325 0.42385 -0.276 0.887125 -0.41365 1.401325Z" stroke-width="0.25"></path>
</svg>
</file>

<file path="frontend/public/site.webmanifest">
{"name":"","short_name":"","icons":[{"src":"/android-chrome-192x192.png","sizes":"192x192","type":"image/png"},{"src":"/android-chrome-512x512.png","sizes":"512x512","type":"image/png"}],"theme_color":"#ffffff","background_color":"#ffffff","display":"standalone"}
</file>

<file path="frontend/public/vibe-kanban-logo-dark.svg">
<svg width="604" height="74" viewBox="0 0 604 74" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M0 13.6035V0.00976562H7.20117V13.6035H0ZM7.20703 13.6035V0.00976562H14.4082V13.6035H7.20703ZM18.5215 13.6035V6.42578H14.4141V5.56445H19.3828V13.6035H18.5215ZM16.6465 8.30078H14.4141V7.43945H17.5078V13.6035H16.6465V8.30078ZM43.2422 13.6035V0.00976562H50.4434V13.6035H43.2422ZM50.4492 13.6035V0.00976562H57.6504V13.6035H50.4492ZM61.7637 13.6035V6.42578H57.6562V5.56445H62.625V13.6035H61.7637ZM59.8887 8.30078H57.6562V7.43945H60.75V13.6035H59.8887V8.30078ZM64.8633 13.6035V0.00976562H72.0645V13.6035H64.8633ZM72.0703 13.6035V0.00976562H79.2715V13.6035H72.0703ZM83.3848 13.6035V6.42578H79.2773V5.56445H84.2461V13.6035H83.3848ZM81.5098 8.30078H79.2773V7.43945H82.3711V13.6035H81.5098V8.30078ZM86.4844 13.6035V0.00976562H93.6855V13.6035H86.4844ZM93.6914 13.6035V0.00976562H100.893V13.6035H93.6914ZM100.898 13.6035V0.00976562H108.1V13.6035H100.898ZM108.105 13.6035V0.00976562H115.307V13.6035H108.105ZM115.312 13.6035V0.00976562H122.514V13.6035H115.312ZM122.52 13.6035V0.00976562H129.721V13.6035H122.52ZM133.834 13.6035V6.42578H129.727V5.56445H134.695V13.6035H133.834ZM131.959 8.30078H129.727V7.43945H132.82V13.6035H131.959V8.30078ZM144.141 13.6035V0.00976562H151.342V13.6035H144.141ZM151.348 13.6035V0.00976562H158.549V13.6035H151.348ZM158.555 13.6035V0.00976562H165.756V13.6035H158.555ZM165.762 13.6035V0.00976562H172.963V13.6035H165.762ZM172.969 13.6035V0.00976562H180.17V13.6035H172.969ZM180.176 13.6035V0.00976562H187.377V13.6035H180.176ZM187.383 13.6035V0.00976562H194.584V13.6035H187.383ZM198.697 13.6035V6.42578H194.59V5.56445H199.559V13.6035H198.697ZM196.822 8.30078H194.59V7.43945H197.684V13.6035H196.822V8.30078ZM230.625 13.6035V0.00976562H237.826V13.6035H230.625ZM237.832 13.6035V0.00976562H245.033V13.6035H237.832ZM249.146 13.6035V6.42578H245.039V5.56445H250.008V13.6035H249.146ZM247.271 8.30078H245.039V7.43945H248.133V13.6035H247.271V8.30078ZM266.66 13.6035V0.00976562H273.861V13.6035H266.66ZM273.867 13.6035V0.00976562H281.068V13.6035H273.867ZM285.182 13.6035V6.42578H281.074V5.56445H286.043V13.6035H285.182ZM283.307 8.30078H281.074V7.43945H284.168V13.6035H283.307V8.30078ZM295.488 13.6035V0.00976562H302.689V13.6035H295.488ZM302.695 13.6035V0.00976562H309.896V13.6035H302.695ZM309.902 13.6035V0.00976562H317.104V13.6035H309.902ZM317.109 13.6035V0.00976562H324.311V13.6035H317.109ZM324.316 13.6035V0.00976562H331.518V13.6035H324.316ZM335.631 13.6035V6.42578H331.523V5.56445H336.492V13.6035H335.631ZM333.756 8.30078H331.523V7.43945H334.617V13.6035H333.756V8.30078ZM345.938 13.6035V0.00976562H353.139V13.6035H345.938ZM353.145 13.6035V0.00976562H360.346V13.6035H353.145ZM360.352 13.6035V0.00976562H367.553V13.6035H360.352ZM371.666 13.6035V6.42578H367.559V5.56445H372.527V13.6035H371.666ZM369.791 8.30078H367.559V7.43945H370.652V13.6035H369.791V8.30078ZM396.387 13.6035V0.00976562H403.588V13.6035H396.387ZM403.594 13.6035V0.00976562H410.795V13.6035H403.594ZM414.908 13.6035V6.42578H410.801V5.56445H415.77V13.6035H414.908ZM413.033 8.30078H410.801V7.43945H413.895V13.6035H413.033V8.30078ZM418.008 13.6035V0.00976562H425.209V13.6035H418.008ZM425.215 13.6035V0.00976562H432.416V13.6035H425.215ZM432.422 13.6035V0.00976562H439.623V13.6035H432.422ZM439.629 13.6035V0.00976562H446.83V13.6035H439.629ZM446.836 13.6035V0.00976562H454.037V13.6035H446.836ZM454.043 13.6035V0.00976562H461.244V13.6035H454.043ZM465.357 13.6035V6.42578H461.25V5.56445H466.219V13.6035H465.357ZM463.482 8.30078H461.25V7.43945H464.344V13.6035H463.482V8.30078ZM482.871 13.6035V0.00976562H490.072V13.6035H482.871ZM490.078 13.6035V0.00976562H497.279V13.6035H490.078ZM497.285 13.6035V0.00976562H504.486V13.6035H497.285ZM504.492 13.6035V0.00976562H511.693V13.6035H504.492ZM511.699 13.6035V0.00976562H518.9V13.6035H511.699ZM523.014 13.6035V6.42578H518.906V5.56445H523.875V13.6035H523.014ZM521.139 8.30078H518.906V7.43945H522V13.6035H521.139V8.30078ZM533.32 13.6035V0.00976562H540.521V13.6035H533.32ZM540.527 13.6035V0.00976562H547.729V13.6035H540.527ZM547.734 13.6035V0.00976562H554.936V13.6035H547.734ZM559.049 13.6035V6.42578H554.941V5.56445H559.91V13.6035H559.049ZM557.174 8.30078H554.941V7.43945H558.035V13.6035H557.174V8.30078ZM583.77 13.6035V0.00976562H590.971V13.6035H583.77ZM590.977 13.6035V0.00976562H598.178V13.6035H590.977ZM602.291 13.6035V6.42578H598.184V5.56445H603.152V13.6035H602.291ZM600.416 8.30078H598.184V7.43945H601.277V13.6035H600.416V8.30078ZM0 26.6035V13.0098H7.20117V26.6035H0ZM7.20703 26.6035V13.0098H14.4082V26.6035H7.20703ZM18.5215 26.6035V13.0098H19.3828V26.6035H18.5215ZM16.6465 26.6035V13.0098H17.5078V26.6035H16.6465ZM43.2422 26.6035V13.0098H50.4434V26.6035H43.2422ZM50.4492 26.6035V13.0098H57.6504V26.6035H50.4492ZM61.7637 26.6035V13.0098H62.625V26.6035H61.7637ZM59.8887 26.6035V13.0098H60.75V26.6035H59.8887ZM64.8633 26.6035V13.0098H72.0645V26.6035H64.8633ZM72.0703 26.6035V13.0098H79.2715V26.6035H72.0703ZM83.3848 26.6035V13.0098H84.2461V26.6035H83.3848ZM81.5098 26.6035V13.0098H82.3711V26.6035H81.5098ZM86.4844 26.6035V13.0098H93.6855V26.6035H86.4844ZM93.6914 26.6035V13.0098H100.893V26.6035H93.6914ZM103.992 26.6035H103.131V18.5645H108.1V19.4258H103.992V26.6035ZM105.867 21.3008V26.6035H105.006V20.4395H108.1V21.3008H105.867ZM115.307 19.4258H108.105V18.5645H115.307V19.4258ZM115.307 21.3008H108.105V20.4395H115.307V21.3008ZM122.514 19.4258H115.312V18.5645H122.514V19.4258ZM122.514 21.3008H115.312V20.4395H122.514V21.3008ZM122.52 26.6035V13.0098H129.721V26.6035H122.52ZM129.727 26.6035V13.0098H136.928V26.6035H129.727ZM141.041 26.6035V19.4258H136.934V18.5645H141.902V26.6035H141.041ZM139.166 21.3008H136.934V20.4395H140.027V26.6035H139.166V21.3008ZM144.141 26.6035V13.0098H151.342V26.6035H144.141ZM151.348 26.6035V13.0098H158.549V26.6035H151.348ZM161.648 26.6035H160.787V18.5645H165.756V19.4258H161.648V26.6035ZM163.523 21.3008V26.6035H162.662V20.4395H165.756V21.3008H163.523ZM172.963 19.4258H165.762V18.5645H172.963V19.4258ZM172.963 21.3008H165.762V20.4395H172.963V21.3008ZM180.17 19.4258H172.969V18.5645H180.17V19.4258ZM180.17 21.3008H172.969V20.4395H180.17V21.3008ZM187.377 19.4258H180.176V18.5645H187.377V19.4258ZM187.377 21.3008H180.176V20.4395H187.377V21.3008ZM194.584 19.4258H187.383V18.5645H194.584V19.4258ZM194.584 21.3008H187.383V20.4395H194.584V21.3008ZM198.697 13.0098H199.559V21.3008H194.59V20.4395H198.697V13.0098ZM196.822 18.5645V13.0098H197.684V19.4258H194.59V18.5645H196.822ZM230.625 26.6035V13.0098H237.826V26.6035H230.625ZM237.832 26.6035V13.0098H245.033V26.6035H237.832ZM249.146 26.6035V13.0098H250.008V26.6035H249.146ZM247.271 26.6035V13.0098H248.133V26.6035H247.271ZM259.453 26.6035V13.0098H266.654V26.6035H259.453ZM266.66 26.6035V13.0098H273.861V26.6035H266.66ZM276.961 26.6035H276.1V18.5645H281.068V19.4258H276.961V26.6035ZM278.836 21.3008V26.6035H277.975V20.4395H281.068V21.3008H278.836ZM285.182 13.0098H286.043V21.3008H281.074V20.4395H285.182V13.0098ZM283.307 18.5645V13.0098H284.168V19.4258H281.074V18.5645H283.307ZM288.281 26.6035V13.0098H295.482V26.6035H288.281ZM295.488 26.6035V13.0098H302.689V26.6035H295.488ZM305.789 26.6035H304.928V18.5645H309.896V19.4258H305.789V26.6035ZM307.664 21.3008V26.6035H306.803V20.4395H309.896V21.3008H307.664ZM317.104 19.4258H309.902V18.5645H317.104V19.4258ZM317.104 21.3008H309.902V20.4395H317.104V21.3008ZM324.311 19.4258H317.109V18.5645H324.311V19.4258ZM324.311 21.3008H317.109V20.4395H324.311V21.3008ZM324.316 26.6035V13.0098H331.518V26.6035H324.316ZM331.523 26.6035V13.0098H338.725V26.6035H331.523ZM342.838 26.6035V19.4258H338.73V18.5645H343.699V26.6035H342.838ZM340.963 21.3008H338.73V20.4395H341.824V26.6035H340.963V21.3008ZM345.938 26.6035V13.0098H353.139V26.6035H345.938ZM353.145 26.6035V13.0098H360.346V26.6035H353.145ZM360.352 26.6035V13.0098H367.553V26.6035H360.352ZM367.559 26.6035V13.0098H374.76V26.6035H367.559ZM378.873 26.6035V19.4258H374.766V18.5645H379.734V26.6035H378.873ZM376.998 21.3008H374.766V20.4395H377.859V26.6035H376.998V21.3008ZM396.387 26.6035V13.0098H403.588V26.6035H396.387ZM403.594 26.6035V13.0098H410.795V26.6035H403.594ZM414.908 26.6035V13.0098H415.77V26.6035H414.908ZM413.033 26.6035V13.0098H413.895V26.6035H413.033ZM418.008 26.6035V13.0098H425.209V26.6035H418.008ZM425.215 26.6035V13.0098H432.416V26.6035H425.215ZM435.516 26.6035H434.654V18.5645H439.623V19.4258H435.516V26.6035ZM437.391 21.3008V26.6035H436.529V20.4395H439.623V21.3008H437.391ZM446.83 19.4258H439.629V18.5645H446.83V19.4258ZM446.83 21.3008H439.629V20.4395H446.83V21.3008ZM454.037 19.4258H446.836V18.5645H454.037V19.4258ZM454.037 21.3008H446.836V20.4395H454.037V21.3008ZM454.043 26.6035V13.0098H461.244V26.6035H454.043ZM461.25 26.6035V13.0098H468.451V26.6035H461.25ZM472.564 26.6035V19.4258H468.457V18.5645H473.426V26.6035H472.564ZM470.689 21.3008H468.457V20.4395H471.551V26.6035H470.689V21.3008ZM475.664 26.6035V13.0098H482.865V26.6035H475.664ZM482.871 26.6035V13.0098H490.072V26.6035H482.871ZM493.172 26.6035H492.311V18.5645H497.279V19.4258H493.172V26.6035ZM495.047 21.3008V26.6035H494.186V20.4395H497.279V21.3008H495.047ZM504.486 19.4258H497.285V18.5645H504.486V19.4258ZM504.486 21.3008H497.285V20.4395H504.486V21.3008ZM511.693 19.4258H504.492V18.5645H511.693V19.4258ZM511.693 21.3008H504.492V20.4395H511.693V21.3008ZM511.699 26.6035V13.0098H518.9V26.6035H511.699ZM518.906 26.6035V13.0098H526.107V26.6035H518.906ZM530.221 26.6035V19.4258H526.113V18.5645H531.082V26.6035H530.221ZM528.346 21.3008H526.113V20.4395H529.207V26.6035H528.346V21.3008ZM533.32 26.6035V13.0098H540.521V26.6035H533.32ZM540.527 26.6035V13.0098H547.729V26.6035H540.527ZM547.734 26.6035V13.0098H554.936V26.6035H547.734ZM554.941 26.6035V13.0098H562.143V26.6035H554.941ZM566.256 26.6035V19.4258H562.148V18.5645H567.117V26.6035H566.256ZM564.381 21.3008H562.148V20.4395H565.242V26.6035H564.381V21.3008ZM583.77 26.6035V13.0098H590.971V26.6035H583.77ZM590.977 26.6035V13.0098H598.178V26.6035H590.977ZM602.291 26.6035V13.0098H603.152V26.6035H602.291ZM600.416 26.6035V13.0098H601.277V26.6035H600.416ZM0 39.6035V26.0098H7.20117V39.6035H0ZM7.20703 39.6035V26.0098H14.4082V39.6035H7.20703ZM18.5215 39.6035V26.0098H19.3828V39.6035H18.5215ZM16.6465 39.6035V26.0098H17.5078V39.6035H16.6465ZM43.2422 39.6035V26.0098H50.4434V39.6035H43.2422ZM50.4492 39.6035V26.0098H57.6504V39.6035H50.4492ZM61.7637 39.6035V26.0098H62.625V39.6035H61.7637ZM59.8887 39.6035V26.0098H60.75V39.6035H59.8887ZM64.8633 39.6035V26.0098H72.0645V39.6035H64.8633ZM72.0703 39.6035V26.0098H79.2715V39.6035H72.0703ZM83.3848 39.6035V26.0098H84.2461V39.6035H83.3848ZM81.5098 39.6035V26.0098H82.3711V39.6035H81.5098ZM86.4844 39.6035V26.0098H93.6855V39.6035H86.4844ZM93.6914 39.6035V26.0098H100.893V39.6035H93.6914ZM100.898 39.6035V26.0098H108.1V39.6035H100.898ZM108.105 39.6035V26.0098H115.307V39.6035H108.105ZM115.312 39.6035V26.0098H122.514V39.6035H115.312ZM122.52 39.6035V26.0098H129.721V39.6035H122.52ZM132.82 39.6035H131.959V31.5645H136.928V32.4258H132.82V39.6035ZM134.695 34.3008V39.6035H133.834V33.4395H136.928V34.3008H134.695ZM141.041 26.0098H141.902V34.3008H136.934V33.4395H141.041V26.0098ZM139.166 31.5645V26.0098H140.027V32.4258H136.934V31.5645H139.166ZM144.141 39.6035V26.0098H151.342V39.6035H144.141ZM151.348 39.6035V26.0098H158.549V39.6035H151.348ZM158.555 39.6035V26.0098H165.756V39.6035H158.555ZM165.762 39.6035V26.0098H172.963V39.6035H165.762ZM172.969 39.6035V26.0098H180.17V39.6035H172.969ZM184.283 39.6035V32.4258H180.176V31.5645H185.145V39.6035H184.283ZM182.408 34.3008H180.176V33.4395H183.27V39.6035H182.408V34.3008ZM187.383 39.6035V26.0098H194.584V39.6035H187.383ZM194.59 39.6035V26.0098H201.791V39.6035H194.59ZM201.797 39.6035V26.0098H208.998V39.6035H201.797ZM209.004 39.6035V26.0098H216.205V39.6035H209.004ZM216.211 39.6035V26.0098H223.412V39.6035H216.211ZM227.525 39.6035V32.4258H223.418V31.5645H228.387V39.6035H227.525ZM225.65 34.3008H223.418V33.4395H226.512V39.6035H225.65V34.3008ZM230.625 39.6035V26.0098H237.826V39.6035H230.625ZM237.832 39.6035V26.0098H245.033V39.6035H237.832ZM245.039 39.6035V26.0098H252.24V39.6035H245.039ZM252.246 39.6035V26.0098H259.447V39.6035H252.246ZM259.453 39.6035V26.0098H266.654V39.6035H259.453ZM269.754 39.6035H268.893V31.5645H273.861V32.4258H269.754V39.6035ZM271.629 34.3008V39.6035H270.768V33.4395H273.861V34.3008H271.629ZM277.975 26.0098H278.836V34.3008H273.867V33.4395H277.975V26.0098ZM276.1 31.5645V26.0098H276.961V32.4258H273.867V31.5645H276.1ZM288.281 39.6035V26.0098H295.482V39.6035H288.281ZM295.488 39.6035V26.0098H302.689V39.6035H295.488ZM302.695 39.6035V26.0098H309.896V39.6035H302.695ZM309.902 39.6035V26.0098H317.104V39.6035H309.902ZM317.109 39.6035V26.0098H324.311V39.6035H317.109ZM324.316 39.6035V26.0098H331.518V39.6035H324.316ZM331.523 39.6035V26.0098H338.725V39.6035H331.523ZM342.838 39.6035V26.0098H343.699V39.6035H342.838ZM340.963 39.6035V26.0098H341.824V39.6035H340.963ZM345.938 39.6035V26.0098H353.139V39.6035H345.938ZM353.145 39.6035V26.0098H360.346V39.6035H353.145ZM363.445 39.6035H362.584V31.5645H367.553V32.4258H363.445V39.6035ZM365.32 34.3008V39.6035H364.459V33.4395H367.553V34.3008H365.32ZM367.559 39.6035V26.0098H374.76V39.6035H367.559ZM374.766 39.6035V26.0098H381.967V39.6035H374.766ZM386.08 39.6035V32.4258H381.973V31.5645H386.941V39.6035H386.08ZM384.205 34.3008H381.973V33.4395H385.066V39.6035H384.205V34.3008ZM396.387 39.6035V26.0098H403.588V39.6035H396.387ZM403.594 39.6035V26.0098H410.795V39.6035H403.594ZM414.908 39.6035V26.0098H415.77V39.6035H414.908ZM413.033 39.6035V26.0098H413.895V39.6035H413.033ZM418.008 39.6035V26.0098H425.209V39.6035H418.008ZM425.215 39.6035V26.0098H432.416V39.6035H425.215ZM432.422 39.6035V26.0098H439.623V39.6035H432.422ZM439.629 39.6035V26.0098H446.83V39.6035H439.629ZM446.836 39.6035V26.0098H454.037V39.6035H446.836ZM454.043 39.6035V26.0098H461.244V39.6035H454.043ZM464.344 39.6035H463.482V31.5645H468.451V32.4258H464.344V39.6035ZM466.219 34.3008V39.6035H465.357V33.4395H468.451V34.3008H466.219ZM472.564 26.0098H473.426V34.3008H468.457V33.4395H472.564V26.0098ZM470.689 31.5645V26.0098H471.551V32.4258H468.457V31.5645H470.689ZM475.664 39.6035V26.0098H482.865V39.6035H475.664ZM482.871 39.6035V26.0098H490.072V39.6035H482.871ZM490.078 39.6035V26.0098H497.279V39.6035H490.078ZM497.285 39.6035V26.0098H504.486V39.6035H497.285ZM504.492 39.6035V26.0098H511.693V39.6035H504.492ZM511.699 39.6035V26.0098H518.9V39.6035H511.699ZM518.906 39.6035V26.0098H526.107V39.6035H518.906ZM530.221 39.6035V26.0098H531.082V39.6035H530.221ZM528.346 39.6035V26.0098H529.207V39.6035H528.346ZM533.32 39.6035V26.0098H540.521V39.6035H533.32ZM540.527 39.6035V26.0098H547.729V39.6035H540.527ZM550.828 39.6035H549.967V31.5645H554.936V32.4258H550.828V39.6035ZM552.703 34.3008V39.6035H551.842V33.4395H554.936V34.3008H552.703ZM554.941 39.6035V26.0098H562.143V39.6035H554.941ZM562.148 39.6035V26.0098H569.35V39.6035H562.148ZM573.463 39.6035V32.4258H569.355V31.5645H574.324V39.6035H573.463ZM571.588 34.3008H569.355V33.4395H572.449V39.6035H571.588V34.3008ZM583.77 39.6035V26.0098H590.971V39.6035H583.77ZM590.977 39.6035V26.0098H598.178V39.6035H590.977ZM602.291 39.6035V26.0098H603.152V39.6035H602.291ZM600.416 39.6035V26.0098H601.277V39.6035H600.416ZM3.09375 39.0098V46.4395H7.20117V47.3008H2.23242V39.0098H3.09375ZM4.96875 44.5645H7.20117V45.4258H4.10742V39.0098H4.96875V44.5645ZM7.20703 52.6035V39.0098H14.4082V52.6035H7.20703ZM14.4141 52.6035V39.0098H21.6152V52.6035H14.4141ZM25.7285 52.6035V45.4258H21.6211V44.5645H26.5898V52.6035H25.7285ZM23.8535 47.3008H21.6211V46.4395H24.7148V52.6035H23.8535V47.3008ZM36.0352 52.6035V39.0098H43.2363V52.6035H36.0352ZM43.2422 52.6035V39.0098H50.4434V52.6035H43.2422ZM53.543 52.6035H52.6816V44.5645H57.6504V45.4258H53.543V52.6035ZM55.418 47.3008V52.6035H54.5566V46.4395H57.6504V47.3008H55.418ZM61.7637 39.0098H62.625V47.3008H57.6562V46.4395H61.7637V39.0098ZM59.8887 44.5645V39.0098H60.75V45.4258H57.6562V44.5645H59.8887ZM64.8633 52.6035V39.0098H72.0645V52.6035H64.8633ZM72.0703 52.6035V39.0098H79.2715V52.6035H72.0703ZM83.3848 52.6035V39.0098H84.2461V52.6035H83.3848ZM81.5098 52.6035V39.0098H82.3711V52.6035H81.5098ZM86.4844 52.6035V39.0098H93.6855V52.6035H86.4844ZM93.6914 52.6035V39.0098H100.893V52.6035H93.6914ZM103.992 52.6035H103.131V44.5645H108.1V45.4258H103.992V52.6035ZM105.867 47.3008V52.6035H105.006V46.4395H108.1V47.3008H105.867ZM115.307 45.4258H108.105V44.5645H115.307V45.4258ZM115.307 47.3008H108.105V46.4395H115.307V47.3008ZM122.514 45.4258H115.312V44.5645H122.514V45.4258ZM122.514 47.3008H115.312V46.4395H122.514V47.3008ZM122.52 52.6035V39.0098H129.721V52.6035H122.52ZM129.727 52.6035V39.0098H136.928V52.6035H129.727ZM141.041 52.6035V45.4258H136.934V44.5645H141.902V52.6035H141.041ZM139.166 47.3008H136.934V46.4395H140.027V52.6035H139.166V47.3008ZM144.141 52.6035V39.0098H151.342V52.6035H144.141ZM151.348 52.6035V39.0098H158.549V52.6035H151.348ZM161.648 52.6035H160.787V44.5645H165.756V45.4258H161.648V52.6035ZM163.523 47.3008V52.6035H162.662V46.4395H165.756V47.3008H163.523ZM172.963 45.4258H165.762V44.5645H172.963V45.4258ZM172.963 47.3008H165.762V46.4395H172.963V47.3008ZM180.17 45.4258H172.969V44.5645H180.17V45.4258ZM180.17 47.3008H172.969V46.4395H180.17V47.3008ZM184.283 39.0098H185.145V47.3008H180.176V46.4395H184.283V39.0098ZM182.408 44.5645V39.0098H183.27V45.4258H180.176V44.5645H182.408ZM190.477 39.0098V46.4395H194.584V47.3008H189.615V39.0098H190.477ZM192.352 44.5645H194.584V45.4258H191.49V39.0098H192.352V44.5645ZM201.791 45.4258H194.59V44.5645H201.791V45.4258ZM201.791 47.3008H194.59V46.4395H201.791V47.3008ZM208.998 45.4258H201.797V44.5645H208.998V45.4258ZM208.998 47.3008H201.797V46.4395H208.998V47.3008ZM216.205 45.4258H209.004V44.5645H216.205V45.4258ZM216.205 47.3008H209.004V46.4395H216.205V47.3008ZM223.412 45.4258H216.211V44.5645H223.412V45.4258ZM223.412 47.3008H216.211V46.4395H223.412V47.3008ZM227.525 39.0098H228.387V47.3008H223.418V46.4395H227.525V39.0098ZM225.65 44.5645V39.0098H226.512V45.4258H223.418V44.5645H225.65ZM230.625 52.6035V39.0098H237.826V52.6035H230.625ZM237.832 52.6035V39.0098H245.033V52.6035H237.832ZM248.133 52.6035H247.271V44.5645H252.24V45.4258H248.133V52.6035ZM250.008 47.3008V52.6035H249.146V46.4395H252.24V47.3008H250.008ZM259.447 45.4258H252.246V44.5645H259.447V45.4258ZM259.447 47.3008H252.246V46.4395H259.447V47.3008ZM259.453 52.6035V39.0098H266.654V52.6035H259.453ZM266.66 52.6035V39.0098H273.861V52.6035H266.66ZM277.975 52.6035V45.4258H273.867V44.5645H278.836V52.6035H277.975ZM276.1 47.3008H273.867V46.4395H276.961V52.6035H276.1V47.3008ZM288.281 52.6035V39.0098H295.482V52.6035H288.281ZM295.488 52.6035V39.0098H302.689V52.6035H295.488ZM305.789 52.6035H304.928V44.5645H309.896V45.4258H305.789V52.6035ZM307.664 47.3008V52.6035H306.803V46.4395H309.896V47.3008H307.664ZM317.104 45.4258H309.902V44.5645H317.104V45.4258ZM317.104 47.3008H309.902V46.4395H317.104V47.3008ZM324.311 45.4258H317.109V44.5645H324.311V45.4258ZM324.311 47.3008H317.109V46.4395H324.311V47.3008ZM324.316 52.6035V39.0098H331.518V52.6035H324.316ZM331.523 52.6035V39.0098H338.725V52.6035H331.523ZM342.838 52.6035V39.0098H343.699V52.6035H342.838ZM340.963 52.6035V39.0098H341.824V52.6035H340.963ZM345.938 52.6035V39.0098H353.139V52.6035H345.938ZM353.145 52.6035V39.0098H360.346V52.6035H353.145ZM364.459 52.6035V39.0098H365.32V52.6035H364.459ZM362.584 52.6035V39.0098H363.445V52.6035H362.584ZM370.652 39.0098V46.4395H374.76V47.3008H369.791V39.0098H370.652ZM372.527 44.5645H374.76V45.4258H371.666V39.0098H372.527V44.5645ZM374.766 52.6035V39.0098H381.967V52.6035H374.766ZM381.973 52.6035V39.0098H389.174V52.6035H381.973ZM393.287 52.6035V45.4258H389.18V44.5645H394.148V52.6035H393.287ZM391.412 47.3008H389.18V46.4395H392.273V52.6035H391.412V47.3008ZM396.387 52.6035V39.0098H403.588V52.6035H396.387ZM403.594 52.6035V39.0098H410.795V52.6035H403.594ZM414.908 52.6035V39.0098H415.77V52.6035H414.908ZM413.033 52.6035V39.0098H413.895V52.6035H413.033ZM418.008 52.6035V39.0098H425.209V52.6035H418.008ZM425.215 52.6035V39.0098H432.416V52.6035H425.215ZM435.516 52.6035H434.654V44.5645H439.623V45.4258H435.516V52.6035ZM437.391 47.3008V52.6035H436.529V46.4395H439.623V47.3008H437.391ZM446.83 45.4258H439.629V44.5645H446.83V45.4258ZM446.83 47.3008H439.629V46.4395H446.83V47.3008ZM454.037 45.4258H446.836V44.5645H454.037V45.4258ZM454.037 47.3008H446.836V46.4395H454.037V47.3008ZM454.043 52.6035V39.0098H461.244V52.6035H454.043ZM461.25 52.6035V39.0098H468.451V52.6035H461.25ZM472.564 52.6035V45.4258H468.457V44.5645H473.426V52.6035H472.564ZM470.689 47.3008H468.457V46.4395H471.551V52.6035H470.689V47.3008ZM475.664 52.6035V39.0098H482.865V52.6035H475.664ZM482.871 52.6035V39.0098H490.072V52.6035H482.871ZM493.172 52.6035H492.311V44.5645H497.279V45.4258H493.172V52.6035ZM495.047 47.3008V52.6035H494.186V46.4395H497.279V47.3008H495.047ZM504.486 45.4258H497.285V44.5645H504.486V45.4258ZM504.486 47.3008H497.285V46.4395H504.486V47.3008ZM511.693 45.4258H504.492V44.5645H511.693V45.4258ZM511.693 47.3008H504.492V46.4395H511.693V47.3008ZM511.699 52.6035V39.0098H518.9V52.6035H511.699ZM518.906 52.6035V39.0098H526.107V52.6035H518.906ZM530.221 52.6035V39.0098H531.082V52.6035H530.221ZM528.346 52.6035V39.0098H529.207V52.6035H528.346ZM533.32 52.6035V39.0098H540.521V52.6035H533.32ZM540.527 52.6035V39.0098H547.729V52.6035H540.527ZM551.842 52.6035V39.0098H552.703V52.6035H551.842ZM549.967 52.6035V39.0098H550.828V52.6035H549.967ZM558.035 39.0098V46.4395H562.143V47.3008H557.174V39.0098H558.035ZM559.91 44.5645H562.143V45.4258H559.049V39.0098H559.91V44.5645ZM562.148 52.6035V39.0098H569.35V52.6035H562.148ZM569.355 52.6035V39.0098H576.557V52.6035H569.355ZM580.67 52.6035V45.4258H576.562V44.5645H581.531V52.6035H580.67ZM578.795 47.3008H576.562V46.4395H579.656V52.6035H578.795V47.3008ZM583.77 52.6035V39.0098H590.971V52.6035H583.77ZM590.977 52.6035V39.0098H598.178V52.6035H590.977ZM602.291 52.6035V39.0098H603.152V52.6035H602.291ZM600.416 52.6035V39.0098H601.277V52.6035H600.416ZM10.3008 52.0098V59.4395H14.4082V60.3008H9.43945V52.0098H10.3008ZM12.1758 57.5645H14.4082V58.4258H11.3145V52.0098H12.1758V57.5645ZM14.4141 65.6035V52.0098H21.6152V65.6035H14.4141ZM21.6211 65.6035V52.0098H28.8223V65.6035H21.6211ZM28.8281 65.6035V52.0098H36.0293V65.6035H28.8281ZM36.0352 65.6035V52.0098H43.2363V65.6035H36.0352ZM46.3359 65.6035H45.4746V57.5645H50.4434V58.4258H46.3359V65.6035ZM48.2109 60.3008V65.6035H47.3496V59.4395H50.4434V60.3008H48.2109ZM54.5566 52.0098H55.418V60.3008H50.4492V59.4395H54.5566V52.0098ZM52.6816 57.5645V52.0098H53.543V58.4258H50.4492V57.5645H52.6816ZM64.8633 65.6035V52.0098H72.0645V65.6035H64.8633ZM72.0703 65.6035V52.0098H79.2715V65.6035H72.0703ZM83.3848 65.6035V52.0098H84.2461V65.6035H83.3848ZM81.5098 65.6035V52.0098H82.3711V65.6035H81.5098ZM86.4844 65.6035V52.0098H93.6855V65.6035H86.4844ZM93.6914 65.6035V52.0098H100.893V65.6035H93.6914ZM100.898 65.6035V52.0098H108.1V65.6035H100.898ZM108.105 65.6035V52.0098H115.307V65.6035H108.105ZM115.312 65.6035V52.0098H122.514V65.6035H115.312ZM122.52 65.6035V52.0098H129.721V65.6035H122.52ZM132.82 65.6035H131.959V57.5645H136.928V58.4258H132.82V65.6035ZM134.695 60.3008V65.6035H133.834V59.4395H136.928V60.3008H134.695ZM141.041 52.0098H141.902V60.3008H136.934V59.4395H141.041V52.0098ZM139.166 57.5645V52.0098H140.027V58.4258H136.934V57.5645H139.166ZM144.141 65.6035V52.0098H151.342V65.6035H144.141ZM151.348 65.6035V52.0098H158.549V65.6035H151.348ZM158.555 65.6035V52.0098H165.756V65.6035H158.555ZM165.762 65.6035V52.0098H172.963V65.6035H165.762ZM172.969 65.6035V52.0098H180.17V65.6035H172.969ZM180.176 65.6035V52.0098H187.377V65.6035H180.176ZM187.383 65.6035V52.0098H194.584V65.6035H187.383ZM198.697 65.6035V58.4258H194.59V57.5645H199.559V65.6035H198.697ZM196.822 60.3008H194.59V59.4395H197.684V65.6035H196.822V60.3008ZM230.625 65.6035V52.0098H237.826V65.6035H230.625ZM237.832 65.6035V52.0098H245.033V65.6035H237.832ZM249.146 65.6035V52.0098H250.008V65.6035H249.146ZM247.271 65.6035V52.0098H248.133V65.6035H247.271ZM266.66 65.6035V52.0098H273.861V65.6035H266.66ZM273.867 65.6035V52.0098H281.068V65.6035H273.867ZM285.182 65.6035V58.4258H281.074V57.5645H286.043V65.6035H285.182ZM283.307 60.3008H281.074V59.4395H284.168V65.6035H283.307V60.3008ZM288.281 65.6035V52.0098H295.482V65.6035H288.281ZM295.488 65.6035V52.0098H302.689V65.6035H295.488ZM306.803 65.6035V52.0098H307.664V65.6035H306.803ZM304.928 65.6035V52.0098H305.789V65.6035H304.928ZM324.316 65.6035V52.0098H331.518V65.6035H324.316ZM331.523 65.6035V52.0098H338.725V65.6035H331.523ZM342.838 65.6035V52.0098H343.699V65.6035H342.838ZM340.963 65.6035V52.0098H341.824V65.6035H340.963ZM345.938 65.6035V52.0098H353.139V65.6035H345.938ZM353.145 65.6035V52.0098H360.346V65.6035H353.145ZM364.459 65.6035V52.0098H365.32V65.6035H364.459ZM362.584 65.6035V52.0098H363.445V65.6035H362.584ZM377.859 52.0098V59.4395H381.967V60.3008H376.998V52.0098H377.859ZM379.734 57.5645H381.967V58.4258H378.873V52.0098H379.734V57.5645ZM381.973 65.6035V52.0098H389.174V65.6035H381.973ZM389.18 65.6035V52.0098H396.381V65.6035H389.18ZM396.387 65.6035V52.0098H403.588V65.6035H396.387ZM403.594 65.6035V52.0098H410.795V65.6035H403.594ZM414.908 65.6035V52.0098H415.77V65.6035H414.908ZM413.033 65.6035V52.0098H413.895V65.6035H413.033ZM418.008 65.6035V52.0098H425.209V65.6035H418.008ZM425.215 65.6035V52.0098H432.416V65.6035H425.215ZM432.422 65.6035V52.0098H439.623V65.6035H432.422ZM439.629 65.6035V52.0098H446.83V65.6035H439.629ZM446.836 65.6035V52.0098H454.037V65.6035H446.836ZM454.043 65.6035V52.0098H461.244V65.6035H454.043ZM464.344 65.6035H463.482V57.5645H468.451V58.4258H464.344V65.6035ZM466.219 60.3008V65.6035H465.357V59.4395H468.451V60.3008H466.219ZM472.564 52.0098H473.426V60.3008H468.457V59.4395H472.564V52.0098ZM470.689 57.5645V52.0098H471.551V58.4258H468.457V57.5645H470.689ZM475.664 65.6035V52.0098H482.865V65.6035H475.664ZM482.871 65.6035V52.0098H490.072V65.6035H482.871ZM494.186 65.6035V52.0098H495.047V65.6035H494.186ZM492.311 65.6035V52.0098H493.172V65.6035H492.311ZM511.699 65.6035V52.0098H518.9V65.6035H511.699ZM518.906 65.6035V52.0098H526.107V65.6035H518.906ZM530.221 65.6035V52.0098H531.082V65.6035H530.221ZM528.346 65.6035V52.0098H529.207V65.6035H528.346ZM533.32 65.6035V52.0098H540.521V65.6035H533.32ZM540.527 65.6035V52.0098H547.729V65.6035H540.527ZM551.842 65.6035V52.0098H552.703V65.6035H551.842ZM549.967 65.6035V52.0098H550.828V65.6035H549.967ZM565.242 52.0098V59.4395H569.35V60.3008H564.381V52.0098H565.242ZM567.117 57.5645H569.35V58.4258H566.256V52.0098H567.117V57.5645ZM569.355 65.6035V52.0098H576.557V65.6035H569.355ZM576.562 65.6035V52.0098H583.764V65.6035H576.562ZM583.77 65.6035V52.0098H590.971V65.6035H583.77ZM590.977 65.6035V52.0098H598.178V65.6035H590.977ZM602.291 65.6035V52.0098H603.152V65.6035H602.291ZM600.416 65.6035V52.0098H601.277V65.6035H600.416ZM17.5078 65.0098V72.4395H21.6152V73.3008H16.6465V65.0098H17.5078ZM19.3828 70.5645H21.6152V71.4258H18.5215V65.0098H19.3828V70.5645ZM28.8223 71.4258H21.6211V70.5645H28.8223V71.4258ZM28.8223 73.3008H21.6211V72.4395H28.8223V73.3008ZM36.0293 71.4258H28.8281V70.5645H36.0293V71.4258ZM36.0293 73.3008H28.8281V72.4395H36.0293V73.3008ZM43.2363 71.4258H36.0352V70.5645H43.2363V71.4258ZM43.2363 73.3008H36.0352V72.4395H43.2363V73.3008ZM47.3496 65.0098H48.2109V73.3008H43.2422V72.4395H47.3496V65.0098ZM45.4746 70.5645V65.0098H46.3359V71.4258H43.2422V70.5645H45.4746ZM67.957 65.0098V72.4395H72.0645V73.3008H67.0957V65.0098H67.957ZM69.832 70.5645H72.0645V71.4258H68.9707V65.0098H69.832V70.5645ZM79.2715 71.4258H72.0703V70.5645H79.2715V71.4258ZM79.2715 73.3008H72.0703V72.4395H79.2715V73.3008ZM83.3848 65.0098H84.2461V73.3008H79.2773V72.4395H83.3848V65.0098ZM81.5098 70.5645V65.0098H82.3711V71.4258H79.2773V70.5645H81.5098ZM89.5781 65.0098V72.4395H93.6855V73.3008H88.7168V65.0098H89.5781ZM91.4531 70.5645H93.6855V71.4258H90.5918V65.0098H91.4531V70.5645ZM100.893 71.4258H93.6914V70.5645H100.893V71.4258ZM100.893 73.3008H93.6914V72.4395H100.893V73.3008ZM108.1 71.4258H100.898V70.5645H108.1V71.4258ZM108.1 73.3008H100.898V72.4395H108.1V73.3008ZM115.307 71.4258H108.105V70.5645H115.307V71.4258ZM115.307 73.3008H108.105V72.4395H115.307V73.3008ZM122.514 71.4258H115.312V70.5645H122.514V71.4258ZM122.514 73.3008H115.312V72.4395H122.514V73.3008ZM129.721 71.4258H122.52V70.5645H129.721V71.4258ZM129.721 73.3008H122.52V72.4395H129.721V73.3008ZM133.834 65.0098H134.695V73.3008H129.727V72.4395H133.834V65.0098ZM131.959 70.5645V65.0098H132.82V71.4258H129.727V70.5645H131.959ZM147.234 65.0098V72.4395H151.342V73.3008H146.373V65.0098H147.234ZM149.109 70.5645H151.342V71.4258H148.248V65.0098H149.109V70.5645ZM158.549 71.4258H151.348V70.5645H158.549V71.4258ZM158.549 73.3008H151.348V72.4395H158.549V73.3008ZM165.756 71.4258H158.555V70.5645H165.756V71.4258ZM165.756 73.3008H158.555V72.4395H165.756V73.3008ZM172.963 71.4258H165.762V70.5645H172.963V71.4258ZM172.963 73.3008H165.762V72.4395H172.963V73.3008ZM180.17 71.4258H172.969V70.5645H180.17V71.4258ZM180.17 73.3008H172.969V72.4395H180.17V73.3008ZM187.377 71.4258H180.176V70.5645H187.377V71.4258ZM187.377 73.3008H180.176V72.4395H187.377V73.3008ZM194.584 71.4258H187.383V70.5645H194.584V71.4258ZM194.584 73.3008H187.383V72.4395H194.584V73.3008ZM198.697 65.0098H199.559V73.3008H194.59V72.4395H198.697V65.0098ZM196.822 70.5645V65.0098H197.684V71.4258H194.59V70.5645H196.822ZM233.719 65.0098V72.4395H237.826V73.3008H232.857V65.0098H233.719ZM235.594 70.5645H237.826V71.4258H234.732V65.0098H235.594V70.5645ZM245.033 71.4258H237.832V70.5645H245.033V71.4258ZM245.033 73.3008H237.832V72.4395H245.033V73.3008ZM249.146 65.0098H250.008V73.3008H245.039V72.4395H249.146V65.0098ZM247.271 70.5645V65.0098H248.133V71.4258H245.039V70.5645H247.271ZM269.754 65.0098V72.4395H273.861V73.3008H268.893V65.0098H269.754ZM271.629 70.5645H273.861V71.4258H270.768V65.0098H271.629V70.5645ZM281.068 71.4258H273.867V70.5645H281.068V71.4258ZM281.068 73.3008H273.867V72.4395H281.068V73.3008ZM285.182 65.0098H286.043V73.3008H281.074V72.4395H285.182V65.0098ZM283.307 70.5645V65.0098H284.168V71.4258H281.074V70.5645H283.307ZM291.375 65.0098V72.4395H295.482V73.3008H290.514V65.0098H291.375ZM293.25 70.5645H295.482V71.4258H292.389V65.0098H293.25V70.5645ZM302.689 71.4258H295.488V70.5645H302.689V71.4258ZM302.689 73.3008H295.488V72.4395H302.689V73.3008ZM306.803 65.0098H307.664V73.3008H302.695V72.4395H306.803V65.0098ZM304.928 70.5645V65.0098H305.789V71.4258H302.695V70.5645H304.928ZM327.41 65.0098V72.4395H331.518V73.3008H326.549V65.0098H327.41ZM329.285 70.5645H331.518V71.4258H328.424V65.0098H329.285V70.5645ZM338.725 71.4258H331.523V70.5645H338.725V71.4258ZM338.725 73.3008H331.523V72.4395H338.725V73.3008ZM342.838 65.0098H343.699V73.3008H338.73V72.4395H342.838V65.0098ZM340.963 70.5645V65.0098H341.824V71.4258H338.73V70.5645H340.963ZM349.031 65.0098V72.4395H353.139V73.3008H348.17V65.0098H349.031ZM350.906 70.5645H353.139V71.4258H350.045V65.0098H350.906V70.5645ZM360.346 71.4258H353.145V70.5645H360.346V71.4258ZM360.346 73.3008H353.145V72.4395H360.346V73.3008ZM364.459 65.0098H365.32V73.3008H360.352V72.4395H364.459V65.0098ZM362.584 70.5645V65.0098H363.445V71.4258H360.352V70.5645H362.584ZM385.066 65.0098V72.4395H389.174V73.3008H384.205V65.0098H385.066ZM386.941 70.5645H389.174V71.4258H386.08V65.0098H386.941V70.5645ZM396.381 71.4258H389.18V70.5645H396.381V71.4258ZM396.381 73.3008H389.18V72.4395H396.381V73.3008ZM403.588 71.4258H396.387V70.5645H403.588V71.4258ZM403.588 73.3008H396.387V72.4395H403.588V73.3008ZM410.795 71.4258H403.594V70.5645H410.795V71.4258ZM410.795 73.3008H403.594V72.4395H410.795V73.3008ZM414.908 65.0098H415.77V73.3008H410.801V72.4395H414.908V65.0098ZM413.033 70.5645V65.0098H413.895V71.4258H410.801V70.5645H413.033ZM421.102 65.0098V72.4395H425.209V73.3008H420.24V65.0098H421.102ZM422.977 70.5645H425.209V71.4258H422.115V65.0098H422.977V70.5645ZM432.416 71.4258H425.215V70.5645H432.416V71.4258ZM432.416 73.3008H425.215V72.4395H432.416V73.3008ZM439.623 71.4258H432.422V70.5645H439.623V71.4258ZM439.623 73.3008H432.422V72.4395H439.623V73.3008ZM446.83 71.4258H439.629V70.5645H446.83V71.4258ZM446.83 73.3008H439.629V72.4395H446.83V73.3008ZM454.037 71.4258H446.836V70.5645H454.037V71.4258ZM454.037 73.3008H446.836V72.4395H454.037V73.3008ZM461.244 71.4258H454.043V70.5645H461.244V71.4258ZM461.244 73.3008H454.043V72.4395H461.244V73.3008ZM465.357 65.0098H466.219V73.3008H461.25V72.4395H465.357V65.0098ZM463.482 70.5645V65.0098H464.344V71.4258H461.25V70.5645H463.482ZM478.758 65.0098V72.4395H482.865V73.3008H477.896V65.0098H478.758ZM480.633 70.5645H482.865V71.4258H479.771V65.0098H480.633V70.5645ZM490.072 71.4258H482.871V70.5645H490.072V71.4258ZM490.072 73.3008H482.871V72.4395H490.072V73.3008ZM494.186 65.0098H495.047V73.3008H490.078V72.4395H494.186V65.0098ZM492.311 70.5645V65.0098H493.172V71.4258H490.078V70.5645H492.311ZM514.793 65.0098V72.4395H518.9V73.3008H513.932V65.0098H514.793ZM516.668 70.5645H518.9V71.4258H515.807V65.0098H516.668V70.5645ZM526.107 71.4258H518.906V70.5645H526.107V71.4258ZM526.107 73.3008H518.906V72.4395H526.107V73.3008ZM530.221 65.0098H531.082V73.3008H526.113V72.4395H530.221V65.0098ZM528.346 70.5645V65.0098H529.207V71.4258H526.113V70.5645H528.346ZM536.414 65.0098V72.4395H540.521V73.3008H535.553V65.0098H536.414ZM538.289 70.5645H540.521V71.4258H537.428V65.0098H538.289V70.5645ZM547.729 71.4258H540.527V70.5645H547.729V71.4258ZM547.729 73.3008H540.527V72.4395H547.729V73.3008ZM551.842 65.0098H552.703V73.3008H547.734V72.4395H551.842V65.0098ZM549.967 70.5645V65.0098H550.828V71.4258H547.734V70.5645H549.967ZM572.449 65.0098V72.4395H576.557V73.3008H571.588V65.0098H572.449ZM574.324 70.5645H576.557V71.4258H573.463V65.0098H574.324V70.5645ZM583.764 71.4258H576.562V70.5645H583.764V71.4258ZM583.764 73.3008H576.562V72.4395H583.764V73.3008ZM590.971 71.4258H583.77V70.5645H590.971V71.4258ZM590.971 73.3008H583.77V72.4395H590.971V73.3008ZM598.178 71.4258H590.977V70.5645H598.178V71.4258ZM598.178 73.3008H590.977V72.4395H598.178V73.3008ZM602.291 65.0098H603.152V73.3008H598.184V72.4395H602.291V65.0098ZM600.416 70.5645V65.0098H601.277V71.4258H598.184V70.5645H600.416Z" fill="white"/>
</svg>
</file>

<file path="frontend/public/vibe-kanban-logo.svg">
<svg width="604" height="74" viewBox="0 0 604 74" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M0 13.6035V0.00976562H7.20117V13.6035H0ZM7.20703 13.6035V0.00976562H14.4082V13.6035H7.20703ZM18.5215 13.6035V6.42578H14.4141V5.56445H19.3828V13.6035H18.5215ZM16.6465 8.30078H14.4141V7.43945H17.5078V13.6035H16.6465V8.30078ZM43.2422 13.6035V0.00976562H50.4434V13.6035H43.2422ZM50.4492 13.6035V0.00976562H57.6504V13.6035H50.4492ZM61.7637 13.6035V6.42578H57.6562V5.56445H62.625V13.6035H61.7637ZM59.8887 8.30078H57.6562V7.43945H60.75V13.6035H59.8887V8.30078ZM64.8633 13.6035V0.00976562H72.0645V13.6035H64.8633ZM72.0703 13.6035V0.00976562H79.2715V13.6035H72.0703ZM83.3848 13.6035V6.42578H79.2773V5.56445H84.2461V13.6035H83.3848ZM81.5098 8.30078H79.2773V7.43945H82.3711V13.6035H81.5098V8.30078ZM86.4844 13.6035V0.00976562H93.6855V13.6035H86.4844ZM93.6914 13.6035V0.00976562H100.893V13.6035H93.6914ZM100.898 13.6035V0.00976562H108.1V13.6035H100.898ZM108.105 13.6035V0.00976562H115.307V13.6035H108.105ZM115.312 13.6035V0.00976562H122.514V13.6035H115.312ZM122.52 13.6035V0.00976562H129.721V13.6035H122.52ZM133.834 13.6035V6.42578H129.727V5.56445H134.695V13.6035H133.834ZM131.959 8.30078H129.727V7.43945H132.82V13.6035H131.959V8.30078ZM144.141 13.6035V0.00976562H151.342V13.6035H144.141ZM151.348 13.6035V0.00976562H158.549V13.6035H151.348ZM158.555 13.6035V0.00976562H165.756V13.6035H158.555ZM165.762 13.6035V0.00976562H172.963V13.6035H165.762ZM172.969 13.6035V0.00976562H180.17V13.6035H172.969ZM180.176 13.6035V0.00976562H187.377V13.6035H180.176ZM187.383 13.6035V0.00976562H194.584V13.6035H187.383ZM198.697 13.6035V6.42578H194.59V5.56445H199.559V13.6035H198.697ZM196.822 8.30078H194.59V7.43945H197.684V13.6035H196.822V8.30078ZM230.625 13.6035V0.00976562H237.826V13.6035H230.625ZM237.832 13.6035V0.00976562H245.033V13.6035H237.832ZM249.146 13.6035V6.42578H245.039V5.56445H250.008V13.6035H249.146ZM247.271 8.30078H245.039V7.43945H248.133V13.6035H247.271V8.30078ZM266.66 13.6035V0.00976562H273.861V13.6035H266.66ZM273.867 13.6035V0.00976562H281.068V13.6035H273.867ZM285.182 13.6035V6.42578H281.074V5.56445H286.043V13.6035H285.182ZM283.307 8.30078H281.074V7.43945H284.168V13.6035H283.307V8.30078ZM295.488 13.6035V0.00976562H302.689V13.6035H295.488ZM302.695 13.6035V0.00976562H309.896V13.6035H302.695ZM309.902 13.6035V0.00976562H317.104V13.6035H309.902ZM317.109 13.6035V0.00976562H324.311V13.6035H317.109ZM324.316 13.6035V0.00976562H331.518V13.6035H324.316ZM335.631 13.6035V6.42578H331.523V5.56445H336.492V13.6035H335.631ZM333.756 8.30078H331.523V7.43945H334.617V13.6035H333.756V8.30078ZM345.938 13.6035V0.00976562H353.139V13.6035H345.938ZM353.145 13.6035V0.00976562H360.346V13.6035H353.145ZM360.352 13.6035V0.00976562H367.553V13.6035H360.352ZM371.666 13.6035V6.42578H367.559V5.56445H372.527V13.6035H371.666ZM369.791 8.30078H367.559V7.43945H370.652V13.6035H369.791V8.30078ZM396.387 13.6035V0.00976562H403.588V13.6035H396.387ZM403.594 13.6035V0.00976562H410.795V13.6035H403.594ZM414.908 13.6035V6.42578H410.801V5.56445H415.77V13.6035H414.908ZM413.033 8.30078H410.801V7.43945H413.895V13.6035H413.033V8.30078ZM418.008 13.6035V0.00976562H425.209V13.6035H418.008ZM425.215 13.6035V0.00976562H432.416V13.6035H425.215ZM432.422 13.6035V0.00976562H439.623V13.6035H432.422ZM439.629 13.6035V0.00976562H446.83V13.6035H439.629ZM446.836 13.6035V0.00976562H454.037V13.6035H446.836ZM454.043 13.6035V0.00976562H461.244V13.6035H454.043ZM465.357 13.6035V6.42578H461.25V5.56445H466.219V13.6035H465.357ZM463.482 8.30078H461.25V7.43945H464.344V13.6035H463.482V8.30078ZM482.871 13.6035V0.00976562H490.072V13.6035H482.871ZM490.078 13.6035V0.00976562H497.279V13.6035H490.078ZM497.285 13.6035V0.00976562H504.486V13.6035H497.285ZM504.492 13.6035V0.00976562H511.693V13.6035H504.492ZM511.699 13.6035V0.00976562H518.9V13.6035H511.699ZM523.014 13.6035V6.42578H518.906V5.56445H523.875V13.6035H523.014ZM521.139 8.30078H518.906V7.43945H522V13.6035H521.139V8.30078ZM533.32 13.6035V0.00976562H540.521V13.6035H533.32ZM540.527 13.6035V0.00976562H547.729V13.6035H540.527ZM547.734 13.6035V0.00976562H554.936V13.6035H547.734ZM559.049 13.6035V6.42578H554.941V5.56445H559.91V13.6035H559.049ZM557.174 8.30078H554.941V7.43945H558.035V13.6035H557.174V8.30078ZM583.77 13.6035V0.00976562H590.971V13.6035H583.77ZM590.977 13.6035V0.00976562H598.178V13.6035H590.977ZM602.291 13.6035V6.42578H598.184V5.56445H603.152V13.6035H602.291ZM600.416 8.30078H598.184V7.43945H601.277V13.6035H600.416V8.30078ZM0 26.6035V13.0098H7.20117V26.6035H0ZM7.20703 26.6035V13.0098H14.4082V26.6035H7.20703ZM18.5215 26.6035V13.0098H19.3828V26.6035H18.5215ZM16.6465 26.6035V13.0098H17.5078V26.6035H16.6465ZM43.2422 26.6035V13.0098H50.4434V26.6035H43.2422ZM50.4492 26.6035V13.0098H57.6504V26.6035H50.4492ZM61.7637 26.6035V13.0098H62.625V26.6035H61.7637ZM59.8887 26.6035V13.0098H60.75V26.6035H59.8887ZM64.8633 26.6035V13.0098H72.0645V26.6035H64.8633ZM72.0703 26.6035V13.0098H79.2715V26.6035H72.0703ZM83.3848 26.6035V13.0098H84.2461V26.6035H83.3848ZM81.5098 26.6035V13.0098H82.3711V26.6035H81.5098ZM86.4844 26.6035V13.0098H93.6855V26.6035H86.4844ZM93.6914 26.6035V13.0098H100.893V26.6035H93.6914ZM103.992 26.6035H103.131V18.5645H108.1V19.4258H103.992V26.6035ZM105.867 21.3008V26.6035H105.006V20.4395H108.1V21.3008H105.867ZM115.307 19.4258H108.105V18.5645H115.307V19.4258ZM115.307 21.3008H108.105V20.4395H115.307V21.3008ZM122.514 19.4258H115.312V18.5645H122.514V19.4258ZM122.514 21.3008H115.312V20.4395H122.514V21.3008ZM122.52 26.6035V13.0098H129.721V26.6035H122.52ZM129.727 26.6035V13.0098H136.928V26.6035H129.727ZM141.041 26.6035V19.4258H136.934V18.5645H141.902V26.6035H141.041ZM139.166 21.3008H136.934V20.4395H140.027V26.6035H139.166V21.3008ZM144.141 26.6035V13.0098H151.342V26.6035H144.141ZM151.348 26.6035V13.0098H158.549V26.6035H151.348ZM161.648 26.6035H160.787V18.5645H165.756V19.4258H161.648V26.6035ZM163.523 21.3008V26.6035H162.662V20.4395H165.756V21.3008H163.523ZM172.963 19.4258H165.762V18.5645H172.963V19.4258ZM172.963 21.3008H165.762V20.4395H172.963V21.3008ZM180.17 19.4258H172.969V18.5645H180.17V19.4258ZM180.17 21.3008H172.969V20.4395H180.17V21.3008ZM187.377 19.4258H180.176V18.5645H187.377V19.4258ZM187.377 21.3008H180.176V20.4395H187.377V21.3008ZM194.584 19.4258H187.383V18.5645H194.584V19.4258ZM194.584 21.3008H187.383V20.4395H194.584V21.3008ZM198.697 13.0098H199.559V21.3008H194.59V20.4395H198.697V13.0098ZM196.822 18.5645V13.0098H197.684V19.4258H194.59V18.5645H196.822ZM230.625 26.6035V13.0098H237.826V26.6035H230.625ZM237.832 26.6035V13.0098H245.033V26.6035H237.832ZM249.146 26.6035V13.0098H250.008V26.6035H249.146ZM247.271 26.6035V13.0098H248.133V26.6035H247.271ZM259.453 26.6035V13.0098H266.654V26.6035H259.453ZM266.66 26.6035V13.0098H273.861V26.6035H266.66ZM276.961 26.6035H276.1V18.5645H281.068V19.4258H276.961V26.6035ZM278.836 21.3008V26.6035H277.975V20.4395H281.068V21.3008H278.836ZM285.182 13.0098H286.043V21.3008H281.074V20.4395H285.182V13.0098ZM283.307 18.5645V13.0098H284.168V19.4258H281.074V18.5645H283.307ZM288.281 26.6035V13.0098H295.482V26.6035H288.281ZM295.488 26.6035V13.0098H302.689V26.6035H295.488ZM305.789 26.6035H304.928V18.5645H309.896V19.4258H305.789V26.6035ZM307.664 21.3008V26.6035H306.803V20.4395H309.896V21.3008H307.664ZM317.104 19.4258H309.902V18.5645H317.104V19.4258ZM317.104 21.3008H309.902V20.4395H317.104V21.3008ZM324.311 19.4258H317.109V18.5645H324.311V19.4258ZM324.311 21.3008H317.109V20.4395H324.311V21.3008ZM324.316 26.6035V13.0098H331.518V26.6035H324.316ZM331.523 26.6035V13.0098H338.725V26.6035H331.523ZM342.838 26.6035V19.4258H338.73V18.5645H343.699V26.6035H342.838ZM340.963 21.3008H338.73V20.4395H341.824V26.6035H340.963V21.3008ZM345.938 26.6035V13.0098H353.139V26.6035H345.938ZM353.145 26.6035V13.0098H360.346V26.6035H353.145ZM360.352 26.6035V13.0098H367.553V26.6035H360.352ZM367.559 26.6035V13.0098H374.76V26.6035H367.559ZM378.873 26.6035V19.4258H374.766V18.5645H379.734V26.6035H378.873ZM376.998 21.3008H374.766V20.4395H377.859V26.6035H376.998V21.3008ZM396.387 26.6035V13.0098H403.588V26.6035H396.387ZM403.594 26.6035V13.0098H410.795V26.6035H403.594ZM414.908 26.6035V13.0098H415.77V26.6035H414.908ZM413.033 26.6035V13.0098H413.895V26.6035H413.033ZM418.008 26.6035V13.0098H425.209V26.6035H418.008ZM425.215 26.6035V13.0098H432.416V26.6035H425.215ZM435.516 26.6035H434.654V18.5645H439.623V19.4258H435.516V26.6035ZM437.391 21.3008V26.6035H436.529V20.4395H439.623V21.3008H437.391ZM446.83 19.4258H439.629V18.5645H446.83V19.4258ZM446.83 21.3008H439.629V20.4395H446.83V21.3008ZM454.037 19.4258H446.836V18.5645H454.037V19.4258ZM454.037 21.3008H446.836V20.4395H454.037V21.3008ZM454.043 26.6035V13.0098H461.244V26.6035H454.043ZM461.25 26.6035V13.0098H468.451V26.6035H461.25ZM472.564 26.6035V19.4258H468.457V18.5645H473.426V26.6035H472.564ZM470.689 21.3008H468.457V20.4395H471.551V26.6035H470.689V21.3008ZM475.664 26.6035V13.0098H482.865V26.6035H475.664ZM482.871 26.6035V13.0098H490.072V26.6035H482.871ZM493.172 26.6035H492.311V18.5645H497.279V19.4258H493.172V26.6035ZM495.047 21.3008V26.6035H494.186V20.4395H497.279V21.3008H495.047ZM504.486 19.4258H497.285V18.5645H504.486V19.4258ZM504.486 21.3008H497.285V20.4395H504.486V21.3008ZM511.693 19.4258H504.492V18.5645H511.693V19.4258ZM511.693 21.3008H504.492V20.4395H511.693V21.3008ZM511.699 26.6035V13.0098H518.9V26.6035H511.699ZM518.906 26.6035V13.0098H526.107V26.6035H518.906ZM530.221 26.6035V19.4258H526.113V18.5645H531.082V26.6035H530.221ZM528.346 21.3008H526.113V20.4395H529.207V26.6035H528.346V21.3008ZM533.32 26.6035V13.0098H540.521V26.6035H533.32ZM540.527 26.6035V13.0098H547.729V26.6035H540.527ZM547.734 26.6035V13.0098H554.936V26.6035H547.734ZM554.941 26.6035V13.0098H562.143V26.6035H554.941ZM566.256 26.6035V19.4258H562.148V18.5645H567.117V26.6035H566.256ZM564.381 21.3008H562.148V20.4395H565.242V26.6035H564.381V21.3008ZM583.77 26.6035V13.0098H590.971V26.6035H583.77ZM590.977 26.6035V13.0098H598.178V26.6035H590.977ZM602.291 26.6035V13.0098H603.152V26.6035H602.291ZM600.416 26.6035V13.0098H601.277V26.6035H600.416ZM0 39.6035V26.0098H7.20117V39.6035H0ZM7.20703 39.6035V26.0098H14.4082V39.6035H7.20703ZM18.5215 39.6035V26.0098H19.3828V39.6035H18.5215ZM16.6465 39.6035V26.0098H17.5078V39.6035H16.6465ZM43.2422 39.6035V26.0098H50.4434V39.6035H43.2422ZM50.4492 39.6035V26.0098H57.6504V39.6035H50.4492ZM61.7637 39.6035V26.0098H62.625V39.6035H61.7637ZM59.8887 39.6035V26.0098H60.75V39.6035H59.8887ZM64.8633 39.6035V26.0098H72.0645V39.6035H64.8633ZM72.0703 39.6035V26.0098H79.2715V39.6035H72.0703ZM83.3848 39.6035V26.0098H84.2461V39.6035H83.3848ZM81.5098 39.6035V26.0098H82.3711V39.6035H81.5098ZM86.4844 39.6035V26.0098H93.6855V39.6035H86.4844ZM93.6914 39.6035V26.0098H100.893V39.6035H93.6914ZM100.898 39.6035V26.0098H108.1V39.6035H100.898ZM108.105 39.6035V26.0098H115.307V39.6035H108.105ZM115.312 39.6035V26.0098H122.514V39.6035H115.312ZM122.52 39.6035V26.0098H129.721V39.6035H122.52ZM132.82 39.6035H131.959V31.5645H136.928V32.4258H132.82V39.6035ZM134.695 34.3008V39.6035H133.834V33.4395H136.928V34.3008H134.695ZM141.041 26.0098H141.902V34.3008H136.934V33.4395H141.041V26.0098ZM139.166 31.5645V26.0098H140.027V32.4258H136.934V31.5645H139.166ZM144.141 39.6035V26.0098H151.342V39.6035H144.141ZM151.348 39.6035V26.0098H158.549V39.6035H151.348ZM158.555 39.6035V26.0098H165.756V39.6035H158.555ZM165.762 39.6035V26.0098H172.963V39.6035H165.762ZM172.969 39.6035V26.0098H180.17V39.6035H172.969ZM184.283 39.6035V32.4258H180.176V31.5645H185.145V39.6035H184.283ZM182.408 34.3008H180.176V33.4395H183.27V39.6035H182.408V34.3008ZM187.383 39.6035V26.0098H194.584V39.6035H187.383ZM194.59 39.6035V26.0098H201.791V39.6035H194.59ZM201.797 39.6035V26.0098H208.998V39.6035H201.797ZM209.004 39.6035V26.0098H216.205V39.6035H209.004ZM216.211 39.6035V26.0098H223.412V39.6035H216.211ZM227.525 39.6035V32.4258H223.418V31.5645H228.387V39.6035H227.525ZM225.65 34.3008H223.418V33.4395H226.512V39.6035H225.65V34.3008ZM230.625 39.6035V26.0098H237.826V39.6035H230.625ZM237.832 39.6035V26.0098H245.033V39.6035H237.832ZM245.039 39.6035V26.0098H252.24V39.6035H245.039ZM252.246 39.6035V26.0098H259.447V39.6035H252.246ZM259.453 39.6035V26.0098H266.654V39.6035H259.453ZM269.754 39.6035H268.893V31.5645H273.861V32.4258H269.754V39.6035ZM271.629 34.3008V39.6035H270.768V33.4395H273.861V34.3008H271.629ZM277.975 26.0098H278.836V34.3008H273.867V33.4395H277.975V26.0098ZM276.1 31.5645V26.0098H276.961V32.4258H273.867V31.5645H276.1ZM288.281 39.6035V26.0098H295.482V39.6035H288.281ZM295.488 39.6035V26.0098H302.689V39.6035H295.488ZM302.695 39.6035V26.0098H309.896V39.6035H302.695ZM309.902 39.6035V26.0098H317.104V39.6035H309.902ZM317.109 39.6035V26.0098H324.311V39.6035H317.109ZM324.316 39.6035V26.0098H331.518V39.6035H324.316ZM331.523 39.6035V26.0098H338.725V39.6035H331.523ZM342.838 39.6035V26.0098H343.699V39.6035H342.838ZM340.963 39.6035V26.0098H341.824V39.6035H340.963ZM345.938 39.6035V26.0098H353.139V39.6035H345.938ZM353.145 39.6035V26.0098H360.346V39.6035H353.145ZM363.445 39.6035H362.584V31.5645H367.553V32.4258H363.445V39.6035ZM365.32 34.3008V39.6035H364.459V33.4395H367.553V34.3008H365.32ZM367.559 39.6035V26.0098H374.76V39.6035H367.559ZM374.766 39.6035V26.0098H381.967V39.6035H374.766ZM386.08 39.6035V32.4258H381.973V31.5645H386.941V39.6035H386.08ZM384.205 34.3008H381.973V33.4395H385.066V39.6035H384.205V34.3008ZM396.387 39.6035V26.0098H403.588V39.6035H396.387ZM403.594 39.6035V26.0098H410.795V39.6035H403.594ZM414.908 39.6035V26.0098H415.77V39.6035H414.908ZM413.033 39.6035V26.0098H413.895V39.6035H413.033ZM418.008 39.6035V26.0098H425.209V39.6035H418.008ZM425.215 39.6035V26.0098H432.416V39.6035H425.215ZM432.422 39.6035V26.0098H439.623V39.6035H432.422ZM439.629 39.6035V26.0098H446.83V39.6035H439.629ZM446.836 39.6035V26.0098H454.037V39.6035H446.836ZM454.043 39.6035V26.0098H461.244V39.6035H454.043ZM464.344 39.6035H463.482V31.5645H468.451V32.4258H464.344V39.6035ZM466.219 34.3008V39.6035H465.357V33.4395H468.451V34.3008H466.219ZM472.564 26.0098H473.426V34.3008H468.457V33.4395H472.564V26.0098ZM470.689 31.5645V26.0098H471.551V32.4258H468.457V31.5645H470.689ZM475.664 39.6035V26.0098H482.865V39.6035H475.664ZM482.871 39.6035V26.0098H490.072V39.6035H482.871ZM490.078 39.6035V26.0098H497.279V39.6035H490.078ZM497.285 39.6035V26.0098H504.486V39.6035H497.285ZM504.492 39.6035V26.0098H511.693V39.6035H504.492ZM511.699 39.6035V26.0098H518.9V39.6035H511.699ZM518.906 39.6035V26.0098H526.107V39.6035H518.906ZM530.221 39.6035V26.0098H531.082V39.6035H530.221ZM528.346 39.6035V26.0098H529.207V39.6035H528.346ZM533.32 39.6035V26.0098H540.521V39.6035H533.32ZM540.527 39.6035V26.0098H547.729V39.6035H540.527ZM550.828 39.6035H549.967V31.5645H554.936V32.4258H550.828V39.6035ZM552.703 34.3008V39.6035H551.842V33.4395H554.936V34.3008H552.703ZM554.941 39.6035V26.0098H562.143V39.6035H554.941ZM562.148 39.6035V26.0098H569.35V39.6035H562.148ZM573.463 39.6035V32.4258H569.355V31.5645H574.324V39.6035H573.463ZM571.588 34.3008H569.355V33.4395H572.449V39.6035H571.588V34.3008ZM583.77 39.6035V26.0098H590.971V39.6035H583.77ZM590.977 39.6035V26.0098H598.178V39.6035H590.977ZM602.291 39.6035V26.0098H603.152V39.6035H602.291ZM600.416 39.6035V26.0098H601.277V39.6035H600.416ZM3.09375 39.0098V46.4395H7.20117V47.3008H2.23242V39.0098H3.09375ZM4.96875 44.5645H7.20117V45.4258H4.10742V39.0098H4.96875V44.5645ZM7.20703 52.6035V39.0098H14.4082V52.6035H7.20703ZM14.4141 52.6035V39.0098H21.6152V52.6035H14.4141ZM25.7285 52.6035V45.4258H21.6211V44.5645H26.5898V52.6035H25.7285ZM23.8535 47.3008H21.6211V46.4395H24.7148V52.6035H23.8535V47.3008ZM36.0352 52.6035V39.0098H43.2363V52.6035H36.0352ZM43.2422 52.6035V39.0098H50.4434V52.6035H43.2422ZM53.543 52.6035H52.6816V44.5645H57.6504V45.4258H53.543V52.6035ZM55.418 47.3008V52.6035H54.5566V46.4395H57.6504V47.3008H55.418ZM61.7637 39.0098H62.625V47.3008H57.6562V46.4395H61.7637V39.0098ZM59.8887 44.5645V39.0098H60.75V45.4258H57.6562V44.5645H59.8887ZM64.8633 52.6035V39.0098H72.0645V52.6035H64.8633ZM72.0703 52.6035V39.0098H79.2715V52.6035H72.0703ZM83.3848 52.6035V39.0098H84.2461V52.6035H83.3848ZM81.5098 52.6035V39.0098H82.3711V52.6035H81.5098ZM86.4844 52.6035V39.0098H93.6855V52.6035H86.4844ZM93.6914 52.6035V39.0098H100.893V52.6035H93.6914ZM103.992 52.6035H103.131V44.5645H108.1V45.4258H103.992V52.6035ZM105.867 47.3008V52.6035H105.006V46.4395H108.1V47.3008H105.867ZM115.307 45.4258H108.105V44.5645H115.307V45.4258ZM115.307 47.3008H108.105V46.4395H115.307V47.3008ZM122.514 45.4258H115.312V44.5645H122.514V45.4258ZM122.514 47.3008H115.312V46.4395H122.514V47.3008ZM122.52 52.6035V39.0098H129.721V52.6035H122.52ZM129.727 52.6035V39.0098H136.928V52.6035H129.727ZM141.041 52.6035V45.4258H136.934V44.5645H141.902V52.6035H141.041ZM139.166 47.3008H136.934V46.4395H140.027V52.6035H139.166V47.3008ZM144.141 52.6035V39.0098H151.342V52.6035H144.141ZM151.348 52.6035V39.0098H158.549V52.6035H151.348ZM161.648 52.6035H160.787V44.5645H165.756V45.4258H161.648V52.6035ZM163.523 47.3008V52.6035H162.662V46.4395H165.756V47.3008H163.523ZM172.963 45.4258H165.762V44.5645H172.963V45.4258ZM172.963 47.3008H165.762V46.4395H172.963V47.3008ZM180.17 45.4258H172.969V44.5645H180.17V45.4258ZM180.17 47.3008H172.969V46.4395H180.17V47.3008ZM184.283 39.0098H185.145V47.3008H180.176V46.4395H184.283V39.0098ZM182.408 44.5645V39.0098H183.27V45.4258H180.176V44.5645H182.408ZM190.477 39.0098V46.4395H194.584V47.3008H189.615V39.0098H190.477ZM192.352 44.5645H194.584V45.4258H191.49V39.0098H192.352V44.5645ZM201.791 45.4258H194.59V44.5645H201.791V45.4258ZM201.791 47.3008H194.59V46.4395H201.791V47.3008ZM208.998 45.4258H201.797V44.5645H208.998V45.4258ZM208.998 47.3008H201.797V46.4395H208.998V47.3008ZM216.205 45.4258H209.004V44.5645H216.205V45.4258ZM216.205 47.3008H209.004V46.4395H216.205V47.3008ZM223.412 45.4258H216.211V44.5645H223.412V45.4258ZM223.412 47.3008H216.211V46.4395H223.412V47.3008ZM227.525 39.0098H228.387V47.3008H223.418V46.4395H227.525V39.0098ZM225.65 44.5645V39.0098H226.512V45.4258H223.418V44.5645H225.65ZM230.625 52.6035V39.0098H237.826V52.6035H230.625ZM237.832 52.6035V39.0098H245.033V52.6035H237.832ZM248.133 52.6035H247.271V44.5645H252.24V45.4258H248.133V52.6035ZM250.008 47.3008V52.6035H249.146V46.4395H252.24V47.3008H250.008ZM259.447 45.4258H252.246V44.5645H259.447V45.4258ZM259.447 47.3008H252.246V46.4395H259.447V47.3008ZM259.453 52.6035V39.0098H266.654V52.6035H259.453ZM266.66 52.6035V39.0098H273.861V52.6035H266.66ZM277.975 52.6035V45.4258H273.867V44.5645H278.836V52.6035H277.975ZM276.1 47.3008H273.867V46.4395H276.961V52.6035H276.1V47.3008ZM288.281 52.6035V39.0098H295.482V52.6035H288.281ZM295.488 52.6035V39.0098H302.689V52.6035H295.488ZM305.789 52.6035H304.928V44.5645H309.896V45.4258H305.789V52.6035ZM307.664 47.3008V52.6035H306.803V46.4395H309.896V47.3008H307.664ZM317.104 45.4258H309.902V44.5645H317.104V45.4258ZM317.104 47.3008H309.902V46.4395H317.104V47.3008ZM324.311 45.4258H317.109V44.5645H324.311V45.4258ZM324.311 47.3008H317.109V46.4395H324.311V47.3008ZM324.316 52.6035V39.0098H331.518V52.6035H324.316ZM331.523 52.6035V39.0098H338.725V52.6035H331.523ZM342.838 52.6035V39.0098H343.699V52.6035H342.838ZM340.963 52.6035V39.0098H341.824V52.6035H340.963ZM345.938 52.6035V39.0098H353.139V52.6035H345.938ZM353.145 52.6035V39.0098H360.346V52.6035H353.145ZM364.459 52.6035V39.0098H365.32V52.6035H364.459ZM362.584 52.6035V39.0098H363.445V52.6035H362.584ZM370.652 39.0098V46.4395H374.76V47.3008H369.791V39.0098H370.652ZM372.527 44.5645H374.76V45.4258H371.666V39.0098H372.527V44.5645ZM374.766 52.6035V39.0098H381.967V52.6035H374.766ZM381.973 52.6035V39.0098H389.174V52.6035H381.973ZM393.287 52.6035V45.4258H389.18V44.5645H394.148V52.6035H393.287ZM391.412 47.3008H389.18V46.4395H392.273V52.6035H391.412V47.3008ZM396.387 52.6035V39.0098H403.588V52.6035H396.387ZM403.594 52.6035V39.0098H410.795V52.6035H403.594ZM414.908 52.6035V39.0098H415.77V52.6035H414.908ZM413.033 52.6035V39.0098H413.895V52.6035H413.033ZM418.008 52.6035V39.0098H425.209V52.6035H418.008ZM425.215 52.6035V39.0098H432.416V52.6035H425.215ZM435.516 52.6035H434.654V44.5645H439.623V45.4258H435.516V52.6035ZM437.391 47.3008V52.6035H436.529V46.4395H439.623V47.3008H437.391ZM446.83 45.4258H439.629V44.5645H446.83V45.4258ZM446.83 47.3008H439.629V46.4395H446.83V47.3008ZM454.037 45.4258H446.836V44.5645H454.037V45.4258ZM454.037 47.3008H446.836V46.4395H454.037V47.3008ZM454.043 52.6035V39.0098H461.244V52.6035H454.043ZM461.25 52.6035V39.0098H468.451V52.6035H461.25ZM472.564 52.6035V45.4258H468.457V44.5645H473.426V52.6035H472.564ZM470.689 47.3008H468.457V46.4395H471.551V52.6035H470.689V47.3008ZM475.664 52.6035V39.0098H482.865V52.6035H475.664ZM482.871 52.6035V39.0098H490.072V52.6035H482.871ZM493.172 52.6035H492.311V44.5645H497.279V45.4258H493.172V52.6035ZM495.047 47.3008V52.6035H494.186V46.4395H497.279V47.3008H495.047ZM504.486 45.4258H497.285V44.5645H504.486V45.4258ZM504.486 47.3008H497.285V46.4395H504.486V47.3008ZM511.693 45.4258H504.492V44.5645H511.693V45.4258ZM511.693 47.3008H504.492V46.4395H511.693V47.3008ZM511.699 52.6035V39.0098H518.9V52.6035H511.699ZM518.906 52.6035V39.0098H526.107V52.6035H518.906ZM530.221 52.6035V39.0098H531.082V52.6035H530.221ZM528.346 52.6035V39.0098H529.207V52.6035H528.346ZM533.32 52.6035V39.0098H540.521V52.6035H533.32ZM540.527 52.6035V39.0098H547.729V52.6035H540.527ZM551.842 52.6035V39.0098H552.703V52.6035H551.842ZM549.967 52.6035V39.0098H550.828V52.6035H549.967ZM558.035 39.0098V46.4395H562.143V47.3008H557.174V39.0098H558.035ZM559.91 44.5645H562.143V45.4258H559.049V39.0098H559.91V44.5645ZM562.148 52.6035V39.0098H569.35V52.6035H562.148ZM569.355 52.6035V39.0098H576.557V52.6035H569.355ZM580.67 52.6035V45.4258H576.562V44.5645H581.531V52.6035H580.67ZM578.795 47.3008H576.562V46.4395H579.656V52.6035H578.795V47.3008ZM583.77 52.6035V39.0098H590.971V52.6035H583.77ZM590.977 52.6035V39.0098H598.178V52.6035H590.977ZM602.291 52.6035V39.0098H603.152V52.6035H602.291ZM600.416 52.6035V39.0098H601.277V52.6035H600.416ZM10.3008 52.0098V59.4395H14.4082V60.3008H9.43945V52.0098H10.3008ZM12.1758 57.5645H14.4082V58.4258H11.3145V52.0098H12.1758V57.5645ZM14.4141 65.6035V52.0098H21.6152V65.6035H14.4141ZM21.6211 65.6035V52.0098H28.8223V65.6035H21.6211ZM28.8281 65.6035V52.0098H36.0293V65.6035H28.8281ZM36.0352 65.6035V52.0098H43.2363V65.6035H36.0352ZM46.3359 65.6035H45.4746V57.5645H50.4434V58.4258H46.3359V65.6035ZM48.2109 60.3008V65.6035H47.3496V59.4395H50.4434V60.3008H48.2109ZM54.5566 52.0098H55.418V60.3008H50.4492V59.4395H54.5566V52.0098ZM52.6816 57.5645V52.0098H53.543V58.4258H50.4492V57.5645H52.6816ZM64.8633 65.6035V52.0098H72.0645V65.6035H64.8633ZM72.0703 65.6035V52.0098H79.2715V65.6035H72.0703ZM83.3848 65.6035V52.0098H84.2461V65.6035H83.3848ZM81.5098 65.6035V52.0098H82.3711V65.6035H81.5098ZM86.4844 65.6035V52.0098H93.6855V65.6035H86.4844ZM93.6914 65.6035V52.0098H100.893V65.6035H93.6914ZM100.898 65.6035V52.0098H108.1V65.6035H100.898ZM108.105 65.6035V52.0098H115.307V65.6035H108.105ZM115.312 65.6035V52.0098H122.514V65.6035H115.312ZM122.52 65.6035V52.0098H129.721V65.6035H122.52ZM132.82 65.6035H131.959V57.5645H136.928V58.4258H132.82V65.6035ZM134.695 60.3008V65.6035H133.834V59.4395H136.928V60.3008H134.695ZM141.041 52.0098H141.902V60.3008H136.934V59.4395H141.041V52.0098ZM139.166 57.5645V52.0098H140.027V58.4258H136.934V57.5645H139.166ZM144.141 65.6035V52.0098H151.342V65.6035H144.141ZM151.348 65.6035V52.0098H158.549V65.6035H151.348ZM158.555 65.6035V52.0098H165.756V65.6035H158.555ZM165.762 65.6035V52.0098H172.963V65.6035H165.762ZM172.969 65.6035V52.0098H180.17V65.6035H172.969ZM180.176 65.6035V52.0098H187.377V65.6035H180.176ZM187.383 65.6035V52.0098H194.584V65.6035H187.383ZM198.697 65.6035V58.4258H194.59V57.5645H199.559V65.6035H198.697ZM196.822 60.3008H194.59V59.4395H197.684V65.6035H196.822V60.3008ZM230.625 65.6035V52.0098H237.826V65.6035H230.625ZM237.832 65.6035V52.0098H245.033V65.6035H237.832ZM249.146 65.6035V52.0098H250.008V65.6035H249.146ZM247.271 65.6035V52.0098H248.133V65.6035H247.271ZM266.66 65.6035V52.0098H273.861V65.6035H266.66ZM273.867 65.6035V52.0098H281.068V65.6035H273.867ZM285.182 65.6035V58.4258H281.074V57.5645H286.043V65.6035H285.182ZM283.307 60.3008H281.074V59.4395H284.168V65.6035H283.307V60.3008ZM288.281 65.6035V52.0098H295.482V65.6035H288.281ZM295.488 65.6035V52.0098H302.689V65.6035H295.488ZM306.803 65.6035V52.0098H307.664V65.6035H306.803ZM304.928 65.6035V52.0098H305.789V65.6035H304.928ZM324.316 65.6035V52.0098H331.518V65.6035H324.316ZM331.523 65.6035V52.0098H338.725V65.6035H331.523ZM342.838 65.6035V52.0098H343.699V65.6035H342.838ZM340.963 65.6035V52.0098H341.824V65.6035H340.963ZM345.938 65.6035V52.0098H353.139V65.6035H345.938ZM353.145 65.6035V52.0098H360.346V65.6035H353.145ZM364.459 65.6035V52.0098H365.32V65.6035H364.459ZM362.584 65.6035V52.0098H363.445V65.6035H362.584ZM377.859 52.0098V59.4395H381.967V60.3008H376.998V52.0098H377.859ZM379.734 57.5645H381.967V58.4258H378.873V52.0098H379.734V57.5645ZM381.973 65.6035V52.0098H389.174V65.6035H381.973ZM389.18 65.6035V52.0098H396.381V65.6035H389.18ZM396.387 65.6035V52.0098H403.588V65.6035H396.387ZM403.594 65.6035V52.0098H410.795V65.6035H403.594ZM414.908 65.6035V52.0098H415.77V65.6035H414.908ZM413.033 65.6035V52.0098H413.895V65.6035H413.033ZM418.008 65.6035V52.0098H425.209V65.6035H418.008ZM425.215 65.6035V52.0098H432.416V65.6035H425.215ZM432.422 65.6035V52.0098H439.623V65.6035H432.422ZM439.629 65.6035V52.0098H446.83V65.6035H439.629ZM446.836 65.6035V52.0098H454.037V65.6035H446.836ZM454.043 65.6035V52.0098H461.244V65.6035H454.043ZM464.344 65.6035H463.482V57.5645H468.451V58.4258H464.344V65.6035ZM466.219 60.3008V65.6035H465.357V59.4395H468.451V60.3008H466.219ZM472.564 52.0098H473.426V60.3008H468.457V59.4395H472.564V52.0098ZM470.689 57.5645V52.0098H471.551V58.4258H468.457V57.5645H470.689ZM475.664 65.6035V52.0098H482.865V65.6035H475.664ZM482.871 65.6035V52.0098H490.072V65.6035H482.871ZM494.186 65.6035V52.0098H495.047V65.6035H494.186ZM492.311 65.6035V52.0098H493.172V65.6035H492.311ZM511.699 65.6035V52.0098H518.9V65.6035H511.699ZM518.906 65.6035V52.0098H526.107V65.6035H518.906ZM530.221 65.6035V52.0098H531.082V65.6035H530.221ZM528.346 65.6035V52.0098H529.207V65.6035H528.346ZM533.32 65.6035V52.0098H540.521V65.6035H533.32ZM540.527 65.6035V52.0098H547.729V65.6035H540.527ZM551.842 65.6035V52.0098H552.703V65.6035H551.842ZM549.967 65.6035V52.0098H550.828V65.6035H549.967ZM565.242 52.0098V59.4395H569.35V60.3008H564.381V52.0098H565.242ZM567.117 57.5645H569.35V58.4258H566.256V52.0098H567.117V57.5645ZM569.355 65.6035V52.0098H576.557V65.6035H569.355ZM576.562 65.6035V52.0098H583.764V65.6035H576.562ZM583.77 65.6035V52.0098H590.971V65.6035H583.77ZM590.977 65.6035V52.0098H598.178V65.6035H590.977ZM602.291 65.6035V52.0098H603.152V65.6035H602.291ZM600.416 65.6035V52.0098H601.277V65.6035H600.416ZM17.5078 65.0098V72.4395H21.6152V73.3008H16.6465V65.0098H17.5078ZM19.3828 70.5645H21.6152V71.4258H18.5215V65.0098H19.3828V70.5645ZM28.8223 71.4258H21.6211V70.5645H28.8223V71.4258ZM28.8223 73.3008H21.6211V72.4395H28.8223V73.3008ZM36.0293 71.4258H28.8281V70.5645H36.0293V71.4258ZM36.0293 73.3008H28.8281V72.4395H36.0293V73.3008ZM43.2363 71.4258H36.0352V70.5645H43.2363V71.4258ZM43.2363 73.3008H36.0352V72.4395H43.2363V73.3008ZM47.3496 65.0098H48.2109V73.3008H43.2422V72.4395H47.3496V65.0098ZM45.4746 70.5645V65.0098H46.3359V71.4258H43.2422V70.5645H45.4746ZM67.957 65.0098V72.4395H72.0645V73.3008H67.0957V65.0098H67.957ZM69.832 70.5645H72.0645V71.4258H68.9707V65.0098H69.832V70.5645ZM79.2715 71.4258H72.0703V70.5645H79.2715V71.4258ZM79.2715 73.3008H72.0703V72.4395H79.2715V73.3008ZM83.3848 65.0098H84.2461V73.3008H79.2773V72.4395H83.3848V65.0098ZM81.5098 70.5645V65.0098H82.3711V71.4258H79.2773V70.5645H81.5098ZM89.5781 65.0098V72.4395H93.6855V73.3008H88.7168V65.0098H89.5781ZM91.4531 70.5645H93.6855V71.4258H90.5918V65.0098H91.4531V70.5645ZM100.893 71.4258H93.6914V70.5645H100.893V71.4258ZM100.893 73.3008H93.6914V72.4395H100.893V73.3008ZM108.1 71.4258H100.898V70.5645H108.1V71.4258ZM108.1 73.3008H100.898V72.4395H108.1V73.3008ZM115.307 71.4258H108.105V70.5645H115.307V71.4258ZM115.307 73.3008H108.105V72.4395H115.307V73.3008ZM122.514 71.4258H115.312V70.5645H122.514V71.4258ZM122.514 73.3008H115.312V72.4395H122.514V73.3008ZM129.721 71.4258H122.52V70.5645H129.721V71.4258ZM129.721 73.3008H122.52V72.4395H129.721V73.3008ZM133.834 65.0098H134.695V73.3008H129.727V72.4395H133.834V65.0098ZM131.959 70.5645V65.0098H132.82V71.4258H129.727V70.5645H131.959ZM147.234 65.0098V72.4395H151.342V73.3008H146.373V65.0098H147.234ZM149.109 70.5645H151.342V71.4258H148.248V65.0098H149.109V70.5645ZM158.549 71.4258H151.348V70.5645H158.549V71.4258ZM158.549 73.3008H151.348V72.4395H158.549V73.3008ZM165.756 71.4258H158.555V70.5645H165.756V71.4258ZM165.756 73.3008H158.555V72.4395H165.756V73.3008ZM172.963 71.4258H165.762V70.5645H172.963V71.4258ZM172.963 73.3008H165.762V72.4395H172.963V73.3008ZM180.17 71.4258H172.969V70.5645H180.17V71.4258ZM180.17 73.3008H172.969V72.4395H180.17V73.3008ZM187.377 71.4258H180.176V70.5645H187.377V71.4258ZM187.377 73.3008H180.176V72.4395H187.377V73.3008ZM194.584 71.4258H187.383V70.5645H194.584V71.4258ZM194.584 73.3008H187.383V72.4395H194.584V73.3008ZM198.697 65.0098H199.559V73.3008H194.59V72.4395H198.697V65.0098ZM196.822 70.5645V65.0098H197.684V71.4258H194.59V70.5645H196.822ZM233.719 65.0098V72.4395H237.826V73.3008H232.857V65.0098H233.719ZM235.594 70.5645H237.826V71.4258H234.732V65.0098H235.594V70.5645ZM245.033 71.4258H237.832V70.5645H245.033V71.4258ZM245.033 73.3008H237.832V72.4395H245.033V73.3008ZM249.146 65.0098H250.008V73.3008H245.039V72.4395H249.146V65.0098ZM247.271 70.5645V65.0098H248.133V71.4258H245.039V70.5645H247.271ZM269.754 65.0098V72.4395H273.861V73.3008H268.893V65.0098H269.754ZM271.629 70.5645H273.861V71.4258H270.768V65.0098H271.629V70.5645ZM281.068 71.4258H273.867V70.5645H281.068V71.4258ZM281.068 73.3008H273.867V72.4395H281.068V73.3008ZM285.182 65.0098H286.043V73.3008H281.074V72.4395H285.182V65.0098ZM283.307 70.5645V65.0098H284.168V71.4258H281.074V70.5645H283.307ZM291.375 65.0098V72.4395H295.482V73.3008H290.514V65.0098H291.375ZM293.25 70.5645H295.482V71.4258H292.389V65.0098H293.25V70.5645ZM302.689 71.4258H295.488V70.5645H302.689V71.4258ZM302.689 73.3008H295.488V72.4395H302.689V73.3008ZM306.803 65.0098H307.664V73.3008H302.695V72.4395H306.803V65.0098ZM304.928 70.5645V65.0098H305.789V71.4258H302.695V70.5645H304.928ZM327.41 65.0098V72.4395H331.518V73.3008H326.549V65.0098H327.41ZM329.285 70.5645H331.518V71.4258H328.424V65.0098H329.285V70.5645ZM338.725 71.4258H331.523V70.5645H338.725V71.4258ZM338.725 73.3008H331.523V72.4395H338.725V73.3008ZM342.838 65.0098H343.699V73.3008H338.73V72.4395H342.838V65.0098ZM340.963 70.5645V65.0098H341.824V71.4258H338.73V70.5645H340.963ZM349.031 65.0098V72.4395H353.139V73.3008H348.17V65.0098H349.031ZM350.906 70.5645H353.139V71.4258H350.045V65.0098H350.906V70.5645ZM360.346 71.4258H353.145V70.5645H360.346V71.4258ZM360.346 73.3008H353.145V72.4395H360.346V73.3008ZM364.459 65.0098H365.32V73.3008H360.352V72.4395H364.459V65.0098ZM362.584 70.5645V65.0098H363.445V71.4258H360.352V70.5645H362.584ZM385.066 65.0098V72.4395H389.174V73.3008H384.205V65.0098H385.066ZM386.941 70.5645H389.174V71.4258H386.08V65.0098H386.941V70.5645ZM396.381 71.4258H389.18V70.5645H396.381V71.4258ZM396.381 73.3008H389.18V72.4395H396.381V73.3008ZM403.588 71.4258H396.387V70.5645H403.588V71.4258ZM403.588 73.3008H396.387V72.4395H403.588V73.3008ZM410.795 71.4258H403.594V70.5645H410.795V71.4258ZM410.795 73.3008H403.594V72.4395H410.795V73.3008ZM414.908 65.0098H415.77V73.3008H410.801V72.4395H414.908V65.0098ZM413.033 70.5645V65.0098H413.895V71.4258H410.801V70.5645H413.033ZM421.102 65.0098V72.4395H425.209V73.3008H420.24V65.0098H421.102ZM422.977 70.5645H425.209V71.4258H422.115V65.0098H422.977V70.5645ZM432.416 71.4258H425.215V70.5645H432.416V71.4258ZM432.416 73.3008H425.215V72.4395H432.416V73.3008ZM439.623 71.4258H432.422V70.5645H439.623V71.4258ZM439.623 73.3008H432.422V72.4395H439.623V73.3008ZM446.83 71.4258H439.629V70.5645H446.83V71.4258ZM446.83 73.3008H439.629V72.4395H446.83V73.3008ZM454.037 71.4258H446.836V70.5645H454.037V71.4258ZM454.037 73.3008H446.836V72.4395H454.037V73.3008ZM461.244 71.4258H454.043V70.5645H461.244V71.4258ZM461.244 73.3008H454.043V72.4395H461.244V73.3008ZM465.357 65.0098H466.219V73.3008H461.25V72.4395H465.357V65.0098ZM463.482 70.5645V65.0098H464.344V71.4258H461.25V70.5645H463.482ZM478.758 65.0098V72.4395H482.865V73.3008H477.896V65.0098H478.758ZM480.633 70.5645H482.865V71.4258H479.771V65.0098H480.633V70.5645ZM490.072 71.4258H482.871V70.5645H490.072V71.4258ZM490.072 73.3008H482.871V72.4395H490.072V73.3008ZM494.186 65.0098H495.047V73.3008H490.078V72.4395H494.186V65.0098ZM492.311 70.5645V65.0098H493.172V71.4258H490.078V70.5645H492.311ZM514.793 65.0098V72.4395H518.9V73.3008H513.932V65.0098H514.793ZM516.668 70.5645H518.9V71.4258H515.807V65.0098H516.668V70.5645ZM526.107 71.4258H518.906V70.5645H526.107V71.4258ZM526.107 73.3008H518.906V72.4395H526.107V73.3008ZM530.221 65.0098H531.082V73.3008H526.113V72.4395H530.221V65.0098ZM528.346 70.5645V65.0098H529.207V71.4258H526.113V70.5645H528.346ZM536.414 65.0098V72.4395H540.521V73.3008H535.553V65.0098H536.414ZM538.289 70.5645H540.521V71.4258H537.428V65.0098H538.289V70.5645ZM547.729 71.4258H540.527V70.5645H547.729V71.4258ZM547.729 73.3008H540.527V72.4395H547.729V73.3008ZM551.842 65.0098H552.703V73.3008H547.734V72.4395H551.842V65.0098ZM549.967 70.5645V65.0098H550.828V71.4258H547.734V70.5645H549.967ZM572.449 65.0098V72.4395H576.557V73.3008H571.588V65.0098H572.449ZM574.324 70.5645H576.557V71.4258H573.463V65.0098H574.324V70.5645ZM583.764 71.4258H576.562V70.5645H583.764V71.4258ZM583.764 73.3008H576.562V72.4395H583.764V73.3008ZM590.971 71.4258H583.77V70.5645H590.971V71.4258ZM590.971 73.3008H583.77V72.4395H590.971V73.3008ZM598.178 71.4258H590.977V70.5645H598.178V71.4258ZM598.178 73.3008H590.977V72.4395H598.178V73.3008ZM602.291 65.0098H603.152V73.3008H598.184V72.4395H602.291V65.0098ZM600.416 70.5645V65.0098H601.277V71.4258H598.184V70.5645H600.416Z" fill="black"/>
</svg>
</file>

<file path="frontend/src/components/common/ProfileVariantBadge.tsx">
import type { ExecutorProfileId } from 'shared/types';
import { cn } from '@/lib/utils';

interface ProfileVariantBadgeProps {
  profileVariant: ExecutorProfileId | null;
  className?: string;
}

export function ProfileVariantBadge({
  profileVariant,
  className,
}: ProfileVariantBadgeProps) {
  if (!profileVariant) {
    return null;
  }

  return (
    <span className={cn('text-xs text-muted-foreground', className)}>
      {profileVariant.executor}
      {profileVariant.variant && (
        <>
          <span className="mx-1">/</span>
          <span className="font-medium">{profileVariant.variant}</span>
        </>
      )}
    </span>
  );
}

export default ProfileVariantBadge;
</file>

<file path="frontend/src/components/common/RawLogText.tsx">
import { memo } from 'react';
import { AnsiHtml } from 'fancy-ansi/react';
import { hasAnsi } from 'fancy-ansi';
import { clsx } from 'clsx';

interface RawLogTextProps {
  content: string;
  channel?: 'stdout' | 'stderr';
  as?: 'div' | 'span';
  className?: string;
}

const RawLogText = memo(
  ({
    content,
    channel = 'stdout',
    as: Component = 'div',
    className,
  }: RawLogTextProps) => {
    // Only apply stderr fallback color when no ANSI codes are present
    const hasAnsiCodes = hasAnsi(content);
    const shouldApplyStderrFallback = channel === 'stderr' && !hasAnsiCodes;

    return (
      <Component
        className={clsx(
          'font-mono text-xs break-all whitespace-pre-wrap',
          shouldApplyStderrFallback && 'text-destructive',
          className
        )}
      >
        <AnsiHtml text={content} />
      </Component>
    );
  }
);

RawLogText.displayName = 'RawLogText';

export default RawLogText;
</file>

<file path="frontend/src/components/dialogs/auth/GitHubLoginDialog.tsx">
import { useEffect, useState } from 'react';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Button } from '@/components/ui/button';
import { useUserSystem } from '@/components/config-provider';
import { Check, Clipboard, Github } from 'lucide-react';
import { Loader } from '@/components/ui/loader';
import { githubAuthApi } from '@/lib/api';
import { DeviceFlowStartResponse, DevicePollStatus } from 'shared/types';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

const GitHubLoginDialog = NiceModal.create(() => {
  const modal = useModal();
  const { config, loading, githubTokenInvalid, reloadSystem } = useUserSystem();
  const [fetching, setFetching] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [deviceState, setDeviceState] =
    useState<null | DeviceFlowStartResponse>(null);
  const [polling, setPolling] = useState(false);
  const [copied, setCopied] = useState(false);

  const isAuthenticated =
    !!(config?.github?.username && config?.github?.oauth_token) &&
    !githubTokenInvalid;

  const handleLogin = async () => {
    setFetching(true);
    setError(null);
    setDeviceState(null);
    try {
      const data = await githubAuthApi.start();
      setDeviceState(data);
      setPolling(true);
    } catch (e: any) {
      console.error(e);
      setError(e?.message || 'Network error');
    } finally {
      setFetching(false);
    }
  };

  // Poll for completion
  useEffect(() => {
    let timer: ReturnType<typeof setTimeout> | null = null;
    if (polling && deviceState) {
      const poll = async () => {
        try {
          const poll_status = await githubAuthApi.poll();
          switch (poll_status) {
            case DevicePollStatus.SUCCESS:
              setPolling(false);
              setDeviceState(null);
              setError(null);
              await reloadSystem();
              break;
            case DevicePollStatus.AUTHORIZATION_PENDING:
              timer = setTimeout(poll, deviceState.interval * 1000);
              break;
            case DevicePollStatus.SLOW_DOWN:
              timer = setTimeout(poll, (deviceState.interval + 5) * 1000);
          }
        } catch (e: any) {
          if (e?.message === 'expired_token') {
            setPolling(false);
            setError('Device code expired. Please try again.');
            setDeviceState(null);
          } else {
            setPolling(false);
            setError(e?.message || 'Login failed.');
            setDeviceState(null);
          }
        }
      };
      timer = setTimeout(poll, deviceState.interval * 1000);
    }
    return () => {
      if (timer) clearTimeout(timer);
    };
  }, [polling, deviceState]);

  // Automatically copy code to clipboard and open GitHub URL when deviceState is set
  useEffect(() => {
    if (deviceState?.user_code) {
      copyToClipboard(deviceState.user_code);
    }
  }, [deviceState?.user_code, deviceState?.verification_uri]);

  const copyToClipboard = async (text: string) => {
    try {
      if (navigator.clipboard && navigator.clipboard.writeText) {
        await navigator.clipboard.writeText(text);
        setCopied(true);
        setTimeout(() => setCopied(false), 2000);
      } else {
        // Fallback for environments where clipboard API is not available
        const textArea = document.createElement('textarea');
        textArea.value = text;
        textArea.style.position = 'fixed';
        textArea.style.left = '-999999px';
        textArea.style.top = '-999999px';
        document.body.appendChild(textArea);
        textArea.focus();
        textArea.select();
        try {
          document.execCommand('copy');
          setCopied(true);
          setTimeout(() => setCopied(false), 2000);
        } catch (err) {
          console.warn('Copy to clipboard failed:', err);
        }
        document.body.removeChild(textArea);
      }
    } catch (err) {
      console.warn('Copy to clipboard failed:', err);
    }
  };

  return (
    <Dialog
      open={modal.visible}
      onOpenChange={(open) => {
        if (!open) {
          modal.resolve(isAuthenticated ? true : false);
          modal.hide();
        }
      }}
    >
      <DialogContent>
        <DialogHeader>
          <div className="flex items-center gap-3">
            <Github className="h-6 w-6" />
            <DialogTitle>Sign in with GitHub</DialogTitle>
          </div>
          <DialogDescription className="text-left pt-1">
            Connect your GitHub account to create and manage pull requests
            directly from Vibe Kanban.
          </DialogDescription>
        </DialogHeader>
        {loading ? (
          <Loader message="Loading…" size={32} className="py-8" />
        ) : isAuthenticated ? (
          <div className="space-y-4 py-3">
            <Card>
              <CardContent className="text-center py-8">
                <div className="flex items-center justify-center gap-3 mb-4">
                  <Check className="h-8 w-8 text-green-500" />
                  <Github className="h-8 w-8 text-muted-foreground" />
                </div>
                <div className="text-lg font-medium mb-1">
                  Successfully connected!
                </div>
                <div className="text-sm text-muted-foreground">
                  You are signed in as <b>{config?.github?.username ?? ''}</b>
                </div>
              </CardContent>
            </Card>
            <DialogFooter>
              <Button
                onClick={() => {
                  modal.resolve(true);
                  modal.hide();
                }}
                className="w-full"
              >
                Close
              </Button>
            </DialogFooter>
          </div>
        ) : deviceState ? (
          <div className="space-y-4">
            <div className="flex items-start gap-3">
              <span className="flex-shrink-0 w-10 h-10 bg-background border rounded-full flex items-center justify-center text-lg font-semibold">
                1
              </span>
              <div>
                <p className="text-sm font-medium mb-1">
                  Go to GitHub Device Authorization
                </p>
                <a
                  href={deviceState.verification_uri}
                  target="_blank"
                  rel="noopener noreferrer"
                  className="text-sm underline"
                >
                  {deviceState.verification_uri}
                </a>
              </div>
            </div>

            <div className="flex items-start gap-3">
              <span className="flex-shrink-0 w-10 h-10 bg-background border rounded-full flex items-center justify-center text-lg font-semibold">
                2
              </span>
              <div className="flex-1">
                <p className="text-sm font-medium mb-3">Enter this code:</p>
                <div className="flex items-center gap-3">
                  <span className="text-sm font-mono font-bold tracking-[0.2em] bg-muted border flex h-9 px-2 items-center">
                    <span>{deviceState.user_code}</span>
                  </span>
                  <Button
                    variant="outline"
                    size="sm"
                    onClick={() => copyToClipboard(deviceState.user_code)}
                    disabled={copied}
                  >
                    {copied ? (
                      <>
                        <Check className="w-4 h-4 mr-1" />
                        Copied
                      </>
                    ) : (
                      <>
                        <Clipboard className="w-4 h-4 mr-1" />
                        Copy
                      </>
                    )}
                  </Button>
                </div>
              </div>
            </div>

            <div className="flex items-center gap-2 text-xs text-muted-foreground bg-muted/50 p-2 rounded-lg">
              <Github className="h-3 w-3 flex-shrink-0" />
              <span>
                {copied
                  ? 'Code copied to clipboard! Complete the authorization on GitHub.'
                  : 'Waiting for you to authorize this application on GitHub...'}
              </span>
            </div>

            {error && (
              <div className="p-3 bg-destructive/10 border border-destructive/20 rounded-lg">
                <div className="text-destructive text-sm">{error}</div>
              </div>
            )}

            <DialogFooter>
              <Button
                variant="outline"
                onClick={() => {
                  modal.resolve(false);
                  modal.hide();
                }}
              >
                Skip
              </Button>
            </DialogFooter>
          </div>
        ) : (
          <div className="space-y-4 py-3">
            <Card>
              <CardHeader className="pb-3">
                <CardTitle className="text-base">
                  Why do you need GitHub access?
                </CardTitle>
              </CardHeader>
              <CardContent className="space-y-3 pt-0">
                <div className="flex items-start gap-3">
                  <Check className="h-4 w-4 text-green-500 mt-0.5 flex-shrink-0" />
                  <div>
                    <p className="text-sm font-medium">Create pull requests</p>
                    <p className="text-xs text-muted-foreground">
                      Generate PRs directly from your task attempts
                    </p>
                  </div>
                </div>
                <div className="flex items-start gap-3">
                  <Check className="h-4 w-4 text-green-500 mt-0.5 flex-shrink-0" />
                  <div>
                    <p className="text-sm font-medium">Manage repositories</p>
                    <p className="text-xs text-muted-foreground">
                      Access your repos to push changes and create branches
                    </p>
                  </div>
                </div>
                <div className="flex items-start gap-3">
                  <Check className="h-4 w-4 text-green-500 mt-0.5 flex-shrink-0" />
                  <div>
                    <p className="text-sm font-medium">Streamline workflow</p>
                    <p className="text-xs text-muted-foreground">
                      Skip manual PR creation and focus on coding
                    </p>
                  </div>
                </div>
              </CardContent>
            </Card>

            {error && (
              <div className="p-3 bg-destructive/10 border border-destructive/20 rounded-lg">
                <div className="text-destructive text-sm">{error}</div>
              </div>
            )}

            <DialogFooter className="gap-3 flex-col sm:flex-row">
              <Button
                variant="outline"
                onClick={() => {
                  modal.resolve(false);
                  modal.hide();
                }}
                className="flex-1"
              >
                Skip
              </Button>
              <Button
                onClick={handleLogin}
                disabled={fetching}
                className="flex-1"
              >
                <Github className="h-4 w-4 mr-2" />
                {fetching ? 'Starting…' : 'Sign in with GitHub'}
              </Button>
            </DialogFooter>
          </div>
        )}
      </DialogContent>
    </Dialog>
  );
});

export { GitHubLoginDialog };
</file>

<file path="frontend/src/components/dialogs/auth/ProvidePatDialog.tsx">
import { useState } from 'react';
import {
  Dialog,
  DialogContent,
  DialogHeader,
  DialogTitle,
  DialogFooter,
} from '@/components/ui/dialog';
import { Input } from '@/components/ui/input';
import { Button } from '@/components/ui/button';
import { useUserSystem } from '@/components/config-provider';
import { Alert, AlertDescription } from '@/components/ui/alert';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

export interface ProvidePatDialogProps {
  errorMessage?: string;
}

export const ProvidePatDialog = NiceModal.create<ProvidePatDialogProps>(
  ({ errorMessage }) => {
    const modal = useModal();
    const { config, updateAndSaveConfig } = useUserSystem();
    const [pat, setPat] = useState('');
    const [saving, setSaving] = useState(false);
    const [error, setError] = useState<string | null>(null);

    const handleSave = async () => {
      if (!config) return;
      setSaving(true);
      setError(null);
      try {
        await updateAndSaveConfig({
          github: {
            ...config.github,
            pat,
          },
        });
        modal.resolve(true);
        modal.hide();
      } catch (err) {
        setError('Failed to save Personal Access Token');
      } finally {
        setSaving(false);
      }
    };

    return (
      <Dialog
        open={modal.visible}
        onOpenChange={(open) => !open && modal.hide()}
      >
        <DialogContent>
          <DialogHeader>
            <DialogTitle>Provide GitHub Personal Access Token</DialogTitle>
          </DialogHeader>
          <div className="space-y-2">
            <p>
              {errorMessage ||
                'Your GitHub OAuth token does not have sufficient permissions to open a PR in this repository.'}
              <br />
              <br />
              Please provide a Personal Access Token with <b>repo</b>{' '}
              permissions.
            </p>
            <Input
              placeholder="ghp_xxxxxxxxxxxxxxxxxxxx"
              value={pat}
              onChange={(e) => setPat(e.target.value)}
              autoFocus
            />
            <p className="text-sm text-muted-foreground">
              <a
                href="https://github.com/settings/tokens"
                target="_blank"
                rel="noopener noreferrer"
                className="text-blue-600 hover:underline"
              >
                Create a token here
              </a>
            </p>
            {error && (
              <Alert variant="destructive">
                <AlertDescription>{error}</AlertDescription>
              </Alert>
            )}
          </div>
          <DialogFooter>
            <Button onClick={handleSave} disabled={saving || !pat || !config}>
              {saving ? 'Saving...' : 'Save'}
            </Button>
            <Button
              variant="outline"
              onClick={() => {
                modal.resolve(false);
                modal.hide();
              }}
              disabled={saving}
            >
              Cancel
            </Button>
          </DialogFooter>
        </DialogContent>
      </Dialog>
    );
  }
);
</file>

<file path="frontend/src/components/dialogs/global/DisclaimerDialog.tsx">
import { useState } from 'react';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Button } from '@/components/ui/button';
import { Checkbox } from '@/components/ui/checkbox';
import { AlertTriangle } from 'lucide-react';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

const DisclaimerDialog = NiceModal.create(() => {
  const modal = useModal();
  const [acknowledged, setAcknowledged] = useState(false);

  const handleAccept = () => {
    if (acknowledged) {
      modal.resolve('accepted');
    }
  };

  return (
    <Dialog open={modal.visible} uncloseable={true}>
      <DialogContent className="sm:max-w-[600px]">
        <DialogHeader>
          <div className="flex items-center gap-3">
            <AlertTriangle className="h-6 w-6 text-destructive" />
            <DialogTitle>Important Safety Warning</DialogTitle>
          </div>
          <DialogDescription className="text-left space-y-4 pt-4">
            <p className="font-semibold text-foreground">
              Please read and acknowledge the following before proceeding:
            </p>
            <div className="space-y-3">
              <p>
                <strong>Coding agents have full access to your computer</strong>{' '}
                and can execute any terminal commands, including:
              </p>
              <ul className="list-disc list-inside space-y-1 ml-4">
                <li>Installing, modifying, or deleting software</li>
                <li>Accessing, creating, or removing files and directories</li>
                <li>Making network requests and connections</li>
                <li>Running system-level commands with your permissions</li>
              </ul>
              <p>
                <strong>
                  This software is experimental and may cause catastrophic
                  damage
                </strong>{' '}
                to your system, data, or projects. By using this software, you
                acknowledge that:
              </p>
              <ul className="list-disc list-inside space-y-1 ml-4">
                <li>You use this software entirely at your own risk</li>
                <li>
                  The developers are not responsible for any damage, data loss,
                  or security issues
                </li>
                <li>
                  You should have proper backups of important data before using
                  this software
                </li>
                <li>
                  You understand the potential consequences of granting
                  unrestricted system access
                </li>
              </ul>
            </div>
          </DialogDescription>
        </DialogHeader>
        <div className="flex items-center space-x-2 py-4">
          <Checkbox
            id="acknowledge"
            checked={acknowledged}
            onCheckedChange={(checked: boolean) =>
              setAcknowledged(checked === true)
            }
          />
          <label
            htmlFor="acknowledge"
            className="text-sm font-medium leading-none peer-disabled:cursor-not-allowed peer-disabled:opacity-70"
          >
            I understand and acknowledge the risks described above. I am aware
            that coding agents have full access to my computer and may cause
            catastrophic damage.
          </label>
        </div>
        <DialogFooter>
          <Button
            onClick={handleAccept}
            disabled={!acknowledged}
            variant="destructive"
          >
            I Accept the Risks and Want to Proceed
          </Button>
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
});

export { DisclaimerDialog };
</file>

<file path="frontend/src/components/dialogs/global/OnboardingDialog.tsx">
import { useState } from 'react';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Button } from '@/components/ui/button';
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from '@/components/ui/select';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import { Label } from '@/components/ui/label';
import { Input } from '@/components/ui/input';
import { Sparkles, Code, ChevronDown, HandMetal } from 'lucide-react';
import { BaseCodingAgent, EditorType } from 'shared/types';
import type { ExecutorProfileId } from 'shared/types';
import { useUserSystem } from '@/components/config-provider';

import { toPrettyCase } from '@/utils/string';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

export type OnboardingResult = {
  profile: ExecutorProfileId;
  editor: { editor_type: EditorType; custom_command: string | null };
};

const OnboardingDialog = NiceModal.create(() => {
  const modal = useModal();
  const { profiles, config } = useUserSystem();

  const [profile, setProfile] = useState<ExecutorProfileId>(
    config?.executor_profile || {
      executor: BaseCodingAgent.CLAUDE_CODE,
      variant: null,
    }
  );
  const [editorType, setEditorType] = useState<EditorType>(EditorType.VS_CODE);
  const [customCommand, setCustomCommand] = useState<string>('');

  const handleComplete = () => {
    modal.resolve({
      profile,
      editor: {
        editor_type: editorType,
        custom_command:
          editorType === EditorType.CUSTOM ? customCommand || null : null,
      },
    } as OnboardingResult);
  };

  const isValid =
    editorType !== EditorType.CUSTOM ||
    (editorType === EditorType.CUSTOM && customCommand.trim() !== '');

  return (
    <Dialog open={modal.visible} uncloseable={true}>
      <DialogContent className="sm:max-w-[600px] space-y-4">
        <DialogHeader>
          <div className="flex items-center gap-3">
            <HandMetal className="h-6 w-6 text-primary text-primary-foreground" />
            <DialogTitle>Welcome to Vibe Kanban</DialogTitle>
          </div>
          <DialogDescription className="text-left pt-2">
            Let's set up your coding preferences. You can always change these
            later in Settings.
          </DialogDescription>
        </DialogHeader>
        <div className="space-y-2">
          <h2 className="text-xl flex items-center gap-2">
            <Sparkles className="h-4 w-4" />
            Choose Your Coding Agent
          </h2>
          <div className="space-y-2">
            <Label htmlFor="profile">Default Agent</Label>
            <div className="flex gap-2">
              <Select
                value={profile.executor}
                onValueChange={(v) =>
                  setProfile({ executor: v as BaseCodingAgent, variant: null })
                }
              >
                <SelectTrigger id="profile" className="flex-1">
                  <SelectValue placeholder="Select your preferred coding agent" />
                </SelectTrigger>
                <SelectContent>
                  {profiles &&
                    (Object.keys(profiles) as BaseCodingAgent[]).map(
                      (agent) => (
                        <SelectItem key={agent} value={agent}>
                          {agent}
                        </SelectItem>
                      )
                    )}
                </SelectContent>
              </Select>

              {/* Show variant selector if selected profile has variants */}
              {(() => {
                const selectedProfile = profiles?.[profile.executor];
                const hasVariants =
                  selectedProfile && Object.keys(selectedProfile).length > 0;

                if (hasVariants) {
                  return (
                    <DropdownMenu>
                      <DropdownMenuTrigger asChild>
                        <Button
                          variant="outline"
                          className="w-24 px-2 flex items-center justify-between"
                        >
                          <span className="text-xs truncate flex-1 text-left">
                            {profile.variant || 'DEFAULT'}
                          </span>
                          <ChevronDown className="h-3 w-3 ml-1 flex-shrink-0" />
                        </Button>
                      </DropdownMenuTrigger>
                      <DropdownMenuContent>
                        {Object.keys(selectedProfile).map((variant) => (
                          <DropdownMenuItem
                            key={variant}
                            onClick={() =>
                              setProfile({
                                ...profile,
                                variant: variant,
                              })
                            }
                            className={
                              profile.variant === variant ? 'bg-accent' : ''
                            }
                          >
                            {variant}
                          </DropdownMenuItem>
                        ))}
                      </DropdownMenuContent>
                    </DropdownMenu>
                  );
                } else if (selectedProfile) {
                  // Show disabled button when profile exists but has no variants
                  return (
                    <Button
                      variant="outline"
                      className="w-24 px-2 flex items-center justify-between"
                      disabled
                    >
                      <span className="text-xs truncate flex-1 text-left">
                        Default
                      </span>
                    </Button>
                  );
                }
                return null;
              })()}
            </div>
          </div>
        </div>

        <div className="space-y-2">
          <h2 className="text-xl flex items-center gap-2">
            <Code className="h-4 w-4" />
            Choose Your Code Editor
          </h2>

          <div className="space-y-2">
            <Label htmlFor="editor">Preferred Editor</Label>
            <Select
              value={editorType}
              onValueChange={(value: EditorType) => setEditorType(value)}
            >
              <SelectTrigger id="editor">
                <SelectValue placeholder="Select your preferred editor" />
              </SelectTrigger>
              <SelectContent>
                {Object.values(EditorType).map((type) => (
                  <SelectItem key={type} value={type}>
                    {toPrettyCase(type)}
                  </SelectItem>
                ))}
              </SelectContent>
            </Select>
            <p className="text-sm text-muted-foreground">
              This editor will be used to open task attempts and project files.
            </p>

            {editorType === EditorType.CUSTOM && (
              <div className="space-y-2">
                <Label htmlFor="custom-command">Custom Command</Label>
                <Input
                  id="custom-command"
                  placeholder="e.g., code, subl, vim"
                  value={customCommand}
                  onChange={(e) => setCustomCommand(e.target.value)}
                />
                <p className="text-sm text-muted-foreground">
                  Enter the command to run your custom editor. Use spaces for
                  arguments (e.g., "code --wait").
                </p>
              </div>
            )}
          </div>
        </div>

        <DialogFooter>
          <Button
            onClick={handleComplete}
            disabled={!isValid}
            className="w-full"
          >
            Continue
          </Button>
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
});

export { OnboardingDialog };
</file>

<file path="frontend/src/components/dialogs/global/PrivacyOptInDialog.tsx">
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Button } from '@/components/ui/button';
import { Shield, CheckCircle, XCircle, Settings } from 'lucide-react';
import { useUserSystem } from '@/components/config-provider';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

const PrivacyOptInDialog = NiceModal.create(() => {
  const modal = useModal();
  const { config } = useUserSystem();

  // Check if user is authenticated with GitHub
  const isGitHubAuthenticated =
    config?.github?.username && config?.github?.oauth_token;

  const handleOptIn = () => {
    modal.resolve(true);
  };

  const handleOptOut = () => {
    modal.resolve(false);
  };

  return (
    <Dialog open={modal.visible} uncloseable={true}>
      <DialogContent className="sm:max-w-[700px]">
        <DialogHeader>
          <div className="flex items-center gap-3">
            <Shield className="h-6 w-6 text-primary text-primary-foreground" />
            <DialogTitle>Feedback</DialogTitle>
          </div>
          <DialogDescription className="text-left pt-1">
            Help us improve Vibe Kanban by sharing usage data and allowing us to
            contact you if needed.
          </DialogDescription>
        </DialogHeader>

        <div className="space-y-3">
          <h2>What data do we collect?</h2>
          <div>
            {isGitHubAuthenticated && (
              <div className="flex items-start gap-2">
                <CheckCircle className="h-4 w-4 text-green-500 mt-0.5 flex-shrink-0" />
                <div className="min-w-0">
                  <p className="text-sm font-medium">
                    GitHub profile information
                  </p>
                  <p className="text-xs text-muted-foreground">
                    Username and email address to send you only very important
                    updates about the project. We promise not to abuse this
                  </p>
                </div>
              </div>
            )}
            <div className="flex items-start gap-2">
              <CheckCircle className="h-4 w-4 text-green-500 mt-0.5 flex-shrink-0" />
              <div className="min-w-0">
                <p className="text-sm font-medium">High-level usage metrics</p>
                <p className="text-xs text-muted-foreground">
                  Number of tasks created, projects managed, feature usage
                </p>
              </div>
            </div>
            <div className="flex items-start gap-2">
              <CheckCircle className="h-4 w-4 text-green-500 mt-0.5 flex-shrink-0" />
              <div className="min-w-0">
                <p className="text-sm font-medium">
                  Performance and error data
                </p>
                <p className="text-xs text-muted-foreground">
                  Application crashes, response times, technical issues
                </p>
              </div>
            </div>
            <div className="flex items-start gap-2">
              <XCircle className="h-4 w-4 text-destructive mt-0.5 flex-shrink-0" />
              <div className="min-w-0">
                <p className="text-sm font-medium">We do NOT collect</p>
                <p className="text-xs text-muted-foreground">
                  Task contents, code snippets, project names, or other personal
                  data
                </p>
              </div>
            </div>
          </div>

          <div className="flex items-center gap-2 text-xs text-muted-foreground bg-muted/50 p-2 rounded-lg">
            <Settings className="h-3 w-3 flex-shrink-0" />
            <span>
              This helps us prioritize improvements. You can change this
              preference anytime in Settings.
            </span>
          </div>
        </div>

        <DialogFooter className="gap-3 flex-col sm:flex-row pt-2">
          <Button variant="outline" onClick={handleOptOut} className="flex-1">
            <XCircle className="h-4 w-4 mr-2" />
            No thanks
          </Button>
          <Button onClick={handleOptIn} className="flex-1">
            <CheckCircle className="h-4 w-4 mr-2" />
            Yes, help improve Vibe Kanban
          </Button>
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
});

export { PrivacyOptInDialog };
export default PrivacyOptInDialog;
</file>

<file path="frontend/src/components/dialogs/global/ReleaseNotesDialog.tsx">
import { useState } from 'react';
import {
  Dialog,
  DialogContent,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Button } from '@/components/ui/button';
import { AlertCircle, ExternalLink } from 'lucide-react';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

const RELEASE_NOTES_URL = 'https://vibekanban.com/release-notes';

export const ReleaseNotesDialog = NiceModal.create(() => {
  const modal = useModal();
  const [iframeError, setIframeError] = useState(false);

  const handleOpenInBrowser = () => {
    window.open(RELEASE_NOTES_URL, '_blank');
    modal.resolve();
  };

  const handleIframeError = () => {
    setIframeError(true);
  };

  return (
    <Dialog
      open={modal.visible}
      onOpenChange={(open) => !open && modal.resolve()}
      className="h-[calc(100%-4rem)]"
    >
      <DialogContent className="flex flex-col w-full h-full max-w-7xl max-h-[calc(100dvh-1rem)] p-0">
        <DialogHeader className="p-4 border-b flex-shrink-0">
          <DialogTitle className="text-xl font-semibold">
            We've updated Vibe Kanban! Check out what's new...
          </DialogTitle>
        </DialogHeader>

        {iframeError ? (
          <div className="flex flex-col items-center justify-center flex-1 text-center space-y-4 p-4">
            <AlertCircle className="h-12 w-12 text-muted-foreground" />
            <div className="space-y-2">
              <h3 className="text-lg font-medium">
                Unable to load release notes
              </h3>
              <p className="text-sm text-muted-foreground max-w-md">
                We couldn't display the release notes in this window. Click
                below to view them in your browser.
              </p>
            </div>
            <Button onClick={handleOpenInBrowser} className="mt-4">
              <ExternalLink className="h-4 w-4 mr-2" />
              Open Release Notes in Browser
            </Button>
          </div>
        ) : (
          <iframe
            src={RELEASE_NOTES_URL}
            className="flex-1 w-full border-0"
            sandbox="allow-scripts allow-same-origin allow-popups"
            referrerPolicy="no-referrer"
            title="Release Notes"
            onError={handleIframeError}
            onLoad={(e) => {
              // Check if iframe content loaded successfully
              try {
                const iframe = e.target as HTMLIFrameElement;
                // If iframe is accessible but empty, it might indicate loading issues
                if (iframe.contentDocument?.body?.children.length === 0) {
                  setTimeout(() => setIframeError(true), 5000); // Wait 5s then show fallback
                }
              } catch {
                // Cross-origin access blocked (expected), iframe loaded successfully
              }
            }}
          />
        )}

        <DialogFooter className="p-4 border-t flex-shrink-0">
          <Button variant="outline" onClick={handleOpenInBrowser}>
            <ExternalLink className="h-4 w-4 mr-2" />
            Open in Browser
          </Button>
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
});
</file>

<file path="frontend/src/components/dialogs/projects/ProjectEditorSelectionDialog.tsx">
import { useState } from 'react';
import { Button } from '@/components/ui/button';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from '@/components/ui/select';
import { EditorType, Project } from 'shared/types';
import { useOpenProjectInEditor } from '@/hooks/useOpenProjectInEditor';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

export interface ProjectEditorSelectionDialogProps {
  selectedProject: Project | null;
}

export const ProjectEditorSelectionDialog =
  NiceModal.create<ProjectEditorSelectionDialogProps>(({ selectedProject }) => {
    const modal = useModal();
    const handleOpenInEditor = useOpenProjectInEditor(selectedProject, () =>
      modal.hide()
    );
    const [selectedEditor, setSelectedEditor] = useState<EditorType>(
      EditorType.VS_CODE
    );

    const handleConfirm = () => {
      handleOpenInEditor(selectedEditor);
      modal.resolve(selectedEditor);
      modal.hide();
    };

    const handleCancel = () => {
      modal.resolve(null);
      modal.hide();
    };

    return (
      <Dialog
        open={modal.visible}
        onOpenChange={(open) => !open && handleCancel()}
      >
        <DialogContent className="sm:max-w-[425px]">
          <DialogHeader>
            <DialogTitle>Choose Editor</DialogTitle>
            <DialogDescription>
              The default editor failed to open. Please select an alternative
              editor to open the project.
            </DialogDescription>
          </DialogHeader>
          <div className="grid gap-4 py-4">
            <div className="space-y-2">
              <label className="text-sm font-medium">Editor</label>
              <Select
                value={selectedEditor}
                onValueChange={(value) =>
                  setSelectedEditor(value as EditorType)
                }
              >
                <SelectTrigger>
                  <SelectValue />
                </SelectTrigger>
                <SelectContent>
                  {Object.values(EditorType).map((editor) => (
                    <SelectItem key={editor} value={editor}>
                      {editor}
                    </SelectItem>
                  ))}
                </SelectContent>
              </Select>
            </div>
          </div>
          <DialogFooter>
            <Button variant="outline" onClick={handleCancel}>
              Cancel
            </Button>
            <Button onClick={handleConfirm}>Open Editor</Button>
          </DialogFooter>
        </DialogContent>
      </Dialog>
    );
  });
</file>

<file path="frontend/src/components/dialogs/projects/ProjectFormDialog.tsx">
import { useEffect, useState } from 'react';
import { Button } from '@/components/ui/button';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';
import { TaskTemplateManager } from '@/components/TaskTemplateManager';
import { ProjectFormFields } from '@/components/projects/project-form-fields';
import { CreateProject, Project, UpdateProject } from 'shared/types';
import { projectsApi } from '@/lib/api';
import { generateProjectNameFromPath } from '@/utils/string';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

export interface ProjectFormDialogProps {
  project?: Project | null;
}

export type ProjectFormDialogResult = 'saved' | 'canceled';

export const ProjectFormDialog = NiceModal.create<ProjectFormDialogProps>(
  ({ project }) => {
    const modal = useModal();
    const [name, setName] = useState(project?.name || '');
    const [gitRepoPath, setGitRepoPath] = useState(
      project?.git_repo_path || ''
    );
    const [setupScript, setSetupScript] = useState(project?.setup_script ?? '');
    const [devScript, setDevScript] = useState(project?.dev_script ?? '');
    const [cleanupScript, setCleanupScript] = useState(
      project?.cleanup_script ?? ''
    );
    const [copyFiles, setCopyFiles] = useState(project?.copy_files ?? '');
    const [loading, setLoading] = useState(false);
    const [error, setError] = useState('');
    const [repoMode, setRepoMode] = useState<'existing' | 'new'>('existing');
    const [parentPath, setParentPath] = useState('');
    const [folderName, setFolderName] = useState('');

    const isEditing = !!project;

    // Update form fields when project prop changes
    useEffect(() => {
      if (project) {
        setName(project.name || '');
        setGitRepoPath(project.git_repo_path || '');
        setSetupScript(project.setup_script ?? '');
        setDevScript(project.dev_script ?? '');
        setCleanupScript(project.cleanup_script ?? '');
        setCopyFiles(project.copy_files ?? '');
      } else {
        setName('');
        setGitRepoPath('');
        setSetupScript('');
        setDevScript('');
        setCleanupScript('');
        setCopyFiles('');
      }
    }, [project]);

    // Auto-populate project name from directory name
    const handleGitRepoPathChange = (path: string) => {
      setGitRepoPath(path);

      // Only auto-populate name for new projects
      if (!isEditing && path) {
        const cleanName = generateProjectNameFromPath(path);
        if (cleanName) setName(cleanName);
      }
    };

    // Handle direct project creation from repo selection
    const handleDirectCreate = async (path: string, suggestedName: string) => {
      setError('');
      setLoading(true);

      try {
        const createData: CreateProject = {
          name: suggestedName,
          git_repo_path: path,
          use_existing_repo: true,
          setup_script: null,
          dev_script: null,
          cleanup_script: null,
          copy_files: null,
        };

        await projectsApi.create(createData);
        modal.resolve('saved' as ProjectFormDialogResult);
        modal.hide();
      } catch (error) {
        setError(error instanceof Error ? error.message : 'An error occurred');
      } finally {
        setLoading(false);
      }
    };

    const handleSubmit = async (e: React.FormEvent) => {
      e.preventDefault();
      setError('');
      setLoading(true);

      try {
        let finalGitRepoPath = gitRepoPath;
        if (repoMode === 'new') {
          // Use home directory (~) if parentPath is empty
          const effectiveParentPath = parentPath.trim() || '~';
          finalGitRepoPath = `${effectiveParentPath}/${folderName}`.replace(
            /\/+/g,
            '/'
          );
        }
        // Auto-populate name from git repo path if not provided
        const finalName =
          name.trim() || generateProjectNameFromPath(finalGitRepoPath);

        if (isEditing) {
          const updateData: UpdateProject = {
            name: finalName,
            git_repo_path: finalGitRepoPath,
            setup_script: setupScript.trim() || null,
            dev_script: devScript.trim() || null,
            cleanup_script: cleanupScript.trim() || null,
            copy_files: copyFiles.trim() || null,
          };

          await projectsApi.update(project!.id, updateData);
        } else {
          // Creating new project
          const createData: CreateProject = {
            name: finalName,
            git_repo_path: finalGitRepoPath,
            use_existing_repo: repoMode === 'existing',
            setup_script: null,
            dev_script: null,
            cleanup_script: null,
            copy_files: null,
          };

          await projectsApi.create(createData);
        }

        modal.resolve('saved' as ProjectFormDialogResult);
        modal.hide();
      } catch (error) {
        setError(error instanceof Error ? error.message : 'An error occurred');
      } finally {
        setLoading(false);
      }
    };

    const handleCancel = () => {
      // Reset form
      if (project) {
        setName(project.name || '');
        setGitRepoPath(project.git_repo_path || '');
        setSetupScript(project.setup_script ?? '');
        setDevScript(project.dev_script ?? '');
        setCopyFiles(project.copy_files ?? '');
      } else {
        setName('');
        setGitRepoPath('');
        setSetupScript('');
        setDevScript('');
        setCopyFiles('');
      }
      setParentPath('');
      setFolderName('');
      setError('');

      modal.resolve('canceled' as ProjectFormDialogResult);
      modal.hide();
    };

    const handleOpenChange = (open: boolean) => {
      if (!open) {
        handleCancel();
      }
    };

    return (
      <Dialog open={modal.visible} onOpenChange={handleOpenChange}>
        <DialogContent className="overflow-x-hidden">
          <DialogHeader>
            <DialogTitle>
              {isEditing ? 'Edit Project' : 'Create Project'}
            </DialogTitle>
            <DialogDescription>
              {isEditing
                ? "Make changes to your project here. Click save when you're done."
                : 'Choose your repository source'}
            </DialogDescription>
          </DialogHeader>

          <div className="mx-auto w-full max-w-2xl overflow-x-hidden px-1">
            {isEditing ? (
              <Tabs defaultValue="general" className="w-full -mt-2">
                <TabsList className="grid w-full grid-cols-2 mb-4">
                  <TabsTrigger value="general">General</TabsTrigger>
                  <TabsTrigger value="templates">Task Templates</TabsTrigger>
                </TabsList>
                <TabsContent value="general" className="space-y-4">
                  <form onSubmit={handleSubmit} className="space-y-4">
                    <ProjectFormFields
                      isEditing={isEditing}
                      repoMode={repoMode}
                      setRepoMode={setRepoMode}
                      gitRepoPath={gitRepoPath}
                      handleGitRepoPathChange={handleGitRepoPathChange}
                      parentPath={parentPath}
                      setParentPath={setParentPath}
                      setFolderName={setFolderName}
                      setName={setName}
                      name={name}
                      setupScript={setupScript}
                      setSetupScript={setSetupScript}
                      devScript={devScript}
                      setDevScript={setDevScript}
                      cleanupScript={cleanupScript}
                      setCleanupScript={setCleanupScript}
                      copyFiles={copyFiles}
                      setCopyFiles={setCopyFiles}
                      error={error}
                      setError={setError}
                      projectId={project ? project.id : undefined}
                    />
                    <DialogFooter>
                      <Button
                        type="submit"
                        disabled={loading || !gitRepoPath.trim()}
                      >
                        {loading ? 'Saving...' : 'Save Changes'}
                      </Button>
                    </DialogFooter>
                  </form>
                </TabsContent>
                <TabsContent value="templates" className="mt-0 pt-0">
                  <TaskTemplateManager
                    projectId={project ? project.id : undefined}
                  />
                </TabsContent>
              </Tabs>
            ) : (
              <form onSubmit={handleSubmit} className="space-y-4">
                <ProjectFormFields
                  isEditing={isEditing}
                  repoMode={repoMode}
                  setRepoMode={setRepoMode}
                  gitRepoPath={gitRepoPath}
                  handleGitRepoPathChange={handleGitRepoPathChange}
                  parentPath={parentPath}
                  setParentPath={setParentPath}
                  setFolderName={setFolderName}
                  setName={setName}
                  name={name}
                  setupScript={setupScript}
                  setSetupScript={setSetupScript}
                  devScript={devScript}
                  setDevScript={setDevScript}
                  cleanupScript={cleanupScript}
                  setCleanupScript={setCleanupScript}
                  copyFiles={copyFiles}
                  setCopyFiles={setCopyFiles}
                  error={error}
                  setError={setError}
                  projectId={undefined}
                  onCreateProject={handleDirectCreate}
                />
                {repoMode === 'new' && (
                  <DialogFooter>
                    <Button
                      type="submit"
                      disabled={loading || !folderName.trim()}
                    >
                      {loading ? 'Creating...' : 'Create Project'}
                    </Button>
                  </DialogFooter>
                )}
              </form>
            )}
          </div>
        </DialogContent>
      </Dialog>
    );
  }
);
</file>

<file path="frontend/src/components/dialogs/settings/CreateConfigurationDialog.tsx">
import { useState, useEffect } from 'react';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Label } from '@/components/ui/label';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from '@/components/ui/select';
import { Alert, AlertDescription } from '@/components/ui/alert';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

export interface CreateConfigurationDialogProps {
  executorType: string;
  existingConfigs: string[];
}

export type CreateConfigurationResult = {
  action: 'created' | 'canceled';
  configName?: string;
  cloneFrom?: string | null;
};

export const CreateConfigurationDialog =
  NiceModal.create<CreateConfigurationDialogProps>(
    ({ executorType, existingConfigs }) => {
      const modal = useModal();
      const [configName, setConfigName] = useState('');
      const [cloneFrom, setCloneFrom] = useState<string | null>(null);
      const [error, setError] = useState<string | null>(null);

      useEffect(() => {
        // Reset form when dialog opens
        if (modal.visible) {
          setConfigName('');
          setCloneFrom(null);
          setError(null);
        }
      }, [modal.visible]);

      const validateConfigName = (name: string): string | null => {
        const trimmedName = name.trim();
        if (!trimmedName) return 'Configuration name cannot be empty';
        if (trimmedName.length > 40)
          return 'Configuration name must be 40 characters or less';
        if (!/^[a-zA-Z0-9_-]+$/.test(trimmedName)) {
          return 'Configuration name can only contain letters, numbers, underscores, and hyphens';
        }
        if (existingConfigs.includes(trimmedName)) {
          return 'A configuration with this name already exists';
        }
        return null;
      };

      const handleCreate = () => {
        const validationError = validateConfigName(configName);
        if (validationError) {
          setError(validationError);
          return;
        }

        modal.resolve({
          action: 'created',
          configName: configName.trim(),
          cloneFrom,
        } as CreateConfigurationResult);
        modal.hide();
      };

      const handleCancel = () => {
        modal.resolve({ action: 'canceled' } as CreateConfigurationResult);
        modal.hide();
      };

      const handleOpenChange = (open: boolean) => {
        if (!open) {
          handleCancel();
        }
      };

      return (
        <Dialog open={modal.visible} onOpenChange={handleOpenChange}>
          <DialogContent className="sm:max-w-md">
            <DialogHeader>
              <DialogTitle>Create New Configuration</DialogTitle>
              <DialogDescription>
                Add a new configuration for the {executorType} executor.
              </DialogDescription>
            </DialogHeader>

            <div className="space-y-4">
              <div className="space-y-2">
                <Label htmlFor="config-name">Configuration Name</Label>
                <Input
                  id="config-name"
                  value={configName}
                  onChange={(e) => {
                    setConfigName(e.target.value);
                    setError(null);
                  }}
                  placeholder="e.g., PRODUCTION, DEVELOPMENT"
                  maxLength={40}
                  autoFocus
                />
              </div>

              <div className="space-y-2">
                <Label htmlFor="clone-from">Clone from (optional)</Label>
                <Select
                  value={cloneFrom || '__blank__'}
                  onValueChange={(value) =>
                    setCloneFrom(value === '__blank__' ? null : value)
                  }
                >
                  <SelectTrigger id="clone-from">
                    <SelectValue placeholder="Start blank or clone existing" />
                  </SelectTrigger>
                  <SelectContent>
                    <SelectItem value="__blank__">Start blank</SelectItem>
                    {existingConfigs.map((configuration) => (
                      <SelectItem key={configuration} value={configuration}>
                        Clone from {configuration}
                      </SelectItem>
                    ))}
                  </SelectContent>
                </Select>
              </div>

              {error && (
                <Alert variant="destructive">
                  <AlertDescription>{error}</AlertDescription>
                </Alert>
              )}
            </div>

            <DialogFooter>
              <Button variant="outline" onClick={handleCancel}>
                Cancel
              </Button>
              <Button onClick={handleCreate} disabled={!configName.trim()}>
                Create Configuration
              </Button>
            </DialogFooter>
          </DialogContent>
        </Dialog>
      );
    }
  );
</file>

<file path="frontend/src/components/dialogs/settings/DeleteConfigurationDialog.tsx">
import { useState } from 'react';
import { Button } from '@/components/ui/button';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Alert, AlertDescription } from '@/components/ui/alert';
import { Loader2 } from 'lucide-react';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

export interface DeleteConfigurationDialogProps {
  configName: string;
  executorType: string;
}

export type DeleteConfigurationResult = 'deleted' | 'canceled';

export const DeleteConfigurationDialog =
  NiceModal.create<DeleteConfigurationDialogProps>(
    ({ configName, executorType }) => {
      const modal = useModal();
      const [isDeleting, setIsDeleting] = useState(false);
      const [error, setError] = useState<string | null>(null);

      const handleDelete = async () => {
        setIsDeleting(true);
        setError(null);

        try {
          // Resolve with 'deleted' to let parent handle the deletion
          modal.resolve('deleted' as DeleteConfigurationResult);
          modal.hide();
        } catch (error) {
          setError('Failed to delete configuration. Please try again.');
          setIsDeleting(false);
        }
      };

      const handleCancel = () => {
        modal.resolve('canceled' as DeleteConfigurationResult);
        modal.hide();
      };

      const handleOpenChange = (open: boolean) => {
        if (!open) {
          handleCancel();
        }
      };

      return (
        <Dialog open={modal.visible} onOpenChange={handleOpenChange}>
          <DialogContent className="sm:max-w-md">
            <DialogHeader>
              <DialogTitle>Delete Configuration?</DialogTitle>
              <DialogDescription>
                This will permanently remove "{configName}" from the{' '}
                {executorType} executor. You can't undo this action.
              </DialogDescription>
            </DialogHeader>

            {error && (
              <Alert variant="destructive">
                <AlertDescription>{error}</AlertDescription>
              </Alert>
            )}

            <DialogFooter>
              <Button
                variant="outline"
                onClick={handleCancel}
                disabled={isDeleting}
              >
                Cancel
              </Button>
              <Button
                variant="destructive"
                onClick={handleDelete}
                disabled={isDeleting}
              >
                {isDeleting && (
                  <Loader2 className="mr-2 h-4 w-4 animate-spin" />
                )}
                Delete
              </Button>
            </DialogFooter>
          </DialogContent>
        </Dialog>
      );
    }
  );
</file>

<file path="frontend/src/components/dialogs/shared/ConfirmDialog.tsx">
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Button } from '@/components/ui/button';
import NiceModal, { useModal } from '@ebay/nice-modal-react';
import { AlertTriangle, Info, CheckCircle, XCircle } from 'lucide-react';
import type { ConfirmResult } from '@/lib/modals';

export interface ConfirmDialogProps {
  title: string;
  message: string;
  confirmText?: string;
  cancelText?: string;
  variant?: 'default' | 'destructive' | 'info' | 'success';
  icon?: boolean;
}

const ConfirmDialog = NiceModal.create<ConfirmDialogProps>((props) => {
  const modal = useModal();
  const {
    title,
    message,
    confirmText = 'Confirm',
    cancelText = 'Cancel',
    variant = 'default',
    icon = true,
  } = props;

  const handleConfirm = () => {
    modal.resolve('confirmed' as ConfirmResult);
  };

  const handleCancel = () => {
    modal.resolve('canceled' as ConfirmResult);
  };

  const getIcon = () => {
    if (!icon) return null;

    switch (variant) {
      case 'destructive':
        return <AlertTriangle className="h-6 w-6 text-destructive" />;
      case 'info':
        return <Info className="h-6 w-6 text-blue-500" />;
      case 'success':
        return <CheckCircle className="h-6 w-6 text-green-500" />;
      default:
        return <XCircle className="h-6 w-6 text-muted-foreground" />;
    }
  };

  const getConfirmButtonVariant = () => {
    return variant === 'destructive' ? 'destructive' : 'default';
  };

  return (
    <Dialog open={modal.visible} onOpenChange={handleCancel}>
      <DialogContent className="sm:max-w-[425px]">
        <DialogHeader>
          <div className="flex items-center gap-3">
            {getIcon()}
            <DialogTitle>{title}</DialogTitle>
          </div>
          <DialogDescription className="text-left pt-2">
            {message}
          </DialogDescription>
        </DialogHeader>
        <DialogFooter className="gap-2">
          <Button variant="outline" onClick={handleCancel}>
            {cancelText}
          </Button>
          <Button variant={getConfirmButtonVariant()} onClick={handleConfirm}>
            {confirmText}
          </Button>
        </DialogFooter>
      </DialogContent>
    </Dialog>
  );
});

export { ConfirmDialog };
</file>

<file path="frontend/src/components/dialogs/shared/FolderPickerDialog.tsx">
import React, { useEffect, useMemo, useState } from 'react';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Alert, AlertDescription } from '@/components/ui/alert';
import {
  AlertCircle,
  ChevronUp,
  File,
  Folder,
  FolderOpen,
  Home,
  Search,
} from 'lucide-react';
import { fileSystemApi } from '@/lib/api';
import { DirectoryEntry, DirectoryListResponse } from 'shared/types';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

export interface FolderPickerDialogProps {
  value?: string;
  title?: string;
  description?: string;
}

export const FolderPickerDialog = NiceModal.create<FolderPickerDialogProps>(
  ({
    value = '',
    title = 'Select Folder',
    description = 'Choose a folder for your project',
  }) => {
    const modal = useModal();
    const [currentPath, setCurrentPath] = useState<string>('');
    const [entries, setEntries] = useState<DirectoryEntry[]>([]);
    const [loading, setLoading] = useState(false);
    const [error, setError] = useState('');
    const [manualPath, setManualPath] = useState(value);
    const [searchTerm, setSearchTerm] = useState('');

    const filteredEntries = useMemo(() => {
      if (!searchTerm.trim()) return entries;
      return entries.filter((entry) =>
        entry.name.toLowerCase().includes(searchTerm.toLowerCase())
      );
    }, [entries, searchTerm]);

    useEffect(() => {
      if (modal.visible) {
        setManualPath(value);
        loadDirectory();
      }
    }, [modal.visible, value]);

    const loadDirectory = async (path?: string) => {
      setLoading(true);
      setError('');

      try {
        const result: DirectoryListResponse = await fileSystemApi.list(path);

        // Ensure result exists and has the expected structure
        if (!result || typeof result !== 'object') {
          throw new Error('Invalid response from file system API');
        }
        // Safely access entries, ensuring it's an array
        const entries = Array.isArray(result.entries) ? result.entries : [];
        setEntries(entries);
        const newPath = result.current_path || '';
        setCurrentPath(newPath);
        // Update manual path if we have a specific path (not for initial home directory load)
        if (path) {
          setManualPath(newPath);
        }
      } catch (err) {
        setError(
          err instanceof Error ? err.message : 'Failed to load directory'
        );
        // Reset entries to empty array on error
        setEntries([]);
      } finally {
        setLoading(false);
      }
    };

    const handleFolderClick = (entry: DirectoryEntry) => {
      if (entry.is_directory) {
        loadDirectory(entry.path);
        setManualPath(entry.path); // Auto-populate the manual path field
      }
    };

    const handleParentDirectory = () => {
      const parentPath = currentPath.split('/').slice(0, -1).join('/');
      const newPath = parentPath || '/';
      loadDirectory(newPath);
      setManualPath(newPath);
    };

    const handleHomeDirectory = () => {
      loadDirectory();
      // Don't set manual path here since home directory path varies by system
    };

    const handleManualPathChange = (e: React.ChangeEvent<HTMLInputElement>) => {
      setManualPath(e.target.value);
    };

    const handleManualPathSubmit = () => {
      loadDirectory(manualPath);
    };

    const handleSelectCurrent = () => {
      const selectedPath = manualPath || currentPath;
      modal.resolve(selectedPath);
      modal.hide();
    };

    const handleSelectManual = () => {
      modal.resolve(manualPath);
      modal.hide();
    };

    const handleCancel = () => {
      modal.resolve(null);
      modal.hide();
    };

    const handleOpenChange = (open: boolean) => {
      if (!open) {
        handleCancel();
      }
    };

    return (
      <Dialog open={modal.visible} onOpenChange={handleOpenChange}>
        <DialogContent className="max-w-[600px] w-full h-[700px] flex flex-col overflow-hidden">
          <DialogHeader>
            <DialogTitle>{title}</DialogTitle>
            <DialogDescription>{description}</DialogDescription>
          </DialogHeader>

          <div className="flex-1 flex flex-col space-y-4 overflow-hidden">
            {/* Legend */}
            <div className="text-xs text-muted-foreground border-b pb-2">
              Click folder names to navigate • Use action buttons to select
            </div>

            {/* Manual path input */}
            <div className="space-y-2">
              <div className="text-sm font-medium">Enter path manually:</div>
              <div className="flex space-x-2 min-w-0">
                <Input
                  value={manualPath}
                  onChange={handleManualPathChange}
                  placeholder="/path/to/your/project"
                  className="flex-1 min-w-0"
                />
                <Button
                  onClick={handleManualPathSubmit}
                  variant="outline"
                  size="sm"
                  className="flex-shrink-0"
                >
                  Go
                </Button>
              </div>
            </div>

            {/* Search input */}
            <div className="space-y-2">
              <div className="text-sm font-medium">
                Search current directory:
              </div>
              <div className="relative">
                <Search className="absolute left-3 top-1/2 transform -translate-y-1/2 h-4 w-4 text-muted-foreground" />
                <Input
                  value={searchTerm}
                  onChange={(e) => setSearchTerm(e.target.value)}
                  placeholder="Filter folders and files..."
                  className="pl-10"
                />
              </div>
            </div>

            {/* Navigation */}
            <div className="flex items-center space-x-2 min-w-0">
              <Button
                onClick={handleHomeDirectory}
                variant="outline"
                size="sm"
                className="flex-shrink-0"
              >
                <Home className="h-4 w-4" />
              </Button>
              <Button
                onClick={handleParentDirectory}
                variant="outline"
                size="sm"
                disabled={!currentPath || currentPath === '/'}
                className="flex-shrink-0"
              >
                <ChevronUp className="h-4 w-4" />
              </Button>
              <div className="text-sm text-muted-foreground flex-1 truncate min-w-0">
                {currentPath || 'Home'}
              </div>
              <Button
                onClick={handleSelectCurrent}
                variant="outline"
                size="sm"
                disabled={!currentPath}
                className="flex-shrink-0"
              >
                Select Current
              </Button>
            </div>

            {/* Directory listing */}
            <div className="flex-1 border rounded-md overflow-auto">
              {loading ? (
                <div className="p-4 text-center text-muted-foreground">
                  Loading...
                </div>
              ) : error ? (
                <Alert variant="destructive" className="m-4">
                  <AlertCircle className="h-4 w-4" />
                  <AlertDescription>{error}</AlertDescription>
                </Alert>
              ) : filteredEntries.length === 0 ? (
                <div className="p-4 text-center text-muted-foreground">
                  {searchTerm.trim() ? 'No matches found' : 'No folders found'}
                </div>
              ) : (
                <div className="p-2">
                  {filteredEntries.map((entry, index) => (
                    <div
                      key={index}
                      className={`flex items-center space-x-2 p-2 rounded cursor-pointer hover:bg-accent ${
                        !entry.is_directory
                          ? 'opacity-50 cursor-not-allowed'
                          : ''
                      }`}
                      onClick={() =>
                        entry.is_directory && handleFolderClick(entry)
                      }
                      title={entry.name} // Show full name on hover
                    >
                      {entry.is_directory ? (
                        entry.is_git_repo ? (
                          <FolderOpen className="h-4 w-4 text-success flex-shrink-0" />
                        ) : (
                          <Folder className="h-4 w-4 text-blue-600 flex-shrink-0" />
                        )
                      ) : (
                        <File className="h-4 w-4 text-gray-400 flex-shrink-0" />
                      )}
                      <span className="text-sm flex-1 truncate min-w-0">
                        {entry.name}
                      </span>
                      {entry.is_git_repo && (
                        <span className="text-xs text-success bg-green-100 px-2 py-1 rounded flex-shrink-0">
                          git repo
                        </span>
                      )}
                    </div>
                  ))}
                </div>
              )}
            </div>
          </div>

          <DialogFooter>
            <Button type="button" variant="outline" onClick={handleCancel}>
              Cancel
            </Button>
            <Button onClick={handleSelectManual} disabled={!manualPath.trim()}>
              Select Path
            </Button>
          </DialogFooter>
        </DialogContent>
      </Dialog>
    );
  }
);
</file>

<file path="frontend/src/components/dialogs/tasks/CreatePRDialog.tsx">
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Label } from '@radix-ui/react-label';
import { Textarea } from '@/components/ui/textarea.tsx';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import BranchSelector from '@/components/tasks/BranchSelector';
import { useCallback, useEffect, useState } from 'react';
import { attemptsApi } from '@/lib/api.ts';

import {
  GitBranch,
  GitHubServiceError,
  TaskAttempt,
  TaskWithAttemptStatus,
} from 'shared/types';
import { projectsApi } from '@/lib/api.ts';
import { Loader2 } from 'lucide-react';
import NiceModal, { useModal } from '@ebay/nice-modal-react';
const CreatePrDialog = NiceModal.create(() => {
  const modal = useModal();
  const data = modal.args as
    | { attempt: TaskAttempt; task: TaskWithAttemptStatus; projectId: string }
    | undefined;
  const [prTitle, setPrTitle] = useState('');
  const [prBody, setPrBody] = useState('');
  const [prBaseBranch, setPrBaseBranch] = useState('');
  const [creatingPR, setCreatingPR] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [branches, setBranches] = useState<GitBranch[]>([]);
  const [branchesLoading, setBranchesLoading] = useState(false);

  useEffect(() => {
    if (modal.visible && data) {
      setPrTitle(`${data.task.title} (vibe-kanban)`);
      setPrBody(data.task.description || '');

      // Always fetch branches for dropdown population
      if (data.projectId) {
        setBranchesLoading(true);
        projectsApi
          .getBranches(data.projectId)
          .then((projectBranches) => {
            setBranches(projectBranches);

            // Set smart default: task base branch OR current branch
            if (data.attempt.base_branch) {
              setPrBaseBranch(data.attempt.base_branch);
            } else {
              const currentBranch = projectBranches.find((b) => b.is_current);
              if (currentBranch) {
                setPrBaseBranch(currentBranch.name);
              }
            }
          })
          .catch(console.error)
          .finally(() => setBranchesLoading(false));
      }

      setError(null); // Reset error when opening
    }
  }, [modal.visible, data]);

  const handleConfirmCreatePR = useCallback(async () => {
    if (!data?.projectId || !data?.attempt.id) return;

    setCreatingPR(true);

    const result = await attemptsApi.createPR(data.attempt.id, {
      title: prTitle,
      body: prBody || null,
      base_branch: prBaseBranch || null,
    });

    if (result.success) {
      setError(null); // Clear any previous errors on success
      // Reset form and close dialog
      setPrTitle('');
      setPrBody('');
      setPrBaseBranch('');
      modal.hide();
    } else {
      if (result.error) {
        modal.hide();
        switch (result.error) {
          case GitHubServiceError.TOKEN_INVALID:
            NiceModal.show('github-login');
            break;
          case GitHubServiceError.INSUFFICIENT_PERMISSIONS:
            NiceModal.show('provide-pat');
            break;
          case GitHubServiceError.REPO_NOT_FOUND_OR_NO_ACCESS:
            NiceModal.show('provide-pat', {
              errorMessage:
                'Your token does not have access to this repository, or the repository does not exist. Please check the repository URL and/or provide a Personal Access Token with access.',
            });
            break;
        }
      } else if (result.message) {
        setError(result.message);
      } else {
        setError('Failed to create GitHub PR');
      }
    }
    setCreatingPR(false);
  }, [data, prBaseBranch, prBody, prTitle, modal]);

  const handleCancelCreatePR = useCallback(() => {
    modal.hide();
    // Reset form to empty state
    setPrTitle('');
    setPrBody('');
    setPrBaseBranch('');
  }, [modal]);

  // Don't render if no data
  if (!data) return null;

  return (
    <>
      <Dialog open={modal.visible} onOpenChange={() => handleCancelCreatePR()}>
        <DialogContent className="sm:max-w-[525px]">
          <DialogHeader>
            <DialogTitle>Create GitHub Pull Request</DialogTitle>
            <DialogDescription>
              Create a pull request for this task attempt on GitHub.
            </DialogDescription>
          </DialogHeader>
          <div className="space-y-4 py-4">
            <div className="space-y-2">
              <Label htmlFor="pr-title">Title</Label>
              <Input
                id="pr-title"
                value={prTitle}
                onChange={(e) => setPrTitle(e.target.value)}
                placeholder="Enter PR title"
              />
            </div>
            <div className="space-y-2">
              <Label htmlFor="pr-body">Description (optional)</Label>
              <Textarea
                id="pr-body"
                value={prBody}
                onChange={(e) => setPrBody(e.target.value)}
                placeholder="Enter PR description"
                rows={4}
              />
            </div>
            <div className="space-y-2">
              <Label htmlFor="pr-base">Base Branch</Label>
              <BranchSelector
                branches={branches}
                selectedBranch={prBaseBranch}
                onBranchSelect={setPrBaseBranch}
                placeholder={
                  branchesLoading ? 'Loading branches...' : 'Select base branch'
                }
                className={
                  branchesLoading ? 'opacity-50 cursor-not-allowed' : ''
                }
              />
            </div>
            {error && (
              <div className="text-sm text-destructive bg-red-50 p-2 rounded">
                {error}
              </div>
            )}
          </div>
          <DialogFooter>
            <Button variant="outline" onClick={handleCancelCreatePR}>
              Cancel
            </Button>
            <Button
              onClick={handleConfirmCreatePR}
              disabled={creatingPR || !prTitle.trim()}
              className="bg-blue-600 hover:bg-blue-700"
            >
              {creatingPR ? (
                <>
                  <Loader2 className="mr-2 h-4 w-4 animate-spin" />
                  Creating...
                </>
              ) : (
                'Create PR'
              )}
            </Button>
          </DialogFooter>
        </DialogContent>
      </Dialog>
    </>
  );
});

export { CreatePrDialog as CreatePRDialog };
export default CreatePrDialog;
</file>

<file path="frontend/src/components/dialogs/tasks/DeleteTaskConfirmationDialog.tsx">
import { useState } from 'react';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Button } from '@/components/ui/button';
import { Alert } from '@/components/ui/alert';
import { tasksApi } from '@/lib/api';
import type { TaskWithAttemptStatus } from 'shared/types';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

export interface DeleteTaskConfirmationDialogProps {
  task: TaskWithAttemptStatus;
  projectId: string;
}

const DeleteTaskConfirmationDialog =
  NiceModal.create<DeleteTaskConfirmationDialogProps>(({ task }) => {
    const modal = useModal();
    const [isDeleting, setIsDeleting] = useState(false);
    const [error, setError] = useState<string | null>(null);

    const handleConfirmDelete = async () => {
      setIsDeleting(true);
      setError(null);

      try {
        await tasksApi.delete(task.id);
        modal.resolve(true);
        modal.hide();
      } catch (err: unknown) {
        const errorMessage =
          err instanceof Error ? err.message : 'Failed to delete task';
        setError(errorMessage);
      } finally {
        setIsDeleting(false);
      }
    };

    const handleCancelDelete = () => {
      modal.resolve(false);
      modal.hide();
    };

    return (
      <Dialog
        open={modal.visible}
        onOpenChange={(open) => !open && handleCancelDelete()}
      >
        <DialogContent>
          <DialogHeader>
            <DialogTitle>Delete Task</DialogTitle>
            <DialogDescription>
              Are you sure you want to delete{' '}
              <span className="font-semibold">"{task.title}"</span>?
            </DialogDescription>
          </DialogHeader>

          <div className="py-4">
            <div className="bg-red-50 border border-red-200 rounded-md p-3">
              <p className="text-sm text-red-800">
                <strong>Warning:</strong> This action will permanently delete
                the task and cannot be undone.
              </p>
            </div>
          </div>

          {error && (
            <Alert variant="destructive" className="mb-4">
              {error}
            </Alert>
          )}

          <DialogFooter>
            <Button
              variant="outline"
              onClick={handleCancelDelete}
              disabled={isDeleting}
              autoFocus
            >
              Cancel
            </Button>
            <Button
              variant="destructive"
              onClick={handleConfirmDelete}
              disabled={isDeleting}
            >
              {isDeleting ? 'Deleting...' : 'Delete Task'}
            </Button>
          </DialogFooter>
        </DialogContent>
      </Dialog>
    );
  });

export { DeleteTaskConfirmationDialog };
export default DeleteTaskConfirmationDialog;
</file>

<file path="frontend/src/components/dialogs/tasks/EditorSelectionDialog.tsx">
import { useState } from 'react';
import { Button } from '@/components/ui/button';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from '@/components/ui/select';
import { EditorType, TaskAttempt } from 'shared/types';
import { useOpenInEditor } from '@/hooks/useOpenInEditor';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

export interface EditorSelectionDialogProps {
  selectedAttempt: TaskAttempt | null;
}

export const EditorSelectionDialog =
  NiceModal.create<EditorSelectionDialogProps>(({ selectedAttempt }) => {
    const modal = useModal();
    const handleOpenInEditor = useOpenInEditor(selectedAttempt, () =>
      modal.hide()
    );
    const [selectedEditor, setSelectedEditor] = useState<EditorType>(
      EditorType.VS_CODE
    );

    const handleConfirm = () => {
      handleOpenInEditor(selectedEditor);
      modal.resolve(selectedEditor);
      modal.hide();
    };

    const handleCancel = () => {
      modal.resolve(null);
      modal.hide();
    };

    return (
      <Dialog
        open={modal.visible}
        onOpenChange={(open) => !open && handleCancel()}
      >
        <DialogContent className="sm:max-w-[425px]">
          <DialogHeader>
            <DialogTitle>Choose Editor</DialogTitle>
            <DialogDescription>
              The default editor failed to open. Please select an alternative
              editor to open the task worktree.
            </DialogDescription>
          </DialogHeader>
          <div className="grid gap-4 py-4">
            <div className="space-y-2">
              <label className="text-sm font-medium">Editor</label>
              <Select
                value={selectedEditor}
                onValueChange={(value) =>
                  setSelectedEditor(value as EditorType)
                }
              >
                <SelectTrigger>
                  <SelectValue />
                </SelectTrigger>
                <SelectContent>
                  {Object.values(EditorType).map((editor) => (
                    <SelectItem key={editor} value={editor}>
                      {editor}
                    </SelectItem>
                  ))}
                </SelectContent>
              </Select>
            </div>
          </div>
          <DialogFooter>
            <Button variant="outline" onClick={handleCancel}>
              Cancel
            </Button>
            <Button onClick={handleConfirm}>Open Editor</Button>
          </DialogFooter>
        </DialogContent>
      </Dialog>
    );
  });
</file>

<file path="frontend/src/components/dialogs/tasks/RebaseDialog.tsx">
import { useState } from 'react';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Button } from '@/components/ui/button';
import BranchSelector from '@/components/tasks/BranchSelector';
import type { GitBranch } from 'shared/types';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

export interface RebaseDialogProps {
  branches: GitBranch[];
  isRebasing?: boolean;
}

export type RebaseDialogResult = {
  action: 'confirmed' | 'canceled';
  branchName?: string;
};

export const RebaseDialog = NiceModal.create<RebaseDialogProps>(
  ({ branches, isRebasing = false }) => {
    const modal = useModal();
    const [selectedBranch, setSelectedBranch] = useState<string>('');

    const handleConfirm = () => {
      if (selectedBranch) {
        modal.resolve({
          action: 'confirmed',
          branchName: selectedBranch,
        } as RebaseDialogResult);
        modal.hide();
      }
    };

    const handleCancel = () => {
      modal.resolve({ action: 'canceled' } as RebaseDialogResult);
      modal.hide();
    };

    const handleOpenChange = (open: boolean) => {
      if (!open) {
        handleCancel();
      }
    };

    return (
      <Dialog open={modal.visible} onOpenChange={handleOpenChange}>
        <DialogContent className="sm:max-w-md">
          <DialogHeader>
            <DialogTitle>Rebase Task Attempt</DialogTitle>
            <DialogDescription>
              Choose a new base branch to rebase this task attempt onto.
            </DialogDescription>
          </DialogHeader>

          <div className="space-y-4">
            <div className="space-y-2">
              <label htmlFor="base-branch" className="text-sm font-medium">
                Base Branch
              </label>
              <BranchSelector
                branches={branches}
                selectedBranch={selectedBranch}
                onBranchSelect={setSelectedBranch}
                placeholder="Select a base branch"
                excludeCurrentBranch={false}
              />
            </div>
          </div>

          <DialogFooter>
            <Button
              variant="outline"
              onClick={handleCancel}
              disabled={isRebasing}
            >
              Cancel
            </Button>
            <Button
              onClick={handleConfirm}
              disabled={isRebasing || !selectedBranch}
            >
              {isRebasing ? 'Rebasing...' : 'Rebase'}
            </Button>
          </DialogFooter>
        </DialogContent>
      </Dialog>
    );
  }
);
</file>

<file path="frontend/src/components/dialogs/tasks/RestoreLogsDialog.tsx">
import { useState } from 'react';
import {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Button } from '@/components/ui/button';
import { AlertTriangle, GitCommit } from 'lucide-react';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

export interface RestoreLogsDialogProps {
  targetSha: string | null;
  targetSubject: string | null;
  commitsToReset: number | null;
  isLinear: boolean | null;
  laterCount: number;
  laterCoding: number;
  laterSetup: number;
  laterCleanup: number;
  needGitReset: boolean;
  canGitReset: boolean;
  hasRisk: boolean;
  uncommittedCount: number;
  untrackedCount: number;
  initialWorktreeResetOn: boolean;
  initialForceReset: boolean;
}

export type RestoreLogsDialogResult = {
  action: 'confirmed' | 'canceled';
  performGitReset?: boolean;
  forceWhenDirty?: boolean;
};

export const RestoreLogsDialog = NiceModal.create<RestoreLogsDialogProps>(
  ({
    targetSha,
    targetSubject,
    commitsToReset,
    isLinear,
    laterCount,
    laterCoding,
    laterSetup,
    laterCleanup,
    needGitReset,
    canGitReset,
    hasRisk,
    uncommittedCount,
    untrackedCount,
    initialWorktreeResetOn,
    initialForceReset,
  }) => {
    const modal = useModal();
    const [worktreeResetOn, setWorktreeResetOn] = useState(
      initialWorktreeResetOn
    );
    const [forceReset, setForceReset] = useState(initialForceReset);

    const hasLater = laterCount > 0;
    const short = targetSha?.slice(0, 7);
    // Note: confirm enabling logic handled in footer based on uncommitted changes

    const handleConfirm = () => {
      modal.resolve({
        action: 'confirmed',
        performGitReset: worktreeResetOn,
        forceWhenDirty: forceReset,
      } as RestoreLogsDialogResult);
      modal.hide();
    };

    const handleCancel = () => {
      modal.resolve({ action: 'canceled' } as RestoreLogsDialogResult);
      modal.hide();
    };

    const handleOpenChange = (open: boolean) => {
      if (!open) {
        handleCancel();
      }
    };

    return (
      <Dialog open={modal.visible} onOpenChange={handleOpenChange}>
        <DialogContent
          className="max-h-[92vh] sm:max-h-[88vh] overflow-y-auto overflow-x-hidden"
          onKeyDownCapture={(e) => {
            if (e.key === 'Escape') {
              e.stopPropagation();
              handleCancel();
            }
          }}
        >
          <DialogHeader>
            <DialogTitle className="flex items-center gap-2 mb-3 md:mb-4">
              <AlertTriangle className="h-4 w-4 text-destructive" /> Confirm
              Retry
            </DialogTitle>
            <DialogDescription className="mt-6 break-words">
              <div className="space-y-3">
                {hasLater && (
                  <div className="flex items-start gap-3 rounded-md border border-destructive/30 bg-destructive/10 p-3">
                    <AlertTriangle className="h-4 w-4 text-destructive mt-0.5" />
                    <div className="text-sm min-w-0 w-full break-words">
                      <p className="font-medium text-destructive mb-2">
                        History change
                      </p>
                      <>
                        <p className="mt-0.5">
                          Will delete this process
                          {laterCount > 0 && (
                            <>
                              {' '}
                              and {laterCount} later process
                              {laterCount === 1 ? '' : 'es'}
                            </>
                          )}{' '}
                          from history.
                        </p>
                        <ul className="mt-1 text-xs text-muted-foreground list-disc pl-5">
                          {laterCoding > 0 && (
                            <li>
                              {laterCoding} coding agent run
                              {laterCoding === 1 ? '' : 's'}
                            </li>
                          )}
                          {laterSetup + laterCleanup > 0 && (
                            <li>
                              {laterSetup + laterCleanup} script process
                              {laterSetup + laterCleanup === 1 ? '' : 'es'}
                              {laterSetup > 0 && laterCleanup > 0 && (
                                <>
                                  {' '}
                                  ({laterSetup} setup, {laterCleanup} cleanup)
                                </>
                              )}
                            </li>
                          )}
                        </ul>
                      </>
                      <p className="mt-1 text-xs text-muted-foreground">
                        This permanently alters history and cannot be undone.
                      </p>
                    </div>
                  </div>
                )}

                {needGitReset && canGitReset && (
                  <div
                    className={
                      !worktreeResetOn
                        ? 'flex items-start gap-3 rounded-md border p-3'
                        : hasRisk
                          ? 'flex items-start gap-3 rounded-md border border-destructive/30 bg-destructive/10 p-3'
                          : 'flex items-start gap-3 rounded-md border p-3 border-amber-300/60 bg-amber-50/70 dark:border-amber-400/30 dark:bg-amber-900/20'
                    }
                  >
                    <AlertTriangle
                      className={
                        !worktreeResetOn
                          ? 'h-4 w-4 text-muted-foreground mt-0.5'
                          : hasRisk
                            ? 'h-4 w-4 text-destructive mt-0.5'
                            : 'h-4 w-4 text-amber-600 dark:text-amber-400 mt-0.5'
                      }
                    />
                    <div className="text-sm min-w-0 w-full break-words">
                      <p className="font-medium mb-2">Reset worktree</p>
                      <div
                        className="mt-2 w-full flex items-center cursor-pointer select-none"
                        role="switch"
                        aria-checked={worktreeResetOn}
                        onClick={() => setWorktreeResetOn((v) => !v)}
                      >
                        <div className="text-xs text-muted-foreground">
                          {worktreeResetOn ? 'Enabled' : 'Disabled'}
                        </div>
                        <div className="ml-auto relative inline-flex h-5 w-9 items-center rounded-full">
                          <span
                            className={
                              (worktreeResetOn
                                ? 'bg-emerald-500'
                                : 'bg-muted-foreground/30') +
                              ' absolute inset-0 rounded-full transition-colors'
                            }
                          />
                          <span
                            className={
                              (worktreeResetOn
                                ? 'translate-x-5'
                                : 'translate-x-1') +
                              ' pointer-events-none relative inline-block h-3.5 w-3.5 rounded-full bg-white shadow transition-transform'
                            }
                          />
                        </div>
                      </div>
                      {worktreeResetOn && (
                        <>
                          <p className="mt-2 text-xs text-muted-foreground">
                            Your worktree will be restored to this commit.
                          </p>
                          <div className="mt-1 flex items-center gap-2 min-w-0">
                            <GitCommit className="h-3.5 w-3.5 text-muted-foreground" />
                            {short && (
                              <span className="font-mono text-xs px-2 py-0.5 rounded bg-muted">
                                {short}
                              </span>
                            )}
                            {targetSubject && (
                              <span className="text-muted-foreground break-words whitespace-normal">
                                {targetSubject}
                              </span>
                            )}
                          </div>
                          {((isLinear &&
                            commitsToReset !== null &&
                            commitsToReset > 0) ||
                            uncommittedCount > 0 ||
                            untrackedCount > 0) && (
                            <ul className="mt-2 space-y-1 text-xs text-muted-foreground list-disc pl-5">
                              {isLinear &&
                                commitsToReset !== null &&
                                commitsToReset > 0 && (
                                  <li>
                                    Roll back {commitsToReset} commit
                                    {commitsToReset === 1 ? '' : 's'} from
                                    current HEAD.
                                  </li>
                                )}
                              {uncommittedCount > 0 && (
                                <li>
                                  Discard {uncommittedCount} uncommitted change
                                  {uncommittedCount === 1 ? '' : 's'}.
                                </li>
                              )}
                              {untrackedCount > 0 && (
                                <li>
                                  {untrackedCount} untracked file
                                  {untrackedCount === 1 ? '' : 's'} present (not
                                  affected by reset).
                                </li>
                              )}
                            </ul>
                          )}
                        </>
                      )}
                    </div>
                  </div>
                )}

                {needGitReset && !canGitReset && (
                  <div
                    className={
                      forceReset && worktreeResetOn
                        ? 'flex items-start gap-3 rounded-md border border-destructive/30 bg-destructive/10 p-3'
                        : 'flex items-start gap-3 rounded-md border p-3'
                    }
                  >
                    <AlertTriangle className="h-4 w-4 text-destructive mt-0.5" />
                    <div className="text-sm min-w-0 w-full break-words">
                      <p className="font-medium text-destructive">
                        Reset worktree
                      </p>
                      <div
                        className={`mt-2 w-full flex items-center select-none cursor-pointer`}
                        role="switch"
                        onClick={() => {
                          setWorktreeResetOn((on) => {
                            if (forceReset) return !on; // free toggle when forced
                            // Without force, only allow explicitly disabling reset
                            return false;
                          });
                        }}
                      >
                        <div className="text-xs text-muted-foreground">
                          {forceReset
                            ? worktreeResetOn
                              ? 'Enabled'
                              : 'Disabled'
                            : 'Disabled (uncommitted changes detected)'}
                        </div>
                        <div className="ml-auto relative inline-flex h-5 w-9 items-center rounded-full">
                          <span
                            className={
                              (worktreeResetOn && forceReset
                                ? 'bg-emerald-500'
                                : 'bg-muted-foreground/30') +
                              ' absolute inset-0 rounded-full transition-colors'
                            }
                          />
                          <span
                            className={
                              (worktreeResetOn && forceReset
                                ? 'translate-x-5'
                                : 'translate-x-1') +
                              ' pointer-events-none relative inline-block h-3.5 w-3.5 rounded-full bg-white shadow transition-transform'
                            }
                          />
                        </div>
                      </div>
                      <div
                        className="mt-2 w-full flex items-center cursor-pointer select-none"
                        role="switch"
                        onClick={() => {
                          setForceReset((v) => {
                            const next = !v;
                            if (next) setWorktreeResetOn(true);
                            return next;
                          });
                        }}
                      >
                        <div className="text-xs font-medium text-destructive">
                          Force reset (discard uncommitted changes)
                        </div>
                        <div className="ml-auto relative inline-flex h-5 w-9 items-center rounded-full">
                          <span
                            className={
                              (forceReset
                                ? 'bg-destructive'
                                : 'bg-muted-foreground/30') +
                              ' absolute inset-0 rounded-full transition-colors'
                            }
                          />
                          <span
                            className={
                              (forceReset ? 'translate-x-5' : 'translate-x-1') +
                              ' pointer-events-none relative inline-block h-3.5 w-3.5 rounded-full bg-white shadow transition-transform'
                            }
                          />
                        </div>
                      </div>
                      <p className="mt-2 text-xs text-muted-foreground">
                        {forceReset
                          ? 'Uncommitted changes will be discarded.'
                          : 'Uncommitted changes present. Turn on Force reset or commit/stash to proceed.'}
                      </p>
                      {short && (
                        <>
                          <p className="mt-2 text-xs text-muted-foreground">
                            Your worktree will be restored to this commit.
                          </p>
                          <div className="mt-1 flex items-center gap-2 min-w-0">
                            <GitCommit className="h-3.5 w-3.5 text-muted-foreground" />
                            <span className="font-mono text-xs px-2 py-0.5 rounded bg-muted">
                              {short}
                            </span>
                            {targetSubject && (
                              <span className="text-muted-foreground break-words whitespace-normal">
                                {targetSubject}
                              </span>
                            )}
                          </div>
                        </>
                      )}
                    </div>
                  </div>
                )}
              </div>
            </DialogDescription>
          </DialogHeader>
          <DialogFooter>
            <Button variant="outline" onClick={handleCancel}>
              Cancel
            </Button>
            <Button
              variant="destructive"
              disabled={
                // Disable when uncommitted changes present and user hasn't enabled force
                // or explicitly disabled worktree reset.
                (hasRisk && worktreeResetOn && needGitReset && !forceReset) ||
                false
              }
              onClick={handleConfirm}
            >
              Retry
            </Button>
          </DialogFooter>
        </DialogContent>
      </Dialog>
    );
  }
);
</file>

<file path="frontend/src/components/dialogs/tasks/TaskFormDialog.tsx">
import { useState, useEffect, useCallback } from 'react';
import { Globe2, Settings2, ChevronRight } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { ImageUploadSection } from '@/components/ui/ImageUploadSection';
import {
  Dialog,
  DialogContent,
  DialogHeader,
  DialogTitle,
} from '@/components/ui/dialog';
import { Input } from '@/components/ui/input';
import { FileSearchTextarea } from '@/components/ui/file-search-textarea';
import { Label } from '@/components/ui/label';
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from '@/components/ui/select';
import { templatesApi, imagesApi, projectsApi, attemptsApi } from '@/lib/api';
import { useTaskMutations } from '@/hooks/useTaskMutations';
import { useUserSystem } from '@/components/config-provider';
import { ExecutorProfileSelector } from '@/components/settings';
import BranchSelector from '@/components/tasks/BranchSelector';
import type {
  TaskStatus,
  TaskTemplate,
  ImageResponse,
  GitBranch,
  ExecutorProfileId,
} from 'shared/types';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

interface Task {
  id: string;
  project_id: string;
  title: string;
  description: string | null;
  status: TaskStatus;
  created_at: string;
  updated_at: string;
}

export interface TaskFormDialogProps {
  task?: Task | null; // Optional for create mode
  projectId?: string; // For file search functionality
  initialTemplate?: TaskTemplate | null; // For pre-filling from template
  initialTask?: Task | null; // For duplicating an existing task
  initialBaseBranch?: string; // For pre-selecting base branch in spinoff
  parentTaskAttemptId?: string; // For linking to parent task attempt
}

export const TaskFormDialog = NiceModal.create<TaskFormDialogProps>(
  ({
    task,
    projectId,
    initialTemplate,
    initialTask,
    initialBaseBranch,
    parentTaskAttemptId,
  }) => {
    const modal = useModal();
    const { createTask, createAndStart, updateTask } =
      useTaskMutations(projectId);
    const { system, profiles } = useUserSystem();
    const [title, setTitle] = useState('');
    const [description, setDescription] = useState('');
    const [status, setStatus] = useState<TaskStatus>('todo');
    const [isSubmitting, setIsSubmitting] = useState(false);
    const [isSubmittingAndStart, setIsSubmittingAndStart] = useState(false);
    const [templates, setTemplates] = useState<TaskTemplate[]>([]);
    const [selectedTemplate, setSelectedTemplate] = useState<string>('');
    const [showDiscardWarning, setShowDiscardWarning] = useState(false);
    const [images, setImages] = useState<ImageResponse[]>([]);
    const [newlyUploadedImageIds, setNewlyUploadedImageIds] = useState<
      string[]
    >([]);
    const [branches, setBranches] = useState<GitBranch[]>([]);
    const [selectedBranch, setSelectedBranch] = useState<string>('');
    const [selectedExecutorProfile, setSelectedExecutorProfile] =
      useState<ExecutorProfileId | null>(null);
    const [quickstartExpanded, setQuickstartExpanded] =
      useState<boolean>(false);

    const isEditMode = Boolean(task);

    // Check if there's any content that would be lost
    const hasUnsavedChanges = useCallback(() => {
      if (!isEditMode) {
        // Create mode - warn when there's content
        return title.trim() !== '' || description.trim() !== '';
      } else if (task) {
        // Edit mode - warn when current values differ from original task
        const titleChanged = title.trim() !== task.title.trim();
        const descriptionChanged =
          (description || '').trim() !== (task.description || '').trim();
        const statusChanged = status !== task.status;
        return titleChanged || descriptionChanged || statusChanged;
      }
      return false;
    }, [title, description, status, isEditMode, task]);

    // Warn on browser/tab close if there are unsaved changes
    useEffect(() => {
      if (!modal.visible) return; // dialog closed → nothing to do

      // always re-evaluate latest fields via hasUnsavedChanges()
      const handleBeforeUnload = (e: BeforeUnloadEvent) => {
        if (hasUnsavedChanges()) {
          e.preventDefault();
          // Chrome / Edge still require returnValue to be set
          e.returnValue = '';
          return '';
        }
        // nothing returned → no prompt
      };

      window.addEventListener('beforeunload', handleBeforeUnload);
      return () =>
        window.removeEventListener('beforeunload', handleBeforeUnload);
    }, [modal.visible, hasUnsavedChanges]); // hasUnsavedChanges is memoised with title/descr deps

    useEffect(() => {
      if (task) {
        // Edit mode - populate with existing task data
        setTitle(task.title);
        setDescription(task.description || '');
        setStatus(task.status);

        // Load existing images for the task
        if (modal.visible) {
          imagesApi
            .getTaskImages(task.id)
            .then((taskImages) => setImages(taskImages))
            .catch((err) => {
              console.error('Failed to load task images:', err);
              setImages([]);
            });
        }
      } else if (initialTask) {
        // Duplicate mode - pre-fill from existing task but reset status to 'todo' and no images
        setTitle(initialTask.title);
        setDescription(initialTask.description || '');
        setStatus('todo'); // Always start duplicated tasks as 'todo'
        setSelectedTemplate('');
        setImages([]);
        setNewlyUploadedImageIds([]);
      } else if (initialTemplate) {
        // Create mode with template - pre-fill from template
        setTitle(initialTemplate.title);
        setDescription(initialTemplate.description || '');
        setStatus('todo');
        setSelectedTemplate('');
      } else {
        // Create mode - reset to defaults
        setTitle('');
        setDescription('');
        setStatus('todo');
        setSelectedTemplate('');
        setImages([]);
        setNewlyUploadedImageIds([]);
        setSelectedBranch('');
        setSelectedExecutorProfile(system.config?.executor_profile || null);
        setQuickstartExpanded(false);
      }
    }, [
      task,
      initialTask,
      initialTemplate,
      modal.visible,
      system.config?.executor_profile,
    ]);

    // Fetch templates and branches when dialog opens in create mode
    useEffect(() => {
      if (modal.visible && !isEditMode && projectId) {
        // Fetch templates and branches
        Promise.all([
          templatesApi.listByProject(projectId),
          templatesApi.listGlobal(),
          projectsApi.getBranches(projectId),
        ])
          .then(([projectTemplates, globalTemplates, projectBranches]) => {
            // Combine templates with project templates first
            setTemplates([...projectTemplates, ...globalTemplates]);

            // Set branches and default to initialBaseBranch if provided, otherwise current branch
            setBranches(projectBranches);

            if (
              initialBaseBranch &&
              projectBranches.some((b) => b.name === initialBaseBranch)
            ) {
              // Use initialBaseBranch if it exists in the project branches (for spinoff)
              setSelectedBranch(initialBaseBranch);
            } else {
              // Default behavior: use current branch or first available
              const currentBranch = projectBranches.find((b) => b.is_current);
              const defaultBranch = currentBranch || projectBranches[0];
              if (defaultBranch) {
                setSelectedBranch(defaultBranch.name);
              }
            }
          })
          .catch(console.error);
      }
    }, [modal.visible, isEditMode, projectId, initialBaseBranch]);

    // Fetch parent base branch when parentTaskAttemptId is provided
    useEffect(() => {
      if (
        modal.visible &&
        !isEditMode &&
        parentTaskAttemptId &&
        !initialBaseBranch &&
        branches.length > 0
      ) {
        attemptsApi
          .get(parentTaskAttemptId)
          .then((attempt) => {
            const parentBranch = attempt.branch || attempt.base_branch;
            if (parentBranch && branches.some((b) => b.name === parentBranch)) {
              setSelectedBranch(parentBranch);
            }
          })
          .catch(() => {
            // Silently fail, will use current branch fallback
          });
      }
    }, [
      modal.visible,
      isEditMode,
      parentTaskAttemptId,
      initialBaseBranch,
      branches,
    ]);

    // Set default executor from config (following TaskDetailsToolbar pattern)
    useEffect(() => {
      if (system.config?.executor_profile) {
        setSelectedExecutorProfile(system.config.executor_profile);
      }
    }, [system.config?.executor_profile]);

    // Set default executor from config (following TaskDetailsToolbar pattern)
    useEffect(() => {
      if (system.config?.executor_profile) {
        setSelectedExecutorProfile(system.config.executor_profile);
      }
    }, [system.config?.executor_profile]);

    // Handle template selection
    const handleTemplateChange = (templateId: string) => {
      setSelectedTemplate(templateId);
      if (templateId === 'none') {
        // Clear the form when "No template" is selected
        setTitle('');
        setDescription('');
      } else if (templateId) {
        const template = templates.find((t) => t.id === templateId);
        if (template) {
          setTitle(template.title);
          setDescription(template.description || '');
        }
      }
    };

    // Handle image upload success by inserting markdown into description
    const handleImageUploaded = useCallback((image: ImageResponse) => {
      const markdownText = `![${image.original_name}](${image.file_path})`;
      setDescription((prev) => {
        if (prev.trim() === '') {
          return markdownText;
        } else {
          return prev + ' ' + markdownText;
        }
      });

      setImages((prev) => [...prev, image]);
      // Track as newly uploaded for backend association
      setNewlyUploadedImageIds((prev) => [...prev, image.id]);
    }, []);

    const handleImagesChange = useCallback((updatedImages: ImageResponse[]) => {
      setImages(updatedImages);
      // Also update newlyUploadedImageIds to remove any deleted image IDs
      setNewlyUploadedImageIds((prev) =>
        prev.filter((id) => updatedImages.some((img) => img.id === id))
      );
    }, []);

    const handleSubmit = useCallback(async () => {
      if (!title.trim() || !projectId) return;

      setIsSubmitting(true);
      try {
        let imageIds: string[] | undefined;

        if (isEditMode) {
          // In edit mode, send all current image IDs (existing + newly uploaded)
          imageIds =
            images.length > 0 ? images.map((img) => img.id) : undefined;
        } else {
          // In create mode, only send newly uploaded image IDs
          imageIds =
            newlyUploadedImageIds.length > 0
              ? newlyUploadedImageIds
              : undefined;
        }

        if (isEditMode && task) {
          updateTask.mutate(
            {
              taskId: task.id,
              data: {
                title,
                description: description || null,
                status,
                parent_task_attempt: parentTaskAttemptId || null,
                image_ids: imageIds || null,
              },
            },
            {
              onSuccess: () => {
                modal.hide();
              },
            }
          );
        } else {
          createTask.mutate(
            {
              project_id: projectId,
              title,
              description: description || null,
              parent_task_attempt: parentTaskAttemptId || null,
              image_ids: imageIds || null,
            },
            {
              onSuccess: () => {
                modal.hide();
              },
            }
          );
        }
      } finally {
        setIsSubmitting(false);
      }
    }, [
      title,
      description,
      status,
      isEditMode,
      projectId,
      task,
      modal,
      newlyUploadedImageIds,
      images,
      createTask,
      updateTask,
    ]);

    const handleCreateAndStart = useCallback(async () => {
      if (!title.trim() || !projectId) return;

      setIsSubmittingAndStart(true);
      try {
        if (!isEditMode) {
          const imageIds =
            newlyUploadedImageIds.length > 0
              ? newlyUploadedImageIds
              : undefined;

          // Use selected executor profile or fallback to config default
          const finalExecutorProfile =
            selectedExecutorProfile || system.config?.executor_profile;
          if (!finalExecutorProfile || !selectedBranch) {
            console.warn(
              `Missing ${!finalExecutorProfile ? 'executor profile' : 'branch'} for Create & Start`
            );
            return;
          }

          createAndStart.mutate(
            {
              task: {
                project_id: projectId,
                title,
                description: description || null,
                parent_task_attempt: parentTaskAttemptId || null,
                image_ids: imageIds || null,
              },
              executor_profile_id: finalExecutorProfile,
              base_branch: selectedBranch,
            },
            {
              onSuccess: () => {
                modal.hide();
              },
            }
          );
        }
      } finally {
        setIsSubmittingAndStart(false);
      }
    }, [
      title,
      description,
      isEditMode,
      projectId,
      modal,
      newlyUploadedImageIds,
      createAndStart,
      selectedExecutorProfile,
      selectedBranch,
      system.config?.executor_profile,
    ]);

    const handleCancel = useCallback(() => {
      // Check for unsaved changes before closing
      if (hasUnsavedChanges()) {
        setShowDiscardWarning(true);
      } else {
        modal.hide();
      }
    }, [modal, hasUnsavedChanges]);

    const handleDiscardChanges = useCallback(() => {
      // Close both dialogs
      setShowDiscardWarning(false);
      modal.hide();
    }, [modal]);

    // Handle keyboard shortcuts
    useEffect(() => {
      const handleKeyDown = (event: KeyboardEvent) => {
        // ESC to close dialog (prevent it from reaching TaskDetailsPanel)
        if (event.key === 'Escape') {
          event.preventDefault();
          event.stopPropagation();
          handleCancel();
          return;
        }

        // Command/Ctrl + Enter to Create & Start (create mode) or Save (edit mode)
        if ((event.metaKey || event.ctrlKey) && event.key === 'Enter') {
          if (
            !isEditMode &&
            title.trim() &&
            !isSubmitting &&
            !isSubmittingAndStart
          ) {
            event.preventDefault();
            handleCreateAndStart();
          } else if (
            isEditMode &&
            title.trim() &&
            !isSubmitting &&
            !isSubmittingAndStart
          ) {
            event.preventDefault();
            handleSubmit();
          }
        }
      };

      if (modal.visible) {
        document.addEventListener('keydown', handleKeyDown, true); // Use capture phase to get priority
        return () =>
          document.removeEventListener('keydown', handleKeyDown, true);
      }
    }, [
      modal.visible,
      isEditMode,
      title,
      handleSubmit,
      isSubmitting,
      isSubmittingAndStart,
      handleCreateAndStart,
      handleCancel,
    ]);

    // Handle dialog close attempt
    const handleDialogOpenChange = (open: boolean) => {
      if (!open && hasUnsavedChanges()) {
        // Trying to close with unsaved changes
        setShowDiscardWarning(true);
      } else if (!open) {
        modal.hide();
      }
    };

    return (
      <>
        <Dialog open={modal.visible} onOpenChange={handleDialogOpenChange}>
          <DialogContent className="sm:max-w-[550px]">
            <DialogHeader>
              <DialogTitle>
                {isEditMode ? 'Edit Task' : 'Create New Task'}
              </DialogTitle>
            </DialogHeader>
            <div className="space-y-4">
              <div>
                <Label htmlFor="task-title" className="text-sm font-medium">
                  Title
                </Label>
                <Input
                  id="task-title"
                  value={title}
                  onChange={(e) => setTitle(e.target.value)}
                  placeholder="What needs to be done?"
                  className="mt-1.5"
                  disabled={isSubmitting || isSubmittingAndStart}
                  autoFocus
                />
              </div>

              <div>
                <Label
                  htmlFor="task-description"
                  className="text-sm font-medium"
                >
                  Description
                </Label>
                <FileSearchTextarea
                  value={description}
                  onChange={setDescription}
                  rows={3}
                  maxRows={8}
                  placeholder="Add more details (optional). Type @ to search files."
                  className="mt-1.5"
                  disabled={isSubmitting || isSubmittingAndStart}
                  projectId={projectId}
                />
              </div>

              <ImageUploadSection
                images={images}
                onImagesChange={handleImagesChange}
                onUpload={imagesApi.upload}
                onDelete={imagesApi.delete}
                onImageUploaded={handleImageUploaded}
                disabled={isSubmitting || isSubmittingAndStart}
                readOnly={isEditMode}
                collapsible={true}
                defaultExpanded={false}
              />

              {!isEditMode && templates.length > 0 && (
                <div className="pt-2">
                  <details className="group">
                    <summary className="cursor-pointer text-sm text-muted-foreground hover:text-foreground transition-colors list-none flex items-center gap-2">
                      <svg
                        className="h-3 w-3 transition-transform group-open:rotate-90"
                        viewBox="0 0 20 20"
                        fill="currentColor"
                      >
                        <path
                          fillRule="evenodd"
                          d="M7.293 14.707a1 1 0 010-1.414L10.586 10 7.293 6.707a1 1 0 011.414-1.414l4 4a1 1 0 010 1.414l-4 4a1 1 0 01-1.414 0z"
                          clipRule="evenodd"
                        />
                      </svg>
                      Use a template
                    </summary>
                    <div className="mt-3 space-y-2">
                      <p className="text-xs text-muted-foreground">
                        Templates help you quickly create tasks with predefined
                        content.
                      </p>
                      <Select
                        value={selectedTemplate}
                        onValueChange={handleTemplateChange}
                      >
                        <SelectTrigger id="task-template" className="w-full">
                          <SelectValue placeholder="Choose a template to prefill this form" />
                        </SelectTrigger>
                        <SelectContent>
                          <SelectItem value="none">No template</SelectItem>
                          {templates.map((template) => (
                            <SelectItem key={template.id} value={template.id}>
                              <div className="flex items-center gap-2">
                                {template.project_id === null && (
                                  <Globe2 className="h-3 w-3 text-muted-foreground" />
                                )}
                                <span>{template.template_name}</span>
                              </div>
                            </SelectItem>
                          ))}
                        </SelectContent>
                      </Select>
                    </div>
                  </details>
                </div>
              )}

              {isEditMode && (
                <div className="pt-2">
                  <Label htmlFor="task-status" className="text-sm font-medium">
                    Status
                  </Label>
                  <Select
                    value={status}
                    onValueChange={(value) => setStatus(value as TaskStatus)}
                    disabled={isSubmitting || isSubmittingAndStart}
                  >
                    <SelectTrigger className="mt-1.5">
                      <SelectValue />
                    </SelectTrigger>
                    <SelectContent>
                      <SelectItem value="todo">To Do</SelectItem>
                      <SelectItem value="inprogress">In Progress</SelectItem>
                      <SelectItem value="inreview">In Review</SelectItem>
                      <SelectItem value="done">Done</SelectItem>
                      <SelectItem value="cancelled">Cancelled</SelectItem>
                    </SelectContent>
                  </Select>
                </div>
              )}

              {!isEditMode &&
                (() => {
                  const quickstartSection = (
                    <div className="pt-2">
                      <details
                        className="group"
                        open={quickstartExpanded}
                        onToggle={(e) =>
                          setQuickstartExpanded(
                            (e.target as HTMLDetailsElement).open
                          )
                        }
                      >
                        <summary className="cursor-pointer text-sm text-muted-foreground hover:text-foreground transition-colors list-none flex items-center gap-2">
                          <ChevronRight className="h-3 w-3 transition-transform group-open:rotate-90" />
                          <Settings2 className="h-3 w-3" />
                          Quickstart
                        </summary>
                        <div className="mt-3 space-y-3">
                          <p className="text-xs text-muted-foreground">
                            Configuration for "Create & Start" workflow
                          </p>

                          {/* Executor Profile Selector */}
                          {profiles && selectedExecutorProfile && (
                            <ExecutorProfileSelector
                              profiles={profiles}
                              selectedProfile={selectedExecutorProfile}
                              onProfileSelect={setSelectedExecutorProfile}
                              disabled={isSubmitting || isSubmittingAndStart}
                            />
                          )}

                          {/* Branch Selector */}
                          {branches.length > 0 && (
                            <div>
                              <Label
                                htmlFor="base-branch"
                                className="text-sm font-medium"
                              >
                                Branch
                              </Label>
                              <div className="mt-1.5">
                                <BranchSelector
                                  branches={branches}
                                  selectedBranch={selectedBranch}
                                  onBranchSelect={setSelectedBranch}
                                  placeholder="Select branch"
                                  className={
                                    isSubmitting || isSubmittingAndStart
                                      ? 'opacity-50 cursor-not-allowed'
                                      : ''
                                  }
                                />
                              </div>
                            </div>
                          )}
                        </div>
                      </details>
                    </div>
                  );
                  return quickstartSection;
                })()}

              <div className="flex flex-col-reverse sm:flex-row sm:justify-end gap-2 pt-2">
                <Button
                  variant="outline"
                  onClick={handleCancel}
                  disabled={isSubmitting || isSubmittingAndStart}
                >
                  Cancel
                </Button>
                {isEditMode ? (
                  <Button
                    onClick={handleSubmit}
                    disabled={isSubmitting || !title.trim()}
                  >
                    {isSubmitting ? 'Updating...' : 'Update Task'}
                  </Button>
                ) : (
                  <>
                    <Button
                      variant="outline"
                      onClick={handleSubmit}
                      disabled={
                        isSubmitting || isSubmittingAndStart || !title.trim()
                      }
                    >
                      {isSubmitting ? 'Creating...' : 'Create Task'}
                    </Button>
                    <Button
                      onClick={handleCreateAndStart}
                      disabled={
                        isSubmitting || isSubmittingAndStart || !title.trim()
                      }
                      className={'font-medium'}
                    >
                      {isSubmittingAndStart
                        ? 'Creating & Starting...'
                        : 'Create & Start'}
                    </Button>
                  </>
                )}
              </div>
            </div>
          </DialogContent>
        </Dialog>

        {/* Discard Warning Dialog */}
        <Dialog open={showDiscardWarning} onOpenChange={setShowDiscardWarning}>
          <DialogContent className="sm:max-w-[425px]">
            <DialogHeader>
              <DialogTitle>Discard unsaved changes?</DialogTitle>
            </DialogHeader>
            <div className="py-4">
              <p className="text-sm text-muted-foreground">
                You have unsaved changes. Are you sure you want to discard them?
              </p>
            </div>
            <div className="flex justify-end gap-2">
              <Button
                variant="outline"
                onClick={() => setShowDiscardWarning(false)}
              >
                Continue Editing
              </Button>
              <Button variant="destructive" onClick={handleDiscardChanges}>
                Discard Changes
              </Button>
            </div>
          </DialogContent>
        </Dialog>
      </>
    );
  }
);
</file>

<file path="frontend/src/components/dialogs/tasks/TaskTemplateEditDialog.tsx">
import { useState, useEffect } from 'react';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Label } from '@/components/ui/label';
import { Textarea } from '@/components/ui/textarea';
import {
  Dialog,
  DialogContent,
  DialogHeader,
  DialogTitle,
  DialogFooter,
} from '@/components/ui/dialog';
import { Loader2 } from 'lucide-react';
import { templatesApi } from '@/lib/api';
import type {
  TaskTemplate,
  CreateTaskTemplate,
  UpdateTaskTemplate,
} from 'shared/types';
import NiceModal, { useModal } from '@ebay/nice-modal-react';

export interface TaskTemplateEditDialogProps {
  template?: TaskTemplate | null; // null for create mode
  projectId?: string;
  isGlobal?: boolean;
}

export type TaskTemplateEditResult = 'saved' | 'canceled';

export const TaskTemplateEditDialog =
  NiceModal.create<TaskTemplateEditDialogProps>(
    ({ template, projectId, isGlobal = false }) => {
      const modal = useModal();
      const [formData, setFormData] = useState({
        template_name: '',
        title: '',
        description: '',
      });
      const [saving, setSaving] = useState(false);
      const [error, setError] = useState<string | null>(null);

      const isEditMode = Boolean(template);

      useEffect(() => {
        if (template) {
          setFormData({
            template_name: template.template_name,
            title: template.title,
            description: template.description || '',
          });
        } else {
          setFormData({
            template_name: '',
            title: '',
            description: '',
          });
        }
        setError(null);
      }, [template]);

      // Handle keyboard shortcuts
      useEffect(() => {
        const handleKeyDown = (event: KeyboardEvent) => {
          // Command/Ctrl + Enter to save template
          if ((event.metaKey || event.ctrlKey) && event.key === 'Enter') {
            if (modal.visible && !saving) {
              event.preventDefault();
              handleSave();
            }
          }
        };

        if (modal.visible) {
          document.addEventListener('keydown', handleKeyDown, true);
          return () =>
            document.removeEventListener('keydown', handleKeyDown, true);
        }
      }, [modal.visible, saving]);

      const handleSave = async () => {
        if (!formData.template_name.trim() || !formData.title.trim()) {
          setError('Template name and title are required');
          return;
        }

        setSaving(true);
        setError(null);

        try {
          if (isEditMode && template) {
            const updateData: UpdateTaskTemplate = {
              template_name: formData.template_name,
              title: formData.title,
              description: formData.description || null,
            };
            await templatesApi.update(template.id, updateData);
          } else {
            const createData: CreateTaskTemplate = {
              project_id: isGlobal ? null : projectId || null,
              template_name: formData.template_name,
              title: formData.title,
              description: formData.description || null,
            };
            await templatesApi.create(createData);
          }

          modal.resolve('saved' as TaskTemplateEditResult);
          modal.hide();
        } catch (err: any) {
          setError(err.message || 'Failed to save template');
        } finally {
          setSaving(false);
        }
      };

      const handleCancel = () => {
        modal.resolve('canceled' as TaskTemplateEditResult);
        modal.hide();
      };

      const handleOpenChange = (open: boolean) => {
        if (!open) {
          handleCancel();
        }
      };

      return (
        <Dialog open={modal.visible} onOpenChange={handleOpenChange}>
          <DialogContent className="sm:max-w-[500px]">
            <DialogHeader>
              <DialogTitle>
                {isEditMode ? 'Edit Template' : 'Create Template'}
              </DialogTitle>
            </DialogHeader>
            <div className="space-y-4 py-4">
              <div>
                <Label htmlFor="template-name">Template Name</Label>
                <Input
                  id="template-name"
                  value={formData.template_name}
                  onChange={(e) =>
                    setFormData({ ...formData, template_name: e.target.value })
                  }
                  placeholder="e.g., Bug Fix, Feature Request"
                  disabled={saving}
                  autoFocus
                />
              </div>
              <div>
                <Label htmlFor="template-title">Default Title</Label>
                <Input
                  id="template-title"
                  value={formData.title}
                  onChange={(e) =>
                    setFormData({ ...formData, title: e.target.value })
                  }
                  placeholder="e.g., Fix bug in..."
                  disabled={saving}
                />
              </div>
              <div>
                <Label htmlFor="template-description">
                  Default Description
                </Label>
                <Textarea
                  id="template-description"
                  value={formData.description}
                  onChange={(e) =>
                    setFormData({ ...formData, description: e.target.value })
                  }
                  placeholder="Enter a default description for tasks created with this template"
                  rows={4}
                  disabled={saving}
                />
              </div>
              {error && <div className="text-sm text-destructive">{error}</div>}
            </div>
            <DialogFooter>
              <Button
                variant="outline"
                onClick={handleCancel}
                disabled={saving}
              >
                Cancel
              </Button>
              <Button onClick={handleSave} disabled={saving}>
                {saving && <Loader2 className="mr-2 h-4 w-4 animate-spin" />}
                {isEditMode ? 'Update' : 'Create'}
              </Button>
            </DialogFooter>
          </DialogContent>
        </Dialog>
      );
    }
  );
</file>

<file path="frontend/src/components/dialogs/index.ts">
// Global app dialogs
export { DisclaimerDialog } from './global/DisclaimerDialog';
export { OnboardingDialog } from './global/OnboardingDialog';
export { PrivacyOptInDialog } from './global/PrivacyOptInDialog';
export { ReleaseNotesDialog } from './global/ReleaseNotesDialog';

// Authentication dialogs
export { GitHubLoginDialog } from './auth/GitHubLoginDialog';
export {
  ProvidePatDialog,
  type ProvidePatDialogProps,
} from './auth/ProvidePatDialog';

// Project-related dialogs
export {
  ProjectFormDialog,
  type ProjectFormDialogProps,
  type ProjectFormDialogResult,
} from './projects/ProjectFormDialog';
export {
  ProjectEditorSelectionDialog,
  type ProjectEditorSelectionDialogProps,
} from './projects/ProjectEditorSelectionDialog';

// Task-related dialogs
export {
  TaskFormDialog,
  type TaskFormDialogProps,
} from './tasks/TaskFormDialog';

export { CreatePRDialog } from './tasks/CreatePRDialog';
export {
  EditorSelectionDialog,
  type EditorSelectionDialogProps,
} from './tasks/EditorSelectionDialog';
export {
  DeleteTaskConfirmationDialog,
  type DeleteTaskConfirmationDialogProps,
} from './tasks/DeleteTaskConfirmationDialog';
export {
  TaskTemplateEditDialog,
  type TaskTemplateEditDialogProps,
  type TaskTemplateEditResult,
} from './tasks/TaskTemplateEditDialog';
export {
  RebaseDialog,
  type RebaseDialogProps,
  type RebaseDialogResult,
} from './tasks/RebaseDialog';
export {
  RestoreLogsDialog,
  type RestoreLogsDialogProps,
  type RestoreLogsDialogResult,
} from './tasks/RestoreLogsDialog';

// Settings dialogs
export {
  CreateConfigurationDialog,
  type CreateConfigurationDialogProps,
  type CreateConfigurationResult,
} from './settings/CreateConfigurationDialog';
export {
  DeleteConfigurationDialog,
  type DeleteConfigurationDialogProps,
  type DeleteConfigurationResult,
} from './settings/DeleteConfigurationDialog';

// Shared/Generic dialogs
export { ConfirmDialog, type ConfirmDialogProps } from './shared/ConfirmDialog';
export {
  FolderPickerDialog,
  type FolderPickerDialogProps,
} from './shared/FolderPickerDialog';
</file>

<file path="frontend/src/components/diff/CommentWidgetLine.tsx">
import React, { useState, useRef, useEffect } from 'react';
import { Button } from '@/components/ui/button';
import { FileSearchTextarea } from '@/components/ui/file-search-textarea';
import { useReview, type ReviewDraft } from '@/contexts/ReviewProvider';

interface CommentWidgetLineProps {
  draft: ReviewDraft;
  widgetKey: string;
  onSave: () => void;
  onCancel: () => void;
  projectId?: string;
}

export function CommentWidgetLine({
  draft,
  widgetKey,
  onSave,
  onCancel,
  projectId,
}: CommentWidgetLineProps) {
  const { setDraft, addComment } = useReview();
  const [value, setValue] = useState(draft.text);
  const textareaRef = useRef<HTMLTextAreaElement>(null);

  useEffect(() => {
    textareaRef.current?.focus();
  }, []);

  const handleSave = () => {
    if (value.trim()) {
      addComment({
        filePath: draft.filePath,
        side: draft.side,
        lineNumber: draft.lineNumber,
        text: value.trim(),
        codeLine: draft.codeLine,
      });
    }
    setDraft(widgetKey, null);
    onSave();
  };

  const handleCancel = () => {
    setDraft(widgetKey, null);
    onCancel();
  };

  const handleKeyDown = (e: React.KeyboardEvent) => {
    if (e.key === 'Escape') {
      handleCancel();
    } else if (e.key === 'Enter' && (e.ctrlKey || e.metaKey)) {
      handleSave();
    }
  };

  return (
    <div className="p-4 border-y">
      <FileSearchTextarea
        value={value}
        onChange={setValue}
        onKeyDown={handleKeyDown}
        placeholder="Add a comment... (type @ to search files)"
        rows={3}
        maxRows={10}
        className="w-full bg-primary text-primary-foreground text-sm font-mono resize-none min-h-[60px] focus:outline-none focus:ring-1 focus:ring-primary"
        projectId={projectId}
      />
      <div className="mt-2 flex gap-2">
        <Button size="xs" onClick={handleSave} disabled={!value.trim()}>
          Add review comment
        </Button>
        <Button
          size="xs"
          variant="ghost"
          onClick={handleCancel}
          className="text-secondary-foreground"
        >
          Cancel
        </Button>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/diff/ReviewCommentRenderer.tsx">
import React, { useState, useRef, useEffect } from 'react';
import { Trash2, Pencil } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { FileSearchTextarea } from '@/components/ui/file-search-textarea';
import { useReview, type ReviewComment } from '@/contexts/ReviewProvider';

interface ReviewCommentRendererProps {
  comment: ReviewComment;
  projectId?: string;
}

export function ReviewCommentRenderer({
  comment,
  projectId,
}: ReviewCommentRendererProps) {
  const { deleteComment, updateComment } = useReview();
  const [isEditing, setIsEditing] = useState(false);
  const [editText, setEditText] = useState(comment.text);
  const textareaRef = useRef<HTMLTextAreaElement>(null);

  useEffect(() => {
    if (isEditing) {
      textareaRef.current?.focus();
    }
  }, [isEditing]);

  const handleDelete = () => {
    deleteComment(comment.id);
  };

  const handleEdit = () => {
    setEditText(comment.text);
    setIsEditing(true);
  };

  const handleSave = () => {
    if (editText.trim()) {
      updateComment(comment.id, editText.trim());
    }
    setIsEditing(false);
  };

  const handleCancel = () => {
    setEditText(comment.text);
    setIsEditing(false);
  };

  const handleKeyDown = (e: React.KeyboardEvent) => {
    if (e.key === 'Escape') {
      handleCancel();
    } else if (e.key === 'Enter' && (e.ctrlKey || e.metaKey)) {
      handleSave();
    }
  };

  if (isEditing) {
    return (
      <div className="border-y bg-background p-4">
        <FileSearchTextarea
          value={editText}
          onChange={setEditText}
          onKeyDown={handleKeyDown}
          placeholder="Edit comment... (type @ to search files)"
          rows={3}
          maxRows={10}
          className="w-full bg-background text-foreground text-sm font-mono resize-none min-h-[60px] focus:outline-none"
          projectId={projectId}
        />
        <div className="mt-2 flex gap-2">
          <Button size="xs" onClick={handleSave} disabled={!editText.trim()}>
            Save changes
          </Button>
          <Button
            size="xs"
            variant="ghost"
            onClick={handleCancel}
            className="text-secondary-foreground"
          >
            Cancel
          </Button>
        </div>
      </div>
    );
  }

  return (
    <div className="border-y bg-background p-4 flex gap-2 items-center">
      <div className="flex-1 text-sm whitespace-pre-wrap text-foreground">
        {comment.text}
      </div>
      <div className="flex gap-1">
        <Button
          variant="ghost"
          size="xs"
          onClick={handleEdit}
          title="Edit comment"
          className="h-auto"
        >
          <Pencil className="h-3 w-3" />
        </Button>
        <Button
          variant="ghost"
          size="xs"
          onClick={handleDelete}
          title="Delete comment"
          className="h-auto"
        >
          <Trash2 className="h-3 w-4" />
        </Button>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/layout/navbar.tsx">
import { Link, useLocation } from 'react-router-dom';
import { Button } from '@/components/ui/button';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuSeparator,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import {
  FolderOpen,
  Settings,
  BookOpen,
  MessageCircleQuestion,
  Menu,
  Plus,
} from 'lucide-react';
import { Logo } from '@/components/logo';
import { SearchBar } from '@/components/search-bar';
import { useSearch } from '@/contexts/search-context';
import { openTaskForm } from '@/lib/openTaskForm';
import { useProject } from '@/contexts/project-context';
import { showProjectForm } from '@/lib/modals';
import { useOpenProjectInEditor } from '@/hooks/useOpenProjectInEditor';

const INTERNAL_NAV = [
  { label: 'Projects', icon: FolderOpen, to: '/projects' },
  { label: 'Settings', icon: Settings, to: '/settings' },
];

const EXTERNAL_LINKS = [
  {
    label: 'Docs',
    icon: BookOpen,
    href: 'https://vibekanban.com/docs',
  },
  {
    label: 'Support',
    icon: MessageCircleQuestion,
    href: 'https://github.com/BloopAI/vibe-kanban/issues',
  },
];

export function Navbar() {
  const location = useLocation();
  const { projectId, project } = useProject();
  const { query, setQuery, active, clear } = useSearch();
  const handleOpenInEditor = useOpenProjectInEditor(project || null);

  const handleCreateTask = () => {
    if (projectId) {
      openTaskForm({ projectId });
    }
  };

  const handleOpenInIDE = () => {
    handleOpenInEditor();
  };

  const handleProjectSettings = async () => {
    try {
      await showProjectForm({ project });
      // Settings saved successfully - no additional action needed
    } catch (error) {
      // User cancelled - do nothing
    }
  };

  return (
    <div className="border-b bg-background">
      <div className="w-full px-3">
        <div className="flex items-center h-12 py-2">
          <div className="flex-1">
            <Link to="/projects">
              <Logo />
            </Link>
          </div>

          <SearchBar
            className="hidden sm:flex"
            value={query}
            onChange={setQuery}
            disabled={!active}
            onClear={clear}
            project={project || null}
          />

          <div className="flex-1 flex justify-end">
            {projectId && (
              <>
                <Button
                  variant="ghost"
                  size="icon"
                  onClick={handleOpenInIDE}
                  aria-label="Open project in IDE"
                >
                  <FolderOpen className="h-4 w-4" />
                </Button>
                <Button
                  variant="ghost"
                  size="icon"
                  onClick={handleProjectSettings}
                  aria-label="Project settings"
                >
                  <Settings className="h-4 w-4" />
                </Button>
                <Button
                  variant="ghost"
                  size="icon"
                  onClick={handleCreateTask}
                  aria-label="Create new task"
                >
                  <Plus className="h-4 w-4" />
                </Button>
              </>
            )}
            <DropdownMenu>
              <DropdownMenuTrigger asChild>
                <Button
                  variant="ghost"
                  size="icon"
                  aria-label="Main navigation"
                >
                  <Menu className="h-4 w-4" />
                </Button>
              </DropdownMenuTrigger>

              <DropdownMenuContent align="end">
                {INTERNAL_NAV.map((item) => {
                  const active = location.pathname.startsWith(item.to);
                  const Icon = item.icon;
                  return (
                    <DropdownMenuItem
                      key={item.to}
                      asChild
                      className={active ? 'bg-accent' : ''}
                    >
                      <Link to={item.to}>
                        <Icon className="mr-2 h-4 w-4" />
                        {item.label}
                      </Link>
                    </DropdownMenuItem>
                  );
                })}

                <DropdownMenuSeparator />

                {EXTERNAL_LINKS.map((item) => {
                  const Icon = item.icon;
                  return (
                    <DropdownMenuItem key={item.href} asChild>
                      <a
                        href={item.href}
                        target="_blank"
                        rel="noopener noreferrer"
                      >
                        <Icon className="mr-2 h-4 w-4" />
                        {item.label}
                      </a>
                    </DropdownMenuItem>
                  );
                })}
              </DropdownMenuContent>
            </DropdownMenu>
          </div>
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/logs/VirtualizedList.tsx">
import {
  DataWithScrollModifier,
  ScrollModifier,
  VirtuosoMessageList,
  VirtuosoMessageListLicense,
  VirtuosoMessageListMethods,
  VirtuosoMessageListProps,
} from '@virtuoso.dev/message-list';
import { useEffect, useRef, useState } from 'react';
import DisplayConversationEntry from '../NormalizedConversation/DisplayConversationEntry';
import { useEntries } from '@/contexts/EntriesContext';
import {
  AddEntryType,
  PatchTypeWithKey,
  useConversationHistory,
} from '@/hooks/useConversationHistory';
import { TaskAttempt } from 'shared/types';
import { Loader2 } from 'lucide-react';

interface VirtualizedListProps {
  attempt: TaskAttempt;
}

interface MessageListContext {
  attempt: TaskAttempt;
}

type ChannelData = DataWithScrollModifier<PatchTypeWithKey> | null;

const InitialDataScrollModifier: ScrollModifier = {
  type: 'item-location',
  location: {
    index: 'LAST',
    align: 'end',
  },
  purgeItemSizes: true,
};

const AutoScrollToBottom: ScrollModifier = {
  type: 'auto-scroll-to-bottom',
  autoScroll: ({ atBottom, scrollInProgress }) => {
    if (atBottom || scrollInProgress) {
      return 'smooth';
    }
    return false;
  },
};

const ItemContent: VirtuosoMessageListProps<
  PatchTypeWithKey,
  MessageListContext
>['ItemContent'] = ({ data, context }) => {
  if (data.type === 'STDOUT') {
    return <p>{data.content}</p>;
  } else if (data.type === 'STDERR') {
    return <p>{data.content}</p>;
  } else if (data.type === 'NORMALIZED_ENTRY') {
    return (
      <DisplayConversationEntry
        key={data.patchKey}
        expansionKey={data.patchKey}
        entry={data.content}
        executionProcessId={data.executionProcessId}
        taskAttempt={context.attempt}
      />
    );
  }
};

const VirtualizedList = ({ attempt }: VirtualizedListProps) => {
  const [channelData, setChannelData] = useState<ChannelData>(null);
  const [loading, setLoading] = useState(true);
  const { setEntries, reset } = useEntries();

  // When attempt changes, set loading and reset entries
  useEffect(() => {
    setLoading(true);
    reset();
  }, [attempt.id, reset]);

  const onEntriesUpdated = (
    newEntries: PatchTypeWithKey[],
    addType: AddEntryType,
    newLoading: boolean
  ) => {
    // initial defaults to scrolling to the latest
    let scrollModifier: ScrollModifier = InitialDataScrollModifier;

    if (addType === 'running' && !loading) {
      scrollModifier = AutoScrollToBottom;
    }

    setChannelData({ data: newEntries, scrollModifier });
    setEntries(newEntries); // Update shared context
    if (loading) {
      setLoading(newLoading);
    }
  };
  useConversationHistory({ attempt, onEntriesUpdated });

  const messageListRef = useRef<VirtuosoMessageListMethods | null>(null);

  return (
    <>
      <VirtuosoMessageListLicense
        licenseKey={import.meta.env.VITE_PUBLIC_REACT_VIRTUOSO_LICENSE_KEY}
      >
        <VirtuosoMessageList<PatchTypeWithKey, MessageListContext>
          ref={messageListRef}
          className="flex-1"
          data={channelData}
          context={{ attempt }}
          itemIdentity={(item) => item.patchKey}
          computeItemKey={({ data }) => data.patchKey}
          ItemContent={ItemContent}
          Header={() => <div className="h-2"></div>} // Padding
          Footer={() => <div className="h-2"></div>} // Padding
        />
      </VirtuosoMessageListLicense>
      {loading && (
        <div className="float-left top-0 left-0 w-full h-full bg-primary flex flex-col gap-2 justify-center items-center">
          <Loader2 className="h-8 w-8 animate-spin" />
          <p>Loading History</p>
        </div>
      )}
    </>
  );
};

export default VirtualizedList;
</file>

<file path="frontend/src/components/NormalizedConversation/DisplayConversationEntry.tsx">
import MarkdownRenderer from '@/components/ui/markdown-renderer.tsx';
import {
  ActionType,
  NormalizedEntry,
  TaskAttempt,
  type NormalizedEntryType,
} from 'shared/types.ts';
import type { ProcessStartPayload } from '@/types/logs';
import FileChangeRenderer from './FileChangeRenderer';
import { renderJson } from './ToolDetails';
import { useExpandable } from '@/stores/useExpandableStore';
import {
  AlertCircle,
  Bot,
  Brain,
  CheckSquare,
  ChevronDown,
  Hammer,
  Edit,
  Eye,
  Globe,
  Plus,
  Search,
  Settings,
  Terminal,
  User,
} from 'lucide-react';
import RawLogText from '../common/RawLogText';
import UserMessage from './UserMessage';

type Props = {
  entry: NormalizedEntry | ProcessStartPayload;
  expansionKey: string;
  diffDeletable?: boolean;
  executionProcessId?: string;
  taskAttempt?: TaskAttempt;
};

type FileEditAction = Extract<ActionType, { action: 'file_edit' }>;

const getEntryIcon = (entryType: NormalizedEntryType) => {
  const iconSize = 'h-3 w-3';
  if (entryType.type === 'user_message') {
    return <User className={iconSize} />;
  }
  if (entryType.type === 'assistant_message') {
    return <Bot className={iconSize} />;
  }
  if (entryType.type === 'system_message') {
    return <Settings className={iconSize} />;
  }
  if (entryType.type === 'thinking') {
    return <Brain className={iconSize} />;
  }
  if (entryType.type === 'error_message') {
    return <AlertCircle className={iconSize} />;
  }
  if (entryType.type === 'tool_use') {
    const { action_type, tool_name } = entryType;

    // Special handling for TODO tools
    if (
      action_type.action === 'todo_management' ||
      (tool_name &&
        (tool_name.toLowerCase() === 'todowrite' ||
          tool_name.toLowerCase() === 'todoread' ||
          tool_name.toLowerCase() === 'todo_write' ||
          tool_name.toLowerCase() === 'todo_read' ||
          tool_name.toLowerCase() === 'todo'))
    ) {
      return <CheckSquare className={iconSize} />;
    }

    if (action_type.action === 'file_read') {
      return <Eye className={iconSize} />;
    } else if (action_type.action === 'file_edit') {
      return <Edit className={iconSize} />;
    } else if (action_type.action === 'command_run') {
      return <Terminal className={iconSize} />;
    } else if (action_type.action === 'search') {
      return <Search className={iconSize} />;
    } else if (action_type.action === 'web_fetch') {
      return <Globe className={iconSize} />;
    } else if (action_type.action === 'task_create') {
      return <Plus className={iconSize} />;
    } else if (action_type.action === 'plan_presentation') {
      return <CheckSquare className={iconSize} />;
    } else if (action_type.action === 'tool') {
      return <Hammer className={iconSize} />;
    }
    return <Settings className={iconSize} />;
  }
  return <Settings className={iconSize} />;
};

type ExitStatusVisualisation = 'success' | 'error' | 'pending';

const getStatusIndicator = (entryType: NormalizedEntryType) => {
  let status_visualisation: ExitStatusVisualisation | null = null;
  if (
    entryType.type === 'tool_use' &&
    entryType.action_type.action === 'command_run'
  ) {
    status_visualisation = 'pending';
    if (entryType.action_type.result?.exit_status?.type === 'success') {
      if (entryType.action_type.result?.exit_status?.success) {
        status_visualisation = 'success';
      } else {
        status_visualisation = 'error';
      }
    } else if (
      entryType.action_type.result?.exit_status?.type === 'exit_code'
    ) {
      if (entryType.action_type.result?.exit_status?.code === 0) {
        status_visualisation = 'success';
      } else {
        status_visualisation = 'error';
      }
    }
  }

  // If pending, should be a pulsing primary-foreground
  const colorMap: Record<ExitStatusVisualisation, string> = {
    success: 'bg-green-300',
    error: 'bg-red-300',
    pending: 'bg-primary-foreground/50',
  };

  if (!status_visualisation) return null;

  return (
    <div className="relative">
      <div
        className={`${colorMap[status_visualisation]} h-1.5 w-1.5 rounded-full absolute -left-1 -bottom-4`}
      />
      {status_visualisation === 'pending' && (
        <div
          className={`${colorMap[status_visualisation]} h-1.5 w-1.5 rounded-full absolute -left-1 -bottom-4 animate-ping`}
        />
      )}
    </div>
  );
};

/**********************
 * Helper definitions *
 **********************/

const shouldRenderMarkdown = (entryType: NormalizedEntryType) =>
  entryType.type === 'assistant_message' ||
  entryType.type === 'system_message' ||
  entryType.type === 'thinking' ||
  entryType.type === 'tool_use';

const getContentClassName = (entryType: NormalizedEntryType) => {
  const base = ' whitespace-pre-wrap break-words';
  if (
    entryType.type === 'tool_use' &&
    entryType.action_type.action === 'command_run'
  )
    return `${base} font-mono`;

  // Keep content-only styling — no bg/padding/rounded here.
  if (entryType.type === 'error_message')
    return `${base} font-mono text-destructive`;

  if (entryType.type === 'thinking') return `${base} opacity-60`;

  if (
    entryType.type === 'tool_use' &&
    (entryType.action_type.action === 'todo_management' ||
      (entryType.tool_name &&
        ['todowrite', 'todoread', 'todo_write', 'todo_read', 'todo'].includes(
          entryType.tool_name.toLowerCase()
        )))
  )
    return `${base} font-mono text-zinc-800 dark:text-zinc-200`;

  if (
    entryType.type === 'tool_use' &&
    entryType.action_type.action === 'plan_presentation'
  )
    return `${base} text-blue-700 dark:text-blue-300 bg-blue-50 dark:bg-blue-950/20 px-3 py-2 border-l-4 border-blue-400`;

  return base;
};

/*********************
 * Unified card      *
 *********************/

type CardVariant = 'system' | 'error';

const MessageCard: React.FC<{
  children: React.ReactNode;
  variant: CardVariant;
  expanded?: boolean;
  onToggle?: () => void;
}> = ({ children, variant, expanded, onToggle }) => {
  const frameBase =
    'border px-3 py-2 w-full cursor-pointer  bg-[hsl(var(--card))] border-[hsl(var(--border))]';
  const systemTheme = 'border-400/40 text-zinc-500';
  const errorTheme =
    'border-red-400/40 bg-red-50 dark:bg-[hsl(var(--card))] text-[hsl(var(--foreground))]';

  return (
    <div
      className={`${frameBase} ${
        variant === 'system' ? systemTheme : errorTheme
      }`}
      onClick={onToggle}
    >
      <div className="flex items-center gap-1.5">
        <div className="min-w-0 flex-1">{children}</div>
        {onToggle && (
          <ExpandChevron
            expanded={!!expanded}
            onClick={onToggle}
            variant={variant}
          />
        )}
      </div>
    </div>
  );
};

/************************
 * Collapsible container *
 ************************/

type CollapsibleVariant = 'system' | 'error';

const ExpandChevron: React.FC<{
  expanded: boolean;
  onClick: () => void;
  variant: CollapsibleVariant;
}> = ({ expanded, onClick, variant }) => {
  const color =
    variant === 'system'
      ? 'text-700 dark:text-300'
      : 'text-red-700 dark:text-red-300';

  return (
    <ChevronDown
      onClick={onClick}
      className={`h-4 w-4 cursor-pointer transition-transform ${color} ${
        expanded ? '' : '-rotate-90'
      }`}
    />
  );
};

const CollapsibleEntry: React.FC<{
  content: string;
  markdown: boolean;
  expansionKey: string;
  variant: CollapsibleVariant;
  contentClassName: string;
}> = ({ content, markdown, expansionKey, variant, contentClassName }) => {
  const multiline = content.includes('\n');
  const [expanded, toggle] = useExpandable(`entry:${expansionKey}`, false);

  const Inner = (
    <div className={contentClassName}>
      {markdown ? (
        <MarkdownRenderer
          content={content}
          className="whitespace-pre-wrap break-words"
          enableCopyButton={false}
        />
      ) : (
        content
      )}
    </div>
  );

  const firstLine = content.split('\n')[0];
  const PreviewInner = (
    <div className={contentClassName}>
      {markdown ? (
        <MarkdownRenderer
          content={firstLine}
          className="whitespace-pre-wrap break-words"
          enableCopyButton={false}
        />
      ) : (
        firstLine
      )}
    </div>
  );

  if (!multiline) {
    return <MessageCard variant={variant}>{Inner}</MessageCard>;
  }

  return expanded ? (
    <MessageCard variant={variant} expanded={expanded} onToggle={toggle}>
      {Inner}
    </MessageCard>
  ) : (
    <MessageCard variant={variant} expanded={expanded} onToggle={toggle}>
      {PreviewInner}
    </MessageCard>
  );
};

const PlanPresentationCard: React.FC<{
  plan: string;
  expansionKey: string;
}> = ({ plan, expansionKey }) => {
  const [expanded, toggle] = useExpandable(`plan-entry:${expansionKey}`, true);

  return (
    <div className="inline-block w-full">
      <div className="border w-full overflow-hidden  border-blue-400/40">
        <button
          onClick={(e: React.MouseEvent) => {
            e.preventDefault();
            toggle();
          }}
          title={expanded ? 'Hide plan' : 'Show plan'}
          className="w-full px-2 py-1.5 flex items-center gap-1.5 text-left bg-blue-50 dark:bg-blue-950/20 text-blue-700 dark:text-blue-300 border-b border-blue-400/40"
        >
          <span className=" min-w-0 truncate">
            <span className="font-semibold">Plan</span>
          </span>
          <div className="ml-auto flex items-center gap-2">
            <ExpandChevron
              expanded={expanded}
              onClick={toggle}
              variant="system"
            />
          </div>
        </button>

        {expanded && (
          <div className="px-3 py-2 max-h-[65vh] overflow-y-auto overscroll-contain bg-blue-50 dark:bg-blue-950/20">
            <div className=" text-blue-700 dark:text-blue-300">
              <MarkdownRenderer
                content={plan}
                className="whitespace-pre-wrap break-words"
                enableCopyButton
              />
            </div>
          </div>
        )}
      </div>
    </div>
  );
};

const ToolCallCard: React.FC<{
  entryType?: Extract<NormalizedEntryType, { type: 'tool_use' }>;
  action?: any;
  expansionKey: string;
  content?: string;
  entryContent?: string;
}> = ({ entryType, action, expansionKey, content, entryContent }) => {
  const at: any = entryType?.action_type || action;
  const [expanded, toggle] = useExpandable(`tool-entry:${expansionKey}`, false);

  const label =
    at?.action === 'command_run'
      ? 'Ran'
      : entryType?.tool_name || at?.tool_name || 'Tool';

  const isCommand = at?.action === 'command_run';

  const inlineText = (entryContent || content || '').trim();
  const isSingleLine = inlineText !== '' && !/\r?\n/.test(inlineText);
  const showInlineSummary = isSingleLine;

  const hasArgs = at?.action === 'tool' && !!at?.arguments;
  const hasResult = at?.action === 'tool' && !!at?.result;

  const output: string | null = isCommand ? (at?.result?.output ?? null) : null;
  let argsText: string | null = null;
  if (isCommand) {
    const fromArgs =
      typeof at?.arguments === 'string'
        ? at.arguments
        : at?.arguments != null
          ? JSON.stringify(at.arguments, null, 2)
          : '';

    const fallback = (entryContent || content || '').trim();
    argsText = (fromArgs || fallback).trim();
  }

  const hasExpandableDetails = isCommand
    ? Boolean(argsText) || Boolean(output)
    : hasArgs || hasResult;

  const HeaderWrapper: React.ElementType = hasExpandableDetails
    ? 'button'
    : 'div';
  const headerProps = hasExpandableDetails
    ? {
        onClick: (e: React.MouseEvent) => {
          e.preventDefault();
          toggle();
        },
        title: expanded ? 'Hide details' : 'Show details',
      }
    : {};

  return (
    <div className="inline-block w-full  flex flex-col gap-4">
      <HeaderWrapper
        {...headerProps}
        className="w-full flex items-center gap-1.5 text-left text-secondary-foreground"
      >
        <span className=" min-w-0 flex items-center gap-1.5">
          {entryType ? (
            <span>
              {getStatusIndicator(entryType)}
              {getEntryIcon(entryType)}
            </span>
          ) : (
            <span className="font-normal flex">{label}</span>
          )}
          {showInlineSummary && (
            <span className="font-light">{inlineText}</span>
          )}
        </span>
      </HeaderWrapper>

      {expanded && (
        <div className="max-h-[200px] overflow-y-auto border">
          {isCommand ? (
            <>
              {argsText && (
                <>
                  <div className="font-normal uppercase bg-background border-b border-dashed px-2 py-1">
                    Args
                  </div>
                  <div className="px-2 py-1">{argsText}</div>
                </>
              )}

              {output && (
                <>
                  <div className="font-normal uppercase bg-background border-y border-dashed px-2 py-1">
                    Output
                  </div>
                  <div className="px-2 py-1">
                    <RawLogText content={output} />
                  </div>
                </>
              )}
            </>
          ) : (
            <>
              {entryType?.action_type.action === 'tool' && (
                <>
                  <div className="font-normal uppercase bg-background border-b border-dashed px-2 py-1">
                    Args
                  </div>
                  <div className="px-2 py-1">
                    {renderJson(entryType.action_type.arguments)}
                  </div>
                  <div className="font-normal uppercase bg-background border-y border-dashed px-2 py-1">
                    Result
                  </div>
                  <div className="px-2 py-1">
                    {entryType.action_type.result?.type.type === 'markdown' &&
                      entryType.action_type.result.value && (
                        <MarkdownRenderer
                          content={entryType.action_type.result.value?.toString()}
                        />
                      )}
                    {entryType.action_type.result?.type.type === 'json' &&
                      renderJson(entryType.action_type.result.value)}
                  </div>
                </>
              )}
            </>
          )}
        </div>
      )}
    </div>
  );
};

const LoadingCard = () => {
  return (
    <div className="flex animate-pulse space-x-2 items-center">
      <div className="size-3 bg-foreground/10"></div>
      <div className="flex-1 h-3 bg-foreground/10"></div>
      <div className="flex-1 h-3"></div>
      <div className="flex-1 h-3"></div>
    </div>
  );
};

/*******************
 * Main component  *
 *******************/

function DisplayConversationEntry({
  entry,
  expansionKey,
  executionProcessId,
  taskAttempt,
}: Props) {
  const isNormalizedEntry = (
    entry: NormalizedEntry | ProcessStartPayload
  ): entry is NormalizedEntry => 'entry_type' in entry;

  const isProcessStart = (
    entry: NormalizedEntry | ProcessStartPayload
  ): entry is ProcessStartPayload => 'processId' in entry;

  if (isProcessStart(entry)) {
    const toolAction: any = entry.action ?? null;
    return (
      <ToolCallCard
        action={toolAction}
        expansionKey={expansionKey}
        content={toolAction?.message ?? toolAction?.summary ?? undefined}
      />
    );
  }

  // Handle NormalizedEntry
  const entryType = entry.entry_type;
  const isSystem = entryType.type === 'system_message';
  const isError = entryType.type === 'error_message';
  const isToolUse = entryType.type === 'tool_use';
  const isUserMessage = entryType.type === 'user_message';
  const isLoading = entryType.type === 'loading';
  const isFileEdit = (a: ActionType): a is FileEditAction =>
    a.action === 'file_edit';

  if (isUserMessage) {
    return (
      <UserMessage
        content={entry.content}
        executionProcessId={executionProcessId}
        taskAttempt={taskAttempt}
      />
    );
  }

  return (
    <div className="px-4 py-2 text-sm">
      {isSystem || isError ? (
        <CollapsibleEntry
          content={isNormalizedEntry(entry) ? entry.content : ''}
          markdown={shouldRenderMarkdown(entryType)}
          expansionKey={expansionKey}
          variant={isSystem ? 'system' : 'error'}
          contentClassName={getContentClassName(entryType)}
        />
      ) : isToolUse && isFileEdit(entryType.action_type) ? (
        // Only FileChangeRenderer for file_edit
        (() => {
          const fileEditAction = entryType.action_type as FileEditAction;
          return fileEditAction.changes.map((change, idx) => (
            <FileChangeRenderer
              key={idx}
              path={fileEditAction.path}
              change={change}
              expansionKey={`edit:${expansionKey}:${idx}`}
            />
          ));
        })()
      ) : isToolUse && entryType.action_type.action === 'plan_presentation' ? (
        <PlanPresentationCard
          plan={entryType.action_type.plan}
          expansionKey={expansionKey}
        />
      ) : isToolUse ? (
        <ToolCallCard
          entryType={entryType}
          expansionKey={expansionKey}
          entryContent={isNormalizedEntry(entry) ? entry.content : ''}
        />
      ) : isLoading ? (
        <LoadingCard />
      ) : (
        <div className={getContentClassName(entryType)}>
          {shouldRenderMarkdown(entryType) ? (
            <MarkdownRenderer
              content={isNormalizedEntry(entry) ? entry.content : ''}
              className="whitespace-pre-wrap break-words flex flex-col gap-1 font-light"
              enableCopyButton={entryType.type === 'assistant_message'}
            />
          ) : isNormalizedEntry(entry) ? (
            entry.content
          ) : (
            ''
          )}
        </div>
      )}
    </div>
  );
}

export default DisplayConversationEntry;
</file>

<file path="frontend/src/components/NormalizedConversation/EditDiffRenderer.tsx">
import { useMemo } from 'react';
import {
  DiffView,
  DiffModeEnum,
  DiffLineType,
  parseInstance,
} from '@git-diff-view/react';
import { SquarePen } from 'lucide-react';
import { useUserSystem } from '@/components/config-provider';
import { getHighLightLanguageFromPath } from '@/utils/extToLanguage';
import { getActualTheme } from '@/utils/theme';
import '@/styles/diff-style-overrides.css';
import '@/styles/edit-diff-overrides.css';
import { useDiffViewMode } from '@/stores/useDiffViewStore';
import DiffViewSwitch from '@/components/diff-view-switch';

type Props = {
  path: string;
  unifiedDiff: string;
  hasLineNumbers: boolean;
  expansionKey: string;
};

/**
 * Process hunks for @git-diff-view/react
 * - Extract additions/deletions for display
 * - Decide whether to hide line numbers based on backend data
 */
function processUnifiedDiff(unifiedDiff: string, hasLineNumbers: boolean) {
  // Hide line numbers when backend says they are unreliable
  const hideNums = !hasLineNumbers;
  let isValidDiff;

  // Pre-compute additions/deletions using the library parser so counts are available while collapsed
  let additions = 0;
  let deletions = 0;
  try {
    const parsed = parseInstance.parse(unifiedDiff);
    for (const h of parsed.hunks) {
      for (const line of h.lines) {
        if (line.type === DiffLineType.Add) additions++;
        else if (line.type === DiffLineType.Delete) deletions++;
      }
    }
    isValidDiff = parsed.hunks.length > 0;
  } catch (err) {
    console.error('Failed to parse diff hunks:', err);
    isValidDiff = false;
  }

  return {
    hunks: [unifiedDiff],
    hideLineNumbers: hideNums,
    additions,
    deletions,
    isValidDiff,
  };
}

import { useExpandable } from '@/stores/useExpandableStore';

function EditDiffRenderer({
  path,
  unifiedDiff,
  hasLineNumbers,
  expansionKey,
}: Props) {
  const { config } = useUserSystem();
  const [expanded, setExpanded] = useExpandable(expansionKey, false);

  const theme = getActualTheme(config?.theme);
  const globalMode = useDiffViewMode();

  const { hunks, hideLineNumbers, additions, deletions, isValidDiff } = useMemo(
    () => processUnifiedDiff(unifiedDiff, hasLineNumbers),
    [path, unifiedDiff, hasLineNumbers]
  );

  const hideLineNumbersClass = hideLineNumbers ? ' edit-diff-hide-nums' : '';

  const diffData = useMemo(() => {
    const lang = getHighLightLanguageFromPath(path) || 'plaintext';
    return {
      hunks,
      oldFile: { fileName: path, fileLang: lang },
      newFile: { fileName: path, fileLang: lang },
    };
  }, [hunks, path]);

  return (
    <div>
      <div className="flex items-center text-secondary-foreground gap-1.5">
        <SquarePen className="h-3 w-3" />
        <p
          onClick={() => setExpanded()}
          className="text-sm font-mono overflow-x-auto flex-1 cursor-pointer"
        >
          {path}{' '}
          <span style={{ color: 'hsl(var(--console-success))' }}>
            +{additions}
          </span>{' '}
          <span style={{ color: 'hsl(var(--console-error))' }}>
            -{deletions}
          </span>
        </p>
      </div>

      {expanded && (
        <div className={'mt-2 border ' + hideLineNumbersClass}>
          <div className="flex items-center justify-end border-b px-2 py-1">
            <DiffViewSwitch />
          </div>
          {isValidDiff ? (
            <DiffView
              data={diffData}
              diffViewWrap={false}
              diffViewTheme={theme}
              diffViewHighlight
              diffViewMode={
                globalMode === 'split'
                  ? DiffModeEnum.Split
                  : DiffModeEnum.Unified
              }
              diffViewFontSize={12}
            />
          ) : (
            <>
              <pre
                className="px-4 pb-4 text-xs font-mono overflow-x-auto whitespace-pre-wrap"
                style={{ color: 'hsl(var(--muted-foreground) / 0.9)' }}
              >
                {unifiedDiff}
              </pre>
            </>
          )}
        </div>
      )}
    </div>
  );
}

export default EditDiffRenderer;
</file>

<file path="frontend/src/components/NormalizedConversation/FileChangeRenderer.tsx">
import { type FileChange } from 'shared/types';
import { useUserSystem } from '@/components/config-provider';
import { Trash2, FilePlus2, ArrowRight } from 'lucide-react';
import { getHighLightLanguageFromPath } from '@/utils/extToLanguage';
import { getActualTheme } from '@/utils/theme';
import EditDiffRenderer from './EditDiffRenderer';
import FileContentView from './FileContentView';
import '@/styles/diff-style-overrides.css';
import { useExpandable } from '@/stores/useExpandableStore';

type Props = {
  path: string;
  change: FileChange;
  expansionKey: string;
};

function isWrite(
  change: FileChange
): change is Extract<FileChange, { action: 'write'; content: string }> {
  return change?.action === 'write';
}
function isDelete(
  change: FileChange
): change is Extract<FileChange, { action: 'delete' }> {
  return change?.action === 'delete';
}
function isRename(
  change: FileChange
): change is Extract<FileChange, { action: 'rename'; new_path: string }> {
  return change?.action === 'rename';
}
function isEdit(
  change: FileChange
): change is Extract<FileChange, { action: 'edit' }> {
  return change?.action === 'edit';
}

const FileChangeRenderer = ({ path, change, expansionKey }: Props) => {
  const { config } = useUserSystem();
  const [expanded, setExpanded] = useExpandable(expansionKey, false);

  const theme = getActualTheme(config?.theme);

  // Edit: delegate to EditDiffRenderer for identical styling and behavior
  if (isEdit(change)) {
    return (
      <EditDiffRenderer
        path={path}
        unifiedDiff={change.unified_diff}
        hasLineNumbers={change.has_line_numbers}
        expansionKey={expansionKey}
      />
    );
  }

  // Title row content and whether the row is expandable
  const { titleNode, icon, expandable } = (() => {
    if (isDelete(change)) {
      return {
        titleNode: path,
        icon: <Trash2 className="h-3 w-3" />,
        expandable: false,
      };
    }

    if (isRename(change)) {
      return {
        titleNode: (
          <>
            Rename {path} to {change.new_path}
          </>
        ),
        icon: <ArrowRight className="h-3 w-3" />,
        expandable: false,
      };
    }

    if (isWrite(change)) {
      return {
        titleNode: path,
        icon: <FilePlus2 className="h-3 w-3" />,
        expandable: true,
      };
    }

    // No fallback: render nothing for unknown change types
    return {
      titleNode: null,
      icon: null,
      expandable: false,
    };
  })();

  // nothing to display
  if (!titleNode) {
    return null;
  }

  return (
    <div>
      <div className="flex items-center text-secondary-foreground gap-1.5">
        {icon}
        <p
          onClick={() => expandable && setExpanded()}
          className="text-sm font-light overflow-x-auto flex-1 cursor-pointer"
        >
          {titleNode}
        </p>
      </div>

      {/* Body */}
      {isWrite(change) && expanded && (
        <FileContentView
          content={change.content}
          lang={getHighLightLanguageFromPath(path)}
          theme={theme}
        />
      )}
    </div>
  );
};

export default FileChangeRenderer;
</file>

<file path="frontend/src/components/NormalizedConversation/FileContentView.tsx">
import { useMemo } from 'react';
import { DiffView, DiffModeEnum } from '@git-diff-view/react';
import { generateDiffFile } from '@git-diff-view/file';
import '@/styles/diff-style-overrides.css';
import '@/styles/edit-diff-overrides.css';

type Props = {
  content: string;
  lang: string | null;
  theme?: 'light' | 'dark';
};

/**
 * View syntax highlighted file content.
 */
function FileContentView({ content, lang, theme }: Props) {
  // Uses the syntax highlighter from @git-diff-view/react without any diff-related features.
  // This allows uniform styling with EditDiffRenderer.
  const diffFile = useMemo(() => {
    try {
      const instance = generateDiffFile(
        '', // old file
        '', // old content (empty)
        '', // new file
        content, // new content
        '', // old lang
        lang || 'plaintext' // new lang
      );
      instance.initRaw();
      return instance;
    } catch {
      return null;
    }
  }, [content, lang]);

  return diffFile ? (
    <div className="border mt-2">
      <DiffView
        diffFile={diffFile}
        diffViewWrap={false}
        diffViewTheme={theme}
        diffViewHighlight
        diffViewMode={DiffModeEnum.Unified}
        diffViewFontSize={12}
      />
    </div>
  ) : (
    <pre className="text-xs font-mono overflow-x-auto whitespace-pre">
      {content}
    </pre>
  );
}

export default FileContentView;
</file>

<file path="frontend/src/components/NormalizedConversation/ToolDetails.tsx">
import MarkdownRenderer from '@/components/ui/markdown-renderer.tsx';
import RawLogText from '@/components/common/RawLogText';
import { Braces, FileText } from 'lucide-react';

type JsonValue = any;

type ToolResult = {
  type: 'markdown' | 'json';
  value: JsonValue;
};

type Props = {
  arguments?: JsonValue | null;
  result?: ToolResult | null;
  commandOutput?: string | null; // presence => command mode
  commandExit?:
    | { type: 'success'; success: boolean }
    | { type: 'exit_code'; code: number }
    | null;
};

export const renderJson = (v: JsonValue) => (
  <pre className="whitespace-pre-wrap">{JSON.stringify(v, null, 2)}</pre>
);

export default function ToolDetails({
  arguments: args,
  result,
  commandOutput,
}: Props) {
  const isCommandMode = commandOutput !== undefined;

  return (
    <div className="space-y-3">
      {args && (
        <section>
          {!isCommandMode ? (
            <>
              <div className="flex items-center gap-2 text-xs text-zinc-500">
                <Braces className="h-3 w-3" />
                <span>Arguments</span>
              </div>
              {renderJson(args)}
            </>
          ) : (
            <>
              <RawLogText
                content={
                  typeof args === 'string'
                    ? args
                    : JSON.stringify(args, null, 2)
                }
              />
            </>
          )}
        </section>
      )}

      {result && !isCommandMode && (
        <section>
          <div className="flex items-center gap-2 text-xs text-zinc-500">
            {result.type === 'json' ? (
              <Braces className="h-3 w-3" />
            ) : (
              <FileText className="h-3 w-3" />
            )}
            <span>Result</span>
          </div>
          <div className="mt-1">
            {result.type === 'markdown' ? (
              <MarkdownRenderer content={String(result.value ?? '')} />
            ) : (
              renderJson(result.value)
            )}
          </div>
        </section>
      )}

      {isCommandMode && (
        <section>
          <div className="mt-1">
            <RawLogText content={commandOutput ?? ''} />
          </div>
        </section>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/NormalizedConversation/UserMessage.tsx">
import MarkdownRenderer from '@/components/ui/markdown-renderer';
import { Button } from '@/components/ui/button';
import { Pencil, Send, X } from 'lucide-react';
import { useState } from 'react';
import { Textarea } from '@/components/ui/textarea';
import { useProcessRetry } from '@/hooks/useProcessRetry';
import { TaskAttempt, type BaseAgentCapability } from 'shared/types';
import { useUserSystem } from '@/components/config-provider';

const UserMessage = ({
  content,
  executionProcessId,
  taskAttempt,
}: {
  content: string;
  executionProcessId?: string;
  taskAttempt?: TaskAttempt;
}) => {
  const [isEditing, setIsEditing] = useState(false);
  const [editContent, setEditContent] = useState(content);
  const retryHook = useProcessRetry(taskAttempt);
  const { capabilities } = useUserSystem();

  const canFork = !!(
    taskAttempt?.executor &&
    capabilities?.[taskAttempt.executor]?.includes(
      'SESSION_FORK' as BaseAgentCapability
    )
  );

  const handleEdit = () => {
    if (!executionProcessId) return;
    retryHook?.retryProcess(executionProcessId, editContent).then(() => {
      setIsEditing(false);
    });
  };

  return (
    <div className="py-2">
      <div className="bg-background px-4 py-2 text-sm border-y border-dashed flex gap-2">
        <div className="flex-1">
          {isEditing ? (
            <Textarea
              value={editContent}
              onChange={(e) => setEditContent(e.target.value)}
            />
          ) : (
            <MarkdownRenderer
              content={content}
              className="whitespace-pre-wrap break-words flex flex-col gap-1 font-light py-3"
            />
          )}
        </div>
        {executionProcessId && canFork && (
          <div className="flex flex-col">
            <Button
              onClick={() => setIsEditing(!isEditing)}
              variant="ghost"
              className="p-2"
            >
              {isEditing ? (
                <X className="w-3 h-3" />
              ) : (
                <Pencil className="w-3 h-3" />
              )}
            </Button>
            {isEditing && (
              <Button onClick={handleEdit} variant="ghost" className="p-2">
                <Send className="w-3 h-3" />
              </Button>
            )}
          </div>
        )}
      </div>
    </div>
  );
};

export default UserMessage;
</file>

<file path="frontend/src/components/projects/copy-files-field.tsx">
import { MultiFileSearchTextarea } from '@/components/ui/multi-file-search-textarea';

interface CopyFilesFieldProps {
  value: string;
  onChange: (value: string) => void;
  projectId?: string;
  disabled?: boolean;
}

export function CopyFilesField({
  value,
  onChange,
  projectId,
  disabled = false,
}: CopyFilesFieldProps) {
  if (projectId) {
    // Editing existing project - use file search
    return (
      <MultiFileSearchTextarea
        value={value}
        onChange={onChange}
        placeholder="Start typing a file path... (.env, config.local.json, .local/settings.yml)"
        rows={3}
        disabled={disabled}
        className="w-full px-3 py-2 text-sm border border-input bg-background text-foreground disabled:opacity-50 rounded-md resize-vertical focus:outline-none focus:ring-2 focus:ring-ring"
        projectId={projectId}
        maxRows={6}
      />
    );
  }

  // Creating new project - fall back to plain textarea
  return (
    <textarea
      value={value}
      onChange={(e) => onChange(e.target.value)}
      placeholder=".env,config.local.json,.local/settings.yml"
      rows={3}
      disabled={disabled}
      className="w-full px-3 py-2 text-sm border border-input bg-background text-foreground rounded-md resize-vertical focus:outline-none focus:ring-2 focus:ring-ring"
    />
  );
}
</file>

<file path="frontend/src/components/projects/github-repository-picker.tsx">
import { useState, useEffect, useCallback, useRef } from 'react';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Label } from '@/components/ui/label';
import { Alert, AlertDescription } from '@/components/ui/alert';
import { Loader2, Github } from 'lucide-react';
import { githubApi, RepositoryInfo } from '@/lib/api';

interface GitHubRepositoryPickerProps {
  selectedRepository: RepositoryInfo | null;
  onRepositorySelect: (repository: RepositoryInfo | null) => void;
  onNameChange: (name: string) => void;
  name: string;
  error: string;
}

// Simple in-memory cache for repositories
const repositoryCache = new Map<number, RepositoryInfo[]>();
const CACHE_DURATION = 5 * 60 * 1000; // 5 minutes
const cacheTimestamps = new Map<number, number>();

function isCacheValid(page: number): boolean {
  const timestamp = cacheTimestamps.get(page);
  return timestamp ? Date.now() - timestamp < CACHE_DURATION : false;
}

export function GitHubRepositoryPicker({
  selectedRepository,
  onRepositorySelect,
  onNameChange,
  name,
  error,
}: GitHubRepositoryPickerProps) {
  const [repositories, setRepositories] = useState<RepositoryInfo[]>([]);
  const [loading, setLoading] = useState(false);
  const [loadError, setLoadError] = useState('');
  const [page, setPage] = useState(1);
  const [hasMorePages, setHasMorePages] = useState(true);
  const [loadingMore, setLoadingMore] = useState(false);
  const scrollContainerRef = useRef<HTMLDivElement>(null);

  const loadRepositories = useCallback(
    async (pageNum: number = 1, isLoadingMore: boolean = false) => {
      if (isLoadingMore) {
        setLoadingMore(true);
      } else {
        setLoading(true);
      }
      setLoadError('');

      try {
        // Check cache first
        if (isCacheValid(pageNum)) {
          const cachedRepos = repositoryCache.get(pageNum);
          if (cachedRepos) {
            if (pageNum === 1) {
              setRepositories(cachedRepos);
            } else {
              setRepositories((prev) => [...prev, ...cachedRepos]);
            }
            setPage(pageNum);
            return;
          }
        }

        const repos = await githubApi.listRepositories(pageNum);

        // Cache the results
        repositoryCache.set(pageNum, repos);
        cacheTimestamps.set(pageNum, Date.now());

        if (pageNum === 1) {
          setRepositories(repos);
        } else {
          setRepositories((prev) => [...prev, ...repos]);
        }
        setPage(pageNum);

        // If we got fewer than expected results, we've reached the end
        if (repos.length < 30) {
          // GitHub typically returns 30 repos per page
          setHasMorePages(false);
        }
      } catch (err) {
        setLoadError(
          err instanceof Error ? err.message : 'Failed to load repositories'
        );
      } finally {
        if (isLoadingMore) {
          setLoadingMore(false);
        } else {
          setLoading(false);
        }
      }
    },
    []
  );

  useEffect(() => {
    loadRepositories(1);
  }, [loadRepositories]);

  const handleRepositorySelect = (repository: RepositoryInfo) => {
    onRepositorySelect(repository);
    // Auto-populate project name from repository name if name is empty
    if (!name) {
      const cleanName = repository.name
        .replace(/[-_]/g, ' ')
        .replace(/\b\w/g, (l) => l.toUpperCase());
      onNameChange(cleanName);
    }
  };

  const loadMoreRepositories = useCallback(() => {
    if (!loading && !loadingMore && hasMorePages) {
      loadRepositories(page + 1, true);
    }
  }, [loading, loadingMore, hasMorePages, page, loadRepositories]);

  // Infinite scroll handler
  const handleScroll = useCallback(
    (e: React.UIEvent<HTMLDivElement>) => {
      const { scrollTop, scrollHeight, clientHeight } = e.currentTarget;
      const isNearBottom = scrollHeight - scrollTop <= clientHeight + 100; // 100px threshold

      if (isNearBottom && !loading && !loadingMore && hasMorePages) {
        loadMoreRepositories();
      }
    },
    [loading, loadingMore, hasMorePages, loadMoreRepositories]
  );

  if (loadError) {
    return (
      <Alert>
        <AlertDescription>
          {loadError}
          <Button
            variant="link"
            className="h-auto p-0 ml-2"
            onClick={() => loadRepositories(1)}
          >
            Try again
          </Button>
        </AlertDescription>
      </Alert>
    );
  }

  return (
    <div className="space-y-4">
      <div className="space-y-2">
        <Label>Select Repository</Label>
        {loading && repositories.length === 0 ? (
          <div className="flex items-center justify-center py-8">
            <Loader2 className="h-6 w-6 animate-spin" />
            <span className="ml-2">Loading repositories...</span>
          </div>
        ) : (
          <div
            ref={scrollContainerRef}
            className="max-h-64 overflow-y-auto border rounded-md p-4 space-y-3"
            onScroll={handleScroll}
          >
            {repositories.map((repository) => (
              <div
                key={repository.id}
                className={`p-3 border rounded-lg cursor-pointer hover:bg-accent ${
                  selectedRepository?.id === repository.id
                    ? 'bg-accent border-primary'
                    : ''
                }`}
                onClick={() => handleRepositorySelect(repository)}
              >
                <div className="flex items-start space-x-3">
                  <Github className="h-4 w-4 mt-1" />
                  <div className="flex-1 space-y-1">
                    <div className="flex items-center space-x-2">
                      <span className="font-medium">{repository.name}</span>
                      {repository.private && (
                        <span className="text-xs bg-yellow-100 text-yellow-800 px-2 py-0.5 rounded">
                          Private
                        </span>
                      )}
                    </div>
                    <div className="text-sm text-muted-foreground">
                      <div>{repository.full_name}</div>
                      {repository.description && (
                        <div className="mt-1">{repository.description}</div>
                      )}
                    </div>
                  </div>
                </div>
              </div>
            ))}

            {repositories.length === 0 && !loading && (
              <div className="text-center py-4 text-muted-foreground">
                No repositories found
              </div>
            )}

            {/* Loading more indicator */}
            {loadingMore && (
              <div className="flex items-center justify-center py-4">
                <Loader2 className="h-4 w-4 animate-spin mr-2" />
                <span className="text-sm text-muted-foreground">
                  Loading more repositories...
                </span>
              </div>
            )}

            {/* Manual load more button (fallback if infinite scroll doesn't work) */}
            {hasMorePages && !loadingMore && repositories.length > 0 && (
              <div className="pt-4 border-t">
                <Button
                  variant="outline"
                  onClick={loadMoreRepositories}
                  disabled={loading || loadingMore}
                  className="w-full"
                >
                  Load more repositories
                </Button>
              </div>
            )}

            {/* End of results indicator */}
            {!hasMorePages && repositories.length > 0 && (
              <div className="text-center py-2 text-xs text-muted-foreground border-t">
                All repositories loaded
              </div>
            )}
          </div>
        )}
      </div>

      {selectedRepository && (
        <div className="space-y-2">
          <Label htmlFor="project-name">Project Name</Label>
          <Input
            id="project-name"
            placeholder="Enter project name"
            value={name}
            onChange={(e) => onNameChange(e.target.value)}
          />
        </div>
      )}

      {error && (
        <Alert>
          <AlertDescription>{error}</AlertDescription>
        </Alert>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/projects/project-detail.tsx">
import { useCallback, useEffect, useState } from 'react';
import { useNavigate } from 'react-router-dom';
import { Button } from '@/components/ui/button';
import {
  Card,
  CardContent,
  CardDescription,
  CardHeader,
  CardTitle,
} from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { Alert, AlertDescription } from '@/components/ui/alert';
import { Project } from 'shared/types';
import { showProjectForm } from '@/lib/modals';
import { projectsApi } from '@/lib/api';
import {
  AlertCircle,
  ArrowLeft,
  Calendar,
  CheckSquare,
  Clock,
  Edit,
  Loader2,
  Trash2,
} from 'lucide-react';
import { useKeyboardShortcuts } from '@/lib/keyboard-shortcuts';

interface ProjectDetailProps {
  projectId: string;
  onBack: () => void;
}

export function ProjectDetail({ projectId, onBack }: ProjectDetailProps) {
  const navigate = useNavigate();
  const [project, setProject] = useState<Project | null>(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState('');

  useKeyboardShortcuts({
    navigate,
    currentPath: `/projects/${projectId}`,
  });

  const fetchProject = useCallback(async () => {
    setLoading(true);
    setError('');

    try {
      const result = await projectsApi.getById(projectId);
      setProject(result);
    } catch (error) {
      console.error('Failed to fetch project:', error);
      // @ts-expect-error it is type ApiError
      setError(error.message || 'Failed to load project');
    }

    setLoading(false);
  }, [projectId]);

  const handleDelete = async () => {
    if (!project) return;
    if (
      !confirm(
        `Are you sure you want to delete "${project.name}"? This action cannot be undone.`
      )
    )
      return;

    try {
      await projectsApi.delete(projectId);
      onBack();
    } catch (error) {
      console.error('Failed to delete project:', error);
      // @ts-expect-error it is type ApiError
      setError(error.message || 'Failed to delete project');
    }
  };

  const handleEditClick = async () => {
    try {
      const result = await showProjectForm({ project });
      if (result === 'saved') {
        fetchProject();
      }
    } catch (error) {
      // User cancelled - do nothing
    }
  };

  useEffect(() => {
    fetchProject();
  }, [fetchProject]);

  if (loading) {
    return (
      <div className="flex items-center justify-center py-12">
        <Loader2 className="mr-2 h-4 w-4 animate-spin" />
        Loading project...
      </div>
    );
  }

  if (error || !project) {
    return (
      <div className="space-y-4 py-12 px-4">
        <Button variant="outline" onClick={onBack}>
          <ArrowLeft className="mr-2 h-4 w-4" />
          Back to Projects
        </Button>
        <Card>
          <CardContent className="py-12 text-center">
            <div className="mx-auto flex h-12 w-12 items-center justify-center rounded-lg bg-muted">
              <AlertCircle className="h-6 w-6 text-muted-foreground" />
            </div>
            <h3 className="mt-4 text-lg font-semibold">Project not found</h3>
            <p className="mt-2 text-sm text-muted-foreground">
              {error ||
                "The project you're looking for doesn't exist or has been deleted."}
            </p>
            <Button className="mt-4" onClick={onBack}>
              Back to Projects
            </Button>
          </CardContent>
        </Card>
      </div>
    );
  }

  return (
    <div className="space-y-6 py-12 px-4">
      <div className="flex justify-between items-start">
        <div className="flex items-center space-x-4">
          <Button variant="outline" onClick={onBack}>
            <ArrowLeft className="mr-2 h-4 w-4" />
            Back to Projects
          </Button>
          <div>
            <div className="flex items-center gap-3">
              <h1 className="text-2xl font-bold">{project.name}</h1>
            </div>
            <p className="text-sm text-muted-foreground">
              Project details and settings
            </p>
          </div>
        </div>
        <div className="flex gap-2">
          <Button onClick={() => navigate(`/projects/${projectId}/tasks`)}>
            <CheckSquare className="mr-2 h-4 w-4" />
            View Tasks
          </Button>
          <Button variant="outline" onClick={handleEditClick}>
            <Edit className="mr-2 h-4 w-4" />
            Edit
          </Button>
          <Button
            variant="outline"
            onClick={handleDelete}
            className="text-destructive hover:text-destructive-foreground hover:bg-destructive/10"
          >
            <Trash2 className="mr-2 h-4 w-4" />
            Delete
          </Button>
        </div>
      </div>

      {error && (
        <Alert variant="destructive">
          <AlertCircle className="h-4 w-4" />
          <AlertDescription>{error}</AlertDescription>
        </Alert>
      )}

      <div className="grid gap-6 md:grid-cols-2">
        <Card>
          <CardHeader>
            <CardTitle className="flex items-center">
              <Calendar className="mr-2 h-5 w-5" />
              Project Information
            </CardTitle>
          </CardHeader>
          <CardContent className="space-y-4">
            <div className="flex items-center justify-between">
              <span className="text-sm font-medium text-muted-foreground">
                Status
              </span>
              <Badge variant="secondary">Active</Badge>
            </div>
            <div className="space-y-2">
              <div className="flex items-center text-sm">
                <Calendar className="mr-2 h-4 w-4 text-muted-foreground" />
                <span className="text-muted-foreground">Created:</span>
                <span className="ml-2">
                  {new Date(project.created_at).toLocaleDateString()}
                </span>
              </div>
              <div className="flex items-center text-sm">
                <Clock className="mr-2 h-4 w-4 text-muted-foreground" />
                <span className="text-muted-foreground">Last Updated:</span>
                <span className="ml-2">
                  {new Date(project.updated_at).toLocaleDateString()}
                </span>
              </div>
            </div>
          </CardContent>
        </Card>

        <Card>
          <CardHeader>
            <CardTitle>Project Details</CardTitle>
            <CardDescription>
              Technical information about this project
            </CardDescription>
          </CardHeader>
          <CardContent className="space-y-3">
            <div>
              <h4 className="text-sm font-medium text-muted-foreground">
                Project ID
              </h4>
              <code className="mt-1 block text-xs bg-muted p-2 rounded font-mono">
                {project.id}
              </code>
            </div>
            <div>
              <h4 className="text-sm font-medium text-muted-foreground">
                Created At
              </h4>
              <p className="mt-1 text-sm">
                {new Date(project.created_at).toLocaleString()}
              </p>
            </div>
            <div>
              <h4 className="text-sm font-medium text-muted-foreground">
                Last Modified
              </h4>
              <p className="mt-1 text-sm">
                {new Date(project.updated_at).toLocaleString()}
              </p>
            </div>
          </CardContent>
        </Card>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/projects/project-form-fields.tsx">
import { useState, useEffect } from 'react';
import { Label } from '@/components/ui/label';
import { Input } from '@/components/ui/input';
import { Button } from '@/components/ui/button';
import { Alert, AlertDescription } from '@/components/ui/alert';
import {
  AlertCircle,
  Folder,
  Search,
  FolderGit,
  FolderPlus,
  ArrowLeft,
} from 'lucide-react';
import {
  createScriptPlaceholderStrategy,
  ScriptPlaceholderContext,
} from '@/utils/script-placeholders';
import { useUserSystem } from '@/components/config-provider';
import { CopyFilesField } from './copy-files-field';
// Removed collapsible sections for simplicity; show fields always in edit mode
import { fileSystemApi } from '@/lib/api';
import { showFolderPicker } from '@/lib/modals';
import { DirectoryEntry } from 'shared/types';
import { generateProjectNameFromPath } from '@/utils/string';

interface ProjectFormFieldsProps {
  isEditing: boolean;
  repoMode: 'existing' | 'new';
  setRepoMode: (mode: 'existing' | 'new') => void;
  gitRepoPath: string;
  handleGitRepoPathChange: (path: string) => void;
  parentPath: string;
  setParentPath: (path: string) => void;
  setFolderName: (name: string) => void;
  setName: (name: string) => void;
  name: string;
  setupScript: string;
  setSetupScript: (script: string) => void;
  devScript: string;
  setDevScript: (script: string) => void;
  cleanupScript: string;
  setCleanupScript: (script: string) => void;
  copyFiles: string;
  setCopyFiles: (files: string) => void;
  error: string;
  setError: (error: string) => void;
  projectId?: string;
  onCreateProject?: (path: string, name: string) => void;
}

export function ProjectFormFields({
  isEditing,
  repoMode,
  setRepoMode,
  gitRepoPath,
  handleGitRepoPathChange,
  parentPath,
  setParentPath,
  setFolderName,
  setName,
  name,
  setupScript,
  setSetupScript,
  devScript,
  setDevScript,
  cleanupScript,
  setCleanupScript,
  copyFiles,
  setCopyFiles,
  error,
  setError,
  projectId,
  onCreateProject,
}: ProjectFormFieldsProps) {
  const { system } = useUserSystem();

  // Create strategy-based placeholders
  const placeholders = system.environment
    ? new ScriptPlaceholderContext(
        createScriptPlaceholderStrategy(system.environment.os_type)
      ).getPlaceholders()
    : {
        setup: '#!/bin/bash\nnpm install\n# Add any setup commands here...',
        dev: '#!/bin/bash\nnpm run dev\n# Add dev server start command here...',
        cleanup:
          '#!/bin/bash\n# Add cleanup commands here...\n# This runs after coding agent execution',
      };

  // Repository loading state
  const [allRepos, setAllRepos] = useState<DirectoryEntry[]>([]);
  const [loading, setLoading] = useState(false);
  const [reposError, setReposError] = useState('');
  const [showMoreOptions, setShowMoreOptions] = useState(false);
  const [showRecentRepos, setShowRecentRepos] = useState(false);

  // Lazy-load repositories when the user navigates to the repo list
  useEffect(() => {
    if (!isEditing && showRecentRepos && !loading && allRepos.length === 0) {
      loadRecentRepos();
    }
  }, [isEditing, showRecentRepos]);

  const loadRecentRepos = async () => {
    setLoading(true);
    setReposError('');

    try {
      const discoveredRepos = await fileSystemApi.listGitRepos();
      setAllRepos(discoveredRepos);
    } catch (err) {
      setReposError('Failed to load repositories');
      console.error('Failed to load repos:', err);
    } finally {
      setLoading(false);
    }
  };

  return (
    <>
      {!isEditing && repoMode === 'existing' && (
        <div className="space-y-4">
          {/* Show selection interface only when no repo is selected */}
          <>
            {/* Initial choice cards - Stage 1 */}
            {!showRecentRepos && (
              <>
                {/* From Git Repository card */}
                <div
                  className="p-4 border cursor-pointer hover:shadow-md transition-shadow rounded-lg bg-card"
                  onClick={() => setShowRecentRepos(true)}
                >
                  <div className="flex items-start gap-3">
                    <FolderGit className="h-5 w-5 mt-0.5 flex-shrink-0 text-muted-foreground" />
                    <div className="min-w-0 flex-1">
                      <div className="font-medium text-foreground">
                        From Git Repository
                      </div>
                      <div className="text-xs text-muted-foreground mt-1">
                        Use an existing repository as your project base
                      </div>
                    </div>
                  </div>
                </div>

                {/* Create Blank Project card */}
                <div
                  className="p-4 border cursor-pointer hover:shadow-md transition-shadow rounded-lg bg-card"
                  onClick={() => {
                    setRepoMode('new');
                    setError('');
                  }}
                >
                  <div className="flex items-start gap-3">
                    <FolderPlus className="h-5 w-5 mt-0.5 flex-shrink-0 text-muted-foreground" />
                    <div className="min-w-0 flex-1">
                      <div className="font-medium text-foreground">
                        Create Blank Project
                      </div>
                      <div className="text-xs text-muted-foreground mt-1">
                        Start a new project from scratch
                      </div>
                    </div>
                  </div>
                </div>
              </>
            )}

            {/* Repository selection - Stage 2A */}
            {showRecentRepos && (
              <>
                {/* Back button */}
                <button
                  className="text-sm text-muted-foreground hover:text-foreground flex items-center gap-1 mb-4"
                  onClick={() => {
                    setShowRecentRepos(false);
                    setError('');
                  }}
                >
                  <ArrowLeft className="h-3 w-3" />
                  Back to options
                </button>

                {/* Repository cards */}
                {!loading && allRepos.length > 0 && (
                  <div className="space-y-2">
                    {allRepos
                      .slice(0, showMoreOptions ? allRepos.length : 3)
                      .map((repo) => (
                        <div
                          key={repo.path}
                          className="p-4 border cursor-pointer hover:shadow-md transition-shadow rounded-lg bg-card"
                          onClick={() => {
                            setError('');
                            const cleanName = generateProjectNameFromPath(
                              repo.path
                            );
                            onCreateProject?.(repo.path, cleanName);
                          }}
                        >
                          <div className="flex items-start gap-3">
                            <FolderGit className="h-5 w-5 mt-0.5 flex-shrink-0 text-muted-foreground" />
                            <div className="min-w-0 flex-1">
                              <div className="font-medium text-foreground">
                                {repo.name}
                              </div>
                              <div className="text-xs text-muted-foreground truncate mt-1">
                                {repo.path}
                              </div>
                            </div>
                          </div>
                        </div>
                      ))}

                    {/* Show more/less for repositories */}
                    {!showMoreOptions && allRepos.length > 3 && (
                      <button
                        className="text-sm text-muted-foreground hover:text-foreground transition-colors text-left"
                        onClick={() => setShowMoreOptions(true)}
                      >
                        Show {allRepos.length - 3} more repositories
                      </button>
                    )}
                    {showMoreOptions && allRepos.length > 3 && (
                      <button
                        className="text-sm text-muted-foreground hover:text-foreground transition-colors text-left"
                        onClick={() => setShowMoreOptions(false)}
                      >
                        Show less
                      </button>
                    )}
                  </div>
                )}

                {/* Loading state */}
                {loading && (
                  <div className="p-4 border rounded-lg bg-card">
                    <div className="flex items-center gap-3">
                      <div className="animate-spin h-5 w-5 border-2 border-muted-foreground border-t-transparent rounded-full"></div>
                      <div className="text-sm text-muted-foreground">
                        Loading repositories...
                      </div>
                    </div>
                  </div>
                )}

                {/* Error state */}
                {!loading && reposError && (
                  <div className="p-4 border border-destructive rounded-lg bg-destructive/5">
                    <div className="flex items-center gap-3">
                      <AlertCircle className="h-5 w-5 text-destructive flex-shrink-0" />
                      <div className="text-sm text-destructive">
                        {reposError}
                      </div>
                    </div>
                  </div>
                )}

                {/* Browse for repository card */}
                <div
                  className="p-4 border border-dashed cursor-pointer hover:shadow-md transition-shadow rounded-lg bg-card"
                  onClick={async () => {
                    setError('');
                    const selectedPath = await showFolderPicker({
                      title: 'Select Git Repository',
                      description: 'Choose an existing git repository',
                    });
                    if (selectedPath) {
                      const projectName =
                        generateProjectNameFromPath(selectedPath);
                      if (onCreateProject) {
                        onCreateProject(selectedPath, projectName);
                      }
                    }
                  }}
                >
                  <div className="flex items-start gap-3">
                    <Search className="h-5 w-5 mt-0.5 flex-shrink-0 text-muted-foreground" />
                    <div className="min-w-0 flex-1">
                      <div className="font-medium text-foreground">
                        Search all repos
                      </div>
                      <div className="text-xs text-muted-foreground mt-1">
                        Browse and select any repository on your system
                      </div>
                    </div>
                  </div>
                </div>
              </>
            )}
          </>
        </div>
      )}

      {/* Blank Project Form */}
      {!isEditing && repoMode === 'new' && (
        <div className="space-y-4">
          {/* Back button */}
          <Button
            type="button"
            variant="ghost"
            size="sm"
            onClick={() => {
              setRepoMode('existing');
              setError('');
              setName('');
              setParentPath('');
              setFolderName('');
            }}
            className="flex items-center gap-2"
          >
            <ArrowLeft className="h-4 w-4" />
            Back to options
          </Button>

          <div className="space-y-4">
            <div className="space-y-2">
              <Label htmlFor="new-project-name">
                Project Name <span className="text-red-500">*</span>
              </Label>
              <Input
                id="new-project-name"
                type="text"
                value={name}
                onChange={(e) => {
                  setName(e.target.value);
                  if (e.target.value) {
                    setFolderName(
                      e.target.value
                        .toLowerCase()
                        .replace(/\s+/g, '-')
                        .replace(/[^a-z0-9-]/g, '')
                    );
                  }
                }}
                placeholder="My Awesome Project"
                className="placeholder:text-secondary-foreground placeholder:opacity-100"
                required
              />
              <p className="text-xs text-muted-foreground">
                The folder name will be auto-generated from the project name
              </p>
            </div>

            <div className="space-y-2">
              <Label htmlFor="parent-path">Parent Directory</Label>
              <div className="flex space-x-2">
                <Input
                  id="parent-path"
                  type="text"
                  value={parentPath}
                  onChange={(e) => setParentPath(e.target.value)}
                  placeholder="Home"
                  className="flex-1 placeholder:text-secondary-foreground placeholder:opacity-100"
                />
                <Button
                  type="button"
                  variant="ghost"
                  size="icon"
                  onClick={async () => {
                    const selectedPath = await showFolderPicker({
                      title: 'Select Parent Directory',
                      description: 'Choose where to create the new repository',
                      value: parentPath,
                    });
                    if (selectedPath) {
                      setParentPath(selectedPath);
                    }
                  }}
                >
                  <Folder className="h-4 w-4" />
                </Button>
              </div>
              <p className="text-xs text-muted-foreground">
                Leave empty to use your home directory, or specify a custom
                path.
              </p>
            </div>
          </div>
        </div>
      )}

      {isEditing && (
        <>
          <div className="space-y-2">
            <Label htmlFor="git-repo-path">Git Repository Path</Label>
            <div className="flex space-x-2">
              <Input
                id="git-repo-path"
                type="text"
                value={gitRepoPath}
                onChange={(e) => handleGitRepoPathChange(e.target.value)}
                placeholder="/path/to/your/existing/repo"
                required
                className="flex-1"
              />
              <Button
                type="button"
                variant="outline"
                onClick={async () => {
                  const selectedPath = await showFolderPicker({
                    title: 'Select Git Repository',
                    description: 'Choose an existing git repository',
                    value: gitRepoPath,
                  });
                  if (selectedPath) {
                    handleGitRepoPathChange(selectedPath);
                  }
                }}
              >
                <Folder className="h-4 w-4" />
              </Button>
            </div>
          </div>

          <div className="space-y-2">
            <Label htmlFor="name">Project Name</Label>
            <Input
              id="name"
              type="text"
              value={name}
              onChange={(e) => setName(e.target.value)}
              placeholder="Enter project name"
              required
            />
          </div>
        </>
      )}

      {isEditing && (
        <div className="space-y-4 pt-4 border-t border-border">
          <div className="space-y-2">
            <Label htmlFor="setup-script">Setup Script</Label>
            <textarea
              id="setup-script"
              value={setupScript}
              onChange={(e) => setSetupScript(e.target.value)}
              placeholder={placeholders.setup}
              rows={4}
              className="w-full px-3 py-2 text-sm border border-input bg-background text-foreground rounded-md resize-vertical focus:outline-none focus:ring-2 focus:ring-ring"
            />
            <p className="text-sm text-muted-foreground">
              This script will run after creating the worktree and before the
              coding agent starts. Use it for setup tasks like installing
              dependencies or preparing the environment.
            </p>
          </div>

          <div className="space-y-2">
            <Label htmlFor="dev-script">Dev Server Script</Label>
            <textarea
              id="dev-script"
              value={devScript}
              onChange={(e) => setDevScript(e.target.value)}
              placeholder={placeholders.dev}
              rows={4}
              className="w-full px-3 py-2 text-sm border border-input bg-background text-foreground rounded-md resize-vertical focus:outline-none focus:ring-2 focus:ring-ring"
            />
            <p className="text-sm text-muted-foreground">
              This script can be run from task attempts to start a development
              server. Use it to quickly start your project's dev server for
              testing changes.
            </p>
          </div>

          <div className="space-y-2">
            <Label htmlFor="cleanup-script">Cleanup Script</Label>
            <textarea
              id="cleanup-script"
              value={cleanupScript}
              onChange={(e) => setCleanupScript(e.target.value)}
              placeholder={placeholders.cleanup}
              rows={4}
              className="w-full px-3 py-2 text-sm border border-input bg-background text-foreground rounded-md resize-vertical focus:outline-none focus:ring-2 focus:ring-ring"
            />
            <p className="text-sm text-muted-foreground">
              This script runs after coding agent execution{' '}
              <strong>only if changes were made</strong>. Use it for quality
              assurance tasks like running linters, formatters, tests, or other
              validation steps. If no changes are made, this script is skipped.
            </p>
          </div>

          <div className="space-y-2">
            <Label>Copy Files</Label>
            <CopyFilesField
              value={copyFiles}
              onChange={setCopyFiles}
              projectId={projectId}
            />
            <p className="text-sm text-muted-foreground">
              Comma-separated list of files to copy from the original project
              directory to the worktree. These files will be copied after the
              worktree is created but before the setup script runs. Useful for
              environment-specific files like .env, configuration files, and
              local settings. Make sure these are gitignored or they could get
              committed!
            </p>
          </div>
        </div>
      )}

      {error && (
        <Alert variant="destructive">
          <AlertCircle className="h-4 w-4" />
          <AlertDescription>{error}</AlertDescription>
        </Alert>
      )}
    </>
  );
}
</file>

<file path="frontend/src/components/projects/project-list.tsx">
import { useEffect, useState } from 'react';
import { useNavigate } from 'react-router-dom';
import { useTranslation } from 'react-i18next';
import {
  useKanbanKeyboardNavigation,
  useKeyboardShortcuts,
} from '@/lib/keyboard-shortcuts';
import { Button } from '@/components/ui/button';
import { Card, CardContent } from '@/components/ui/card';
import { Alert, AlertDescription } from '@/components/ui/alert';
import { Project } from 'shared/types';
import { showProjectForm } from '@/lib/modals';
import { projectsApi } from '@/lib/api';
import { AlertCircle, Loader2, Plus } from 'lucide-react';
import ProjectCard from '@/components/projects/ProjectCard.tsx';

export function ProjectList() {
  const navigate = useNavigate();
  const { t } = useTranslation('projects');
  const [projects, setProjects] = useState<Project[]>([]);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState('');
  const [focusedProjectId, setFocusedProjectId] = useState<string | null>(null);
  const [focusedColumn, setFocusedColumn] = useState<string | null>(null);

  const fetchProjects = async () => {
    setLoading(true);
    setError('');

    try {
      const result = await projectsApi.getAll();
      setProjects(result);
    } catch (error) {
      console.error('Failed to fetch projects:', error);
      setError(t('errors.fetchFailed'));
    } finally {
      setLoading(false);
    }
  };

  const handleCreateProject = async () => {
    try {
      const result = await showProjectForm();
      if (result === 'saved') {
        fetchProjects();
      }
    } catch (error) {
      // User cancelled - do nothing
    }
  };

  const handleEditProject = async (project: Project) => {
    try {
      const result = await showProjectForm({ project });
      if (result === 'saved') {
        fetchProjects();
      }
    } catch (error) {
      // User cancelled - do nothing
    }
  };

  // Group projects by grid columns (3 columns for lg, 2 for md, 1 for sm)
  const getGridColumns = () => {
    const screenWidth = window.innerWidth;
    if (screenWidth >= 1024) return 3; // lg
    if (screenWidth >= 768) return 2; // md
    return 1; // sm
  };

  const groupProjectsByColumns = (projects: Project[], columns: number) => {
    const grouped: Record<string, Project[]> = {};
    for (let i = 0; i < columns; i++) {
      grouped[`column-${i}`] = [];
    }

    projects.forEach((project, index) => {
      const columnIndex = index % columns;
      grouped[`column-${columnIndex}`].push(project);
    });

    return grouped;
  };

  const columns = getGridColumns();
  const groupedProjects = groupProjectsByColumns(projects, columns);
  const allColumnKeys = Object.keys(groupedProjects);

  // Set initial focus when projects are loaded
  useEffect(() => {
    if (projects.length > 0 && !focusedProjectId) {
      setFocusedProjectId(projects[0].id);
      setFocusedColumn('column-0');
    }
  }, [projects, focusedProjectId]);

  const handleViewProjectDetails = (project: Project) => {
    navigate(`/projects/${project.id}/tasks`);
  };

  // Setup keyboard navigation
  useKanbanKeyboardNavigation({
    focusedTaskId: focusedProjectId,
    setFocusedTaskId: setFocusedProjectId,
    focusedStatus: focusedColumn,
    setFocusedStatus: setFocusedColumn,
    groupedTasks: groupedProjects,
    filteredTasks: projects,
    allTaskStatuses: allColumnKeys,
    onViewTaskDetails: handleViewProjectDetails,
    preserveIndexOnColumnSwitch: true,
  });

  useKeyboardShortcuts({
    ignoreEscape: true,
    onC: handleCreateProject,
    navigate,
    currentPath: '/projects',
  });

  // Handle window resize to update column layout
  useEffect(() => {
    const handleResize = () => {
      // Reset focus when layout changes
      if (focusedProjectId && projects.length > 0) {
        const newColumns = getGridColumns();

        // Find which column the focused project should be in
        const focusedProject = projects.find((p) => p.id === focusedProjectId);
        if (focusedProject) {
          const projectIndex = projects.indexOf(focusedProject);
          const newColumnIndex = projectIndex % newColumns;
          setFocusedColumn(`column-${newColumnIndex}`);
        }
      }
    };

    window.addEventListener('resize', handleResize);
    return () => window.removeEventListener('resize', handleResize);
  }, [focusedProjectId, projects]);

  useEffect(() => {
    fetchProjects();
  }, []);

  return (
    <div className="space-y-6 p-8 pb-16 md:pb-8 h-full overflow-auto">
      <div className="flex justify-between items-center">
        <div>
          <h1 className="text-3xl font-bold tracking-tight">{t('title')}</h1>
          <p className="text-muted-foreground">{t('subtitle')}</p>
        </div>
        <Button onClick={handleCreateProject}>
          <Plus className="mr-2 h-4 w-4" />
          {t('createProject')}
        </Button>
      </div>

      {error && (
        <Alert variant="destructive">
          <AlertCircle className="h-4 w-4" />
          <AlertDescription>{error}</AlertDescription>
        </Alert>
      )}

      {loading ? (
        <div className="flex items-center justify-center py-12">
          <Loader2 className="mr-2 h-4 w-4 animate-spin" />
          {t('loading')}
        </div>
      ) : projects.length === 0 ? (
        <Card>
          <CardContent className="py-12 text-center">
            <div className="mx-auto flex h-12 w-12 items-center justify-center rounded-lg bg-muted">
              <Plus className="h-6 w-6" />
            </div>
            <h3 className="mt-4 text-lg font-semibold">{t('empty.title')}</h3>
            <p className="mt-2 text-sm text-muted-foreground">
              {t('empty.description')}
            </p>
            <Button className="mt-4" onClick={handleCreateProject}>
              <Plus className="mr-2 h-4 w-4" />
              {t('empty.createFirst')}
            </Button>
          </CardContent>
        </Card>
      ) : (
        <div className="grid gap-6 md:grid-cols-2 lg:grid-cols-3">
          {projects.map((project) => (
            <ProjectCard
              key={project.id}
              project={project}
              isFocused={focusedProjectId === project.id}
              setError={setError}
              onEdit={handleEditProject}
              fetchProjects={fetchProjects}
            />
          ))}
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/projects/ProjectCard.tsx">
import {
  Card,
  CardDescription,
  CardHeader,
  CardTitle,
} from '@/components/ui/card.tsx';
import { Badge } from '@/components/ui/badge.tsx';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu.tsx';
import { Button } from '@/components/ui/button.tsx';
import {
  Calendar,
  Edit,
  ExternalLink,
  FolderOpen,
  MoreHorizontal,
  Trash2,
} from 'lucide-react';
import { useNavigate } from 'react-router-dom';
import { Project } from 'shared/types';
import { useEffect, useRef } from 'react';
import { useOpenProjectInEditor } from '@/hooks/useOpenProjectInEditor';
import { projectsApi } from '@/lib/api';

type Props = {
  project: Project;
  isFocused: boolean;
  fetchProjects: () => void;
  setError: (error: string) => void;
  onEdit: (project: Project) => void;
};

function ProjectCard({
  project,
  isFocused,
  fetchProjects,
  setError,
  onEdit,
}: Props) {
  const navigate = useNavigate();
  const ref = useRef<HTMLDivElement>(null);
  const handleOpenInEditor = useOpenProjectInEditor(project);

  useEffect(() => {
    if (isFocused && ref.current) {
      ref.current.scrollIntoView({ block: 'nearest', behavior: 'smooth' });
      ref.current.focus();
    }
  }, [isFocused]);

  const handleDelete = async (id: string, name: string) => {
    if (
      !confirm(
        `Are you sure you want to delete "${name}"? This action cannot be undone.`
      )
    )
      return;

    try {
      await projectsApi.delete(id);
      fetchProjects();
    } catch (error) {
      console.error('Failed to delete project:', error);
      setError('Failed to delete project');
    }
  };

  const handleEdit = (project: Project) => {
    onEdit(project);
  };

  const handleOpenInIDE = () => {
    handleOpenInEditor();
  };

  return (
    <Card
      className={`hover:shadow-md transition-shadow cursor-pointer focus:ring-2 focus:ring-primary outline-none border`}
      onClick={() => navigate(`/projects/${project.id}/tasks`)}
      tabIndex={isFocused ? 0 : -1}
      ref={ref}
    >
      <CardHeader>
        <div className="flex items-start justify-between">
          <CardTitle className="text-lg">{project.name}</CardTitle>
          <div className="flex items-center gap-2">
            <Badge variant="secondary">Active</Badge>
            <DropdownMenu>
              <DropdownMenuTrigger asChild onClick={(e) => e.stopPropagation()}>
                <Button variant="ghost" size="sm" className="h-8 w-8 p-0">
                  <MoreHorizontal className="h-4 w-4" />
                </Button>
              </DropdownMenuTrigger>
              <DropdownMenuContent align="end">
                <DropdownMenuItem
                  onClick={(e) => {
                    e.stopPropagation();
                    navigate(`/projects/${project.id}`);
                  }}
                >
                  <ExternalLink className="mr-2 h-4 w-4" />
                  View Project
                </DropdownMenuItem>
                <DropdownMenuItem
                  onClick={(e) => {
                    e.stopPropagation();
                    handleOpenInIDE();
                  }}
                >
                  <FolderOpen className="mr-2 h-4 w-4" />
                  Open in IDE
                </DropdownMenuItem>
                <DropdownMenuItem
                  onClick={(e) => {
                    e.stopPropagation();
                    handleEdit(project);
                  }}
                >
                  <Edit className="mr-2 h-4 w-4" />
                  Edit
                </DropdownMenuItem>
                <DropdownMenuItem
                  onClick={(e) => {
                    e.stopPropagation();
                    handleDelete(project.id, project.name);
                  }}
                  className="text-destructive"
                >
                  <Trash2 className="mr-2 h-4 w-4" />
                  Delete
                </DropdownMenuItem>
              </DropdownMenuContent>
            </DropdownMenu>
          </div>
        </div>
        <CardDescription className="flex items-center">
          <Calendar className="mr-1 h-3 w-3" />
          Created {new Date(project.created_at).toLocaleDateString()}
        </CardDescription>
      </CardHeader>
    </Card>
  );
}

export default ProjectCard;
</file>

<file path="frontend/src/components/rjsf/templates/ArrayFieldTemplate.tsx">
import {
  ArrayFieldTemplateProps,
  ArrayFieldTemplateItemType,
} from '@rjsf/utils';
import { Button } from '@/components/ui/button';
import { Plus, X } from 'lucide-react';

export const ArrayFieldTemplate = (props: ArrayFieldTemplateProps) => {
  const { canAdd, items, onAddClick, disabled, readonly } = props;

  if (!items || (items.length === 0 && !canAdd)) {
    return null;
  }

  return (
    <div className="space-y-4">
      <div>
        {items.length > 0 &&
          items.map((element: ArrayFieldTemplateItemType) => (
            <ArrayItem
              key={element.key}
              element={element}
              disabled={disabled}
              readonly={readonly}
            />
          ))}
      </div>

      {canAdd && (
        <Button
          type="button"
          variant="outline"
          size="sm"
          onClick={onAddClick}
          disabled={disabled || readonly}
          className="w-full"
        >
          <Plus className="w-4 h-4 mr-2" />
          Add Item
        </Button>
      )}
    </div>
  );
};

interface ArrayItemProps {
  element: ArrayFieldTemplateItemType;
  disabled?: boolean;
  readonly?: boolean;
}

const ArrayItem = ({ element, disabled, readonly }: ArrayItemProps) => {
  const { children } = element;
  const elementAny = element as any; // Type assertion needed for RJSF v6 beta properties

  return (
    <div className="flex items-center gap-2">
      <div className="flex-1">{children}</div>

      {/* Remove button */}
      {elementAny.buttonsProps?.hasRemove && (
        <Button
          type="button"
          variant="ghost"
          size="sm"
          onClick={elementAny.buttonsProps.onDropIndexClick(
            elementAny.buttonsProps.index
          )}
          disabled={disabled || readonly || elementAny.buttonsProps.disabled}
          className="h-8 w-8 p-0 text-muted-foreground hover:text-destructive hover:bg-destructive/10 transition-all duration-200 shrink-0"
          title="Remove item"
        >
          <X className="w-4 h-4" />
        </Button>
      )}
    </div>
  );
};
</file>

<file path="frontend/src/components/rjsf/templates/FieldTemplate.tsx">
import { FieldTemplateProps } from '@rjsf/utils';

export const FieldTemplate = (props: FieldTemplateProps) => {
  const {
    children,
    rawErrors = [],
    rawHelp,
    rawDescription,
    label,
    required,
    schema,
  } = props;

  if (schema.type === 'object') {
    return children;
  }

  // Two-column layout for other field types
  return (
    <div className="grid grid-cols-1 md:grid-cols-2 gap-4 py-6">
      {/* Left column: Label and description */}
      <div className="space-y-2">
        {label && (
          <div className="text-sm font-bold leading-relaxed">
            {label}
            {required && <span className="text-destructive ml-1">*</span>}
          </div>
        )}

        {rawDescription && (
          <p className="text-sm text-muted-foreground leading-relaxed">
            {rawDescription}
          </p>
        )}

        {rawHelp && (
          <p className="text-sm text-muted-foreground leading-relaxed">
            {rawHelp}
          </p>
        )}
      </div>

      {/* Right column: Field content */}
      <div className="space-y-2">
        {children}

        {rawErrors.length > 0 && (
          <div className="space-y-1">
            {rawErrors.map((error, index) => (
              <p key={index} className="text-sm text-destructive">
                {error}
              </p>
            ))}
          </div>
        )}
      </div>
    </div>
  );
};
</file>

<file path="frontend/src/components/rjsf/templates/FormTemplate.tsx">
export const FormTemplate = (props: any) => {
  const { children } = props;

  return <div className="w-full">{children}</div>;
};
</file>

<file path="frontend/src/components/rjsf/templates/index.ts">
export { ArrayFieldTemplate } from './ArrayFieldTemplate';
export { FieldTemplate } from './FieldTemplate';
export { ObjectFieldTemplate } from './ObjectFieldTemplate';
export { FormTemplate } from './FormTemplate';
</file>

<file path="frontend/src/components/rjsf/templates/ObjectFieldTemplate.tsx">
import { ObjectFieldTemplateProps } from '@rjsf/utils';

export const ObjectFieldTemplate = (props: ObjectFieldTemplateProps) => {
  const { properties } = props;

  return (
    <div className="divide-y">
      {properties.map((element) => (
        <div key={element.name}>{element.content}</div>
      ))}
    </div>
  );
};
</file>

<file path="frontend/src/components/rjsf/widgets/CheckboxWidget.tsx">
import { WidgetProps } from '@rjsf/utils';
import { Checkbox } from '@/components/ui/checkbox';

export const CheckboxWidget = (props: WidgetProps) => {
  const { id, value, disabled, readonly, onChange } = props;

  const handleChange = (checked: boolean) => {
    onChange(checked);
  };

  const checked = Boolean(value);

  return (
    <div className="flex items-center space-x-2">
      <Checkbox
        id={id}
        checked={checked}
        onCheckedChange={handleChange}
        disabled={disabled || readonly}
      />
    </div>
  );
};
</file>

<file path="frontend/src/components/rjsf/widgets/index.ts">
export { TextWidget } from './TextWidget';
export { SelectWidget } from './SelectWidget';
export { CheckboxWidget } from './CheckboxWidget';
export { TextareaWidget } from './TextareaWidget';
</file>

<file path="frontend/src/components/rjsf/widgets/SelectWidget.tsx">
import { WidgetProps } from '@rjsf/utils';
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from '@/components/ui/select';

export const SelectWidget = (props: WidgetProps) => {
  const {
    id,
    value,
    disabled,
    readonly,
    onChange,
    onBlur,
    onFocus,
    options,
    schema,
    placeholder,
  } = props;

  const { enumOptions } = options;

  const handleChange = (newValue: string) => {
    // Handle nullable enum values - '__null__' means null for nullable types
    const finalValue = newValue === '__null__' ? options.emptyValue : newValue;
    onChange(finalValue);
  };

  const handleOpenChange = (open: boolean) => {
    if (!open && onBlur) {
      onBlur(id, value);
    }
    if (open && onFocus) {
      onFocus(id, value);
    }
  };

  // Convert enumOptions to the format expected by our Select component
  const selectOptions = enumOptions || [];

  // Handle nullable types by adding a null option
  const isNullable = Array.isArray(schema.type) && schema.type.includes('null');
  const allOptions = isNullable
    ? [{ value: '__null__', label: 'None' }, ...selectOptions]
    : selectOptions;

  return (
    <Select
      value={value === null ? '__null__' : (value ?? '')}
      onValueChange={handleChange}
      onOpenChange={handleOpenChange}
      disabled={disabled || readonly}
    >
      <SelectTrigger id={id}>
        <SelectValue placeholder={placeholder || 'Select an option...'} />
      </SelectTrigger>
      <SelectContent>
        {allOptions.map((option) => (
          <SelectItem key={option.value} value={String(option.value)}>
            {option.label}
          </SelectItem>
        ))}
      </SelectContent>
    </Select>
  );
};
</file>

<file path="frontend/src/components/rjsf/widgets/TextareaWidget.tsx">
import { WidgetProps } from '@rjsf/utils';
import { Textarea } from '@/components/ui/textarea';

export const TextareaWidget = (props: WidgetProps) => {
  const {
    id,
    value,
    disabled,
    readonly,
    onChange,
    onBlur,
    onFocus,
    placeholder,
    options,
    schema,
  } = props;

  const handleChange = (event: React.ChangeEvent<HTMLTextAreaElement>) => {
    const newValue = event.target.value;
    onChange(newValue === '' ? options.emptyValue : newValue);
  };

  const handleBlur = (event: React.FocusEvent<HTMLTextAreaElement>) => {
    if (onBlur) {
      onBlur(id, event.target.value);
    }
  };

  const handleFocus = (event: React.FocusEvent<HTMLTextAreaElement>) => {
    if (onFocus) {
      onFocus(id, event.target.value);
    }
  };

  // Get rows from ui:options or default based on field name
  const rows =
    options.rows ||
    ((schema.title || '').toLowerCase().includes('prompt') ? 4 : 3);

  return (
    <Textarea
      id={id}
      value={value ?? ''}
      placeholder={placeholder || ''}
      disabled={disabled || readonly}
      onChange={handleChange}
      onBlur={handleBlur}
      onFocus={handleFocus}
      rows={rows}
      className="resize-vertical"
    />
  );
};
</file>

<file path="frontend/src/components/rjsf/widgets/TextWidget.tsx">
import { WidgetProps } from '@rjsf/utils';
import { Input } from '@/components/ui/input';

export const TextWidget = (props: WidgetProps) => {
  const {
    id,
    value,
    disabled,
    readonly,
    onChange,
    onBlur,
    onFocus,
    placeholder,
    options,
  } = props;

  const handleChange = (event: React.ChangeEvent<HTMLInputElement>) => {
    const newValue = event.target.value;
    onChange(newValue === '' ? options.emptyValue : newValue);
  };

  const handleBlur = (event: React.FocusEvent<HTMLInputElement>) => {
    if (onBlur) {
      onBlur(id, event.target.value);
    }
  };

  const handleFocus = (event: React.FocusEvent<HTMLInputElement>) => {
    if (onFocus) {
      onFocus(id, event.target.value);
    }
  };

  return (
    <Input
      id={id}
      value={value ?? ''}
      placeholder={placeholder || ''}
      disabled={disabled || readonly}
      onChange={handleChange}
      onBlur={handleBlur}
      onFocus={handleFocus}
    />
  );
};
</file>

<file path="frontend/src/components/rjsf/index.ts">
export { shadcnTheme, customWidgets, customTemplates } from './theme';
export * from './widgets';
export * from './templates';
</file>

<file path="frontend/src/components/rjsf/theme.ts">
import { RegistryWidgetsType } from '@rjsf/utils';
import {
  TextWidget,
  SelectWidget,
  CheckboxWidget,
  TextareaWidget,
} from './widgets';
import {
  ArrayFieldTemplate,
  FieldTemplate,
  ObjectFieldTemplate,
  FormTemplate,
} from './templates';

export const customWidgets: RegistryWidgetsType = {
  TextWidget,
  SelectWidget,
  CheckboxWidget,
  TextareaWidget,
  textarea: TextareaWidget,
};

export const customTemplates = {
  ArrayFieldTemplate,
  FieldTemplate,
  ObjectFieldTemplate,
  FormTemplate,
};

export const shadcnTheme = {
  widgets: customWidgets,
  templates: customTemplates,
};
</file>

<file path="frontend/src/components/settings/ExecutorProfileSelector.tsx">
import { Settings2, ArrowDown } from 'lucide-react';
import { Button } from '@/components/ui/button';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import { Label } from '@/components/ui/label';
import type {
  BaseCodingAgent,
  ExecutorConfig,
  ExecutorProfileId,
} from 'shared/types';

type Props = {
  profiles: Record<string, ExecutorConfig> | null;
  selectedProfile: ExecutorProfileId | null;
  onProfileSelect: (profile: ExecutorProfileId) => void;
  disabled?: boolean;
  showLabel?: boolean;
  showVariantSelector?: boolean;
};

function ExecutorProfileSelector({
  profiles,
  selectedProfile,
  onProfileSelect,
  disabled = false,
  showLabel = true,
  showVariantSelector = true,
}: Props) {
  if (!profiles) {
    return null;
  }

  const handleExecutorChange = (executor: string) => {
    onProfileSelect({
      executor: executor as BaseCodingAgent,
      variant: null,
    });
  };

  const handleVariantChange = (variant: string) => {
    if (selectedProfile) {
      onProfileSelect({
        ...selectedProfile,
        variant: variant === 'DEFAULT' ? null : variant,
      });
    }
  };

  const currentProfile = selectedProfile
    ? profiles[selectedProfile.executor]
    : null;
  const hasVariants = currentProfile && Object.keys(currentProfile).length > 0;

  return (
    <div className="flex gap-3 flex-col sm:flex-row">
      {/* Executor Profile Selector */}
      <div className="flex-1">
        {showLabel && (
          <Label htmlFor="executor-profile" className="text-sm font-medium">
            Agent
          </Label>
        )}
        <DropdownMenu>
          <DropdownMenuTrigger asChild>
            <Button
              variant="outline"
              size="sm"
              className="w-full justify-between text-xs mt-1.5"
              disabled={disabled}
            >
              <div className="flex items-center gap-1.5">
                <Settings2 className="h-3 w-3" />
                <span className="truncate">
                  {selectedProfile?.executor || 'Select profile'}
                </span>
              </div>
              <ArrowDown className="h-3 w-3" />
            </Button>
          </DropdownMenuTrigger>
          <DropdownMenuContent className="w-full">
            {Object.keys(profiles)
              .sort((a, b) => a.localeCompare(b))
              .map((executorKey) => (
                <DropdownMenuItem
                  key={executorKey}
                  onClick={() => handleExecutorChange(executorKey)}
                  className={
                    selectedProfile?.executor === executorKey ? 'bg-accent' : ''
                  }
                >
                  {executorKey}
                </DropdownMenuItem>
              ))}
          </DropdownMenuContent>
        </DropdownMenu>
      </div>

      {/* Variant Selector (conditional) */}
      {showVariantSelector &&
        selectedProfile &&
        hasVariants &&
        currentProfile && (
          <div className="flex-1">
            <Label htmlFor="executor-variant" className="text-sm font-medium">
              Configuration
            </Label>
            <DropdownMenu>
              <DropdownMenuTrigger asChild>
                <Button
                  variant="outline"
                  size="sm"
                  className="w-full justify-between text-xs mt-1.5"
                  disabled={disabled}
                >
                  <span className="truncate">
                    {selectedProfile.variant || 'DEFAULT'}
                  </span>
                  <ArrowDown className="h-3 w-3" />
                </Button>
              </DropdownMenuTrigger>
              <DropdownMenuContent className="w-full">
                {Object.keys(currentProfile).map((variantKey) => (
                  <DropdownMenuItem
                    key={variantKey}
                    onClick={() => handleVariantChange(variantKey)}
                    className={
                      selectedProfile.variant === variantKey ? 'bg-accent' : ''
                    }
                  >
                    {variantKey}
                  </DropdownMenuItem>
                ))}
              </DropdownMenuContent>
            </DropdownMenu>
          </div>
        )}

      {/* Show disabled variant selector for profiles without variants */}
      {showVariantSelector &&
        selectedProfile &&
        !hasVariants &&
        currentProfile && (
          <div className="flex-1">
            <Label htmlFor="executor-variant" className="text-sm font-medium">
              Configuration
            </Label>
            <Button
              variant="outline"
              size="sm"
              disabled
              className="w-full text-xs justify-start mt-1.5"
            >
              Default
            </Button>
          </div>
        )}

      {/* Show placeholder for variant when no profile selected */}
      {showVariantSelector && !selectedProfile && (
        <div className="flex-1">
          <Label htmlFor="executor-variant" className="text-sm font-medium">
            Configuration
          </Label>
          <Button
            variant="outline"
            size="sm"
            disabled
            className="w-full text-xs justify-start mt-1.5"
          >
            Select agent first
          </Button>
        </div>
      )}
    </div>
  );
}

export default ExecutorProfileSelector;
</file>

<file path="frontend/src/components/settings/index.ts">
export { default as ExecutorProfileSelector } from './ExecutorProfileSelector';
export { default as TaskSettings } from './TaskSettings';
</file>

<file path="frontend/src/components/settings/TaskSettings.tsx">
import { Label } from '@/components/ui/label';
import BranchSelector from '@/components/tasks/BranchSelector';
import ExecutorProfileSelector from './ExecutorProfileSelector';
import type {
  GitBranch,
  ExecutorConfig,
  ExecutorProfileId,
} from 'shared/types';

type Props = {
  // Branch selector props
  branches?: GitBranch[];
  selectedBranch?: string | null;
  onBranchSelect?: (branch: string) => void;
  showBranchSelector?: boolean;
  branchSelectorProps?: {
    placeholder?: string;
    className?: string;
    excludeCurrentBranch?: boolean;
  };

  // Executor profile selector props
  profiles?: Record<string, ExecutorConfig> | null;
  selectedProfile?: ExecutorProfileId | null;
  onProfileSelect?: (profile: ExecutorProfileId) => void;
  showExecutorSelector?: boolean;
  executorSelectorProps?: {
    showLabel?: boolean;
    showVariantSelector?: boolean;
    className?: string;
  };

  // Common props
  disabled?: boolean;
  className?: string;
};

function TaskSettings({
  // Branch selector props
  branches = [],
  selectedBranch,
  onBranchSelect,
  showBranchSelector = true,
  branchSelectorProps = {},

  // Executor profile selector props
  profiles,
  selectedProfile,
  onProfileSelect,
  showExecutorSelector = true,
  executorSelectorProps = {},

  // Common props
  disabled = false,
  className = '',
}: Props) {
  return (
    <div className={`space-y-3 ${className}`}>
      {/* Executor Profile Selector */}
      {showExecutorSelector &&
        profiles &&
        selectedProfile &&
        onProfileSelect && (
          <ExecutorProfileSelector
            profiles={profiles}
            selectedProfile={selectedProfile}
            onProfileSelect={onProfileSelect}
            disabled={disabled}
            {...executorSelectorProps}
          />
        )}

      {/* Branch Selector */}
      {showBranchSelector &&
        branches.length > 0 &&
        selectedBranch !== undefined &&
        onBranchSelect && (
          <div>
            <Label htmlFor="base-branch" className="text-sm font-medium">
              Branch
            </Label>
            <BranchSelector
              branches={branches}
              selectedBranch={selectedBranch}
              onBranchSelect={onBranchSelect}
              placeholder="Select branch"
              className="mt-1.5"
              {...branchSelectorProps}
            />
          </div>
        )}
    </div>
  );
}

export default TaskSettings;
</file>

<file path="frontend/src/components/tasks/follow-up/FollowUpConflictSection.tsx">
import { useCallback } from 'react';
import { attemptsApi } from '@/lib/api';
import { ConflictBanner } from '@/components/tasks/ConflictBanner';
import { buildResolveConflictsInstructions } from '@/lib/conflicts';
import type { BranchStatus } from 'shared/types';

type Props = {
  selectedAttemptId?: string;
  attemptBranch: string | null;
  branchStatus?: BranchStatus;
  isEditable: boolean;
  appendInstructions: (text: string) => void;
  refetchBranchStatus: () => void;
};

export function FollowUpConflictSection({
  selectedAttemptId,
  attemptBranch,
  branchStatus,
  isEditable,
  appendInstructions,
  refetchBranchStatus,
}: Props) {
  const op = branchStatus?.conflict_op ?? null;
  const handleInsertInstructions = useCallback(() => {
    const template = buildResolveConflictsInstructions(
      attemptBranch,
      branchStatus?.base_branch_name,
      branchStatus?.conflicted_files || [],
      op
    );
    appendInstructions(template);
  }, [
    attemptBranch,
    branchStatus?.base_branch_name,
    branchStatus?.conflicted_files,
    op,
    appendInstructions,
  ]);

  const hasConflicts = (branchStatus?.conflicted_files?.length ?? 0) > 0;
  if (!hasConflicts) return null;

  return (
    <ConflictBanner
      attemptBranch={attemptBranch}
      baseBranch={branchStatus?.base_branch_name}
      conflictedFiles={branchStatus?.conflicted_files || []}
      isEditable={isEditable}
      op={op}
      onOpenEditor={async () => {
        if (!selectedAttemptId) return;
        try {
          const first = branchStatus?.conflicted_files?.[0];
          await attemptsApi.openEditor(selectedAttemptId, undefined, first);
        } catch (e) {
          console.error('Failed to open editor', e);
        }
      }}
      onInsertInstructions={handleInsertInstructions}
      onAbort={async () => {
        if (!selectedAttemptId) return;
        try {
          await attemptsApi.abortConflicts(selectedAttemptId);
          refetchBranchStatus();
        } catch (e) {
          console.error('Failed to abort conflicts', e);
        }
      }}
    />
  );
}
</file>

<file path="frontend/src/components/tasks/follow-up/FollowUpEditorCard.tsx">
import { Loader2 } from 'lucide-react';
import { FileSearchTextarea } from '@/components/ui/file-search-textarea';
import { cn } from '@/lib/utils';
import { useProject } from '@/contexts/project-context';

type Props = {
  placeholder: string;
  value: string;
  onChange: (v: string) => void;
  onKeyDown: (e: React.KeyboardEvent<Element>) => void;
  disabled: boolean;
  // Loading overlay
  showLoadingOverlay: boolean;
};

export function FollowUpEditorCard({
  placeholder,
  value,
  onChange,
  onKeyDown,
  disabled,
  showLoadingOverlay,
}: Props) {
  const { projectId } = useProject();
  return (
    <div className="relative">
      <FileSearchTextarea
        placeholder={placeholder}
        value={value}
        onChange={onChange}
        onKeyDown={onKeyDown}
        className={cn('flex-1 min-h-[40px] resize-none')}
        disabled={disabled}
        projectId={projectId}
        rows={1}
        maxRows={6}
      />
      {showLoadingOverlay && (
        <div className="pointer-events-none absolute inset-0 z-20 flex items-center justify-center bg-background/60">
          <Loader2 className="h-4 w-4 animate-spin" />
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/tasks/TaskDetails/DiffTab.tsx">
import { useDiffEntries } from '@/hooks/useDiffEntries';
import { useMemo, useCallback, useState, useEffect } from 'react';
import { Loader } from '@/components/ui/loader';
import { Button } from '@/components/ui/button';
import DiffViewSwitch from '@/components/diff-view-switch';
import DiffCard from '@/components/DiffCard';
import { useDiffSummary } from '@/hooks/useDiffSummary';
import type { TaskAttempt } from 'shared/types';

interface DiffTabProps {
  selectedAttempt: TaskAttempt | null;
}

function DiffTab({ selectedAttempt }: DiffTabProps) {
  const [loading, setLoading] = useState(true);
  const [collapsedIds, setCollapsedIds] = useState<Set<string>>(new Set());
  const [hasInitialized, setHasInitialized] = useState(false);
  const { diffs, error } = useDiffEntries(selectedAttempt?.id ?? null, true);
  const { fileCount, added, deleted } = useDiffSummary(
    selectedAttempt?.id ?? null
  );

  useEffect(() => {
    setLoading(true);
    setHasInitialized(false);
  }, [selectedAttempt?.id]);

  useEffect(() => {
    setLoading(true);
  }, [selectedAttempt?.id]);

  useEffect(() => {
    if (diffs.length > 0 && loading) {
      setLoading(false);
    }
  }, [diffs, loading]);

  // If no diffs arrive within 7 seconds, stop showing the spinner
  useEffect(() => {
    if (!loading) return;
    const timer = setTimeout(() => {
      if (diffs.length === 0) {
        setLoading(false);
      }
    }, 7000);
    return () => clearTimeout(timer);
  }, [loading, diffs.length]);

  // Default-collapse certain change kinds on first load only
  useEffect(() => {
    if (diffs.length === 0) return;
    if (hasInitialized) return; // only run once per attempt
    const kindsToCollapse = new Set([
      'deleted',
      'renamed',
      'copied',
      'permissionChange',
    ]);
    const initial = new Set(
      diffs
        .filter((d) => kindsToCollapse.has(d.change))
        .map((d, i) => d.newPath || d.oldPath || String(i))
    );
    if (initial.size > 0) setCollapsedIds(initial);
    setHasInitialized(true);
  }, [diffs, hasInitialized]);

  const ids = useMemo(() => {
    return diffs.map((d, i) => d.newPath || d.oldPath || String(i));
  }, [diffs]);

  const toggle = useCallback((id: string) => {
    setCollapsedIds((prev) => {
      const next = new Set(prev);
      next.has(id) ? next.delete(id) : next.add(id);
      return next;
    });
  }, []);

  const allCollapsed = collapsedIds.size === diffs.length;
  const handleCollapseAll = useCallback(() => {
    setCollapsedIds(allCollapsed ? new Set() : new Set(ids));
  }, [allCollapsed, ids]);

  if (error) {
    return (
      <div className="bg-red-50 border border-red-200 rounded-lg p-4 m-4">
        <div className="text-red-800 text-sm">Failed to load diff: {error}</div>
      </div>
    );
  }

  if (loading) {
    return (
      <div className="flex items-center justify-center h-full">
        <Loader />
      </div>
    );
  }

  if (!loading && diffs.length === 0) {
    return (
      <div className="flex items-center justify-center h-full text-sm text-muted-foreground">
        No changes have been made yet
      </div>
    );
  }

  return (
    <DiffTabContent
      diffs={diffs}
      fileCount={fileCount}
      added={added}
      deleted={deleted}
      collapsedIds={collapsedIds}
      allCollapsed={allCollapsed}
      handleCollapseAll={handleCollapseAll}
      toggle={toggle}
      selectedAttempt={selectedAttempt}
    />
  );
}

interface DiffTabContentProps {
  diffs: any[];
  fileCount: number;
  added: number;
  deleted: number;
  collapsedIds: Set<string>;
  allCollapsed: boolean;
  handleCollapseAll: () => void;
  toggle: (id: string) => void;
  selectedAttempt: TaskAttempt | null;
}

function DiffTabContent({
  diffs,
  fileCount,
  added,
  deleted,
  collapsedIds,
  allCollapsed,
  handleCollapseAll,
  toggle,
  selectedAttempt,
}: DiffTabContentProps) {
  return (
    <div className="h-full flex flex-col relative">
      {diffs.length > 0 && (
        <div className="sticky top-0 bg-background border-b px-4 py-2 z-10">
          <div className="flex items-center justify-between gap-4">
            <span
              className="text-xs font-mono whitespace-nowrap"
              aria-live="polite"
              style={{ color: 'hsl(var(--muted-foreground) / 0.7)' }}
            >
              {fileCount} file{fileCount === 1 ? '' : 's'} changed,{' '}
              <span style={{ color: 'hsl(var(--console-success))' }}>
                +{added}
              </span>{' '}
              <span style={{ color: 'hsl(var(--console-error))' }}>
                -{deleted}
              </span>
            </span>
            <div className="flex items-center gap-2">
              <DiffViewSwitch />
              <Button
                variant="outline"
                size="xs"
                onClick={handleCollapseAll}
                className="shrink-0"
              >
                {allCollapsed ? 'Expand All' : 'Collapse All'}
              </Button>
            </div>
          </div>
        </div>
      )}
      <div className="flex-1 overflow-y-auto px-4">
        {diffs.map((diff, idx) => {
          const id = diff.newPath || diff.oldPath || String(idx);
          return (
            <DiffCard
              key={id}
              diff={diff}
              expanded={!collapsedIds.has(id)}
              onToggle={() => toggle(id)}
              selectedAttempt={selectedAttempt}
            />
          );
        })}
      </div>
    </div>
  );
}

export default DiffTab;
</file>

<file path="frontend/src/components/tasks/TaskDetails/LogsTab.tsx">
import type { TaskAttempt } from 'shared/types';
import VirtualizedList from '@/components/logs/VirtualizedList';

type Props = {
  selectedAttempt: TaskAttempt;
};

function LogsTab({ selectedAttempt }: Props) {
  return <VirtualizedList key={selectedAttempt.id} attempt={selectedAttempt} />;
}

export default LogsTab;
</file>

<file path="frontend/src/components/tasks/TaskDetails/ProcessesTab.tsx">
import { useState, useEffect, useCallback } from 'react';
import {
  Play,
  Square,
  AlertCircle,
  CheckCircle,
  Clock,
  Cog,
  ArrowLeft,
} from 'lucide-react';
import { executionProcessesApi } from '@/lib/api.ts';
import { ProfileVariantBadge } from '@/components/common/ProfileVariantBadge.tsx';
import { useExecutionProcesses } from '@/hooks/useExecutionProcesses';
import ProcessLogsViewer from './ProcessLogsViewer';
import type { ExecutionProcessStatus, ExecutionProcess } from 'shared/types';

import { useProcessSelection } from '@/contexts/ProcessSelectionContext';

interface ProcessesTabProps {
  attemptId?: string;
}

function ProcessesTab({ attemptId }: ProcessesTabProps) {
  const {
    executionProcesses,
    executionProcessesById,
    isLoading: processesLoading,
    isConnected,
    error: processesError,
  } = useExecutionProcesses(attemptId ?? '', { showSoftDeleted: true });
  const { selectedProcessId, setSelectedProcessId } = useProcessSelection();
  const [loadingProcessId, setLoadingProcessId] = useState<string | null>(null);
  const [localProcessDetails, setLocalProcessDetails] = useState<
    Record<string, ExecutionProcess>
  >({});

  useEffect(() => {
    setLocalProcessDetails({});
    setLoadingProcessId(null);
  }, [attemptId]);

  const getStatusIcon = (status: ExecutionProcessStatus) => {
    switch (status) {
      case 'running':
        return <Play className="h-4 w-4 text-blue-500" />;
      case 'completed':
        return <CheckCircle className="h-4 w-4 text-green-500" />;
      case 'failed':
        return <AlertCircle className="h-4 w-4 text-destructive" />;
      case 'killed':
        return <Square className="h-4 w-4 text-gray-500" />;
      default:
        return <Clock className="h-4 w-4 text-gray-400" />;
    }
  };

  const getStatusColor = (status: ExecutionProcessStatus) => {
    switch (status) {
      case 'running':
        return 'bg-blue-50 border-blue-200 text-blue-800';
      case 'completed':
        return 'bg-green-50 border-green-200 text-green-800';
      case 'failed':
        return 'bg-red-50 border-red-200 text-red-800';
      case 'killed':
        return 'bg-gray-50 border-gray-200 text-gray-800';
      default:
        return 'bg-gray-50 border-gray-200 text-gray-800';
    }
  };

  const formatDate = (dateString: string) => {
    const date = new Date(dateString);
    return date.toLocaleString();
  };

  const fetchProcessDetails = useCallback(async (processId: string) => {
    try {
      setLoadingProcessId(processId);
      const result = await executionProcessesApi.getDetails(processId);

      if (result !== undefined) {
        setLocalProcessDetails((prev) => ({
          ...prev,
          [processId]: result,
        }));
      }
    } catch (err) {
      console.error('Failed to fetch process details:', err);
    } finally {
      setLoadingProcessId((current) =>
        current === processId ? null : current
      );
    }
  }, []);

  // Automatically fetch process details when selectedProcessId changes
  useEffect(() => {
    if (!attemptId || !selectedProcessId) {
      return;
    }

    if (
      !localProcessDetails[selectedProcessId] &&
      loadingProcessId !== selectedProcessId
    ) {
      fetchProcessDetails(selectedProcessId);
    }
  }, [
    attemptId,
    selectedProcessId,
    localProcessDetails,
    loadingProcessId,
    fetchProcessDetails,
  ]);

  const handleProcessClick = async (process: ExecutionProcess) => {
    setSelectedProcessId(process.id);

    // If we don't have details for this process, fetch them
    if (!localProcessDetails[process.id]) {
      await fetchProcessDetails(process.id);
    }
  };

  const selectedProcess = selectedProcessId
    ? localProcessDetails[selectedProcessId] ||
      executionProcessesById[selectedProcessId]
    : null;

  if (!attemptId) {
    return (
      <div className="flex-1 flex items-center justify-center text-muted-foreground">
        <div className="text-center">
          <Cog className="h-12 w-12 mx-auto mb-4 opacity-50" />
          <p>Select an attempt to view execution processes.</p>
        </div>
      </div>
    );
  }

  return (
    <div className="flex-1 flex flex-col min-h-0">
      {!selectedProcessId ? (
        <div className="flex-1 overflow-auto px-4 pb-20 pt-4">
          {processesError && (
            <div className="mb-3 text-sm text-destructive">
              Failed to load live updates for processes.
              {!isConnected && ' Reconnecting...'}
            </div>
          )}
          {processesLoading && executionProcesses.length === 0 ? (
            <div className="flex items-center justify-center text-muted-foreground py-10">
              <p>Loading execution processes...</p>
            </div>
          ) : executionProcesses.length === 0 ? (
            <div className="flex items-center justify-center text-muted-foreground py-10">
              <div className="text-center">
                <Cog className="h-12 w-12 mx-auto mb-4 opacity-50" />
                <p>No execution processes found for this attempt.</p>
              </div>
            </div>
          ) : (
            <div className="space-y-3">
              {executionProcesses.map((process) => (
                <div
                  key={process.id}
                  className={`border rounded-lg p-4 hover:bg-muted/30 cursor-pointer transition-colors ${
                    loadingProcessId === process.id
                      ? 'opacity-50 cursor-wait'
                      : ''
                  }`}
                  onClick={() => handleProcessClick(process)}
                >
                  <div className="flex items-start justify-between">
                    <div className="flex items-center space-x-3">
                      {getStatusIcon(process.status)}
                      <div>
                        <h3 className="font-medium text-sm">
                          {process.run_reason}
                        </h3>
                        <p className="text-sm text-muted-foreground mt-1">
                          Process ID: {process.id}
                        </p>
                        {process.dropped && (
                          <span
                            className="inline-block mt-1 text-[10px] px-1.5 py-0.5 rounded-full bg-amber-100 text-amber-700 border border-amber-200"
                            title="Deleted by restore: timeline was restored to a checkpoint and later executions were removed"
                          >
                            Deleted
                          </span>
                        )}
                        {
                          <p className="text-sm text-muted-foreground mt-1">
                            Agent:{' '}
                            {process.executor_action.typ.type ===
                              'CodingAgentInitialRequest' ||
                            process.executor_action.typ.type ===
                              'CodingAgentFollowUpRequest' ? (
                              <ProfileVariantBadge
                                profileVariant={
                                  process.executor_action.typ
                                    .executor_profile_id
                                }
                              />
                            ) : null}
                          </p>
                        }
                      </div>
                    </div>
                    <div className="text-right">
                      <span
                        className={`inline-block px-2 py-1 text-xs font-medium border rounded-full ${getStatusColor(
                          process.status
                        )}`}
                      >
                        {process.status}
                      </span>
                      {process.exit_code !== null && (
                        <p className="text-xs text-muted-foreground mt-1">
                          Exit: {process.exit_code.toString()}
                        </p>
                      )}
                    </div>
                  </div>
                  <div className="mt-3 text-xs text-muted-foreground">
                    <div className="flex justify-between">
                      <span>Started: {formatDate(process.started_at)}</span>
                      {process.completed_at && (
                        <span>
                          Completed: {formatDate(process.completed_at)}
                        </span>
                      )}
                    </div>
                    <div className="mt-1">Process ID: {process.id}</div>
                  </div>
                </div>
              ))}
            </div>
          )}
        </div>
      ) : (
        <div className="flex-1 flex flex-col min-h-0">
          <div className="flex items-center justify-between px-4 py-2 border-b flex-shrink-0">
            <h2 className="text-lg font-semibold">Process Details</h2>
            <button
              onClick={() => setSelectedProcessId(null)}
              className="flex items-center gap-2 px-3 py-2 text-sm font-medium text-muted-foreground hover:text-foreground hover:bg-muted/50 rounded-md border border-border transition-colors"
            >
              <ArrowLeft className="h-4 w-4" />
              Back to list
            </button>
          </div>
          <div className="flex-1">
            {selectedProcess ? (
              <ProcessLogsViewer processId={selectedProcess.id} />
            ) : loadingProcessId === selectedProcessId ? (
              <div className="text-center text-muted-foreground">
                <p>Loading process details...</p>
              </div>
            ) : (
              <div className="text-center text-muted-foreground">
                <p>Failed to load process details. Please try again.</p>
              </div>
            )}
          </div>
        </div>
      )}
    </div>
  );
}

export default ProcessesTab;
</file>

<file path="frontend/src/components/tasks/TaskDetails/ProcessLogsViewer.tsx">
import { useEffect, useRef, useState } from 'react';
import { Virtuoso, VirtuosoHandle } from 'react-virtuoso';
import { AlertCircle } from 'lucide-react';
import { useLogStream } from '@/hooks/useLogStream';
import RawLogText from '@/components/common/RawLogText';
import type { PatchType } from 'shared/types';

type LogEntry = Extract<PatchType, { type: 'STDOUT' } | { type: 'STDERR' }>;

interface ProcessLogsViewerProps {
  processId: string;
}

export default function ProcessLogsViewer({
  processId,
}: ProcessLogsViewerProps) {
  const virtuosoRef = useRef<VirtuosoHandle>(null);
  const didInitScroll = useRef(false);
  const prevLenRef = useRef(0);
  const [atBottom, setAtBottom] = useState(true);

  const { logs, error } = useLogStream(processId);

  // 1) Initial jump to bottom once data appears.
  useEffect(() => {
    if (!didInitScroll.current && logs.length > 0) {
      didInitScroll.current = true;
      requestAnimationFrame(() => {
        virtuosoRef.current?.scrollToIndex({
          index: logs.length - 1,
          align: 'end',
        });
      });
    }
  }, [logs.length]);

  // 2) If there's a large append and we're at bottom, force-stick to the last item.
  useEffect(() => {
    const prev = prevLenRef.current;
    const grewBy = logs.length - prev;
    prevLenRef.current = logs.length;

    // tweak threshold as you like; this handles "big bursts"
    const LARGE_BURST = 10;
    if (grewBy >= LARGE_BURST && atBottom && logs.length > 0) {
      // defer so Virtuoso can re-measure before jumping
      requestAnimationFrame(() => {
        virtuosoRef.current?.scrollToIndex({
          index: logs.length - 1,
          align: 'end',
        });
      });
    }
  }, [logs.length, atBottom, logs]);

  const formatLogLine = (entry: LogEntry, index: number) => {
    return (
      <RawLogText
        key={index}
        content={entry.content}
        channel={entry.type === 'STDERR' ? 'stderr' : 'stdout'}
        className="text-sm px-4 py-1"
      />
    );
  };

  return (
    <div className="h-full">
      {logs.length === 0 && !error ? (
        <div className="p-4 text-center text-muted-foreground text-sm">
          No logs available
        </div>
      ) : error ? (
        <div className="p-4 text-center text-destructive text-sm">
          <AlertCircle className="h-4 w-4 inline mr-2" />
          {error}
        </div>
      ) : (
        <Virtuoso<LogEntry>
          ref={virtuosoRef}
          className="flex-1 rounded-lg"
          data={logs}
          itemContent={(index, entry) =>
            formatLogLine(entry as LogEntry, index)
          }
          // Keep pinned while user is at bottom; release when they scroll up
          atBottomStateChange={setAtBottom}
          followOutput={atBottom ? 'smooth' : false}
          // Optional: a bit more overscan helps during bursts
          increaseViewportBy={{ top: 0, bottom: 600 }}
        />
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/tasks/TaskDetails/TabNavigation.tsx">
import { GitCompare, MessageSquare, Cog } from 'lucide-react';
import { useAttemptExecution } from '@/hooks/useAttemptExecution';
import type { TabType } from '@/types/tabs';
import type { TaskAttempt } from 'shared/types';

type Props = {
  activeTab: TabType;
  setActiveTab: (tab: TabType) => void;
  rightContent?: React.ReactNode;
  selectedAttempt: TaskAttempt | null;
};

function TabNavigation({
  activeTab,
  setActiveTab,
  rightContent,
  selectedAttempt,
}: Props) {
  const { attemptData } = useAttemptExecution(selectedAttempt?.id);

  const tabs = [
    { id: 'logs' as TabType, label: 'Logs', icon: MessageSquare },
    { id: 'diffs' as TabType, label: 'Diffs', icon: GitCompare },
    { id: 'processes' as TabType, label: 'Processes', icon: Cog },
  ];

  const getTabClassName = (tabId: TabType) => {
    const baseClasses = 'flex items-center py-2 px-2 text-sm font-medium';
    const activeClasses = 'text-primary-foreground';
    const inactiveClasses =
      'text-secondary-foreground hover:text-primary-foreground';

    return `${baseClasses} ${activeTab === tabId ? activeClasses : inactiveClasses}`;
  };

  return (
    <div className="border-b border-dashed bg-background sticky top-0 z-10">
      <div className="flex items-center px-3 space-x-3">
        {tabs.map(({ id, label, icon: Icon }) => (
          <button
            key={id}
            onClick={() => setActiveTab(id)}
            className={getTabClassName(id)}
          >
            <Icon className="h-4 w-4 mr-2" />
            {label}
            {id === 'processes' &&
              attemptData.processes &&
              attemptData.processes.length > 0 && (
                <span className="ml-2 px-1.5 py-0.5 text-xs bg-primary/10 text-primary rounded-full">
                  {attemptData.processes.length}
                </span>
              )}
          </button>
        ))}
        <div className="ml-auto flex items-center">{rightContent}</div>
      </div>
    </div>
  );
}

export default TabNavigation;
</file>

<file path="frontend/src/components/tasks/TaskDetails/TaskTitleDescription.tsx">
import { useState } from 'react';
import { ChevronDown, ChevronUp } from 'lucide-react';
import { Button } from '@/components/ui/button';
import type { TaskWithAttemptStatus } from 'shared/types';

interface TaskTitleDescriptionProps {
  task: TaskWithAttemptStatus;
}

export function TaskTitleDescription({ task }: TaskTitleDescriptionProps) {
  const [isDescriptionExpanded, setIsDescriptionExpanded] = useState(false);

  return (
    <div>
      <h2 className="text-lg font-medium mb-1 line-clamp-2">{task.title}</h2>

      <div className="mt-2">
        <div className="flex items-start gap-2 text-sm text-secondary-foreground">
          {task.description ? (
            <div className="flex-1 min-w-0">
              <p
                className={`whitespace-pre-wrap break-words ${
                  !isDescriptionExpanded && task.description.length > 350
                    ? 'line-clamp-6'
                    : ''
                }`}
              >
                {task.description}
              </p>
              {task.description.length > 150 && (
                <Button
                  variant="ghost"
                  size="sm"
                  onClick={() =>
                    setIsDescriptionExpanded(!isDescriptionExpanded)
                  }
                  className="mt-1 p-0 h-auto text-sm text-secondary-foreground hover:text-foreground"
                >
                  {isDescriptionExpanded ? (
                    <>
                      <ChevronUp className="h-3 w-3 mr-1" />
                      Show less
                    </>
                  ) : (
                    <>
                      <ChevronDown className="h-3 w-3 mr-1" />
                      Show more
                    </>
                  )}
                </Button>
              )}
            </div>
          ) : (
            <p className="italic">No description provided</p>
          )}
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/tasks/Toolbar/CreateAttempt.tsx">
import { Dispatch, SetStateAction, useCallback } from 'react';
import { Button } from '@/components/ui/button.tsx';
import { X } from 'lucide-react';
import type { GitBranch, Task } from 'shared/types';
import type { ExecutorConfig } from 'shared/types';
import type { ExecutorProfileId } from 'shared/types';
import type { TaskAttempt } from 'shared/types';
import { useAttemptCreation } from '@/hooks/useAttemptCreation';
import { useAttemptExecution } from '@/hooks/useAttemptExecution';
import BranchSelector from '@/components/tasks/BranchSelector.tsx';
import { ExecutorProfileSelector } from '@/components/settings';
import { useKeyboardShortcuts } from '@/lib/keyboard-shortcuts.ts';
import { showModal } from '@/lib/modals';
import { Card } from '@/components/ui/card';
import { Label } from '@/components/ui/label';

type Props = {
  task: Task;
  branches: GitBranch[];
  taskAttempts: TaskAttempt[];
  createAttemptBranch: string | null;
  selectedProfile: ExecutorProfileId | null;
  selectedBranch: string | null;
  setIsInCreateAttemptMode: Dispatch<SetStateAction<boolean>>;
  setCreateAttemptBranch: Dispatch<SetStateAction<string | null>>;
  setSelectedProfile: Dispatch<SetStateAction<ExecutorProfileId | null>>;
  availableProfiles: Record<string, ExecutorConfig> | null;
  selectedAttempt: TaskAttempt | null;
};

function CreateAttempt({
  task,
  branches,
  taskAttempts,
  createAttemptBranch,
  selectedProfile,
  selectedBranch,
  setIsInCreateAttemptMode,
  setCreateAttemptBranch,
  setSelectedProfile,
  availableProfiles,
  selectedAttempt,
}: Props) {
  const { isAttemptRunning } = useAttemptExecution(selectedAttempt?.id);
  const { createAttempt, isCreating } = useAttemptCreation(task.id);

  // Create attempt logic
  const actuallyCreateAttempt = useCallback(
    async (profile: ExecutorProfileId, baseBranch?: string) => {
      const effectiveBaseBranch = baseBranch || selectedBranch;

      if (!effectiveBaseBranch) {
        throw new Error('Base branch is required to create an attempt');
      }

      await createAttempt({
        profile,
        baseBranch: effectiveBaseBranch,
      });
    },
    [createAttempt, selectedBranch]
  );

  // Handler for Enter key or Start button
  const onCreateNewAttempt = useCallback(
    async (
      profile: ExecutorProfileId,
      baseBranch?: string,
      isKeyTriggered?: boolean
    ) => {
      if (task.status === 'todo' && isKeyTriggered) {
        try {
          const result = await showModal<'confirmed' | 'canceled'>(
            'create-attempt-confirm',
            {
              title: 'Start New Attempt?',
              message:
                'Are you sure you want to start a new attempt for this task? This will create a new session and branch.',
            }
          );

          if (result === 'confirmed') {
            await actuallyCreateAttempt(profile, baseBranch);
            setIsInCreateAttemptMode(false);
          }
        } catch (error) {
          // User cancelled - do nothing
        }
      } else {
        await actuallyCreateAttempt(profile, baseBranch);
        setIsInCreateAttemptMode(false);
      }
    },
    [task.status, actuallyCreateAttempt, setIsInCreateAttemptMode]
  );

  // Keyboard shortcuts
  useKeyboardShortcuts({
    onEnter: () => {
      if (!selectedProfile) {
        return;
      }
      onCreateNewAttempt(
        selectedProfile,
        createAttemptBranch || undefined,
        true
      );
    },
    hasOpenDialog: false,
    closeDialog: () => {},
  });

  const handleExitCreateAttemptMode = () => {
    setIsInCreateAttemptMode(false);
  };

  const handleCreateAttempt = () => {
    if (!selectedProfile) {
      return;
    }
    onCreateNewAttempt(selectedProfile, createAttemptBranch || undefined);
  };

  return (
    <div className="">
      <Card className="bg-background p-3 text-sm border-y border-dashed">
        Create Attempt
      </Card>
      <div className="space-y-3 p-3">
        <div className="flex items-center justify-between">
          {taskAttempts.length > 0 && (
            <Button
              variant="ghost"
              size="sm"
              onClick={handleExitCreateAttemptMode}
            >
              <X className="h-4 w-4" />
            </Button>
          )}
        </div>
        <div className="flex items-center">
          <label className="text-xs font-medium text-muted-foreground">
            Each time you start an attempt, a new session is initiated with your
            selected coding agent, and a git worktree and corresponding task
            branch are created.
          </label>
        </div>

        <div className="grid grid-cols-1 sm:grid-cols-2 gap-3 items-end">
          {/* Top Row: Executor Profile and Variant (spans 2 columns) */}
          {availableProfiles && (
            <div className="col-span-1 sm:col-span-2">
              <ExecutorProfileSelector
                profiles={availableProfiles}
                selectedProfile={selectedProfile}
                onProfileSelect={setSelectedProfile}
                showLabel={true}
              />
            </div>
          )}

          {/* Bottom Row: Base Branch and Start Button */}
          <div className="space-y-1">
            <Label className="text-sm font-medium">
              Base branch <span className="text-destructive">*</span>
            </Label>
            <BranchSelector
              branches={branches}
              selectedBranch={createAttemptBranch}
              onBranchSelect={setCreateAttemptBranch}
              placeholder="Select branch"
            />
          </div>

          <div className="space-y-1">
            <Label className="text-sm font-medium opacity-0">Start</Label>
            <Button
              onClick={handleCreateAttempt}
              disabled={
                !selectedProfile ||
                !createAttemptBranch ||
                isAttemptRunning ||
                isCreating
              }
              size="sm"
              className="w-full text-xs gap-2 justify-center bg-black text-white hover:bg-black/90"
              title={
                !createAttemptBranch
                  ? 'Base branch is required'
                  : !selectedProfile
                    ? 'Coding agent is required'
                    : undefined
              }
            >
              {isCreating ? 'Creating...' : 'Start'}
            </Button>
          </div>
        </div>
      </div>
    </div>
  );
}

export default CreateAttempt;
</file>

<file path="frontend/src/components/tasks/Toolbar/CurrentAttempt.tsx">
import {
  ExternalLink,
  GitBranch as GitBranchIcon,
  GitFork,
  GitPullRequest,
  History,
  Play,
  Plus,
  RefreshCw,
  ScrollText,
  Settings,
  StopCircle,
} from 'lucide-react';
import {
  Tooltip,
  TooltipContent,
  TooltipProvider,
  TooltipTrigger,
} from '@/components/ui/tooltip.tsx';
import { Button } from '@/components/ui/button.tsx';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu.tsx';
import {
  Dispatch,
  SetStateAction,
  useCallback,
  useMemo,
  useRef,
  useState,
  useEffect,
} from 'react';
import type {
  GitBranch,
  TaskAttempt,
  TaskWithAttemptStatus,
} from 'shared/types';
import { useBranchStatus, useOpenInEditor } from '@/hooks';
import { useAttemptExecution } from '@/hooks/useAttemptExecution';
import { useDevServer } from '@/hooks/useDevServer';
import { useRebase } from '@/hooks/useRebase';
import { useMerge } from '@/hooks/useMerge';
import NiceModal from '@ebay/nice-modal-react';
import { Err } from '@/lib/api';
import type { GitOperationError } from 'shared/types';
import { displayConflictOpLabel } from '@/lib/conflicts';
import { usePush } from '@/hooks/usePush';
import { useUserSystem } from '@/components/config-provider.tsx';
import { useKeyboardShortcuts } from '@/lib/keyboard-shortcuts.ts';
import { writeClipboardViaBridge } from '@/vscode/bridge';
import { useProcessSelection } from '@/contexts/ProcessSelectionContext';
import { openTaskForm } from '@/lib/openTaskForm';
import { showModal } from '@/lib/modals';

// Helper function to get the display name for different editor types
function getEditorDisplayName(editorType: string): string {
  switch (editorType) {
    case 'VS_CODE':
      return 'Visual Studio Code';
    case 'CURSOR':
      return 'Cursor';
    case 'WINDSURF':
      return 'Windsurf';
    case 'INTELLI_J':
      return 'IntelliJ IDEA';
    case 'ZED':
      return 'Zed';
    case 'XCODE':
      return 'Xcode';
    case 'CUSTOM':
      return 'Editor';
    default:
      return 'Editor';
  }
}

type Props = {
  task: TaskWithAttemptStatus;
  projectId: string;
  projectHasDevScript: boolean;
  setError: Dispatch<SetStateAction<string | null>>;

  selectedBranch: string | null;
  selectedAttempt: TaskAttempt;
  taskAttempts: TaskAttempt[];
  creatingPR: boolean;
  handleEnterCreateAttemptMode: () => void;
  branches: GitBranch[];
  setSelectedAttempt: (attempt: TaskAttempt | null) => void;
};

function CurrentAttempt({
  task,
  projectId,
  projectHasDevScript,
  setError,
  selectedBranch,
  selectedAttempt,
  taskAttempts,
  creatingPR,
  handleEnterCreateAttemptMode,
  branches,
  setSelectedAttempt,
}: Props) {
  const { config } = useUserSystem();
  const { isAttemptRunning, stopExecution, isStopping } = useAttemptExecution(
    selectedAttempt?.id,
    task.id
  );
  const { data: branchStatus, refetch: refetchBranchStatus } = useBranchStatus(
    selectedAttempt?.id
  );
  const hasConflicts = useMemo(
    () => Boolean((branchStatus?.conflicted_files?.length ?? 0) > 0),
    [branchStatus?.conflicted_files]
  );
  const conflictOpLabel = useMemo(
    () => displayConflictOpLabel(branchStatus?.conflict_op),
    [branchStatus?.conflict_op]
  );
  const handleOpenInEditor = useOpenInEditor(selectedAttempt);
  const { jumpToProcess } = useProcessSelection();

  // Attempt action hooks
  const {
    start: startDevServer,
    stop: stopDevServer,
    isStarting: isStartingDevServer,
    runningDevServer,
    latestDevServerProcess,
  } = useDevServer(selectedAttempt?.id);
  const rebaseMutation = useRebase(selectedAttempt?.id, projectId);
  const mergeMutation = useMerge(selectedAttempt?.id);
  const pushMutation = usePush(selectedAttempt?.id);

  const [merging, setMerging] = useState(false);
  const [pushing, setPushing] = useState(false);
  const [rebasing, setRebasing] = useState(false);
  const [copied, setCopied] = useState(false);
  const [mergeSuccess, setMergeSuccess] = useState(false);
  const [pushSuccess, setPushSuccess] = useState(false);

  const handleViewDevServerLogs = () => {
    if (latestDevServerProcess) {
      jumpToProcess(latestDevServerProcess.id);
    }
  };

  const handleCreateSubtaskClick = () => {
    openTaskForm({
      projectId,
      initialBaseBranch: selectedAttempt.branch || selectedAttempt.base_branch,
      parentTaskAttemptId: selectedAttempt.id,
    });
  };

  // Use the stopExecution function from the hook

  useKeyboardShortcuts({
    stopExecution: async () => {
      try {
        const result = await showModal<'confirmed' | 'canceled'>(
          'stop-execution-confirm',
          {
            title: 'Stop Current Attempt?',
            message:
              'Are you sure you want to stop the current execution? This action cannot be undone.',
            isExecuting: isStopping,
          }
        );

        if (result === 'confirmed') {
          stopExecution();
        }
      } catch (error) {
        // User cancelled - do nothing
      }
    },
    newAttempt: !isAttemptRunning ? handleEnterCreateAttemptMode : () => {},
    hasOpenDialog: false,
    closeDialog: () => {},
    onEnter: () => {},
  });

  const handleAttemptChange = useCallback(
    (attempt: TaskAttempt) => {
      setSelectedAttempt(attempt);
      // React Query will handle refetching when attemptId changes
    },
    [setSelectedAttempt]
  );

  const handleMergeClick = async () => {
    if (!projectId || !selectedAttempt?.id || !selectedAttempt?.task_id) return;

    // Directly perform merge without checking branch status
    await performMerge();
  };

  const handlePushClick = async () => {
    try {
      setPushing(true);
      await pushMutation.mutateAsync();
      setError(null); // Clear any previous errors on success
      setPushSuccess(true);
      setTimeout(() => setPushSuccess(false), 2000);
    } catch (error: any) {
      setError(error.message || 'Failed to push changes');
    } finally {
      setPushing(false);
    }
  };

  const performMerge = async () => {
    try {
      setMerging(true);
      await mergeMutation.mutateAsync();
      setError(null); // Clear any previous errors on success
      setMergeSuccess(true);
      setTimeout(() => setMergeSuccess(false), 2000);
    } catch (error) {
      // @ts-expect-error it is type ApiError
      setError(error.message || 'Failed to merge changes');
    } finally {
      setMerging(false);
    }
  };

  const handleRebaseClick = async () => {
    setRebasing(true);
    await rebaseMutation
      .mutateAsync(undefined)
      .then(() => setError(null))
      .catch((err: Err<GitOperationError>) => {
        const data = err?.error;
        const isConflict =
          data?.type === 'merge_conflicts' ||
          data?.type === 'rebase_in_progress';
        if (!isConflict) setError(err.message || 'Failed to rebase branch');
      });
    setRebasing(false);
  };

  const handleRebaseWithNewBranch = async (newBaseBranch: string) => {
    setRebasing(true);
    await rebaseMutation
      .mutateAsync(newBaseBranch)
      .then(() => setError(null))
      .catch((err: Err<GitOperationError>) => {
        const data = err?.error;
        const isConflict =
          data?.type === 'merge_conflicts' ||
          data?.type === 'rebase_in_progress';
        if (!isConflict) setError(err.message || 'Failed to rebase branch');
      });
    setRebasing(false);
  };

  const handleRebaseDialogOpen = async () => {
    try {
      const result = await showModal<{
        action: 'confirmed' | 'canceled';
        branchName?: string;
      }>('rebase-dialog', {
        branches,
        isRebasing: rebasing,
      });

      if (result.action === 'confirmed' && result.branchName) {
        await handleRebaseWithNewBranch(result.branchName);
      }
    } catch (error) {
      // User cancelled - do nothing
    }
  };

  const handlePRButtonClick = async () => {
    if (!projectId || !selectedAttempt?.id || !selectedAttempt?.task_id) return;

    // If PR already exists, push to it
    if (mergeInfo.hasOpenPR) {
      await handlePushClick();
      return;
    }

    NiceModal.show('create-pr', {
      attempt: selectedAttempt,
      task,
      projectId,
    });
  };

  // Refresh branch status when a process completes (e.g., rebase resolved by agent)
  const prevRunningRef = useRef<boolean>(isAttemptRunning);
  useEffect(() => {
    if (prevRunningRef.current && !isAttemptRunning && selectedAttempt?.id) {
      refetchBranchStatus();
    }
    prevRunningRef.current = isAttemptRunning;
  }, [isAttemptRunning, selectedAttempt?.id, refetchBranchStatus]);

  // Get display name for selected branch
  const selectedBranchDisplayName = useMemo(() => {
    if (!selectedBranch) return 'current';

    // For remote branches, show just the branch name without the remote prefix
    if (selectedBranch.includes('/')) {
      const parts = selectedBranch.split('/');
      return parts[parts.length - 1];
    }
    return selectedBranch;
  }, [selectedBranch]);

  // Get display name for the configured editor
  const editorDisplayName = useMemo(() => {
    if (!config?.editor?.editor_type) return 'Editor';
    return getEditorDisplayName(config.editor.editor_type);
  }, [config?.editor?.editor_type]);

  // Memoize merge status information to avoid repeated calculations
  const mergeInfo = useMemo(() => {
    if (!branchStatus?.merges)
      return {
        hasOpenPR: false,
        openPR: null,
        hasMergedPR: false,
        mergedPR: null,
        hasMerged: false,
        latestMerge: null,
      };

    const openPR = branchStatus.merges.find(
      (m) => m.type === 'pr' && m.pr_info.status === 'open'
    );

    const mergedPR = branchStatus.merges.find(
      (m) => m.type === 'pr' && m.pr_info.status === 'merged'
    );

    const merges = branchStatus.merges.filter(
      (m) =>
        m.type === 'direct' ||
        (m.type === 'pr' && m.pr_info.status === 'merged')
    );

    return {
      hasOpenPR: !!openPR,
      openPR,
      hasMergedPR: !!mergedPR,
      mergedPR,
      hasMerged: merges.length > 0,
      latestMerge: branchStatus.merges[0] || null, // Most recent merge
    };
  }, [branchStatus?.merges]);

  const handleCopyWorktreePath = useCallback(async () => {
    try {
      await writeClipboardViaBridge(selectedAttempt.container_ref || '');
      setCopied(true);
      setTimeout(() => setCopied(false), 2000);
    } catch (err) {
      console.error('Failed to copy worktree path:', err);
    }
  }, [selectedAttempt.container_ref]);

  // Get status information for display
  const getStatusInfo = useCallback(() => {
    if (hasConflicts) {
      return {
        dotColor: 'bg-orange-500',
        textColor: 'text-orange-700',
        text: `${conflictOpLabel} conflicts`,
        isClickable: false,
      } as const;
    }
    if (branchStatus?.is_rebase_in_progress) {
      return {
        dotColor: 'bg-orange-500',
        textColor: 'text-orange-700',
        text: 'Rebase in progress',
        isClickable: false,
      } as const;
    }
    if (mergeInfo.hasMergedPR && mergeInfo.mergedPR?.type === 'pr') {
      const prMerge = mergeInfo.mergedPR;
      return {
        dotColor: 'bg-green-500',
        textColor: 'text-green-700',
        text: `PR #${prMerge.pr_info.number} merged`,
        isClickable: true,
        onClick: () => window.open(prMerge.pr_info.url, '_blank'),
      };
    }
    if (
      mergeInfo.hasMerged &&
      mergeInfo.latestMerge?.type === 'direct' &&
      (branchStatus?.commits_ahead ?? 0) === 0
    ) {
      return {
        dotColor: 'bg-green-500',
        textColor: 'text-green-700',
        text: `Merged`,
        isClickable: false,
      };
    }

    if (mergeInfo.hasOpenPR && mergeInfo.openPR?.type === 'pr') {
      const prMerge = mergeInfo.openPR;
      return {
        dotColor: 'bg-blue-500',
        textColor: 'text-blue-700 dark:text-blue-400',
        text: `PR #${prMerge.pr_info.number}`,
        isClickable: true,
        onClick: () => window.open(prMerge.pr_info.url, '_blank'),
      };
    }

    if ((branchStatus?.commits_behind ?? 0) > 0) {
      return {
        dotColor: 'bg-orange-500',
        textColor: 'text-orange-700',
        text: `Rebase needed${branchStatus?.has_uncommitted_changes ? ' (dirty)' : ''}`,
        isClickable: false,
      };
    }

    if ((branchStatus?.commits_ahead ?? 0) > 0) {
      return {
        dotColor: 'bg-yellow-500',
        textColor: 'text-yellow-700',
        text:
          branchStatus?.commits_ahead === 1
            ? `1 commit ahead${branchStatus?.has_uncommitted_changes ? ' (dirty)' : ''}`
            : `${branchStatus?.commits_ahead} commits ahead${branchStatus?.has_uncommitted_changes ? ' (dirty)' : ''}`,
        isClickable: false,
      };
    }

    return {
      dotColor: 'bg-gray-500',
      textColor: 'text-gray-700',
      text: `Up to date${branchStatus?.has_uncommitted_changes ? ' (dirty)' : ''}`,
      isClickable: false,
    };
  }, [mergeInfo, branchStatus]);

  return (
    <div className="space-y-2 @container">
      {/* <div className="flex gap-6 items-start"> */}
      <div className="grid grid-cols-2 gap-3 items-start @md:flex @md:items-start">
        <div className="min-w-0">
          <div className="text-xs font-medium text-muted-foreground uppercase tracking-wide mb-1">
            Agent
          </div>
          <div className="text-sm font-medium">{selectedAttempt.executor}</div>
        </div>

        <div className="min-w-0">
          <div className="text-xs font-medium text-muted-foreground uppercase tracking-wide mb-1">
            Task Branch
          </div>
          <div className="flex items-center gap-1.5">
            <GitBranchIcon className="h-3 w-3 text-muted-foreground" />
            <span className="text-sm font-medium truncate">
              {selectedAttempt.branch}
            </span>
          </div>
        </div>

        <div className="min-w-0">
          <div className="flex items-center gap-1.5 text-xs font-medium text-muted-foreground uppercase tracking-wide mb-1">
            <span className="truncate">Base Branch</span>
            <TooltipProvider>
              <Tooltip>
                <TooltipTrigger asChild>
                  <Button
                    variant="ghost"
                    size="xs"
                    onClick={handleRebaseDialogOpen}
                    disabled={rebasing || isAttemptRunning || hasConflicts}
                    className="h-4 w-4 p-0 hover:bg-muted"
                  >
                    <Settings className="h-3 w-3" />
                  </Button>
                </TooltipTrigger>
                <TooltipContent>
                  <p>Change base branch</p>
                </TooltipContent>
              </Tooltip>
            </TooltipProvider>
          </div>
          <div className="flex items-center gap-1.5">
            <GitBranchIcon className="h-3 w-3 text-muted-foreground" />
            <span className="text-sm font-medium truncate">
              {branchStatus?.base_branch_name || selectedBranchDisplayName}
            </span>
          </div>
        </div>

        <div className="min-w-0">
          <div className="text-xs font-medium text-muted-foreground uppercase tracking-wide mb-1">
            Status
          </div>
          <div className="flex items-center gap-1.5">
            {(() => {
              const statusInfo = getStatusInfo();
              return (
                <>
                  <div
                    className={`h-2 w-2 ${statusInfo.dotColor} rounded-full`}
                  />
                  {statusInfo.isClickable ? (
                    <button
                      onClick={statusInfo.onClick}
                      className={`text-sm font-medium ${statusInfo.textColor} hover:underline cursor-pointer`}
                    >
                      {statusInfo.text}
                    </button>
                  ) : (
                    <span
                      className={`text-sm font-medium ${statusInfo.textColor} truncate`}
                      title={statusInfo.text}
                    >
                      {statusInfo.text}
                    </span>
                  )}
                </>
              );
            })()}
          </div>
        </div>
      </div>

      <div>
        <div className="flex items-center gap-1.5 mb-1">
          <div className="text-xs font-medium text-muted-foreground uppercase tracking-wide mb-1 pt-1">
            Path
          </div>
          <Button
            variant="ghost"
            size="xs"
            onClick={() => handleOpenInEditor()}
            className="h-6 px-2 text-xs hover:bg-muted gap-1"
          >
            <ExternalLink className="h-3 w-3" />
            Open in {editorDisplayName}
          </Button>
        </div>
        <div
          className={`text-xs font-mono px-2 py-1 break-all cursor-pointer transition-all duration-300 flex items-center gap-2 ${
            copied
              ? 'bg-green-100 text-green-800 border border-green-300'
              : 'text-muted-foreground bg-muted hover:bg-muted/80'
          }`}
          onClick={handleCopyWorktreePath}
          title={copied ? 'Copied!' : 'Click to copy worktree path'}
        >
          <span
            className={`truncate ${copied ? 'text-green-800' : ''}`}
            dir="rtl"
          >
            {selectedAttempt.container_ref}
          </span>
          {copied && (
            <span className="text-green-700 font-medium whitespace-nowrap">
              Copied!
            </span>
          )}
        </div>
      </div>

      <div>
        <div className="grid grid-cols-2 gap-3 @md:flex @md:flex-wrap @md:items-center">
          <div className="flex gap-2 @md:flex-none">
            <Button
              variant={runningDevServer ? 'destructive' : 'outline'}
              size="xs"
              onClick={() =>
                runningDevServer ? stopDevServer() : startDevServer()
              }
              disabled={
                isStartingDevServer || !projectHasDevScript || hasConflicts
              }
              className="gap-1 flex-1"
            >
              {runningDevServer ? (
                <>
                  <StopCircle className="h-3 w-3" />
                  Stop Dev
                </>
              ) : (
                <>
                  <Play className="h-3 w-3" />
                  Dev
                </>
              )}
            </Button>

            {/* View Dev Server Logs Button */}
            {latestDevServerProcess && (
              <TooltipProvider>
                <Tooltip>
                  <TooltipTrigger asChild>
                    <Button
                      variant="outline"
                      size="xs"
                      onClick={handleViewDevServerLogs}
                      className="gap-1"
                    >
                      <ScrollText className="h-3 w-3" />
                    </Button>
                  </TooltipTrigger>
                  <TooltipContent>
                    <p>View dev server logs</p>
                  </TooltipContent>
                </Tooltip>
              </TooltipProvider>
            )}
          </div>
          {/* Git Operations */}
          {selectedAttempt && branchStatus && !mergeInfo.hasMergedPR && (
            <>
              {(branchStatus.commits_behind ?? 0) > 0 && (
                <Button
                  onClick={handleRebaseClick}
                  disabled={rebasing || isAttemptRunning || hasConflicts}
                  variant="outline"
                  size="xs"
                  className="border-orange-300 text-orange-700 hover:bg-orange-50 gap-1"
                >
                  <RefreshCw
                    className={`h-3 w-3 ${rebasing ? 'animate-spin' : ''}`}
                  />
                  {rebasing ? 'Rebasing...' : `Rebase`}
                </Button>
              )}
              <>
                <Button
                  onClick={handlePRButtonClick}
                  disabled={
                    creatingPR ||
                    pushing ||
                    Boolean((branchStatus.commits_behind ?? 0) > 0) ||
                    isAttemptRunning ||
                    hasConflicts ||
                    (mergeInfo.hasOpenPR &&
                      branchStatus.remote_commits_ahead === 0) ||
                    ((branchStatus.commits_ahead ?? 0) === 0 &&
                      (branchStatus.remote_commits_ahead ?? 0) === 0 &&
                      !pushSuccess &&
                      !mergeSuccess)
                  }
                  variant="outline"
                  size="xs"
                  className="border-blue-300  dark:border-blue-700 text-blue-700 dark:text-blue-500 hover:bg-blue-50 dark:hover:bg-transparent dark:hover:text-blue-400 dark:hover:border-blue-400 gap-1 min-w-[120px]"
                >
                  <GitPullRequest className="h-3 w-3" />
                  {mergeInfo.hasOpenPR
                    ? pushSuccess
                      ? 'Pushed!'
                      : pushing
                        ? 'Pushing...'
                        : branchStatus.remote_commits_ahead === 0
                          ? 'Push to PR'
                          : branchStatus.remote_commits_ahead === 1
                            ? 'Push 1 commit'
                            : `Push ${branchStatus.remote_commits_ahead || 0} commits`
                    : creatingPR
                      ? 'Creating...'
                      : 'Create PR'}
                </Button>
                <Button
                  onClick={handleMergeClick}
                  disabled={
                    mergeInfo.hasOpenPR ||
                    merging ||
                    hasConflicts ||
                    Boolean((branchStatus.commits_behind ?? 0) > 0) ||
                    isAttemptRunning ||
                    ((branchStatus.commits_ahead ?? 0) === 0 &&
                      !pushSuccess &&
                      !mergeSuccess)
                  }
                  size="xs"
                  className="bg-green-600 hover:bg-green-700 dark:bg-green-900 dark:hover:bg-green-700 gap-1 min-w-[120px]"
                >
                  <GitBranchIcon className="h-3 w-3" />
                  {mergeSuccess ? 'Merged!' : merging ? 'Merging...' : 'Merge'}
                </Button>
              </>
            </>
          )}

          <div className="flex gap-2 @md:flex-none">
            {isStopping || isAttemptRunning ? (
              <Button
                variant="destructive"
                size="xs"
                onClick={stopExecution}
                disabled={isStopping}
                className="gap-1 flex-1"
              >
                <StopCircle className="h-4 w-4" />
                {isStopping ? 'Stopping...' : 'Stop Attempt'}
              </Button>
            ) : (
              <Button
                variant="outline"
                size="xs"
                onClick={handleEnterCreateAttemptMode}
                className="gap-1 flex-1"
              >
                <Plus className="h-4 w-4" />
                New Attempt
              </Button>
            )}
            {taskAttempts.length > 1 && (
              <DropdownMenu>
                <TooltipProvider>
                  <Tooltip>
                    <TooltipTrigger asChild>
                      <DropdownMenuTrigger asChild>
                        <Button variant="outline" size="xs" className="gap-1">
                          <History className="h-3 w-4" />
                        </Button>
                      </DropdownMenuTrigger>
                    </TooltipTrigger>
                    <TooltipContent>
                      <p>View attempt history</p>
                    </TooltipContent>
                  </Tooltip>
                </TooltipProvider>
                <DropdownMenuContent align="start" className="w-64">
                  {taskAttempts.map((attempt) => (
                    <DropdownMenuItem
                      key={attempt.id}
                      onClick={() => handleAttemptChange(attempt)}
                      className={
                        selectedAttempt?.id === attempt.id ? 'bg-accent' : ''
                      }
                    >
                      <div className="flex flex-col w-full">
                        <span className="font-medium text-sm">
                          {new Date(attempt.created_at).toLocaleDateString()}{' '}
                          {new Date(attempt.created_at).toLocaleTimeString()}
                        </span>
                        <span className="text-xs text-muted-foreground">
                          {attempt.executor || 'Base Agent'}
                        </span>
                      </div>
                    </DropdownMenuItem>
                  ))}
                </DropdownMenuContent>
              </DropdownMenu>
            )}
          </div>
          <Button
            onClick={handleCreateSubtaskClick}
            variant="outline"
            size="xs"
            className="gap-1 min-w-[120px]"
          >
            <GitFork className="h-3 w-3" />
            Create Subtask
          </Button>
        </div>
      </div>
    </div>
  );
}

export default CurrentAttempt;
</file>

<file path="frontend/src/components/tasks/AttemptHeaderCard.tsx">
import { Card } from '../ui/card';
import { Button } from '../ui/button';
import { MoreHorizontal } from 'lucide-react';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '../ui/dropdown-menu';
import type { TaskAttempt, TaskWithAttemptStatus } from 'shared/types';
import { useDevServer } from '@/hooks/useDevServer';
import { useRebase } from '@/hooks/useRebase';
import { useMerge } from '@/hooks/useMerge';
import { useOpenInEditor } from '@/hooks/useOpenInEditor';
import { useDiffSummary } from '@/hooks/useDiffSummary';
import { useBranchStatus } from '@/hooks';
import { useAttemptExecution } from '@/hooks/useAttemptExecution';
import { useMemo, useState } from 'react';
import NiceModal from '@ebay/nice-modal-react';

interface AttemptHeaderCardProps {
  attemptNumber: number;
  totalAttempts: number;
  selectedAttempt: TaskAttempt | null;
  task: TaskWithAttemptStatus;
  projectId: string;
  // onCreateNewAttempt?: () => void;
  onJumpToDiffFullScreen?: () => void;
}

export function AttemptHeaderCard({
  attemptNumber,
  totalAttempts,
  selectedAttempt,
  task,
  projectId,
  onJumpToDiffFullScreen,
}: AttemptHeaderCardProps) {
  const {
    start: startDevServer,
    stop: stopDevServer,
    runningDevServer,
  } = useDevServer(selectedAttempt?.id);
  const rebaseMutation = useRebase(selectedAttempt?.id, projectId);
  const mergeMutation = useMerge(selectedAttempt?.id);
  const openInEditor = useOpenInEditor(selectedAttempt);
  const { fileCount, added, deleted } = useDiffSummary(
    selectedAttempt?.id ?? null
  );

  // Branch status and execution state
  const { data: branchStatus } = useBranchStatus(selectedAttempt?.id);
  const { isAttemptRunning } = useAttemptExecution(
    selectedAttempt?.id,
    task.id
  );

  // Loading states
  const [rebasing, setRebasing] = useState(false);
  const [merging, setMerging] = useState(false);

  // Check for conflicts
  const hasConflicts = useMemo(
    () => Boolean((branchStatus?.conflicted_files?.length ?? 0) > 0),
    [branchStatus?.conflicted_files]
  );

  // Merge status information
  const mergeInfo = useMemo(() => {
    if (!branchStatus?.merges)
      return {
        hasOpenPR: false,
        openPR: null,
        hasMergedPR: false,
        mergedPR: null,
        hasMerged: false,
      };

    const openPR = branchStatus.merges.find(
      (m) => m.type === 'pr' && m.pr_info.status === 'open'
    );

    const mergedPR = branchStatus.merges.find(
      (m) => m.type === 'pr' && m.pr_info.status === 'merged'
    );

    const merges = branchStatus.merges.filter(
      (m) =>
        m.type === 'direct' ||
        (m.type === 'pr' && m.pr_info.status === 'merged')
    );

    return {
      hasOpenPR: !!openPR,
      openPR,
      hasMergedPR: !!mergedPR,
      mergedPR,
      hasMerged: merges.length > 0,
    };
  }, [branchStatus?.merges]);

  const handleCreatePR = () => {
    if (selectedAttempt) {
      NiceModal.show('create-pr', {
        attempt: selectedAttempt,
        task,
        projectId,
      });
    }
  };

  const handleRebaseClick = async () => {
    setRebasing(true);
    try {
      await rebaseMutation.mutateAsync(undefined);
    } catch (error) {
      // Error handling is done by the mutation
    } finally {
      setRebasing(false);
    }
  };

  const handleMergeClick = async () => {
    setMerging(true);
    try {
      await mergeMutation.mutateAsync();
    } catch (error) {
      // Error handling is done by the mutation
    } finally {
      setMerging(false);
    }
  };

  return (
    <Card className="border-b border-dashed bg-background flex items-center text-sm">
      <div className="flex-1 flex gap-6 p-3 flex-wrap md:flex-nowrap">
        <p>
          <span className="text-secondary-foreground">Attempt &middot; </span>
          {attemptNumber}/{totalAttempts}
        </p>
        <p>
          <span className="text-secondary-foreground">Agent &middot; </span>
          {selectedAttempt?.executor}
        </p>
        {selectedAttempt?.branch && (
          <p className="max-w-30 truncate">
            <span className="text-secondary-foreground">Branch &middot; </span>
            {selectedAttempt.branch}
          </p>
        )}
        {fileCount > 0 && (
          <p className="text-secondary-foreground">
            <Button
              variant="ghost"
              size="sm"
              className="h-4 p-0"
              onClick={onJumpToDiffFullScreen}
            >
              Diffs
            </Button>{' '}
            &middot; <span className="text-success">+{added}</span>{' '}
            <span className="text-destructive">-{deleted}</span>
          </p>
        )}
      </div>
      <DropdownMenu>
        <DropdownMenuTrigger asChild>
          <Button variant="ghost" size="sm" className="h-10 w-10 p-0 mr-3">
            <MoreHorizontal className="h-4 w-4" />
            <span className="sr-only">Open menu</span>
          </Button>
        </DropdownMenuTrigger>
        <DropdownMenuContent align="end">
          <DropdownMenuItem
            onClick={() => openInEditor()}
            disabled={!selectedAttempt}
          >
            Open in IDE
          </DropdownMenuItem>
          <DropdownMenuItem
            onClick={() =>
              runningDevServer ? stopDevServer() : startDevServer()
            }
            disabled={!selectedAttempt}
            className={runningDevServer ? 'text-destructive' : ''}
          >
            {runningDevServer ? 'Stop dev server' : 'Start dev server'}
          </DropdownMenuItem>
          {selectedAttempt &&
            branchStatus &&
            !mergeInfo.hasMergedPR &&
            (branchStatus.commits_behind ?? 0) > 0 && (
              <DropdownMenuItem
                onClick={handleRebaseClick}
                disabled={rebasing || isAttemptRunning || hasConflicts}
              >
                {rebasing ? 'Rebasing...' : 'Rebase'}
              </DropdownMenuItem>
            )}
          <DropdownMenuItem
            onClick={handleCreatePR}
            disabled={!selectedAttempt}
          >
            Create PR
          </DropdownMenuItem>
          {selectedAttempt && branchStatus && !mergeInfo.hasMergedPR && (
            <DropdownMenuItem
              onClick={handleMergeClick}
              disabled={
                mergeInfo.hasOpenPR ||
                merging ||
                hasConflicts ||
                Boolean((branchStatus.commits_behind ?? 0) > 0) ||
                isAttemptRunning ||
                (branchStatus.commits_ahead ?? 0) === 0
              }
            >
              {merging ? 'Merging...' : 'Merge'}
            </DropdownMenuItem>
          )}
          {/* <DropdownMenuItem
            onClick={onCreateNewAttempt}
            disabled={!onCreateNewAttempt}
          >
            Create new attempt
          </DropdownMenuItem> */}
        </DropdownMenuContent>
      </DropdownMenu>
    </Card>
  );
}
</file>

<file path="frontend/src/components/tasks/BranchSelector.tsx">
import { useState, useMemo, useRef, useEffect } from 'react';
import { Button } from '@/components/ui/button.tsx';
import { ArrowDown, GitBranch as GitBranchIcon, Search } from 'lucide-react';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuSeparator,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu.tsx';
import {
  Tooltip,
  TooltipContent,
  TooltipProvider,
  TooltipTrigger,
} from '@/components/ui/tooltip.tsx';
import { Input } from '@/components/ui/input.tsx';
import type { GitBranch } from 'shared/types';

type Props = {
  branches: GitBranch[];
  selectedBranch: string | null;
  onBranchSelect: (branch: string) => void;
  placeholder?: string;
  className?: string;
  excludeCurrentBranch?: boolean;
};

function BranchSelector({
  branches,
  selectedBranch,
  onBranchSelect,
  placeholder = 'Select a branch',
  className = '',
  excludeCurrentBranch = false,
}: Props) {
  const [branchSearchTerm, setBranchSearchTerm] = useState('');
  const [highlighted, setHighlighted] = useState<number | null>(null);
  const [open, setOpen] = useState(false);
  const searchInputRef = useRef<HTMLInputElement>(null);
  const itemRefs = useRef<Array<HTMLDivElement | null>>([]);

  // Filter branches based on search term and options
  const filteredBranches = useMemo(() => {
    let filtered = branches;

    // Don't filter out current branch, we'll handle it in the UI
    if (branchSearchTerm.trim()) {
      filtered = filtered.filter((branch) =>
        branch.name.toLowerCase().includes(branchSearchTerm.toLowerCase())
      );
    }

    return filtered;
  }, [branches, branchSearchTerm]);

  const displayName = useMemo(() => {
    if (!selectedBranch) return placeholder;

    // For remote branches, show just the branch name without the remote prefix
    if (selectedBranch.includes('/')) {
      const parts = selectedBranch.split('/');
      return parts[parts.length - 1];
    }
    return selectedBranch;
  }, [selectedBranch, placeholder]);

  const handleBranchSelect = (branchName: string) => {
    onBranchSelect(branchName);
    setBranchSearchTerm('');
    setHighlighted(null);
    setOpen(false);
  };

  const moveHighlight = (delta: 1 | -1) => {
    if (filteredBranches.length === 0) return;

    setHighlighted((prev) => {
      const next =
        prev === null
          ? delta === 1
            ? 0
            : filteredBranches.length - 1
          : (prev + delta + filteredBranches.length) % filteredBranches.length;

      // Focus the matching item for scroll behavior
      setTimeout(
        () => itemRefs.current[next]?.scrollIntoView({ block: 'nearest' }),
        0
      );
      return next;
    });
  };

  // Reset highlight when filtered branches change
  useEffect(() => {
    if (highlighted !== null && highlighted >= filteredBranches.length) {
      setHighlighted(null);
    }
  }, [filteredBranches, highlighted]);

  // Reset highlight when search changes
  useEffect(() => {
    setHighlighted(null);
  }, [branchSearchTerm]);

  return (
    <DropdownMenu open={open} onOpenChange={setOpen}>
      <DropdownMenuTrigger asChild>
        <Button
          variant="outline"
          size="sm"
          className={`w-full justify-between text-xs ${className}`}
        >
          <div className="flex items-center gap-1.5 w-full">
            <GitBranchIcon className="h-3 w-3" />
            <span className="truncate">{displayName}</span>
          </div>
          <ArrowDown className="h-3 w-3" />
        </Button>
      </DropdownMenuTrigger>
      <DropdownMenuContent className="w-80">
        <div className="p-2">
          <div className="relative">
            <Search className="absolute left-2 top-2.5 h-4 w-4 text-muted-foreground" />
            <Input
              ref={searchInputRef}
              placeholder="Search branches..."
              value={branchSearchTerm}
              onChange={(e) => setBranchSearchTerm(e.target.value)}
              className="pl-8"
              onKeyDown={(e) => {
                // Handle keyboard navigation
                switch (e.key) {
                  case 'ArrowDown':
                    e.preventDefault();
                    e.stopPropagation();
                    moveHighlight(1);
                    break;
                  case 'ArrowUp':
                    e.preventDefault();
                    e.stopPropagation();
                    moveHighlight(-1);
                    break;
                  case 'Enter':
                    if (highlighted !== null && filteredBranches[highlighted]) {
                      e.preventDefault();
                      e.stopPropagation();
                      const branch = filteredBranches[highlighted];
                      const isCurrentAndExcluded =
                        excludeCurrentBranch && branch.is_current;
                      if (!isCurrentAndExcluded) {
                        handleBranchSelect(branch.name);
                      }
                    }
                    break;
                  case 'Escape':
                    e.preventDefault();
                    e.stopPropagation();
                    setOpen(false);
                    break;
                  default:
                    // Prevent dropdown from closing when typing
                    e.stopPropagation();
                }
              }}
              autoFocus
            />
          </div>
        </div>
        <DropdownMenuSeparator />
        <div className="max-h-64 overflow-y-auto">
          {filteredBranches.length === 0 ? (
            <div className="p-2 text-sm text-muted-foreground text-center">
              No branches found
            </div>
          ) : (
            filteredBranches.map((branch, idx) => {
              const isCurrentAndExcluded =
                excludeCurrentBranch && branch.is_current;
              const isHighlighted = idx === highlighted;

              const menuItem = (
                <DropdownMenuItem
                  key={branch.name}
                  ref={(el) => (itemRefs.current[idx] = el)}
                  onClick={() => {
                    if (!isCurrentAndExcluded) {
                      handleBranchSelect(branch.name);
                    }
                  }}
                  onMouseEnter={() => setHighlighted(idx)}
                  disabled={isCurrentAndExcluded}
                  className={`${selectedBranch === branch.name ? 'bg-accent' : ''} ${
                    isCurrentAndExcluded ? 'opacity-50 cursor-not-allowed' : ''
                  } ${isHighlighted ? 'bg-muted' : ''}`}
                >
                  <div className="flex items-center justify-between w-full">
                    <span className={branch.is_current ? 'font-medium' : ''}>
                      {branch.name}
                    </span>
                    <div className="flex gap-1">
                      {branch.is_current && (
                        <span className="text-xs bg-green-100 text-green-800 px-1 rounded">
                          current
                        </span>
                      )}
                      {branch.is_remote && (
                        <span className="text-xs bg-blue-100 text-blue-800 px-1 rounded">
                          remote
                        </span>
                      )}
                    </div>
                  </div>
                </DropdownMenuItem>
              );

              if (isCurrentAndExcluded) {
                return (
                  <TooltipProvider key={branch.name}>
                    <Tooltip>
                      <TooltipTrigger asChild>{menuItem}</TooltipTrigger>
                      <TooltipContent>
                        <p>Cannot rebase a branch onto itself</p>
                      </TooltipContent>
                    </Tooltip>
                  </TooltipProvider>
                );
              }

              return menuItem;
            })
          )}
        </div>
      </DropdownMenuContent>
    </DropdownMenu>
  );
}

export default BranchSelector;
</file>

<file path="frontend/src/components/tasks/ConflictBanner.tsx">
import { AlertCircle } from 'lucide-react';
import { Button } from '@/components/ui/button';
import type { ConflictOp } from 'shared/types';
import { displayConflictOpLabel } from '@/lib/conflicts';

export type Props = Readonly<{
  attemptBranch: string | null;
  baseBranch?: string;
  conflictedFiles: readonly string[];
  isEditable: boolean;
  onOpenEditor: () => void;
  onInsertInstructions: () => void;
  onAbort: () => void;
  op?: ConflictOp | null;
}>;

const MAX_VISIBLE_FILES = 8;

function getOperationTitle(op?: ConflictOp | null): {
  full: string;
  lower: string;
} {
  const title = displayConflictOpLabel(op);
  return { full: title, lower: title.toLowerCase() };
}

function getVisibleFiles(
  files: readonly string[],
  max = MAX_VISIBLE_FILES
): { visible: string[]; total: number; hasMore: boolean } {
  const visible = files.slice(0, max);
  return {
    visible,
    total: files.length,
    hasMore: files.length > visible.length,
  };
}

export function ConflictBanner({
  attemptBranch,
  baseBranch,
  conflictedFiles,
  isEditable,
  onOpenEditor,
  onInsertInstructions,
  onAbort,
  op,
}: Props) {
  const { full: opTitle, lower: opTitleLower } = getOperationTitle(op);
  const {
    visible: visibleFiles,
    total,
    hasMore,
  } = getVisibleFiles(conflictedFiles);

  const heading = attemptBranch
    ? `${opTitle} in progress: '${attemptBranch}' → '${baseBranch}'.`
    : 'A Git operation with merge conflicts is in progress.';

  return (
    <div
      className="flex flex-col gap-2 rounded-md border border-yellow-300 bg-yellow-50 p-3 text-yellow-900"
      role="status"
      aria-live="polite"
    >
      <div className="flex items-start gap-2">
        <AlertCircle className="mt-0.5 h-4 w-4 text-yellow-700" aria-hidden />
        <div className="text-sm leading-relaxed">
          <span>{heading}</span>{' '}
          <span>
            Follow-ups are allowed; some actions may be temporarily unavailable
            until you resolve the conflicts or abort the {opTitleLower}.
          </span>
          {visibleFiles.length > 0 && (
            <div className="mt-1 text-xs text-yellow-800">
              <div className="font-medium">
                Conflicted files ({visibleFiles.length}
                {hasMore ? ` of ${total}` : ''}):
              </div>
              <div className="mt-1 grid grid-cols-1 gap-0.5">
                {visibleFiles.map((f) => (
                  <div key={f} className="truncate">
                    {f}
                  </div>
                ))}
              </div>
            </div>
          )}
        </div>
      </div>

      <div className="flex flex-wrap gap-2">
        <Button
          size="sm"
          variant="outline"
          className="border-yellow-300 text-yellow-800 hover:bg-yellow-100"
          onClick={onOpenEditor}
        >
          Open in Editor
        </Button>

        <Button
          size="sm"
          variant="outline"
          className="border-yellow-300 text-yellow-800 hover:bg-yellow-100"
          onClick={onInsertInstructions}
          disabled={!isEditable}
          aria-disabled={!isEditable}
        >
          Insert Resolve-Conflicts Instructions
        </Button>

        <Button
          size="sm"
          variant="outline"
          className="border-red-300 text-red-700 hover:bg-red-50"
          onClick={onAbort}
        >
          Abort {opTitle}
        </Button>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/tasks/FollowUpStatusRow.tsx">
import { memo, useEffect, useRef, useState } from 'react';
import { CheckCircle2, Clock, Loader2, Send, WifiOff } from 'lucide-react';
import { cn } from '@/lib/utils';

export type SaveStatus = 'idle' | 'saving' | 'saved' | 'offline' | 'sent';

type Status = {
  save: { state: SaveStatus; isSaving: boolean };
  draft: { isLoaded: boolean; isSending: boolean };
  queue: { isUnqueuing: boolean; isQueued: boolean };
};

type Props = { status: Status };

function FollowUpStatusRowImpl({ status }: Props) {
  const { save, draft, queue } = status;

  // Nonce keys to retrigger CSS animation; no JS timers.
  const [savedNonce, setSavedNonce] = useState<number | null>(null);
  const [sentNonce, setSentNonce] = useState<number | null>(null);
  const prevIsSendingRef = useRef<boolean>(draft.isSending);

  // Show "Draft saved" by bumping key to restart CSS animation
  useEffect(() => {
    if (save.state === 'saved') setSavedNonce(Date.now());
  }, [save.state]);

  // Show "Follow-up sent" on isSending rising edge
  useEffect(() => {
    const now = draft.isSending;
    if (now && !prevIsSendingRef.current) {
      setSentNonce(Date.now());
    }
    prevIsSendingRef.current = now;
  }, [draft.isSending]);
  return (
    <div className="flex items-center justify-between text-xs min-h-6 h-6 px-0.5">
      <div className="text-muted-foreground">
        {save.state === 'saving' && save.isSaving ? (
          <span
            className={cn(
              'inline-flex items-center gap-1.5 rounded-md border px-2 py-0.5 bg-muted animate-in fade-in-0',
              'italic'
            )}
          >
            <Loader2 className="animate-spin h-3 w-3" /> Saving…
          </span>
        ) : save.state === 'offline' ? (
          <span className="inline-flex items-center gap-1.5 rounded-md border px-2 py-0.5 bg-muted text-amber-700 animate-in fade-in-0">
            <WifiOff className="h-3 w-3" /> Offline — changes pending
          </span>
        ) : sentNonce ? (
          <span
            key={sentNonce}
            className={cn(
              'inline-flex items-center gap-1.5 rounded-md border px-2 py-0.5 bg-muted text-emerald-700 animate-pill'
            )}
            onAnimationEnd={() => setSentNonce(null)}
          >
            <Send className="h-3 w-3" /> Follow-up sent
          </span>
        ) : savedNonce ? (
          <span
            key={savedNonce}
            className={cn(
              'inline-flex items-center gap-1.5 rounded-md border px-2 py-0.5 bg-muted text-emerald-700 animate-pill'
            )}
            onAnimationEnd={() => setSavedNonce(null)}
          >
            <CheckCircle2 className="h-3 w-3" /> Draft saved
          </span>
        ) : null}
      </div>
      <div className="text-muted-foreground">
        {queue.isUnqueuing ? (
          <span className="inline-flex items-center gap-1.5 rounded-md border px-2 py-0.5 bg-muted animate-in fade-in-0">
            <Loader2 className="animate-spin h-3 w-3" /> Unlocking…
          </span>
        ) : !draft.isLoaded ? (
          <span className="inline-flex items-center gap-1.5 rounded-md border px-2 py-0.5 bg-muted animate-in fade-in-0">
            <Loader2 className="animate-spin h-3 w-3" /> Loading draft…
          </span>
        ) : draft.isSending ? (
          <span className="inline-flex items-center gap-1.5 rounded-md border px-2 py-0.5 bg-muted animate-in fade-in-0">
            <Loader2 className="animate-spin h-3 w-3" /> Sending follow-up…
          </span>
        ) : queue.isQueued ? (
          <span className="inline-flex items-center gap-1.5 rounded-md border px-2 py-0.5 bg-muted animate-in fade-in-0">
            <Clock className="h-3 w-3" /> Queued for next turn. Edits are
            locked.
          </span>
        ) : null}
      </div>
    </div>
  );
}

export const FollowUpStatusRow = memo(FollowUpStatusRowImpl);
</file>

<file path="frontend/src/components/tasks/index.ts">
export { TaskCard } from './TaskCard';
</file>

<file path="frontend/src/components/tasks/TaskCard.tsx">
import { KeyboardEvent, useCallback, useEffect, useRef } from 'react';
import { Button } from '@/components/ui/button';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import { KanbanCard } from '@/components/ui/shadcn-io/kanban';
import {
  CheckCircle,
  Copy,
  Edit,
  Loader2,
  MoreHorizontal,
  Trash2,
  XCircle,
} from 'lucide-react';
import type { TaskWithAttemptStatus } from 'shared/types';

type Task = TaskWithAttemptStatus;

interface TaskCardProps {
  task: Task;
  index: number;
  status: string;
  onEdit: (task: Task) => void;
  onDelete: (taskId: string) => void;
  onDuplicate?: (task: Task) => void;
  onViewDetails: (task: Task) => void;
  isFocused: boolean;
  tabIndex?: number;
}

export function TaskCard({
  task,
  index,
  status,
  onEdit,
  onDelete,
  onDuplicate,
  onViewDetails,
  isFocused,
  tabIndex = -1,
}: TaskCardProps) {
  const localRef = useRef<HTMLDivElement>(null);
  useEffect(() => {
    if (isFocused && localRef.current) {
      localRef.current.scrollIntoView({ block: 'nearest', behavior: 'smooth' });
      localRef.current.focus();
    }
  }, [isFocused]);

  const handleKeyDown = useCallback(
    (e: KeyboardEvent) => {
      if (e.key === 'Backspace') {
        onDelete(task.id);
      } else if (e.key === 'Enter' || e.key === ' ') {
        onViewDetails(task);
      }
    },
    [task, onDelete, onViewDetails]
  );

  const handleClick = useCallback(() => {
    onViewDetails(task);
  }, [task, onViewDetails]);

  return (
    <KanbanCard
      key={task.id}
      id={task.id}
      name={task.title}
      index={index}
      parent={status}
      onClick={handleClick}
      tabIndex={tabIndex}
      forwardedRef={localRef}
      onKeyDown={handleKeyDown}
    >
      <div className="flex flex-1 gap-2 items-center min-w-0">
        <h4 className="flex-1 min-w-0 line-clamp-2 font-light text-sm">
          {task.title}
        </h4>
        <div className="flex items-center space-x-1">
          {/* In Progress Spinner */}
          {task.has_in_progress_attempt && (
            <Loader2 className="h-3 w-3 animate-spin text-blue-500" />
          )}
          {/* Merged Indicator */}
          {task.has_merged_attempt && (
            <CheckCircle className="h-3 w-3 text-green-500" />
          )}
          {/* Failed Indicator */}
          {task.last_attempt_failed && !task.has_merged_attempt && (
            <XCircle className="h-3 w-3 text-destructive" />
          )}
          {/* Actions Menu */}
          <div
            onPointerDown={(e) => e.stopPropagation()}
            onMouseDown={(e) => e.stopPropagation()}
            onClick={(e) => e.stopPropagation()}
            onKeyDown={(e) => e.stopPropagation()}
          >
            <DropdownMenu>
              <DropdownMenuTrigger asChild>
                <Button
                  variant="ghost"
                  size="sm"
                  className="h-6 w-6 p-0 hover:bg-muted"
                >
                  <MoreHorizontal className="h-3 w-3" />
                </Button>
              </DropdownMenuTrigger>
              <DropdownMenuContent align="end">
                <DropdownMenuItem onClick={() => onEdit(task)}>
                  <Edit className="h-4 w-4 mr-2" />
                  Edit
                </DropdownMenuItem>
                {onDuplicate && (
                  <DropdownMenuItem onClick={() => onDuplicate(task)}>
                    <Copy className="h-4 w-4 mr-2" />
                    Duplicate
                  </DropdownMenuItem>
                )}
                <DropdownMenuItem
                  onClick={() => onDelete(task.id)}
                  className="text-destructive"
                >
                  <Trash2 className="h-4 w-4 mr-2" />
                  Delete
                </DropdownMenuItem>
              </DropdownMenuContent>
            </DropdownMenu>
          </div>
        </div>
      </div>
      {task.description && (
        <p className="flex-1 text-sm text-secondary-foreground break-words">
          {task.description.length > 130
            ? `${task.description.substring(0, 130)}...`
            : task.description}
        </p>
      )}
    </KanbanCard>
  );
}
</file>

<file path="frontend/src/components/tasks/TaskDetailsHeader.tsx">
import { memo } from 'react';
import { Edit, Trash2, X, Maximize2, Minimize2 } from 'lucide-react';
import { Button } from '@/components/ui/button';
import {
  Tooltip,
  TooltipContent,
  TooltipProvider,
  TooltipTrigger,
} from '@/components/ui/tooltip';
import type { TaskWithAttemptStatus } from 'shared/types';
import { TaskTitleDescription } from './TaskDetails/TaskTitleDescription';
import { Card } from '../ui/card';
import { statusBoardColors, statusLabels } from '@/utils/status-labels';
import { useTaskViewManager } from '@/hooks/useTaskViewManager';

interface TaskDetailsHeaderProps {
  task: TaskWithAttemptStatus;
  onClose: () => void;
  onEditTask?: (task: TaskWithAttemptStatus) => void;
  onDeleteTask?: (taskId: string) => void;
  hideCloseButton?: boolean;
  isFullScreen?: boolean;
}

// backgroundColor: `hsl(var(${statusBoardColors[task.status]}) / 0.03)`,

function TaskDetailsHeader({
  task,
  onClose,
  onEditTask,
  onDeleteTask,
  hideCloseButton = false,
  isFullScreen,
}: TaskDetailsHeaderProps) {
  const { toggleFullscreen } = useTaskViewManager();
  return (
    <div>
      <Card
        className="flex shrink-0 items-center gap-2 border-b border-dashed bg-background"
        style={{}}
      >
        <div className="p-3 flex flex-1 items-center truncate">
          <div
            className="h-2 w-2 rounded-full inline-block"
            style={{
              backgroundColor: `hsl(var(${statusBoardColors[task.status]}))`,
            }}
          />
          <p className="ml-2 text-sm">{statusLabels[task.status]}</p>
        </div>
        <div className="mr-3">
          <TooltipProvider>
            <Tooltip>
              <TooltipTrigger asChild>
                <Button
                  variant="ghost"
                  size="icon"
                  onClick={() => toggleFullscreen(!isFullScreen)}
                  aria-label={
                    isFullScreen
                      ? 'Collapse to sidebar'
                      : 'Expand to fullscreen'
                  }
                >
                  {isFullScreen ? (
                    <Minimize2 className="h-4 w-4" />
                  ) : (
                    <Maximize2 className="h-4 w-4" />
                  )}
                </Button>
              </TooltipTrigger>
              <TooltipContent>
                <p>
                  {isFullScreen
                    ? 'Collapse to sidebar'
                    : 'Expand to fullscreen'}
                </p>
              </TooltipContent>
            </Tooltip>
          </TooltipProvider>
          {onEditTask && (
            <TooltipProvider>
              <Tooltip>
                <TooltipTrigger asChild>
                  <Button
                    variant="ghost"
                    size="icon"
                    onClick={() => onEditTask(task)}
                  >
                    <Edit className="h-4 w-4" />
                  </Button>
                </TooltipTrigger>
                <TooltipContent>
                  <p>Edit task</p>
                </TooltipContent>
              </Tooltip>
            </TooltipProvider>
          )}
          {onDeleteTask && (
            <TooltipProvider>
              <Tooltip>
                <TooltipTrigger asChild>
                  <Button
                    variant="ghost"
                    size="icon"
                    onClick={() => onDeleteTask(task.id)}
                  >
                    <Trash2 className="h-4 w-4 text-destructive" />
                  </Button>
                </TooltipTrigger>
                <TooltipContent>
                  <p>Delete task</p>
                </TooltipContent>
              </Tooltip>
            </TooltipProvider>
          )}
          {!hideCloseButton && (
            <TooltipProvider>
              <Tooltip>
                <TooltipTrigger asChild>
                  <Button variant="ghost" size="icon" onClick={onClose}>
                    <X className="h-4 w-4" />
                  </Button>
                </TooltipTrigger>
                <TooltipContent>
                  <p>Close panel</p>
                </TooltipContent>
              </Tooltip>
            </TooltipProvider>
          )}
        </div>
      </Card>

      {/* Title and Task Actions */}
      {!isFullScreen && (
        <div className="p-3 border-b border-dashed max-h-96 overflow-y-auto">
          <TaskTitleDescription task={task} />
        </div>
      )}
    </div>
  );
}

export default memo(TaskDetailsHeader);
</file>

<file path="frontend/src/components/tasks/TaskDetailsPanel.tsx">
import { useEffect, useState } from 'react';
import TaskDetailsHeader from './TaskDetailsHeader';
import { TaskFollowUpSection } from './TaskFollowUpSection';
import { TaskTitleDescription } from './TaskDetails/TaskTitleDescription';
import type { TaskAttempt } from 'shared/types';
import {
  getBackdropClasses,
  getTaskPanelClasses,
  getTaskPanelInnerClasses,
} from '@/lib/responsive-config';
import type { TaskWithAttemptStatus } from 'shared/types';
import type { TabType } from '@/types/tabs';
import DiffTab from '@/components/tasks/TaskDetails/DiffTab.tsx';
import LogsTab from '@/components/tasks/TaskDetails/LogsTab.tsx';
import ProcessesTab from '@/components/tasks/TaskDetails/ProcessesTab.tsx';
import TabNavigation from '@/components/tasks/TaskDetails/TabNavigation.tsx';
import TaskDetailsToolbar from './TaskDetailsToolbar.tsx';
import TodoPanel from '@/components/tasks/TodoPanel';
import { TabNavContext } from '@/contexts/TabNavigationContext';
import { ProcessSelectionProvider } from '@/contexts/ProcessSelectionContext';
import { ReviewProvider } from '@/contexts/ReviewProvider';
import { EntriesProvider } from '@/contexts/EntriesContext';
import { AttemptHeaderCard } from './AttemptHeaderCard';
import { inIframe } from '@/vscode/bridge';
import { TaskRelationshipViewer } from './TaskRelationshipViewer';
import { useTaskViewManager } from '@/hooks/useTaskViewManager.ts';

interface TaskDetailsPanelProps {
  task: TaskWithAttemptStatus | null;
  projectHasDevScript?: boolean;
  projectId: string;
  onClose: () => void;
  onEditTask?: (task: TaskWithAttemptStatus) => void;
  onDeleteTask?: (taskId: string) => void;
  onNavigateToTask?: (taskId: string) => void;
  isDialogOpen?: boolean;
  hideBackdrop?: boolean;
  className?: string;
  hideHeader?: boolean;
  isFullScreen?: boolean;
  forceCreateAttempt?: boolean;
  onLeaveForceCreateAttempt?: () => void;
  onNewAttempt?: () => void;
  selectedAttempt: TaskAttempt | null;
  attempts: TaskAttempt[];
  setSelectedAttempt: (attempt: TaskAttempt | null) => void;
  tasksById?: Record<string, TaskWithAttemptStatus>;
}

export function TaskDetailsPanel({
  task,
  projectHasDevScript,
  projectId,
  onClose,
  onEditTask,
  onDeleteTask,
  onNavigateToTask,
  isDialogOpen = false,
  hideBackdrop = false,
  className,
  isFullScreen,
  forceCreateAttempt,
  onLeaveForceCreateAttempt,
  selectedAttempt,
  attempts,
  setSelectedAttempt,
  tasksById,
}: TaskDetailsPanelProps) {
  // Attempt number, find the current attempt number
  const attemptNumber =
    attempts.length -
    attempts.findIndex((attempt) => attempt.id === selectedAttempt?.id);

  // Tab and collapsible state
  const [activeTab, setActiveTab] = useState<TabType>('logs');

  // Handler for jumping to diff tab in full screen
  const { toggleFullscreen } = useTaskViewManager();

  const jumpToDiffFullScreen = () => {
    toggleFullscreen(true);
    setActiveTab('diffs');
  };

  const jumpToLogsTab = () => {
    setActiveTab('logs');
  };

  // Reset to logs tab when task changes
  useEffect(() => {
    if (task?.id) {
      setActiveTab('logs');
    }
  }, [task?.id]);

  // Get selected attempt info for props
  // (now received as props instead of hook)

  // Handle ESC key locally to prevent global navigation
  useEffect(() => {
    if (isDialogOpen) return;

    const handleKeyDown = (event: KeyboardEvent) => {
      if (event.key === 'Escape') {
        event.preventDefault();
        event.stopPropagation();
        onClose();
      }
    };

    document.addEventListener('keydown', handleKeyDown, true);
    return () => document.removeEventListener('keydown', handleKeyDown, true);
  }, [onClose, isDialogOpen]);

  return (
    <>
      {!task ? null : (
        <TabNavContext.Provider value={{ activeTab, setActiveTab }}>
          <ProcessSelectionProvider>
            <ReviewProvider>
              <EntriesProvider>
                {/* Backdrop - only on smaller screens (overlay mode) */}
                {!hideBackdrop && (
                  <div
                    className={getBackdropClasses(isFullScreen || false)}
                    onClick={onClose}
                  />
                )}

                {/* Panel */}
                <div
                  className={
                    className || getTaskPanelClasses(isFullScreen || false)
                  }
                >
                  <div className={getTaskPanelInnerClasses()}>
                    {!inIframe() && (
                      <TaskDetailsHeader
                        task={task}
                        onClose={onClose}
                        onEditTask={onEditTask}
                        onDeleteTask={onDeleteTask}
                        hideCloseButton={hideBackdrop}
                        isFullScreen={isFullScreen}
                      />
                    )}

                    {isFullScreen ? (
                      <div className="flex-1 min-h-0 flex">
                        {/* Sidebar */}
                        <aside
                          className={`w-[28rem] shrink-0 border-r overflow-y-auto ${inIframe() ? 'hidden' : ''}`}
                        >
                          {/* Fullscreen sidebar shows title and description above edit/delete */}
                          <div className="space-y-2 p-3">
                            <TaskTitleDescription task={task} />
                          </div>

                          {/* Current Attempt / Actions */}
                          <TaskDetailsToolbar
                            task={task}
                            projectId={projectId}
                            projectHasDevScript={projectHasDevScript}
                            forceCreateAttempt={forceCreateAttempt}
                            onLeaveForceCreateAttempt={
                              onLeaveForceCreateAttempt
                            }
                            attempts={attempts}
                            selectedAttempt={selectedAttempt}
                            setSelectedAttempt={setSelectedAttempt}
                            // hide actions in sidebar; moved to header in fullscreen
                          />

                          {/* Task Breakdown (TODOs) */}
                          <TodoPanel />

                          {/* Task Relationships */}
                          <TaskRelationshipViewer
                            selectedAttempt={selectedAttempt}
                            onNavigateToTask={onNavigateToTask}
                            task={task}
                            tasksById={tasksById}
                          />
                        </aside>

                        {/* Main content */}
                        <main className="flex-1 min-h-0 min-w-0 flex flex-col">
                          {selectedAttempt && (
                            <>
                              <TabNavigation
                                activeTab={activeTab}
                                setActiveTab={setActiveTab}
                                selectedAttempt={selectedAttempt}
                              />

                              <div className="flex-1 flex flex-col min-h-0">
                                {activeTab === 'diffs' ? (
                                  <DiffTab selectedAttempt={selectedAttempt} />
                                ) : activeTab === 'processes' ? (
                                  <ProcessesTab
                                    attemptId={selectedAttempt?.id}
                                  />
                                ) : (
                                  <LogsTab selectedAttempt={selectedAttempt} />
                                )}
                              </div>

                              <TaskFollowUpSection
                                task={task}
                                selectedAttemptId={selectedAttempt?.id}
                                jumpToLogsTab={jumpToLogsTab}
                              />
                            </>
                          )}
                        </main>
                      </div>
                    ) : (
                      <>
                        {attempts.length === 0 ? (
                          <TaskDetailsToolbar
                            task={task}
                            projectId={projectId}
                            projectHasDevScript={projectHasDevScript}
                            forceCreateAttempt={forceCreateAttempt}
                            onLeaveForceCreateAttempt={
                              onLeaveForceCreateAttempt
                            }
                            attempts={attempts}
                            selectedAttempt={selectedAttempt}
                            setSelectedAttempt={setSelectedAttempt}
                            // hide actions in sidebar; moved to header in fullscreen
                          />
                        ) : (
                          <>
                            <AttemptHeaderCard
                              attemptNumber={attemptNumber}
                              totalAttempts={attempts.length}
                              selectedAttempt={selectedAttempt}
                              task={task}
                              projectId={projectId}
                              // onCreateNewAttempt={() => {
                              //   // TODO: Implement create new attempt
                              //   console.log('Create new attempt');
                              // }}
                              onJumpToDiffFullScreen={jumpToDiffFullScreen}
                            />

                            {selectedAttempt && (
                              <LogsTab selectedAttempt={selectedAttempt} />
                            )}

                            <TaskFollowUpSection
                              task={task}
                              selectedAttemptId={selectedAttempt?.id}
                              jumpToLogsTab={jumpToLogsTab}
                            />
                          </>
                        )}
                      </>
                    )}
                  </div>
                </div>
              </EntriesProvider>
            </ReviewProvider>
          </ProcessSelectionProvider>
        </TabNavContext.Provider>
      )}
    </>
  );
}
</file>

<file path="frontend/src/components/tasks/TaskDetailsToolbar.tsx">
import { useCallback, useEffect, useMemo, useReducer, useState } from 'react';
import { Play } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { projectsApi, attemptsApi } from '@/lib/api';
import type {
  GitBranch,
  TaskAttempt,
  TaskWithAttemptStatus,
} from 'shared/types';
import type { ExecutorProfileId } from 'shared/types';

import { useAttemptExecution } from '@/hooks';
import { useTaskStopping } from '@/stores/useTaskDetailsUiStore';

import CreateAttempt from '@/components/tasks/Toolbar/CreateAttempt.tsx';
import CurrentAttempt from '@/components/tasks/Toolbar/CurrentAttempt.tsx';
import { useUserSystem } from '@/components/config-provider';
import { Card } from '../ui/card';

// UI State Management
type UiAction =
  | { type: 'OPEN_CREATE_PR' }
  | { type: 'CLOSE_CREATE_PR' }
  | { type: 'CREATE_PR_START' }
  | { type: 'CREATE_PR_DONE' }
  | { type: 'ENTER_CREATE_MODE' }
  | { type: 'LEAVE_CREATE_MODE' }
  | { type: 'SET_ERROR'; payload: string | null };

interface UiState {
  showCreatePRDialog: boolean;
  creatingPR: boolean;
  userForcedCreateMode: boolean;
  error: string | null;
}

const initialUi: UiState = {
  showCreatePRDialog: false,
  creatingPR: false,
  userForcedCreateMode: false,
  error: null,
};

function uiReducer(state: UiState, action: UiAction): UiState {
  switch (action.type) {
    case 'OPEN_CREATE_PR':
      return { ...state, showCreatePRDialog: true };
    case 'CLOSE_CREATE_PR':
      return { ...state, showCreatePRDialog: false };
    case 'CREATE_PR_START':
      return { ...state, creatingPR: true };
    case 'CREATE_PR_DONE':
      return { ...state, creatingPR: false };
    case 'ENTER_CREATE_MODE':
      return { ...state, userForcedCreateMode: true };
    case 'LEAVE_CREATE_MODE':
      return { ...state, userForcedCreateMode: false };
    case 'SET_ERROR':
      return { ...state, error: action.payload };
    default:
      return state;
  }
}

function TaskDetailsToolbar({
  task,
  projectId,
  projectHasDevScript,
  forceCreateAttempt,
  onLeaveForceCreateAttempt,
  attempts,
  selectedAttempt,
  setSelectedAttempt,
}: {
  task: TaskWithAttemptStatus;
  projectId: string;
  projectHasDevScript?: boolean;
  forceCreateAttempt?: boolean;
  onLeaveForceCreateAttempt?: () => void;
  attempts: TaskAttempt[];
  selectedAttempt: TaskAttempt | null;
  setSelectedAttempt: (attempt: TaskAttempt | null) => void;
}) {
  // Use props instead of context
  const taskAttempts = attempts;
  // const { setLoading } = useTaskLoading(task.id);
  const { isStopping } = useTaskStopping(task.id);
  const { isAttemptRunning } = useAttemptExecution(selectedAttempt?.id);

  // UI state using reducer
  const [ui, dispatch] = useReducer(uiReducer, initialUi);

  // Data state
  const [branches, setBranches] = useState<GitBranch[]>([]);
  const [selectedBranch, setSelectedBranch] = useState<string | null>(null);
  const [selectedProfile, setSelectedProfile] =
    useState<ExecutorProfileId | null>(null);
  const [parentBaseBranch, setParentBaseBranch] = useState<string | null>(null);
  // const { attemptId: urlAttemptId } = useParams<{ attemptId?: string }>();
  const { system, profiles } = useUserSystem();

  // Memoize latest attempt calculation
  const latestAttempt = useMemo(() => {
    if (taskAttempts.length === 0) return null;
    return taskAttempts.reduce((latest, current) =>
      new Date(current.created_at) > new Date(latest.created_at)
        ? current
        : latest
    );
  }, [taskAttempts]);

  // Derived state
  const isInCreateAttemptMode =
    forceCreateAttempt ??
    (ui.userForcedCreateMode || taskAttempts.length === 0);

  // Derive createAttemptBranch for backward compatibility
  const createAttemptBranch = useMemo(() => {
    // Priority order:
    // 1. User explicitly selected a branch
    if (selectedBranch) {
      return selectedBranch;
    }

    // 2. Latest attempt's base branch (existing behavior for resume/rerun)
    if (
      latestAttempt?.base_branch &&
      branches.some((b: GitBranch) => b.name === latestAttempt.base_branch)
    ) {
      return latestAttempt.base_branch;
    }

    // 3. Parent task attempt's base branch (NEW - for inherited tasks)
    if (parentBaseBranch) {
      return parentBaseBranch;
    }

    // 4. Fall back to current branch
    const currentBranch = branches.find((b) => b.is_current);
    return currentBranch?.name || null;
  }, [latestAttempt, branches, selectedBranch, parentBaseBranch]);

  const fetchProjectBranches = useCallback(async () => {
    const result = await projectsApi.getBranches(projectId);

    setBranches(result);
  }, [projectId]);

  useEffect(() => {
    fetchProjectBranches();
  }, [fetchProjectBranches]);

  // Set default executor from config
  useEffect(() => {
    if (system.config?.executor_profile) {
      setSelectedProfile(system.config.executor_profile);
    }
  }, [system.config?.executor_profile]);

  // Fetch parent task attempt's base branch
  useEffect(() => {
    if (task.parent_task_attempt) {
      attemptsApi
        .get(task.parent_task_attempt)
        .then((attempt) => setParentBaseBranch(attempt.branch))
        .catch(() => setParentBaseBranch(null));
    } else {
      setParentBaseBranch(null);
    }
  }, [task.parent_task_attempt]);

  // Simplified - hooks handle data fetching and navigation
  // const fetchTaskAttempts = useCallback(() => {
  //   // The useSelectedAttempt hook handles all this logic now
  // }, []);

  // Remove fetchTaskAttempts - hooks handle this now

  // Handle entering create attempt mode
  const handleEnterCreateAttemptMode = useCallback(() => {
    dispatch({ type: 'ENTER_CREATE_MODE' });
  }, []);

  // Stub handlers for backward compatibility with CreateAttempt
  const setCreateAttemptBranch = useCallback(
    (branch: string | null | ((prev: string | null) => string | null)) => {
      if (typeof branch === 'function') {
        setSelectedBranch((prev) => branch(prev));
      } else {
        setSelectedBranch(branch);
      }
      // This is now derived state, so no-op
    },
    []
  );

  const setIsInCreateAttemptMode = useCallback(
    (value: boolean | ((prev: boolean) => boolean)) => {
      const boolValue =
        typeof value === 'function' ? value(isInCreateAttemptMode) : value;
      if (boolValue) {
        dispatch({ type: 'ENTER_CREATE_MODE' });
      } else {
        if (onLeaveForceCreateAttempt) onLeaveForceCreateAttempt();
        dispatch({ type: 'LEAVE_CREATE_MODE' });
      }
    },
    [isInCreateAttemptMode, onLeaveForceCreateAttempt]
  );

  // Wrapper functions for UI state dispatch
  const setError = useCallback(
    (value: string | null | ((prev: string | null) => string | null)) => {
      const errorValue = typeof value === 'function' ? value(ui.error) : value;
      dispatch({ type: 'SET_ERROR', payload: errorValue });
    },
    [ui.error]
  );

  return (
    <>
      <div>
        {/* Error Display */}
        {ui.error && (
          <div className="mb-4 p-3 bg-red-50 border border-red-200">
            <div className="text-destructive text-sm">{ui.error}</div>
          </div>
        )}

        {isInCreateAttemptMode ? (
          <CreateAttempt
            task={task}
            createAttemptBranch={createAttemptBranch}
            selectedBranch={selectedBranch}
            selectedProfile={selectedProfile}
            taskAttempts={taskAttempts}
            branches={branches}
            setCreateAttemptBranch={setCreateAttemptBranch}
            setIsInCreateAttemptMode={setIsInCreateAttemptMode}
            setSelectedProfile={setSelectedProfile}
            availableProfiles={profiles}
            selectedAttempt={selectedAttempt}
          />
        ) : (
          <div className="">
            <Card className="bg-background border-y border-dashed p-3 text-sm">
              Actions
            </Card>
            <div className="p-3">
              {/* Current Attempt Info */}
              <div className="space-y-2">
                {selectedAttempt ? (
                  <CurrentAttempt
                    task={task}
                    projectId={projectId}
                    projectHasDevScript={projectHasDevScript ?? false}
                    selectedAttempt={selectedAttempt}
                    taskAttempts={taskAttempts}
                    selectedBranch={selectedBranch}
                    setError={setError}
                    creatingPR={ui.creatingPR}
                    handleEnterCreateAttemptMode={handleEnterCreateAttemptMode}
                    branches={branches}
                    setSelectedAttempt={setSelectedAttempt}
                  />
                ) : (
                  <div className="text-center py-8">
                    <div className="text-lg font-medium text-muted-foreground">
                      No attempts yet
                    </div>
                    <div className="text-sm text-muted-foreground mt-1">
                      Start your first attempt to begin working on this task
                    </div>
                  </div>
                )}
              </div>

              {/* Special Actions: show only in sidebar (non-fullscreen) */}
              {!selectedAttempt && !isAttemptRunning && !isStopping && (
                <div className="space-y-2 pt-3 border-t">
                  <Button
                    onClick={handleEnterCreateAttemptMode}
                    size="sm"
                    className="w-full gap-2 bg-black text-white hover:bg-black/90"
                  >
                    <Play className="h-4 w-4" />
                    Start Attempt
                  </Button>
                </div>
              )}
            </div>
          </div>
        )}
      </div>
    </>
  );
}

export default TaskDetailsToolbar;
</file>

<file path="frontend/src/components/tasks/TaskFollowUpSection.tsx">
import {
  ImageIcon,
  Loader2,
  Send,
  StopCircle,
  AlertCircle,
} from 'lucide-react';
import { Button } from '@/components/ui/button';
import { ImageUploadSection } from '@/components/ui/ImageUploadSection';
import { Alert, AlertDescription } from '@/components/ui/alert';
//
import { useEffect, useMemo, useRef, useState } from 'react';
import { imagesApi } from '@/lib/api.ts';
import type { TaskWithAttemptStatus } from 'shared/types';
import { useBranchStatus } from '@/hooks';
import { useAttemptExecution } from '@/hooks/useAttemptExecution';
import { useUserSystem } from '@/components/config-provider';
import { cn } from '@/lib/utils';
//
import { useReview } from '@/contexts/ReviewProvider';
//
import { VariantSelector } from '@/components/tasks/VariantSelector';
import { FollowUpStatusRow } from '@/components/tasks/FollowUpStatusRow';
import { useAttemptBranch } from '@/hooks/useAttemptBranch';
import { FollowUpConflictSection } from '@/components/tasks/follow-up/FollowUpConflictSection';
import { FollowUpEditorCard } from '@/components/tasks/follow-up/FollowUpEditorCard';
import { useDraftStream } from '@/hooks/follow-up/useDraftStream';
import { useDraftEdits } from '@/hooks/follow-up/useDraftEdits';
import { useDraftAutosave } from '@/hooks/follow-up/useDraftAutosave';
import { useDraftQueue } from '@/hooks/follow-up/useDraftQueue';
import { useFollowUpSend } from '@/hooks/follow-up/useFollowUpSend';
import { useDefaultVariant } from '@/hooks/follow-up/useDefaultVariant';

interface TaskFollowUpSectionProps {
  task: TaskWithAttemptStatus;
  selectedAttemptId?: string;
  jumpToLogsTab: () => void;
}

export function TaskFollowUpSection({
  task,
  selectedAttemptId,
  jumpToLogsTab,
}: TaskFollowUpSectionProps) {
  const { isAttemptRunning, stopExecution, isStopping, processes } =
    useAttemptExecution(selectedAttemptId, task.id);
  const { data: branchStatus, refetch: refetchBranchStatus } =
    useBranchStatus(selectedAttemptId);
  const { branch: attemptBranch, refetch: refetchAttemptBranch } =
    useAttemptBranch(selectedAttemptId);
  const { profiles } = useUserSystem();
  const { comments, generateReviewMarkdown, clearComments } = useReview();

  const reviewMarkdown = useMemo(
    () => generateReviewMarkdown(),
    [generateReviewMarkdown, comments]
  );

  // Draft stream and synchronization
  const {
    draft,
    isDraftLoaded,
    lastServerVersionRef,
    suppressNextSaveRef,
    forceNextApplyRef,
  } = useDraftStream(selectedAttemptId);

  // Editor state
  const {
    message: followUpMessage,
    setMessage: setFollowUpMessage,
    images,
    setImages,
    newlyUploadedImageIds,
    handleImageUploaded,
    clearImagesAndUploads,
  } = useDraftEdits({
    draft,
    lastServerVersionRef,
    suppressNextSaveRef,
    forceNextApplyRef,
    taskId: task.id,
  });

  // Presentation-only: show/hide image upload panel
  const [showImageUpload, setShowImageUpload] = useState(false);

  // Variant selection (with keyboard cycling)
  const { selectedVariant, setSelectedVariant, currentProfile } =
    useDefaultVariant({ processes, profiles: profiles ?? null });

  // Queue management (including derived lock flag)
  const { onQueue, onUnqueue } = useDraftQueue({
    attemptId: selectedAttemptId,
    draft,
    message: followUpMessage,
    selectedVariant,
    images,
    suppressNextSaveRef,
    lastServerVersionRef,
  });

  // Presentation-only queue state
  const [isQueuing, setIsQueuing] = useState(false);
  const [isUnqueuing, setIsUnqueuing] = useState(false);
  // Local queued state override after server action completes; null = rely on server
  const [queuedOptimistic, setQueuedOptimistic] = useState<boolean | null>(
    null
  );

  // Server + presentation derived flags (computed early so they are usable below)
  const isQueued = !!draft?.queued;
  const displayQueued = queuedOptimistic ?? isQueued;

  // Autosave draft when editing
  const { isSaving, saveStatus } = useDraftAutosave({
    attemptId: selectedAttemptId,
    serverDraft: draft,
    current: {
      prompt: followUpMessage,
      variant: selectedVariant,
      image_ids: images.map((img) => img.id),
    },
    isQueuedUI: displayQueued,
    isDraftSending: !!draft?.sending,
    isQueuing: isQueuing,
    isUnqueuing: isUnqueuing,
    suppressNextSaveRef,
    lastServerVersionRef,
    forceNextApplyRef,
  });

  // Send follow-up action
  const { isSendingFollowUp, followUpError, setFollowUpError, onSendFollowUp } =
    useFollowUpSend({
      attemptId: selectedAttemptId,
      message: followUpMessage,
      reviewMarkdown,
      selectedVariant,
      images,
      newlyUploadedImageIds,
      clearComments,
      jumpToLogsTab,
      onAfterSendCleanup: clearImagesAndUploads,
      setMessage: setFollowUpMessage,
    });

  // Profile/variant derived from processes only (see useDefaultVariant)

  // Separate logic for when textarea should be disabled vs when send button should be disabled
  const canTypeFollowUp = useMemo(() => {
    if (!selectedAttemptId || processes.length === 0 || isSendingFollowUp) {
      return false;
    }

    // Check if PR is merged - if so, block follow-ups
    if (branchStatus?.merges) {
      const mergedPR = branchStatus.merges.find(
        (m) => m.type === 'pr' && m.pr_info.status === 'merged'
      );
      if (mergedPR) {
        return false;
      }
    }

    return true;
  }, [
    selectedAttemptId,
    processes.length,
    isSendingFollowUp,
    branchStatus?.merges,
  ]);

  const canSendFollowUp = useMemo(() => {
    if (!canTypeFollowUp) {
      return false;
    }

    // Allow sending if either review comments exist OR follow-up message is present
    return Boolean(reviewMarkdown || followUpMessage.trim());
  }, [canTypeFollowUp, reviewMarkdown, followUpMessage]);
  // currentProfile is provided by useDefaultVariant

  const isDraftLocked =
    displayQueued || isQueuing || isUnqueuing || !!draft?.sending;
  const isEditable = isDraftLoaded && !isDraftLocked;

  const appendToFollowUpMessage = (text: string) => {
    setFollowUpMessage((prev) => {
      const sep =
        prev.trim().length === 0 ? '' : prev.endsWith('\n') ? '\n' : '\n\n';
      return prev + sep + text;
    });
  };

  // When a process completes (e.g., agent resolved conflicts), refresh branch status promptly
  const prevRunningRef = useRef<boolean>(isAttemptRunning);
  useEffect(() => {
    if (prevRunningRef.current && !isAttemptRunning && selectedAttemptId) {
      refetchBranchStatus();
      refetchAttemptBranch();
    }
    prevRunningRef.current = isAttemptRunning;
  }, [
    isAttemptRunning,
    selectedAttemptId,
    refetchBranchStatus,
    refetchAttemptBranch,
  ]);

  // When server indicates sending started, clear draft and images; hide upload panel
  const prevSendingRef = useRef<boolean>(!!draft?.sending);
  useEffect(() => {
    const now = !!draft?.sending;
    if (now && !prevSendingRef.current) {
      if (followUpMessage !== '') setFollowUpMessage('');
      if (images.length > 0 || newlyUploadedImageIds.length > 0) {
        clearImagesAndUploads();
      }
      if (showImageUpload) setShowImageUpload(false);
      if (queuedOptimistic !== null) setQueuedOptimistic(null);
    }
    prevSendingRef.current = now;
  }, [
    draft?.sending,
    followUpMessage,
    setFollowUpMessage,
    images.length,
    newlyUploadedImageIds.length,
    clearImagesAndUploads,
    showImageUpload,
    queuedOptimistic,
  ]);

  // On server queued state change, drop optimistic override and stop spinners accordingly
  useEffect(() => {
    setQueuedOptimistic(null);
    if (isQueued) {
      if (isQueuing) setIsQueuing(false);
    } else {
      if (isUnqueuing) setIsUnqueuing(false);
    }
  }, [isQueued]);

  return (
    selectedAttemptId && (
      <div className="border-t p-4 focus-within:ring ring-inset">
        <div className="space-y-2">
          {followUpError && (
            <Alert variant="destructive">
              <AlertCircle className="h-4 w-4" />
              <AlertDescription>{followUpError}</AlertDescription>
            </Alert>
          )}
          <div className="space-y-2">
            {showImageUpload && (
              <div className="mb-2">
                <ImageUploadSection
                  images={images}
                  onImagesChange={setImages}
                  onUpload={imagesApi.upload}
                  onDelete={imagesApi.delete}
                  onImageUploaded={(image) => {
                    handleImageUploaded(image);
                    const markdownText = `![${image.original_name}](${image.file_path})`;
                    const next =
                      followUpMessage.trim() === ''
                        ? markdownText
                        : followUpMessage + ' ' + markdownText;
                    setFollowUpMessage(next);
                  }}
                  disabled={!isEditable}
                  collapsible={false}
                  defaultExpanded={true}
                />
              </div>
            )}

            {/* Review comments preview */}
            {reviewMarkdown && (
              <div className="text-sm mb-4">
                <div className="whitespace-pre-wrap">{reviewMarkdown}</div>
              </div>
            )}

            {/* Conflict notice and actions (optional UI) */}
            <FollowUpConflictSection
              selectedAttemptId={selectedAttemptId}
              attemptBranch={attemptBranch}
              branchStatus={branchStatus}
              isEditable={isEditable}
              appendInstructions={appendToFollowUpMessage}
              refetchBranchStatus={refetchBranchStatus}
            />

            <div className="flex flex-col gap-2">
              <FollowUpEditorCard
                placeholder={
                  isQueued
                    ? 'Type your follow-up… It will auto-send when ready.'
                    : reviewMarkdown
                      ? '(Optional) Add additional instructions... Type @ to search files.'
                      : 'Continue working on this task attempt... Type @ to search files.'
                }
                value={followUpMessage}
                onChange={(value) => {
                  setFollowUpMessage(value);
                  if (followUpError) setFollowUpError(null);
                }}
                onKeyDown={async (e) => {
                  if ((e.metaKey || e.ctrlKey) && e.key === 'Enter') {
                    e.preventDefault();
                    if (canSendFollowUp && !isSendingFollowUp) {
                      if (isAttemptRunning) {
                        setIsQueuing(true);
                        const ok = await onQueue();
                        setIsQueuing(false);
                        if (ok) setQueuedOptimistic(true);
                      } else {
                        onSendFollowUp();
                      }
                    }
                  } else if (e.key === 'Escape') {
                    e.preventDefault();
                    setFollowUpMessage('');
                  }
                }}
                disabled={!isEditable}
                showLoadingOverlay={isUnqueuing || !isDraftLoaded}
              />
              <FollowUpStatusRow
                status={{
                  save: { state: saveStatus, isSaving },
                  draft: {
                    isLoaded: isDraftLoaded,
                    isSending: !!draft?.sending,
                  },
                  queue: { isUnqueuing: isUnqueuing, isQueued: displayQueued },
                }}
              />
              <div className="flex flex-row gap-2 items-center">
                <div className="flex-1 flex gap-2">
                  {/* Image button */}
                  <Button
                    variant="secondary"
                    size="sm"
                    onClick={() => setShowImageUpload(!showImageUpload)}
                    disabled={!isEditable}
                  >
                    <ImageIcon
                      className={cn(
                        'h-4 w-4',
                        (images.length > 0 || showImageUpload) && 'text-primary'
                      )}
                    />
                  </Button>

                  <VariantSelector
                    currentProfile={currentProfile}
                    selectedVariant={selectedVariant}
                    onChange={setSelectedVariant}
                    disabled={!isEditable}
                  />
                </div>

                {isAttemptRunning ? (
                  <Button
                    onClick={stopExecution}
                    disabled={isStopping}
                    size="sm"
                    variant="destructive"
                  >
                    {isStopping ? (
                      <Loader2 className="animate-spin h-4 w-4 mr-2" />
                    ) : (
                      <>
                        <StopCircle className="h-4 w-4 mr-2" />
                        Stop
                      </>
                    )}
                  </Button>
                ) : (
                  <div className="flex items-center gap-2">
                    {comments.length > 0 && (
                      <Button
                        onClick={clearComments}
                        size="sm"
                        variant="destructive"
                      >
                        Clear Review Comments
                      </Button>
                    )}
                    <Button
                      onClick={onSendFollowUp}
                      disabled={
                        !canSendFollowUp ||
                        isDraftLocked ||
                        !isDraftLoaded ||
                        isSendingFollowUp
                      }
                      size="sm"
                    >
                      {isSendingFollowUp ? (
                        <Loader2 className="animate-spin h-4 w-4 mr-2" />
                      ) : (
                        <>
                          <Send className="h-4 w-4 mr-2" />
                          Send
                        </>
                      )}
                    </Button>
                    {isQueued && (
                      <Button
                        variant="default"
                        size="sm"
                        className="min-w-[180px] transition-all"
                        onClick={async () => {
                          setIsUnqueuing(true);
                          try {
                            const ok = await onUnqueue();
                            if (ok) setQueuedOptimistic(false);
                          } finally {
                            setIsUnqueuing(false);
                          }
                        }}
                        disabled={isUnqueuing}
                      >
                        {isUnqueuing ? (
                          <>
                            <Loader2 className="animate-spin h-4 w-4 mr-2" />
                            Unqueuing…
                          </>
                        ) : (
                          'Edit'
                        )}
                      </Button>
                    )}
                  </div>
                )}
                {isAttemptRunning && (
                  <div className="flex items-center gap-2">
                    <Button
                      onClick={async () => {
                        if (displayQueued) {
                          setIsUnqueuing(true);
                          try {
                            const ok = await onUnqueue();
                            if (ok) setQueuedOptimistic(false);
                          } finally {
                            setIsUnqueuing(false);
                          }
                        } else {
                          setIsQueuing(true);
                          try {
                            const ok = await onQueue();
                            if (ok) setQueuedOptimistic(true);
                          } finally {
                            setIsQueuing(false);
                          }
                        }
                      }}
                      disabled={
                        displayQueued
                          ? isUnqueuing
                          : !canSendFollowUp ||
                            !isDraftLoaded ||
                            isQueuing ||
                            isUnqueuing ||
                            !!draft?.sending
                      }
                      size="sm"
                      variant="default"
                      className="md:min-w-[180px] transition-all"
                    >
                      {displayQueued ? (
                        isUnqueuing ? (
                          <>
                            <Loader2 className="animate-spin h-4 w-4 mr-2" />
                            Unqueuing…
                          </>
                        ) : (
                          'Edit'
                        )
                      ) : isQueuing ? (
                        <>
                          <Loader2 className="animate-spin h-4 w-4 mr-2" />
                          Queuing…
                        </>
                      ) : (
                        'Queue for next turn'
                      )}
                    </Button>
                  </div>
                )}
              </div>
            </div>
          </div>
        </div>
      </div>
    )
  );
}
</file>

<file path="frontend/src/components/tasks/TaskKanbanBoard.tsx">
import { memo, useEffect, useMemo, useState } from 'react';
import {
  type DragEndEvent,
  KanbanBoard,
  KanbanCards,
  KanbanHeader,
  KanbanProvider,
} from '@/components/ui/shadcn-io/kanban';
import { TaskCard } from './TaskCard';
import type { TaskStatus, TaskWithAttemptStatus } from 'shared/types';
import { useNavigate, useParams } from 'react-router-dom';
import {
  useKeyboardShortcuts,
  useKanbanKeyboardNavigation,
} from '@/lib/keyboard-shortcuts.ts';
import { statusBoardColors, statusLabels } from '@/utils/status-labels';

type Task = TaskWithAttemptStatus;

interface TaskKanbanBoardProps {
  tasks: Task[];
  searchQuery?: string;
  onDragEnd: (event: DragEndEvent) => void;
  onEditTask: (task: Task) => void;
  onDeleteTask: (taskId: string) => void;
  onDuplicateTask?: (task: Task) => void;
  onViewTaskDetails: (task: Task) => void;
  isPanelOpen: boolean;
}

const allTaskStatuses: TaskStatus[] = [
  'todo',
  'inprogress',
  'inreview',
  'done',
  'cancelled',
];

function TaskKanbanBoard({
  tasks,
  searchQuery = '',
  onDragEnd,
  onEditTask,
  onDeleteTask,
  onDuplicateTask,
  onViewTaskDetails,
  isPanelOpen,
}: TaskKanbanBoardProps) {
  const { projectId, taskId } = useParams<{
    projectId: string;
    taskId?: string;
  }>();
  const navigate = useNavigate();

  useKeyboardShortcuts({
    navigate,
    currentPath: `/projects/${projectId}/tasks${taskId ? `/${taskId}` : ''}`,
  });

  const [focusedTaskId, setFocusedTaskId] = useState<string | null>(
    taskId || null
  );
  const [focusedStatus, setFocusedStatus] = useState<TaskStatus | null>(null);

  // Memoize filtered tasks
  const filteredTasks = useMemo(() => {
    if (!searchQuery.trim()) {
      return tasks;
    }
    const query = searchQuery.toLowerCase();
    return tasks.filter(
      (task) =>
        task.title.toLowerCase().includes(query) ||
        (task.description && task.description.toLowerCase().includes(query))
    );
  }, [tasks, searchQuery]);

  // Memoize grouped tasks
  const groupedTasks = useMemo(() => {
    const groups: Record<TaskStatus, Task[]> = {} as Record<TaskStatus, Task[]>;
    allTaskStatuses.forEach((status) => {
      groups[status] = [];
    });
    filteredTasks.forEach((task) => {
      const normalizedStatus = task.status.toLowerCase() as TaskStatus;
      if (groups[normalizedStatus]) {
        groups[normalizedStatus].push(task);
      } else {
        groups['todo'].push(task);
      }
    });
    return groups;
  }, [filteredTasks]);

  // Sync focus state with taskId param
  useEffect(() => {
    if (taskId) {
      const found = filteredTasks.find((t) => t.id === taskId);
      if (found) {
        setFocusedTaskId(taskId);
        setFocusedStatus((found.status.toLowerCase() as TaskStatus) || null);
      }
    }
  }, [taskId, filteredTasks]);

  // If no taskId in params, keep last focused, or focus first available
  useEffect(() => {
    if (!taskId && !focusedTaskId) {
      for (const status of allTaskStatuses) {
        if (groupedTasks[status] && groupedTasks[status].length > 0) {
          setFocusedTaskId(groupedTasks[status][0].id);
          setFocusedStatus(status);
          break;
        }
      }
    }
  }, [taskId, focusedTaskId, groupedTasks]);

  // Keyboard navigation handler
  useKanbanKeyboardNavigation({
    focusedTaskId,
    setFocusedTaskId: (id) => {
      setFocusedTaskId(id as string | null);
      if (isPanelOpen) {
        const task = filteredTasks.find((t: any) => t.id === id);
        if (task) {
          onViewTaskDetails(task);
        }
      }
    },
    focusedStatus,
    setFocusedStatus: (status) => setFocusedStatus(status as TaskStatus | null),
    groupedTasks,
    filteredTasks,
    allTaskStatuses,
  });

  return (
    <KanbanProvider onDragEnd={onDragEnd}>
      {Object.entries(groupedTasks).map(([status, statusTasks]) => (
        <KanbanBoard key={status} id={status as TaskStatus}>
          <KanbanHeader
            name={statusLabels[status as TaskStatus]}
            color={statusBoardColors[status as TaskStatus]}
          />
          <KanbanCards>
            {statusTasks.map((task, index) => (
              <TaskCard
                key={task.id}
                task={task}
                index={index}
                status={status}
                onEdit={onEditTask}
                onDelete={onDeleteTask}
                onDuplicate={onDuplicateTask}
                onViewDetails={onViewTaskDetails}
                isFocused={focusedTaskId === task.id}
                tabIndex={focusedTaskId === task.id ? 0 : -1}
              />
            ))}
          </KanbanCards>
        </KanbanBoard>
      ))}
    </KanbanProvider>
  );
}

export default memo(TaskKanbanBoard);
</file>

<file path="frontend/src/components/tasks/TaskRelationshipCard.tsx">
import { Card } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { cn } from '@/lib/utils';
import type { Task } from 'shared/types';

interface TaskRelationshipCardProps {
  task: Task;
  isCurrentTask?: boolean;
  onClick?: () => void;
  className?: string;
}

export function TaskRelationshipCard({
  task,
  isCurrentTask = false,
  onClick,
  className,
}: TaskRelationshipCardProps) {
  const getStatusBadgeVariant = (status: string) => {
    switch (status) {
      case 'todo':
        return 'secondary';
      case 'inprogress':
        return 'default';
      case 'inreview':
        return 'outline';
      case 'done':
        return 'default';
      case 'cancelled':
        return 'destructive';
      default:
        return 'secondary';
    }
  };

  const truncateTitle = (title: string, maxLength: number = 50) => {
    return title.length > maxLength
      ? `${title.substring(0, maxLength)}...`
      : title;
  };

  const truncateDescription = (
    description: string | null,
    maxLength: number = 120
  ) => {
    if (!description) return null;
    return description.length > maxLength
      ? `${description.substring(0, maxLength)}...`
      : description;
  };

  return (
    <Card
      className={cn(
        'p-4 transition-all duration-200 cursor-pointer hover:shadow-md border',
        'min-h-[100px] w-full', // More spacious and responsive
        isCurrentTask && 'bg-accent/10 border-accent ring-1 ring-accent/50',
        !isCurrentTask && 'hover:bg-accent/5',
        onClick && 'cursor-pointer',
        !onClick && 'cursor-default',
        className
      )}
      onClick={onClick}
    >
      <div className="flex flex-col space-y-3">
        {/* Title and Status Row */}
        <div className="flex items-start justify-between gap-3">
          <h4
            className="font-medium text-sm leading-relaxed flex-1 min-w-0"
            title={task.title}
          >
            {truncateTitle(task.title)}
          </h4>
          <div className="flex items-center space-x-1 shrink-0">
            <Badge
              variant={getStatusBadgeVariant(task.status)}
              className="text-xs px-2 py-1 h-auto"
            >
              {task.status}
            </Badge>
          </div>
        </div>

        {/* Description */}
        {task.description && (
          <p
            className="text-xs text-muted-foreground leading-relaxed"
            title={task.description}
          >
            {truncateDescription(task.description)}
          </p>
        )}

        {/* Current task indicator */}
        {isCurrentTask && (
          <div className="flex items-center gap-2 pt-1">
            <div className="w-2 h-2 rounded-full bg-primary animate-pulse" />
            <span className="text-xs text-primary font-medium">
              Current Task
            </span>
          </div>
        )}
      </div>
    </Card>
  );
}
</file>

<file path="frontend/src/components/tasks/TaskRelationshipViewer.tsx">
import { useEffect, useState } from 'react';
import { Card } from '@/components/ui/card';
import { TaskRelationshipCard } from './TaskRelationshipCard';
import { attemptsApi } from '@/lib/api';
import type {
  TaskAttempt,
  TaskRelationships,
  TaskWithAttemptStatus,
} from 'shared/types';
import { ChevronDown, ChevronRight } from 'lucide-react';
import { cn } from '@/lib/utils';

interface TaskRelationshipViewerProps {
  selectedAttempt: TaskAttempt | null;
  onNavigateToTask?: (taskId: string) => void;
  task?: TaskWithAttemptStatus | null;
  tasksById?: Record<string, TaskWithAttemptStatus>;
}

export function TaskRelationshipViewer({
  selectedAttempt,
  onNavigateToTask,
  task,
  tasksById,
}: TaskRelationshipViewerProps) {
  const [relationships, setRelationships] = useState<TaskRelationships | null>(
    null
  );
  const [parentTask, setParentTask] = useState<TaskWithAttemptStatus | null>(
    null
  );
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [childrenExpanded, setChildrenExpanded] = useState(true);

  // Effect for attempt-based relationships (existing behavior)
  useEffect(() => {
    if (!selectedAttempt?.id) {
      setRelationships(null);
      return;
    }

    const fetchRelationships = async () => {
      setLoading(true);
      setError(null);
      try {
        const relationshipData = await attemptsApi.getChildren(
          selectedAttempt.id
        );
        setRelationships(relationshipData);
      } catch (err) {
        console.error('Failed to fetch task relationships:', err);
        setError('Failed to load task relationships');
      } finally {
        setLoading(false);
      }
    };

    fetchRelationships();
  }, [selectedAttempt?.id]);

  // Effect for parent task when child has no attempts (one request + tasksById lookup)
  useEffect(() => {
    if (selectedAttempt?.id) {
      // If we have an attempt, clear parent task since relationships will handle it
      setParentTask(null);
      return;
    }

    if (task?.parent_task_attempt && tasksById) {
      attemptsApi
        .get(task.parent_task_attempt)
        .then((parentAttempt) => {
          // Use existing tasksById instead of second API call
          const parentTaskData = tasksById[parentAttempt.task_id];
          setParentTask(parentTaskData || null);
        })
        .catch(() => setParentTask(null));
    } else {
      setParentTask(null);
    }
  }, [selectedAttempt?.id, task?.parent_task_attempt, tasksById]);

  const displayParentTask = relationships?.parent_task || parentTask;
  const childTasks = relationships?.children || [];
  const hasParent = displayParentTask !== null;
  const hasChildren = childTasks.length > 0;

  // Don't render if no relationships and no current task
  if (!hasParent && !hasChildren && !loading && !error) {
    return null;
  }

  return (
    <div>
      <Card className="bg-background p-3 border border-dashed text-sm">
        Task Relationships
      </Card>
      <div className="p-3 space-y-6">
        {loading ? (
          <div className="text-sm text-muted-foreground py-8 text-center">
            Loading relationships...
          </div>
        ) : error ? (
          <div className="text-sm text-destructive py-8 text-center">
            {error}
          </div>
        ) : (
          <div className="space-y-6">
            {/* Parent Task Section */}
            {hasParent && displayParentTask && (
              <div className="space-y-3">
                <div className="flex items-center gap-2">
                  <h4 className="text-xs font-medium text-muted-foreground uppercase tracking-wide">
                    Parent Task
                  </h4>
                  <div className="flex-1 h-px bg-border"></div>
                </div>
                <div className="flex justify-center">
                  <div className="w-full max-w-md">
                    <TaskRelationshipCard
                      task={displayParentTask}
                      isCurrentTask={false}
                      onClick={() => onNavigateToTask?.(displayParentTask.id)}
                      className="shadow-sm"
                    />
                  </div>
                </div>
              </div>
            )}

            {/* Child Tasks Section */}
            {hasChildren && (
              <div className="space-y-3">
                <div className="flex items-center gap-2">
                  <button
                    onClick={() => setChildrenExpanded(!childrenExpanded)}
                    className="flex items-center gap-1 text-xs font-medium text-muted-foreground uppercase tracking-wide hover:text-foreground transition-colors"
                  >
                    {childrenExpanded ? (
                      <ChevronDown className="w-3 h-3" />
                    ) : (
                      <ChevronRight className="w-3 h-3" />
                    )}
                    Child Tasks ({childTasks.length})
                  </button>
                  <div className="flex-1 h-px bg-border"></div>
                </div>

                {childrenExpanded && (
                  <div
                    className={cn(
                      'grid gap-4',
                      // Responsive grid: 1 col on mobile, 2 on tablet, 3 on desktop
                      'grid-cols-1 md:grid-cols-2 xl:grid-cols-3',
                      // Adjust based on number of children
                      childTasks.length === 1 &&
                        'md:grid-cols-1 xl:grid-cols-1 max-w-md mx-auto',
                      childTasks.length === 2 && 'md:grid-cols-2 xl:grid-cols-2'
                    )}
                  >
                    {childTasks.map((childTask) => (
                      <TaskRelationshipCard
                        key={childTask.id}
                        task={childTask}
                        isCurrentTask={false}
                        onClick={() => onNavigateToTask?.(childTask.id)}
                        className="shadow-sm hover:shadow-md transition-shadow"
                      />
                    ))}
                  </div>
                )}
              </div>
            )}
          </div>
        )}
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/tasks/TodoPanel.tsx">
import { Circle, CircleCheckBig, CircleDotDashed } from 'lucide-react';
import { useEntries } from '@/contexts/EntriesContext';
import { usePinnedTodos } from '@/hooks/usePinnedTodos';
import { Card } from '../ui/card';

function getStatusIcon(status?: string) {
  const s = (status || '').toLowerCase();
  if (s === 'completed')
    return <CircleCheckBig aria-hidden className="h-4 w-4 text-success" />;
  if (s === 'in_progress' || s === 'in-progress')
    return <CircleDotDashed aria-hidden className="h-4 w-4 text-blue-500" />;
  return <Circle aria-hidden className="h-4 w-4 text-muted-foreground" />;
}

export function TodoPanel() {
  const { entries } = useEntries();
  const { todos } = usePinnedTodos(entries);

  // Only show once the agent has created subtasks
  if (!todos || todos.length === 0) return null;

  return (
    <div>
      <Card className="bg-background p-3 border border-dashed text-sm">
        Todos
      </Card>
      <div className="p-3">
        <ul className="space-y-2">
          {todos.map((todo, index) => (
            <li
              key={`${todo.content}-${index}`}
              className="flex items-start gap-2"
            >
              <span className="mt-0.5 h-4 w-4 flex items-center justify-center shrink-0">
                {getStatusIcon(todo.status)}
              </span>
              <span className="text-sm leading-5 break-words">
                {todo.content}
              </span>
            </li>
          ))}
        </ul>
      </div>
    </div>
  );
}

export default TodoPanel;
</file>

<file path="frontend/src/components/tasks/VariantSelector.tsx">
import { memo, forwardRef, useEffect, useState } from 'react';
import { ChevronDown } from 'lucide-react';
import { Button } from '@/components/ui/button';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import { cn } from '@/lib/utils';
import type { ExecutorConfig } from 'shared/types';

type Props = {
  currentProfile: ExecutorConfig | null;
  selectedVariant: string | null;
  onChange: (variant: string | null) => void;
  disabled?: boolean;
  className?: string;
};

const VariantSelectorInner = forwardRef<HTMLButtonElement, Props>(
  ({ currentProfile, selectedVariant, onChange, disabled, className }, ref) => {
    // Bump-effect animation when cycling through variants
    const [isAnimating, setIsAnimating] = useState(false);
    useEffect(() => {
      if (!currentProfile) return;
      setIsAnimating(true);
      const t = setTimeout(() => setIsAnimating(false), 300);
      return () => clearTimeout(t);
    }, [selectedVariant, currentProfile]);

    const hasVariants =
      currentProfile && Object.keys(currentProfile).length > 0;

    if (!currentProfile) return null;

    if (!hasVariants) {
      return (
        <Button
          ref={ref}
          variant="outline"
          size="sm"
          className={cn(
            'h-10 w-24 px-2 flex items-center justify-between',
            className
          )}
          disabled
        >
          <span className="text-xs truncate flex-1 text-left">Default</span>
        </Button>
      );
    }

    return (
      <DropdownMenu>
        <DropdownMenuTrigger asChild>
          <Button
            ref={ref}
            variant="secondary"
            size="sm"
            className={cn(
              'w-18 md:w-24 px-2 flex items-center justify-between transition-all',
              isAnimating && 'scale-105 bg-accent',
              className
            )}
            disabled={disabled}
          >
            <span className="text-xs truncate flex-1 text-left">
              {selectedVariant || 'DEFAULT'}
            </span>
            <ChevronDown className="h-3 w-3 ml-1 flex-shrink-0" />
          </Button>
        </DropdownMenuTrigger>
        <DropdownMenuContent>
          {Object.entries(currentProfile).map(([variantLabel]) => (
            <DropdownMenuItem
              key={variantLabel}
              onClick={() => onChange(variantLabel)}
              className={selectedVariant === variantLabel ? 'bg-accent' : ''}
            >
              {variantLabel}
            </DropdownMenuItem>
          ))}
        </DropdownMenuContent>
      </DropdownMenu>
    );
  }
);

VariantSelectorInner.displayName = 'VariantSelector';
export const VariantSelector = memo(VariantSelectorInner);
</file>

<file path="frontend/src/components/ui/shadcn-io/kanban/index.tsx">
'use client';

import { Card } from '@/components/ui/card';
import { cn } from '@/lib/utils';
import type { DragEndEvent } from '@dnd-kit/core';
import {
  DndContext,
  PointerSensor,
  rectIntersection,
  useDraggable,
  useDroppable,
  useSensor,
  useSensors,
} from '@dnd-kit/core';
import { restrictToFirstScrollableAncestor } from '@dnd-kit/modifiers';
import type { ReactNode, Ref, KeyboardEvent } from 'react';

export type { DragEndEvent } from '@dnd-kit/core';

export type Status = {
  id: string;
  name: string;
  color: string;
};

export type Feature = {
  id: string;
  name: string;
  startAt: Date;
  endAt: Date;
  status: Status;
};

export type KanbanBoardProps = {
  id: Status['id'];
  children: ReactNode;
  className?: string;
};

export const KanbanBoard = ({ id, children, className }: KanbanBoardProps) => {
  const { isOver, setNodeRef } = useDroppable({ id });

  return (
    <div
      className={cn(
        'flex h-full min-h-40 flex-col',
        isOver ? 'outline-primary' : 'outline-black',
        className
      )}
      ref={setNodeRef}
    >
      {children}
    </div>
  );
};

export type KanbanCardProps = Pick<Feature, 'id' | 'name'> & {
  index: number;
  parent: string;
  children?: ReactNode;
  className?: string;
  onClick?: () => void;
  tabIndex?: number;
  forwardedRef?: Ref<HTMLDivElement>;
  onKeyDown?: (e: KeyboardEvent) => void;
};

export const KanbanCard = ({
  id,
  name,
  index,
  parent,
  children,
  className,
  onClick,
  tabIndex,
  forwardedRef,
  onKeyDown,
}: KanbanCardProps) => {
  const { attributes, listeners, setNodeRef, transform, isDragging } =
    useDraggable({
      id,
      data: { index, parent },
    });

  // Combine DnD ref and forwarded ref
  const combinedRef = (node: HTMLDivElement | null) => {
    setNodeRef(node);
    if (typeof forwardedRef === 'function') {
      forwardedRef(node);
    } else if (forwardedRef && typeof forwardedRef === 'object') {
      (forwardedRef as React.MutableRefObject<HTMLDivElement | null>).current =
        node;
    }
  };

  return (
    <Card
      className={cn(
        'p-3 focus:ring-2 ring-secondary-foreground outline-none border-b flex-col space-y-2',
        isDragging && 'cursor-grabbing',
        className
      )}
      {...listeners}
      {...attributes}
      ref={combinedRef}
      tabIndex={tabIndex}
      onClick={onClick}
      onKeyDown={onKeyDown}
      style={{
        zIndex: isDragging ? 1000 : 1,
        transform: transform
          ? `translateX(${transform.x}px) translateY(${transform.y}px)`
          : 'none',
      }}
    >
      {children ?? <p className="m-0 font-medium text-sm">{name}</p>}
    </Card>
  );
};

export type KanbanCardsProps = {
  children: ReactNode;
  className?: string;
};

export const KanbanCards = ({ children, className }: KanbanCardsProps) => (
  <div className={cn('flex flex-1 flex-col', className)}>{children}</div>
);

export type KanbanHeaderProps =
  | {
      children: ReactNode;
    }
  | {
      name: Status['name'];
      color: Status['color'];
      className?: string;
    };

export const KanbanHeader = (props: KanbanHeaderProps) =>
  'children' in props ? (
    props.children
  ) : (
    <Card
      className={cn(
        'sticky top-0 z-20 flex shrink-0 items-center gap-2 p-3 border-b border-dashed',
        'bg-background',
        props.className
      )}
      style={{
        backgroundImage: `linear-gradient(hsl(var(${props.color}) / 0.03), hsl(var(${props.color}) / 0.03))`,
      }}
    >
      <div
        className="h-2 w-2 rounded-full"
        style={{ backgroundColor: `hsl(var(${props.color}))` }}
      />
      <p className="m-0 text-sm">{props.name}</p>
    </Card>
  );

export type KanbanProviderProps = {
  children: ReactNode;
  onDragEnd: (event: DragEndEvent) => void;
  className?: string;
};

export const KanbanProvider = ({
  children,
  onDragEnd,
  className,
}: KanbanProviderProps) => {
  const sensors = useSensors(
    useSensor(PointerSensor, {
      activationConstraint: { distance: 8 },
    })
  );

  return (
    <DndContext
      collisionDetection={rectIntersection}
      onDragEnd={onDragEnd}
      sensors={sensors}
      modifiers={[restrictToFirstScrollableAncestor]}
    >
      <div
        className={cn(
          'inline-grid grid-flow-col auto-cols-[minmax(200px,400px)] divide-x border-x h-full',
          className
        )}
      >
        {children}
      </div>
    </DndContext>
  );
};
</file>

<file path="frontend/src/components/ui/alert.tsx">
import * as React from 'react';
import { cva, type VariantProps } from 'class-variance-authority';

import { cn } from '@/lib/utils';

const alertVariants = cva(
  'relative w-full border p-4 [&>svg~*]:pl-7 [&>svg+div]:translate-y-[-3px] [&>svg]:absolute [&>svg]:left-4 [&>svg]:top-4 [&>svg]:text-foreground',
  {
    variants: {
      variant: {
        default: 'bg-background text-foreground',
        destructive:
          'border-destructive/50 text-destructive dark:border-destructive [&>svg]:text-destructive',
      },
    },
    defaultVariants: {
      variant: 'default',
    },
  }
);

const Alert = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement> & VariantProps<typeof alertVariants>
>(({ className, variant, ...props }, ref) => (
  <div
    ref={ref}
    role="alert"
    className={cn(alertVariants({ variant }), className)}
    {...props}
  />
));
Alert.displayName = 'Alert';

const AlertTitle = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLHeadingElement>
>(({ className, ...props }, ref) => (
  <h5
    ref={ref}
    className={cn('mb-1 font-medium leading-none tracking-tight', className)}
    {...props}
  />
));
AlertTitle.displayName = 'AlertTitle';

const AlertDescription = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLParagraphElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn('text-sm [&_p]:leading-relaxed', className)}
    {...props}
  />
));
AlertDescription.displayName = 'AlertDescription';

export { Alert, AlertTitle, AlertDescription };
</file>

<file path="frontend/src/components/ui/auto-expanding-textarea.tsx">
import * as React from 'react';
import { cn } from '@/lib/utils';

interface AutoExpandingTextareaProps extends React.ComponentProps<'textarea'> {
  maxRows?: number;
}

const AutoExpandingTextarea = React.forwardRef<
  HTMLTextAreaElement,
  AutoExpandingTextareaProps
>(({ className, maxRows = 10, ...props }, ref) => {
  const internalRef = React.useRef<HTMLTextAreaElement>(null);

  // Get the actual ref to use
  const textareaRef = ref || internalRef;

  const adjustHeight = React.useCallback(() => {
    const textarea = (textareaRef as React.RefObject<HTMLTextAreaElement>)
      .current;
    if (!textarea) return;

    // Reset height to auto to get the natural height
    textarea.style.height = 'auto';

    // Calculate line height
    const style = window.getComputedStyle(textarea);
    const lineHeight = parseInt(style.lineHeight) || 20;
    const paddingTop = parseInt(style.paddingTop) || 0;
    const paddingBottom = parseInt(style.paddingBottom) || 0;

    // Calculate max height based on maxRows
    const maxHeight = lineHeight * maxRows + paddingTop + paddingBottom;

    // Set the height to scrollHeight, but cap at maxHeight
    const newHeight = Math.min(textarea.scrollHeight, maxHeight);
    textarea.style.height = `${newHeight}px`;
  }, [maxRows]);

  // Adjust height on mount and when content changes
  React.useEffect(() => {
    adjustHeight();
  }, [adjustHeight, props.value]);

  // Adjust height on input
  const handleInput = React.useCallback(
    (e: React.FormEvent<HTMLTextAreaElement>) => {
      adjustHeight();
      if (props.onInput) {
        props.onInput(e);
      }
    },
    [adjustHeight, props.onInput]
  );

  return (
    <textarea
      className={cn(
        'bg-muted p-0 min-h-[80px] w-full text-sm outline-none disabled:cursor-not-allowed disabled:opacity-50 resize-none overflow-y-auto overflow-x-hidden whitespace-pre-wrap break-words',
        className
      )}
      ref={textareaRef}
      onInput={handleInput}
      {...props}
    />
  );
});

AutoExpandingTextarea.displayName = 'AutoExpandingTextarea';

export { AutoExpandingTextarea };
</file>

<file path="frontend/src/components/ui/badge.tsx">
import * as React from 'react';
import { cva, type VariantProps } from 'class-variance-authority';

import { cn } from '@/lib/utils';

const badgeVariants = cva(
  'inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2',
  {
    variants: {
      variant: {
        default:
          'border-transparent bg-primary text-primary-foreground hover:bg-primary/80',
        secondary:
          'border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80',
        destructive:
          'border-transparent bg-destructive text-destructive-foreground hover:bg-destructive/80',
        outline: 'text-foreground',
      },
    },
    defaultVariants: {
      variant: 'default',
    },
  }
);

export interface BadgeProps
  extends React.HTMLAttributes<HTMLDivElement>,
    VariantProps<typeof badgeVariants> {}

function Badge({ className, variant, ...props }: BadgeProps) {
  return (
    <div className={cn(badgeVariants({ variant }), className)} {...props} />
  );
}

export { Badge, badgeVariants };
</file>

<file path="frontend/src/components/ui/button.tsx">
import * as React from 'react';
import { Slot } from '@radix-ui/react-slot';
import { cva, type VariantProps } from 'class-variance-authority';

import { cn } from '@/lib/utils';

const buttonVariants = cva(
  'inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50',
  {
    variants: {
      variant: {
        default:
          'text-primary-foreground hover:bg-primary/90 border border-foreground',
        destructive:
          'border border-destructive text-destructive hover:bg-destructive/10',
        outline:
          'border border-input hover:bg-accent hover:text-accent-foreground',
        secondary: 'text-secondary-foreground hover:bg-secondary/80 border',
        ghost: 'hover:text-primary-foreground/50',
        link: 'hover:underline',
      },
      size: {
        default: 'h-10 px-4 py-2',
        xs: 'h-8 px-2 text-xs',
        sm: 'h-9 px-3',
        lg: 'h-11 px-8',
        icon: 'h-10 w-10',
      },
    },
    defaultVariants: {
      variant: 'default',
      size: 'default',
    },
  }
);

export interface ButtonProps
  extends React.ButtonHTMLAttributes<HTMLButtonElement>,
    VariantProps<typeof buttonVariants> {
  asChild?: boolean;
}

const Button = React.forwardRef<HTMLButtonElement, ButtonProps>(
  ({ className, variant, size, asChild = false, ...props }, ref) => {
    const Comp = asChild ? Slot : 'button';
    return (
      <Comp
        className={cn(buttonVariants({ variant, size, className }))}
        ref={ref}
        {...props}
      />
    );
  }
);
Button.displayName = 'Button';

export { Button, buttonVariants };
</file>

<file path="frontend/src/components/ui/card.tsx">
import * as React from 'react';

import { cn } from '@/lib/utils';

const Card = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn('bg-card text-card-foreground', className)}
    {...props}
  />
));
Card.displayName = 'Card';

const CardHeader = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn('flex flex-col space-y-1.5 p-6', className)}
    {...props}
  />
));
CardHeader.displayName = 'CardHeader';

const CardTitle = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn(
      'text-2xl font-semibold leading-none tracking-tight',
      className
    )}
    {...props}
  />
));
CardTitle.displayName = 'CardTitle';

const CardDescription = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn('text-sm text-muted-foreground', className)}
    {...props}
  />
));
CardDescription.displayName = 'CardDescription';

const CardContent = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div ref={ref} className={cn('p-6 pt-0', className)} {...props} />
));
CardContent.displayName = 'CardContent';

const CardFooter = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn('flex items-center p-6 pt-0', className)}
    {...props}
  />
));
CardFooter.displayName = 'CardFooter';

export {
  Card,
  CardHeader,
  CardFooter,
  CardTitle,
  CardDescription,
  CardContent,
};
</file>

<file path="frontend/src/components/ui/carousel.tsx">
import * as React from 'react';
import useEmblaCarousel, {
  type UseEmblaCarouselType,
} from 'embla-carousel-react';
import { ArrowLeft, ArrowRight } from 'lucide-react';

import { cn } from '@/lib/utils';
import { Button } from '@/components/ui/button';

type CarouselApi = UseEmblaCarouselType[1];
type UseCarouselParameters = Parameters<typeof useEmblaCarousel>;
type CarouselOptions = UseCarouselParameters[0];
type CarouselPlugin = UseCarouselParameters[1];

type CarouselProps = {
  opts?: CarouselOptions;
  plugins?: CarouselPlugin;
  orientation?: 'horizontal' | 'vertical';
  setApi?: (api: CarouselApi) => void;
};

type CarouselContextProps = {
  carouselRef: ReturnType<typeof useEmblaCarousel>[0];
  api: ReturnType<typeof useEmblaCarousel>[1];
  scrollPrev: () => void;
  scrollNext: () => void;
  canScrollPrev: boolean;
  canScrollNext: boolean;
} & CarouselProps;

const CarouselContext = React.createContext<CarouselContextProps | null>(null);

function useCarousel() {
  const context = React.useContext(CarouselContext);

  if (!context) {
    throw new Error('useCarousel must be used within a <Carousel />');
  }

  return context;
}

const Carousel = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement> & CarouselProps
>(
  (
    {
      orientation = 'horizontal',
      opts,
      setApi,
      plugins,
      className,
      children,
      ...props
    },
    ref
  ) => {
    const [carouselRef, api] = useEmblaCarousel(
      {
        ...opts,
        axis: orientation === 'horizontal' ? 'x' : 'y',
      },
      plugins
    );
    const [canScrollPrev, setCanScrollPrev] = React.useState(false);
    const [canScrollNext, setCanScrollNext] = React.useState(false);

    const onSelect = React.useCallback((api: CarouselApi) => {
      if (!api) {
        return;
      }

      setCanScrollPrev(api.canScrollPrev());
      setCanScrollNext(api.canScrollNext());
    }, []);

    const scrollPrev = React.useCallback(() => {
      api?.scrollPrev();
    }, [api]);

    const scrollNext = React.useCallback(() => {
      api?.scrollNext();
    }, [api]);

    const handleKeyDown = React.useCallback(
      (event: React.KeyboardEvent<HTMLDivElement>) => {
        if (event.key === 'ArrowLeft') {
          event.preventDefault();
          scrollPrev();
        } else if (event.key === 'ArrowRight') {
          event.preventDefault();
          scrollNext();
        }
      },
      [scrollPrev, scrollNext]
    );

    React.useEffect(() => {
      if (!api || !setApi) {
        return;
      }

      setApi(api);
    }, [api, setApi]);

    React.useEffect(() => {
      if (!api) {
        return;
      }

      onSelect(api);
      api.on('reInit', onSelect);
      api.on('select', onSelect);

      return () => {
        api?.off('select', onSelect);
      };
    }, [api, onSelect]);

    return (
      <CarouselContext.Provider
        value={{
          carouselRef,
          api: api,
          opts,
          orientation:
            orientation || (opts?.axis === 'y' ? 'vertical' : 'horizontal'),
          scrollPrev,
          scrollNext,
          canScrollPrev,
          canScrollNext,
        }}
      >
        <div
          ref={ref}
          onKeyDownCapture={handleKeyDown}
          className={cn('relative', className)}
          role="region"
          aria-roledescription="carousel"
          {...props}
        >
          {children}
        </div>
      </CarouselContext.Provider>
    );
  }
);
Carousel.displayName = 'Carousel';

const CarouselContent = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => {
  const { carouselRef, orientation } = useCarousel();

  return (
    <div ref={carouselRef} className="overflow-hidden">
      <div
        ref={ref}
        className={cn(
          'flex',
          orientation === 'horizontal' ? '-ml-4' : '-mt-4 flex-col',
          className
        )}
        {...props}
      />
    </div>
  );
});
CarouselContent.displayName = 'CarouselContent';

const CarouselItem = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => {
  const { orientation } = useCarousel();

  return (
    <div
      ref={ref}
      role="group"
      aria-roledescription="slide"
      className={cn(
        'min-w-0 shrink-0 grow-0 basis-full',
        orientation === 'horizontal' ? 'pl-4' : 'pt-4',
        className
      )}
      {...props}
    />
  );
});
CarouselItem.displayName = 'CarouselItem';

const CarouselPrevious = React.forwardRef<
  HTMLButtonElement,
  React.ComponentProps<typeof Button>
>(({ className, variant = 'outline', size = 'icon', ...props }, ref) => {
  const { orientation, scrollPrev, canScrollPrev } = useCarousel();

  return (
    <Button
      ref={ref}
      variant={variant}
      size={size}
      className={cn(
        'absolute  h-8 w-8 rounded-full',
        orientation === 'horizontal'
          ? '-left-12 top-1/2 -translate-y-1/2'
          : '-top-12 left-1/2 -translate-x-1/2 rotate-90',
        className
      )}
      disabled={!canScrollPrev}
      onClick={scrollPrev}
      {...props}
    >
      <ArrowLeft className="h-4 w-4" />
      <span className="sr-only">Previous slide</span>
    </Button>
  );
});
CarouselPrevious.displayName = 'CarouselPrevious';

const CarouselNext = React.forwardRef<
  HTMLButtonElement,
  React.ComponentProps<typeof Button>
>(({ className, variant = 'outline', size = 'icon', ...props }, ref) => {
  const { orientation, scrollNext, canScrollNext } = useCarousel();

  return (
    <Button
      ref={ref}
      variant={variant}
      size={size}
      className={cn(
        'absolute h-8 w-8 rounded-full',
        orientation === 'horizontal'
          ? '-right-12 top-1/2 -translate-y-1/2'
          : '-bottom-12 left-1/2 -translate-x-1/2 rotate-90',
        className
      )}
      disabled={!canScrollNext}
      onClick={scrollNext}
      {...props}
    >
      <ArrowRight className="h-4 w-4" />
      <span className="sr-only">Next slide</span>
    </Button>
  );
});
CarouselNext.displayName = 'CarouselNext';

export {
  type CarouselApi,
  Carousel,
  CarouselContent,
  CarouselItem,
  CarouselPrevious,
  CarouselNext,
};
</file>

<file path="frontend/src/components/ui/checkbox.tsx">
import * as React from 'react';
import { Check } from 'lucide-react';
import { cn } from '@/lib/utils';

interface CheckboxProps {
  id?: string;
  checked?: boolean;
  onCheckedChange?: (checked: boolean) => void;
  className?: string;
  disabled?: boolean;
}

const Checkbox = React.forwardRef<HTMLButtonElement, CheckboxProps>(
  (
    { className, checked = false, onCheckedChange, disabled, ...props },
    ref
  ) => {
    return (
      <button
        type="button"
        role="checkbox"
        aria-checked={checked}
        ref={ref}
        className={cn(
          'peer h-4 w-4 shrink-0 rounded-sm border border-primary-foreground ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50',
          checked && 'bg-primary text-primary-foreground',
          className
        )}
        disabled={disabled}
        onClick={() => onCheckedChange?.(!checked)}
        {...props}
      >
        {checked && (
          <div className="flex items-center justify-center text-current">
            <Check className="h-4 w-4" />
          </div>
        )}
      </button>
    );
  }
);
Checkbox.displayName = 'Checkbox';

export { Checkbox };
</file>

<file path="frontend/src/components/ui/dialog.tsx">
import * as React from 'react';
import { X } from 'lucide-react';

import { cn } from '@/lib/utils';
import { useDialogKeyboardShortcuts } from '@/lib/keyboard-shortcuts';

const Dialog = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement> & {
    open?: boolean;
    onOpenChange?: (open: boolean) => void;
    uncloseable?: boolean;
  }
>(({ className, open, onOpenChange, children, uncloseable, ...props }, ref) => {
  // Add keyboard shortcut support for closing dialog with Esc
  useDialogKeyboardShortcuts(() => {
    if (open && onOpenChange && !uncloseable) {
      onOpenChange(false);
    }
  });

  if (!open) return null;

  return (
    <div className="fixed inset-0 z-[9999] flex items-start justify-center p-4 overflow-y-auto">
      <div
        className="fixed inset-0 bg-black/50"
        onClick={() => (uncloseable ? {} : onOpenChange?.(false))}
      />
      <div
        ref={ref}
        className={cn(
          'relative z-[9999] grid w-full max-w-lg gap-4 bg-primary p-6 shadow-lg duration-200 sm:rounded-lg my-8',
          className
        )}
        {...props}
      >
        {!uncloseable && (
          <button
            className="absolute right-4 top-4 rounded-sm opacity-70 ring-offset-background transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2"
            onClick={() => onOpenChange?.(false)}
          >
            <X className="h-4 w-4" />
            <span className="sr-only">Close</span>
          </button>
        )}
        {children}
      </div>
    </div>
  );
});
Dialog.displayName = 'Dialog';

const DialogHeader = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      'flex flex-col space-y-1.5 text-center sm:text-left',
      className
    )}
    {...props}
  />
);
DialogHeader.displayName = 'DialogHeader';

const DialogTitle = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLHeadingElement>
>(({ className, ...props }, ref) => (
  <h3
    ref={ref}
    className={cn(
      'text-lg font-semibold leading-none tracking-tight',
      className
    )}
    {...props}
  />
));
DialogTitle.displayName = 'DialogTitle';

const DialogDescription = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLParagraphElement>
>(({ className, ...props }, ref) => (
  <p
    ref={ref}
    className={cn('text-sm text-muted-foreground', className)}
    {...props}
  />
));
DialogDescription.displayName = 'DialogDescription';

const DialogContent = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div ref={ref} className={cn('grid gap-4', className)} {...props} />
));
DialogContent.displayName = 'DialogContent';

const DialogFooter = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      'flex flex-col-reverse sm:flex-row sm:justify-end sm:space-x-2',
      className
    )}
    {...props}
  />
);
DialogFooter.displayName = 'DialogFooter';

export {
  Dialog,
  DialogContent,
  DialogDescription,
  DialogFooter,
  DialogHeader,
  DialogTitle,
};
</file>

<file path="frontend/src/components/ui/dropdown-menu.tsx">
import * as React from 'react';
import * as DropdownMenuPrimitive from '@radix-ui/react-dropdown-menu';
import { Check, ChevronRight, Circle } from 'lucide-react';

import { cn } from '@/lib/utils';

const DropdownMenu = DropdownMenuPrimitive.Root;

const DropdownMenuTrigger = DropdownMenuPrimitive.Trigger;

const DropdownMenuGroup = DropdownMenuPrimitive.Group;

const DropdownMenuPortal = DropdownMenuPrimitive.Portal;

const DropdownMenuSub = DropdownMenuPrimitive.Sub;

const DropdownMenuRadioGroup = DropdownMenuPrimitive.RadioGroup;

const DropdownMenuSubTrigger = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubTrigger>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubTrigger> & {
    inset?: boolean;
  }
>(({ className, inset, children, ...props }, ref) => (
  <DropdownMenuPrimitive.SubTrigger
    ref={ref}
    className={cn(
      'flex cursor-default select-none items-center gap-2 px-2 py-1.5 text-sm outline-none focus:bg-accent data-[state=open]:bg-accent [&_svg]:pointer-events-none [&_svg]:size-4 [&_svg]:shrink-0',
      inset && 'pl-8',
      className
    )}
    {...props}
  >
    {children}
    <ChevronRight className="ml-auto" />
  </DropdownMenuPrimitive.SubTrigger>
));
DropdownMenuSubTrigger.displayName =
  DropdownMenuPrimitive.SubTrigger.displayName;

const DropdownMenuSubContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubContent>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubContent>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.SubContent
    ref={ref}
    className={cn(
      'bg-primary z-[10000] min-w-[8rem] overflow-hidden border bg-popover p-1 text-popover-foreground shadow-lg data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 origin-[--radix-dropdown-menu-content-transform-origin]',
      className
    )}
    {...props}
  />
));
DropdownMenuSubContent.displayName =
  DropdownMenuPrimitive.SubContent.displayName;

const DropdownMenuContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <DropdownMenuPrimitive.Portal>
    <DropdownMenuPrimitive.Content
      ref={ref}
      sideOffset={sideOffset}
      className={cn(
        'bg-primary z-[10000] max-h-[var(--radix-dropdown-menu-content-available-height)] min-w-[8rem] overflow-y-auto overflow-x-hidden border p-1 text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 origin-[--radix-dropdown-menu-content-transform-origin]',
        className
      )}
      {...props}
    />
  </DropdownMenuPrimitive.Portal>
));
DropdownMenuContent.displayName = DropdownMenuPrimitive.Content.displayName;

const DropdownMenuItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Item> & {
    inset?: boolean;
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Item
    ref={ref}
    className={cn(
      'relative flex cursor-default select-none items-center gap-2 px-2 py-1.5 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50 [&_svg]:pointer-events-none [&_svg]:size-4 [&_svg]:shrink-0',
      inset && 'pl-8',
      className
    )}
    {...props}
  />
));
DropdownMenuItem.displayName = DropdownMenuPrimitive.Item.displayName;

const DropdownMenuCheckboxItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.CheckboxItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.CheckboxItem>
>(({ className, children, checked, ...props }, ref) => (
  <DropdownMenuPrimitive.CheckboxItem
    ref={ref}
    className={cn(
      'relative flex cursor-default select-none items-center py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50',
      className
    )}
    checked={checked}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <Check className="h-4 w-4" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.CheckboxItem>
));
DropdownMenuCheckboxItem.displayName =
  DropdownMenuPrimitive.CheckboxItem.displayName;

const DropdownMenuRadioItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.RadioItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.RadioItem>
>(({ className, children, ...props }, ref) => (
  <DropdownMenuPrimitive.RadioItem
    ref={ref}
    className={cn(
      'relative flex cursor-default select-none items-center py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50',
      className
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <Circle className="h-2 w-2 fill-current" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.RadioItem>
));
DropdownMenuRadioItem.displayName = DropdownMenuPrimitive.RadioItem.displayName;

const DropdownMenuLabel = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Label> & {
    inset?: boolean;
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Label
    ref={ref}
    className={cn(
      'px-2 py-1.5 text-sm font-semibold',
      inset && 'pl-8',
      className
    )}
    {...props}
  />
));
DropdownMenuLabel.displayName = DropdownMenuPrimitive.Label.displayName;

const DropdownMenuSeparator = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.Separator
    ref={ref}
    className={cn('-mx-1 my-1 h-px bg-border', className)}
    {...props}
  />
));
DropdownMenuSeparator.displayName = DropdownMenuPrimitive.Separator.displayName;

const DropdownMenuShortcut = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLSpanElement>) => {
  return (
    <span
      className={cn('ml-auto text-xs tracking-widest opacity-60', className)}
      {...props}
    />
  );
};
DropdownMenuShortcut.displayName = 'DropdownMenuShortcut';

export {
  DropdownMenu,
  DropdownMenuTrigger,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuCheckboxItem,
  DropdownMenuRadioItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuShortcut,
  DropdownMenuGroup,
  DropdownMenuPortal,
  DropdownMenuSub,
  DropdownMenuSubContent,
  DropdownMenuSubTrigger,
  DropdownMenuRadioGroup,
};
</file>

<file path="frontend/src/components/ui/file-search-textarea.tsx">
import { KeyboardEvent, useEffect, useRef, useState } from 'react';
import { createPortal } from 'react-dom';
import { AutoExpandingTextarea } from '@/components/ui/auto-expanding-textarea';
import { projectsApi } from '@/lib/api';

import type { SearchResult } from 'shared/types';

interface FileSearchResult extends SearchResult {
  name: string;
}

interface FileSearchTextareaProps {
  value: string;
  onChange: (value: string) => void;
  placeholder?: string;
  rows?: number;
  disabled?: boolean;
  className?: string;
  projectId?: string;
  onKeyDown?: (e: React.KeyboardEvent) => void;
  maxRows?: number;
}

export function FileSearchTextarea({
  value,
  onChange,
  placeholder,
  rows = 3,
  disabled = false,
  className,
  projectId,
  onKeyDown,
  maxRows = 10,
}: FileSearchTextareaProps) {
  const [searchQuery, setSearchQuery] = useState('');
  const [searchResults, setSearchResults] = useState<FileSearchResult[]>([]);
  const [showDropdown, setShowDropdown] = useState(false);
  const [selectedIndex, setSelectedIndex] = useState(-1);

  const [atSymbolPosition, setAtSymbolPosition] = useState(-1);
  const [isLoading, setIsLoading] = useState(false);

  const textareaRef = useRef<HTMLTextAreaElement>(null);
  const dropdownRef = useRef<HTMLDivElement>(null);

  // Search for files when query changes
  useEffect(() => {
    if (!searchQuery || !projectId || searchQuery.length < 1) {
      setSearchResults([]);
      setShowDropdown(false);
      return;
    }

    const searchFiles = async () => {
      setIsLoading(true);

      try {
        const result = await projectsApi.searchFiles(projectId, searchQuery);
        // Transform SearchResult to FileSearchResult by adding name field
        const fileResults: FileSearchResult[] = result.map((item) => ({
          ...item,
          name: item.path.split('/').pop() || item.path,
        }));
        setSearchResults(fileResults);
        setShowDropdown(true);
        setSelectedIndex(-1);
      } catch (error) {
        console.error('Failed to search files:', error);
      } finally {
        setIsLoading(false);
      }
    };

    const debounceTimer = setTimeout(searchFiles, 300);
    return () => clearTimeout(debounceTimer);
  }, [searchQuery, projectId]);

  // Handle text changes and detect @ symbol
  const handleChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    const newValue = e.target.value;
    const newCursorPosition = e.target.selectionStart || 0;

    onChange(newValue);

    // Check if @ was just typed
    const textBeforeCursor = newValue.slice(0, newCursorPosition);
    const lastAtIndex = textBeforeCursor.lastIndexOf('@');

    if (lastAtIndex !== -1) {
      // Check if there's no space after the @ (still typing the search query)
      const textAfterAt = textBeforeCursor.slice(lastAtIndex + 1);
      const hasSpace = textAfterAt.includes(' ') || textAfterAt.includes('\n');

      if (!hasSpace) {
        setAtSymbolPosition(lastAtIndex);
        setSearchQuery(textAfterAt);
        return;
      }
    }

    // If no valid @ context, hide dropdown
    setShowDropdown(false);
    setSearchQuery('');
    setAtSymbolPosition(-1);
  };

  // Handle keyboard navigation
  const handleKeyDown = (e: KeyboardEvent<HTMLTextAreaElement>) => {
    // Handle dropdown navigation first
    if (showDropdown && searchResults.length > 0) {
      switch (e.key) {
        case 'ArrowDown':
          e.preventDefault();
          setSelectedIndex((prev) =>
            prev < searchResults.length - 1 ? prev + 1 : 0
          );
          return;
        case 'ArrowUp':
          e.preventDefault();
          setSelectedIndex((prev) =>
            prev > 0 ? prev - 1 : searchResults.length - 1
          );
          return;
        case 'Enter':
          if (selectedIndex >= 0) {
            e.preventDefault();
            selectFile(searchResults[selectedIndex]);
            return;
          }
          break;
        case 'Escape':
          e.preventDefault();
          setShowDropdown(false);
          setSearchQuery('');
          setAtSymbolPosition(-1);
          return;
      }
    }

    // Call the passed onKeyDown handler
    onKeyDown?.(e);
  };

  // Select a file and insert it into the text
  const selectFile = (file: FileSearchResult) => {
    if (atSymbolPosition === -1) return;

    const beforeAt = value.slice(0, atSymbolPosition);
    const afterQuery = value.slice(atSymbolPosition + 1 + searchQuery.length);
    const newValue = beforeAt + file.path + afterQuery;

    onChange(newValue);
    setShowDropdown(false);
    setSearchQuery('');
    setAtSymbolPosition(-1);

    // Focus back to textarea
    setTimeout(() => {
      if (textareaRef.current) {
        const newCursorPos = atSymbolPosition + file.path.length;
        textareaRef.current.focus();
        textareaRef.current.setSelectionRange(newCursorPos, newCursorPos);
      }
    }, 0);
  };

  // Calculate dropdown position relative to textarea (simpler, more stable approach)
  const getDropdownPosition = () => {
    if (!textareaRef.current) return { top: 0, left: 0, maxHeight: 240 };

    const textareaRect = textareaRef.current.getBoundingClientRect();
    const dropdownWidth = 256; // min-w-64 = 256px
    const maxDropdownHeight = 320;
    const minDropdownHeight = 120;

    // Position dropdown below the textarea by default
    let finalTop = textareaRect.bottom + 4; // 4px gap
    let finalLeft = textareaRect.left;
    let maxHeight = maxDropdownHeight;

    // Ensure dropdown doesn't go off the right edge
    if (finalLeft + dropdownWidth > window.innerWidth - 16) {
      finalLeft = window.innerWidth - dropdownWidth - 16;
    }

    // Ensure dropdown doesn't go off the left edge
    if (finalLeft < 16) {
      finalLeft = 16;
    }

    // Calculate available space below and above textarea
    const availableSpaceBelow = window.innerHeight - textareaRect.bottom - 32;
    const availableSpaceAbove = textareaRect.top - 32;

    // If not enough space below, position above
    if (
      availableSpaceBelow < minDropdownHeight &&
      availableSpaceAbove > availableSpaceBelow
    ) {
      // Get actual height from rendered dropdown
      const actualHeight =
        dropdownRef.current?.getBoundingClientRect().height ||
        minDropdownHeight;
      finalTop = textareaRect.top - actualHeight - 4;
      maxHeight = Math.min(
        maxDropdownHeight,
        Math.max(availableSpaceAbove, minDropdownHeight)
      );
    } else {
      // Position below with available space
      maxHeight = Math.min(
        maxDropdownHeight,
        Math.max(availableSpaceBelow, minDropdownHeight)
      );
    }

    return { top: finalTop, left: finalLeft, maxHeight };
  };

  // Use effect to reposition when dropdown content changes
  useEffect(() => {
    if (showDropdown && dropdownRef.current) {
      // Small delay to ensure content is rendered
      setTimeout(() => {
        const newPosition = getDropdownPosition();
        if (dropdownRef.current) {
          dropdownRef.current.style.top = `${newPosition.top}px`;
          dropdownRef.current.style.left = `${newPosition.left}px`;
          dropdownRef.current.style.maxHeight = `${newPosition.maxHeight}px`;
        }
      }, 0);
    }
  }, [searchResults.length, showDropdown]);

  const dropdownPosition = getDropdownPosition();

  return (
    <div
      className={`relative ${className?.includes('flex-1') ? 'flex-1' : ''}`}
    >
      <AutoExpandingTextarea
        ref={textareaRef}
        value={value}
        onChange={handleChange}
        onKeyDown={handleKeyDown}
        placeholder={placeholder}
        rows={rows}
        disabled={disabled}
        className={className}
        maxRows={maxRows}
      />

      {showDropdown &&
        createPortal(
          <div
            ref={dropdownRef}
            className="fixed bg-background border border-border rounded-md shadow-lg overflow-y-auto min-w-64"
            style={{
              top: dropdownPosition.top,
              left: dropdownPosition.left,
              maxHeight: dropdownPosition.maxHeight,
              zIndex: 10000, // Higher than dialog z-[9999]
            }}
          >
            {isLoading ? (
              <div className="p-2 text-sm text-muted-foreground">
                Searching...
              </div>
            ) : searchResults.length === 0 ? (
              <div className="p-2 text-sm text-muted-foreground">
                No files found
              </div>
            ) : (
              <div className="py-1">
                {searchResults.map((file, index) => (
                  <div
                    key={file.path}
                    className={`px-3 py-2 cursor-pointer text-sm ${
                      index === selectedIndex
                        ? 'bg-blue-50 text-blue-900'
                        : 'hover:bg-muted'
                    }`}
                    onClick={() => selectFile(file)}
                  >
                    <div className="font-medium truncate">{file.name}</div>
                    <div className="text-xs text-muted-foreground truncate">
                      {file.path}
                    </div>
                  </div>
                ))}
              </div>
            )}
          </div>,
          document.body
        )}
    </div>
  );
}
</file>

<file path="frontend/src/components/ui/ImageUploadSection.tsx">
import { useState, useCallback, useRef } from 'react';
import {
  X,
  Image as ImageIcon,
  Upload,
  ChevronRight,
  AlertCircle,
} from 'lucide-react';
import { Button } from './button';
import { Alert, AlertDescription } from './alert';
import { cn } from '@/lib/utils';
import { imagesApi } from '@/lib/api';
import type { ImageResponse } from 'shared/types';

interface ImageUploadSectionProps {
  images: ImageResponse[];
  onImagesChange: (images: ImageResponse[]) => void;
  onUpload: (file: File) => Promise<ImageResponse>;
  onDelete?: (imageId: string) => Promise<void>;
  onImageUploaded?: (image: ImageResponse) => void; // Custom callback for upload success
  isUploading?: boolean;
  disabled?: boolean;
  readOnly?: boolean;
  collapsible?: boolean;
  defaultExpanded?: boolean;
  className?: string;
}

export function ImageUploadSection({
  images,
  onImagesChange,
  onUpload,
  onDelete,
  onImageUploaded,
  isUploading = false,
  disabled = false,
  readOnly = false,
  collapsible = true,
  defaultExpanded = false,
  className,
}: ImageUploadSectionProps) {
  const [isExpanded, setIsExpanded] = useState(
    defaultExpanded || images.length > 0
  );
  const [isDragging, setIsDragging] = useState(false);
  const [uploadingFiles, setUploadingFiles] = useState<Set<string>>(new Set());
  const [errorMessage, setErrorMessage] = useState<string | null>(null);
  const fileInputRef = useRef<HTMLInputElement>(null);

  const handleFileSelect = useCallback(
    async (files: FileList | null) => {
      if (!files || disabled) return;

      setErrorMessage(null);

      const MAX_SIZE = 20 * 1024 * 1024; // 20MB
      const VALID_TYPES = [
        'image/png',
        'image/jpeg',
        'image/jpg',
        'image/gif',
        'image/webp',
        'image/bmp',
        'image/svg+xml',
      ];

      const invalidFiles: string[] = [];
      const oversizedFiles: string[] = [];
      const validFiles: File[] = [];

      Array.from(files).forEach((file) => {
        if (!VALID_TYPES.includes(file.type.toLowerCase())) {
          invalidFiles.push(file.name);
          return;
        }

        if (file.size > MAX_SIZE) {
          oversizedFiles.push(
            `${file.name} (${(file.size / 1048576).toFixed(1)} MB)`
          );
          return;
        }

        validFiles.push(file);
      });

      if (invalidFiles.length > 0 || oversizedFiles.length > 0) {
        const errors: string[] = [];
        if (invalidFiles.length > 0) {
          errors.push(`Unsupported file type: ${invalidFiles.join(', ')}`);
        }
        if (oversizedFiles.length > 0) {
          errors.push(
            `Files too large (max 20 MB): ${oversizedFiles.join(', ')}`
          );
        }
        setErrorMessage(errors.join('. '));
      }

      for (const file of validFiles) {
        const tempId = `uploading-${Date.now()}-${file.name}`;
        setUploadingFiles((prev) => new Set(prev).add(tempId));

        try {
          const uploadedImage = await onUpload(file);

          // Call custom upload callback if provided, otherwise use default behavior
          if (onImageUploaded) {
            onImageUploaded(uploadedImage);
          } else {
            onImagesChange([...images, uploadedImage]);
          }

          setErrorMessage(null);
        } catch (error: any) {
          console.error('Failed to upload image:', error);
          const message =
            error.message || 'Failed to upload image. Please try again.';
          setErrorMessage(message);
        } finally {
          setUploadingFiles((prev) => {
            const next = new Set(prev);
            next.delete(tempId);
            return next;
          });
        }
      }
    },
    [images, onImagesChange, onUpload, disabled]
  );

  const handleDrop = useCallback(
    (e: React.DragEvent) => {
      e.preventDefault();
      setIsDragging(false);
      handleFileSelect(e.dataTransfer.files);
    },
    [handleFileSelect]
  );

  const handleDragOver = useCallback((e: React.DragEvent) => {
    e.preventDefault();
    setIsDragging(true);
  }, []);

  const handleDragLeave = useCallback((e: React.DragEvent) => {
    e.preventDefault();
    setIsDragging(false);
  }, []);

  const handleRemoveImage = useCallback(
    async (imageId: string) => {
      if (onDelete) {
        try {
          await onDelete(imageId);
        } catch (error) {
          console.error('Failed to delete image:', error);
        }
      }
      onImagesChange(images.filter((img) => img.id !== imageId));
    },
    [images, onImagesChange, onDelete]
  );

  const formatFileSize = (bytes: bigint) => {
    const kb = Number(bytes) / 1024;
    if (kb < 1024) {
      return `${kb.toFixed(1)} KB`;
    }
    return `${(kb / 1024).toFixed(1)} MB`;
  };

  const content = (
    <div className={cn('space-y-3', className)}>
      {/* Error message */}
      {errorMessage && (
        <Alert variant="destructive">
          <AlertCircle className="h-4 w-4" />
          <AlertDescription>{errorMessage}</AlertDescription>
        </Alert>
      )}

      {/* Read-only message */}
      {readOnly && images.length === 0 && (
        <p className="text-sm text-muted-foreground">No images attached</p>
      )}

      {/* Drop zone - only show when not read-only */}
      {!readOnly && (
        <div
          className={cn(
            'border-2 border-dashed rounded-lg p-6 text-center transition-colors',
            isDragging
              ? 'border-primary bg-primary/5'
              : 'border-muted-foreground/25 hover:border-muted-foreground/50',
            disabled && 'opacity-50 cursor-not-allowed'
          )}
          onDrop={handleDrop}
          onDragOver={handleDragOver}
          onDragLeave={handleDragLeave}
        >
          <Upload className="h-8 w-8 mx-auto mb-3 text-muted-foreground" />
          <p className="text-sm text-muted-foreground mb-1">
            Drag and drop images here, or click to select
          </p>
          <Button
            variant="secondary"
            size="sm"
            onClick={() => fileInputRef.current?.click()}
            disabled={disabled || isUploading}
          >
            Select Images
          </Button>
          <input
            ref={fileInputRef}
            type="file"
            accept="image/*"
            multiple
            className="hidden"
            onChange={(e) => handleFileSelect(e.target.files)}
            disabled={disabled}
          />
        </div>
      )}

      {/* Image previews */}
      {images.length > 0 && (
        <div className="grid grid-cols-2 gap-2">
          {images.map((image) => (
            <div
              key={image.id}
              className="relative group border rounded-lg p-2 bg-background"
            >
              <div className="flex items-center gap-2">
                <img
                  src={imagesApi.getImageUrl(image.id)}
                  alt={image.original_name}
                  className="h-16 w-16 object-cover rounded"
                />
                <div className="flex-1 min-w-0">
                  <p className="text-xs font-medium truncate">
                    {image.original_name}
                  </p>
                  <p className="text-xs text-muted-foreground">
                    {formatFileSize(image.size_bytes)}
                  </p>
                </div>
              </div>
              {!disabled && !readOnly && (
                <Button
                  variant="ghost"
                  size="icon"
                  className="absolute top-1 right-1 h-6 w-6 opacity-0 group-hover:opacity-100 transition-opacity"
                  onClick={() => handleRemoveImage(image.id)}
                >
                  <X className="h-3 w-3" />
                </Button>
              )}
            </div>
          ))}
        </div>
      )}

      {/* Uploading indicators */}
      {uploadingFiles.size > 0 && (
        <div className="space-y-1">
          {Array.from(uploadingFiles).map((tempId) => (
            <div
              key={tempId}
              className="flex items-center gap-2 text-xs text-muted-foreground"
            >
              <div className="h-3 w-3 border-2 border-primary border-t-transparent rounded-full animate-spin" />
              <span>Uploading...</span>
            </div>
          ))}
        </div>
      )}
    </div>
  );

  if (!collapsible) {
    return content;
  }

  return (
    <div className="space-y-2">
      <button
        type="button"
        onClick={() => setIsExpanded(!isExpanded)}
        className="flex items-center gap-2 text-sm text-muted-foreground hover:text-foreground transition-colors"
      >
        <ChevronRight
          className={cn(
            'h-3 w-3 transition-transform',
            isExpanded && 'rotate-90'
          )}
        />
        <ImageIcon className="h-4 w-4" />
        <span>Images {images.length > 0 && `(${images.length})`}</span>
      </button>
      {isExpanded && content}
    </div>
  );
}
</file>

<file path="frontend/src/components/ui/input.tsx">
import * as React from 'react';

import { cn } from '@/lib/utils';

export interface InputProps
  extends React.InputHTMLAttributes<HTMLInputElement> {}

const Input = React.forwardRef<HTMLInputElement, InputProps>(
  ({ className, type, ...props }, ref) => {
    return (
      <input
        type={type}
        className={cn(
          'flex h-10 w-full border px-3 py-2 text-sm ring-offset-background file:border-0 bg-transparent file:text-sm file:font-medium focus-visible:outline-none disabled:cursor-not-allowed disabled:opacity-50',
          className
        )}
        ref={ref}
        {...props}
      />
    );
  }
);
Input.displayName = 'Input';

export { Input };
</file>

<file path="frontend/src/components/ui/json-editor.tsx">
import React from 'react';
import CodeMirror from '@uiw/react-codemirror';
import { json, jsonParseLinter } from '@codemirror/lang-json';
import { linter } from '@codemirror/lint';
import { indentOnInput } from '@codemirror/language';
import { EditorView } from '@codemirror/view';
import { useTheme } from '@/components/theme-provider';
import { ThemeMode } from 'shared/types';
import { cn } from '@/lib/utils';

interface JSONEditorProps {
  value: string;
  onChange: (value: string) => void;
  placeholder?: string;
  disabled?: boolean;
  minHeight?: number;
  className?: string;
  id?: string;
}

export const JSONEditor: React.FC<JSONEditorProps> = ({
  value,
  onChange,
  placeholder,
  disabled = false,
  minHeight = 300,
  className,
  id,
}) => {
  const { theme } = useTheme();

  // Convert app theme to CodeMirror theme
  const getCodeMirrorTheme = () => {
    if (theme === ThemeMode.SYSTEM) {
      return window.matchMedia('(prefers-color-scheme: dark)').matches
        ? 'dark'
        : 'light';
    }
    return theme === ThemeMode.DARK ? 'dark' : 'light';
  };

  // Avoid SSR errors
  if (typeof window === 'undefined') return null;

  return (
    <div
      id={id}
      className={cn(
        'rounded-md border border-input bg-background overflow-hidden',
        disabled && 'opacity-50 cursor-not-allowed',
        className
      )}
    >
      <CodeMirror
        value={value}
        height={`${minHeight}px`}
        basicSetup={{
          lineNumbers: true,
          autocompletion: true,
          bracketMatching: true,
          closeBrackets: true,
          searchKeymap: true,
        }}
        extensions={[
          json(),
          linter(jsonParseLinter()),
          indentOnInput(),
          EditorView.lineWrapping,
          disabled ? EditorView.editable.of(false) : [],
        ]}
        theme={getCodeMirrorTheme()}
        onChange={onChange}
        placeholder={placeholder}
        style={{
          fontSize: '14px',
          fontFamily:
            'ui-monospace, SFMono-Regular, "SF Mono", Consolas, "Liberation Mono", Menlo, monospace',
        }}
      />
    </div>
  );
};
</file>

<file path="frontend/src/components/ui/label.tsx">
import * as React from 'react';
import * as LabelPrimitive from '@radix-ui/react-label';
import { cva, type VariantProps } from 'class-variance-authority';

import { cn } from '@/lib/utils';

const labelVariants = cva(
  'text-sm font-medium leading-none peer-disabled:cursor-not-allowed peer-disabled:opacity-70'
);

const Label = React.forwardRef<
  React.ElementRef<typeof LabelPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof LabelPrimitive.Root> &
    VariantProps<typeof labelVariants>
>(({ className, ...props }, ref) => (
  <LabelPrimitive.Root
    ref={ref}
    className={cn(labelVariants(), className)}
    {...props}
  />
));
Label.displayName = LabelPrimitive.Root.displayName;

export { Label };
</file>

<file path="frontend/src/components/ui/loader.tsx">
import { Loader2 } from 'lucide-react';
import React from 'react';

interface LoaderProps {
  message?: string | React.ReactElement;
  size?: number;
  className?: string;
}

export const Loader: React.FC<LoaderProps> = ({
  message,
  size = 32,
  className = '',
}) => (
  <div
    className={`flex flex-col items-center justify-center gap-2 ${className}`}
  >
    <Loader2
      className="animate-spin text-muted-foreground"
      style={{ width: size, height: size }}
    />
    {!!message && (
      <div className="text-center text-muted-foreground">{message}</div>
    )}
  </div>
);
</file>

<file path="frontend/src/components/ui/markdown-renderer.tsx">
import Markdown from 'markdown-to-jsx';
import { memo, useMemo, useState, useCallback } from 'react';
import {
  Tooltip,
  TooltipContent,
  TooltipProvider,
  TooltipTrigger,
} from '@/components/ui/tooltip.tsx';
import { Button } from '@/components/ui/button.tsx';
import { Check, Clipboard } from 'lucide-react';
import { writeClipboardViaBridge } from '@/vscode/bridge';

interface MarkdownRendererProps {
  content: string;
  className?: string;
  enableCopyButton?: boolean;
}

function MarkdownRenderer({
  content,
  className = '',
  enableCopyButton = false,
}: MarkdownRendererProps) {
  const overrides = useMemo(
    () => ({
      code: {
        component: ({ children, ...props }: any) => (
          <code
            {...props}
            className="bg-background px-1 py-0.5 text-sm font-mono"
          >
            {children}
          </code>
        ),
      },
      strong: {
        component: ({ children, ...props }: any) => (
          <span {...props} className="">
            {children}
          </span>
        ),
      },
      em: {
        component: ({ children, ...props }: any) => (
          <em {...props} className="italic">
            {children}
          </em>
        ),
      },
      p: {
        component: ({ children, ...props }: any) => (
          <p {...props} className="leading-tight">
            {children}
          </p>
        ),
      },
      h1: {
        component: ({ children, ...props }: any) => (
          <h1 {...props} className="text-lg leading-tight font-medium">
            {children}
          </h1>
        ),
      },
      h2: {
        component: ({ children, ...props }: any) => (
          <h2 {...props} className="text-baseleading-tight font-medium">
            {children}
          </h2>
        ),
      },
      h3: {
        component: ({ children, ...props }: any) => (
          <h3 {...props} className="text-sm leading-tight">
            {children}
          </h3>
        ),
      },
      ul: {
        component: ({ children, ...props }: any) => (
          <ul {...props} className="list-disc list-outside space-y-1 ps-6">
            {children}
          </ul>
        ),
      },
      ol: {
        component: ({ children, ...props }: any) => (
          <ol {...props} className="list-decimal list-outside space-y-1 ps-6">
            {children}
          </ol>
        ),
      },
      li: {
        component: ({ children, ...props }: any) => (
          <li {...props} className="leading-tight">
            {children}
          </li>
        ),
      },
    }),
    []
  );

  const [copied, setCopied] = useState(false);
  const handleCopy = useCallback(async () => {
    try {
      await writeClipboardViaBridge(content);
      setCopied(true);
      window.setTimeout(() => setCopied(false), 400);
    } catch {
      // noop – bridge handles fallback
    }
  }, [content]);

  return (
    <div className={`relative group`}>
      {enableCopyButton && (
        <div className="sticky top-2 right-2 z-10 pointer-events-none h-0">
          <div className="flex justify-end pr-1">
            <TooltipProvider>
              <Tooltip>
                <TooltipTrigger asChild>
                  <div className="relative">
                    <Button
                      type="button"
                      aria-label={copied ? 'Copied!' : 'Copy as Markdown'}
                      title={copied ? 'Copied!' : 'Copy as Markdown'}
                      variant="outline"
                      size="icon"
                      onClick={handleCopy}
                      className="pointer-events-auto opacity-0 group-hover:opacity-100 delay-0 transition-opacity duration-50 h-8 w-8 rounded-md bg-background/95 backdrop-blur border border-border shadow-sm"
                    >
                      {copied ? (
                        <Check className="h-4 w-4 text-green-600" />
                      ) : (
                        <Clipboard className="h-4 w-4" />
                      )}
                    </Button>
                    {copied && (
                      <div
                        className="absolute -right-1 mt-1 translate-y-1.5 select-none text-[11px] leading-none px-2 py-1 rounded bg-green-600 text-white shadow pointer-events-none"
                        role="status"
                        aria-live="polite"
                      >
                        Copied
                      </div>
                    )}
                  </div>
                </TooltipTrigger>
                <TooltipContent>
                  {copied ? 'Copied!' : 'Copy as Markdown'}
                </TooltipContent>
              </Tooltip>
            </TooltipProvider>
          </div>
        </div>
      )}
      <div className={className}>
        <Markdown options={{ overrides }}>{content}</Markdown>
      </div>
    </div>
  );
}

export default memo(MarkdownRenderer);
</file>

<file path="frontend/src/components/ui/multi-file-search-textarea.tsx">
import { KeyboardEvent, useEffect, useRef, useState } from 'react';
import { createPortal } from 'react-dom';
import { AutoExpandingTextarea } from '@/components/ui/auto-expanding-textarea';
import { projectsApi } from '@/lib/api';

import type { SearchResult } from 'shared/types';

interface FileSearchResult extends SearchResult {
  name: string;
}

interface MultiFileSearchTextareaProps {
  value: string;
  onChange: (value: string) => void;
  placeholder?: string;
  rows?: number;
  disabled?: boolean;
  className?: string;
  projectId: string;
  onKeyDown?: (e: React.KeyboardEvent) => void;
  maxRows?: number;
}

export function MultiFileSearchTextarea({
  value,
  onChange,
  placeholder = 'Start typing a file path...',
  rows = 3,
  disabled = false,
  className,
  projectId,
  onKeyDown,
  maxRows = 10,
}: MultiFileSearchTextareaProps) {
  const [searchQuery, setSearchQuery] = useState('');
  const [searchResults, setSearchResults] = useState<FileSearchResult[]>([]);
  const [showDropdown, setShowDropdown] = useState(false);
  const [selectedIndex, setSelectedIndex] = useState(-1);
  const [currentTokenStart, setCurrentTokenStart] = useState(-1);
  const [currentTokenEnd, setCurrentTokenEnd] = useState(-1);
  const [isLoading, setIsLoading] = useState(false);

  const textareaRef = useRef<HTMLTextAreaElement>(null);
  const dropdownRef = useRef<HTMLDivElement>(null);
  const abortControllerRef = useRef<AbortController | null>(null);
  const searchCacheRef = useRef<Map<string, FileSearchResult[]>>(new Map());

  // Search for files when query changes
  useEffect(() => {
    if (!searchQuery || !projectId || searchQuery.length < 2) {
      setSearchResults([]);
      setShowDropdown(false);
      return;
    }

    // Check cache first
    const cached = searchCacheRef.current.get(searchQuery);
    if (cached) {
      setSearchResults(cached);
      setShowDropdown(true);
      setSelectedIndex(-1);
      return;
    }

    const searchFiles = async () => {
      setIsLoading(true);

      // Cancel previous request
      if (abortControllerRef.current) {
        abortControllerRef.current.abort();
      }

      const abortController = new AbortController();
      abortControllerRef.current = abortController;

      try {
        const result = await projectsApi.searchFiles(
          projectId,
          searchQuery,
          'settings',
          {
            signal: abortController.signal,
          }
        );

        // Only process if this request wasn't aborted
        if (!abortController.signal.aborted) {
          const fileResults: FileSearchResult[] = result.map((item) => ({
            ...item,
            name: item.path.split('/').pop() || item.path,
          }));

          // Cache the results
          searchCacheRef.current.set(searchQuery, fileResults);

          setSearchResults(fileResults);
          setShowDropdown(true);
          setSelectedIndex(-1);
        }
      } catch (error) {
        if (!abortController.signal.aborted) {
          console.error('Failed to search files:', error);
        }
      } finally {
        if (!abortController.signal.aborted) {
          setIsLoading(false);
        }
      }
    };

    const debounceTimer = setTimeout(searchFiles, 350);
    return () => {
      clearTimeout(debounceTimer);
      if (abortControllerRef.current) {
        abortControllerRef.current.abort();
      }
    };
  }, [searchQuery, projectId]);

  // Find current token boundaries based on cursor position
  const findCurrentToken = (text: string, cursorPosition: number) => {
    const textBefore = text.slice(0, cursorPosition);
    const textAfter = text.slice(cursorPosition);

    // Find the last separator (comma or newline) before cursor
    const lastSeparatorIndex = Math.max(
      textBefore.lastIndexOf(','),
      textBefore.lastIndexOf('\n')
    );

    // Find the next separator after cursor
    const nextSeparatorIndex = Math.min(
      textAfter.indexOf(',') === -1
        ? Infinity
        : textAfter.indexOf(',') + cursorPosition,
      textAfter.indexOf('\n') === -1
        ? Infinity
        : textAfter.indexOf('\n') + cursorPosition
    );

    const tokenStart = lastSeparatorIndex + 1;
    const tokenEnd =
      nextSeparatorIndex === Infinity ? text.length : nextSeparatorIndex;
    const token = text.slice(tokenStart, tokenEnd).trim();

    return {
      token,
      start: tokenStart,
      end: tokenEnd,
    };
  };

  // Handle text changes and detect current token
  const handleChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    const newValue = e.target.value;
    const cursorPosition = e.target.selectionStart || 0;

    onChange(newValue);

    const { token, start, end } = findCurrentToken(newValue, cursorPosition);

    setCurrentTokenStart(start);
    setCurrentTokenEnd(end);

    // Show search results if token has 2+ characters
    if (token.length >= 2) {
      setSearchQuery(token);
    } else {
      setSearchQuery('');
      setShowDropdown(false);
    }
  };

  // Handle keyboard navigation
  const handleKeyDown = (e: KeyboardEvent<HTMLTextAreaElement>) => {
    // Handle dropdown navigation first
    if (showDropdown && searchResults.length > 0) {
      switch (e.key) {
        case 'ArrowDown':
          e.preventDefault();
          setSelectedIndex((prev) =>
            prev < searchResults.length - 1 ? prev + 1 : 0
          );
          return;
        case 'ArrowUp':
          e.preventDefault();
          setSelectedIndex((prev) =>
            prev > 0 ? prev - 1 : searchResults.length - 1
          );
          return;
        case 'Enter':
        case 'Tab':
          if (selectedIndex >= 0) {
            e.preventDefault();
            selectFile(searchResults[selectedIndex]);
            return;
          }
          break;
        case 'Escape':
          e.preventDefault();
          setShowDropdown(false);
          setSearchQuery('');
          return;
      }
    }

    // Call the passed onKeyDown handler
    onKeyDown?.(e);
  };

  // Select a file and insert it into the text
  const selectFile = (file: FileSearchResult) => {
    if (currentTokenStart === -1) return;

    const before = value.slice(0, currentTokenStart);
    const after = value.slice(currentTokenEnd);

    // Smart comma handling - add ", " if not at end and next char isn't comma/newline
    let insertion = file.path;
    const trimmedAfter = after.trimStart();
    const needsComma =
      trimmedAfter.length > 0 &&
      !trimmedAfter.startsWith(',') &&
      !trimmedAfter.startsWith('\n');

    if (needsComma || trimmedAfter.length === 0) {
      insertion += ', ';
    }

    const newValue =
      before.trimEnd() + (before.trimEnd() ? ' ' : '') + insertion + after;
    onChange(newValue);

    setShowDropdown(false);
    setSearchQuery('');

    // Focus back to textarea and position cursor after insertion
    setTimeout(() => {
      if (textareaRef.current) {
        const newCursorPos =
          currentTokenStart + (before.trimEnd() ? 1 : 0) + insertion.length;
        textareaRef.current.focus();
        textareaRef.current.setSelectionRange(newCursorPos, newCursorPos);
      }
    }, 0);
  };

  // Calculate dropdown position
  const getDropdownPosition = () => {
    if (!textareaRef.current) return { top: 0, left: 0, maxHeight: 240 };

    const textareaRect = textareaRef.current.getBoundingClientRect();
    const dropdownWidth = 256;
    const maxDropdownHeight = 320;
    const minDropdownHeight = 120;

    let finalTop = textareaRect.bottom + 4;
    let finalLeft = textareaRect.left;
    let maxHeight = maxDropdownHeight;

    // Ensure dropdown doesn't go off the right edge
    if (finalLeft + dropdownWidth > window.innerWidth - 16) {
      finalLeft = window.innerWidth - dropdownWidth - 16;
    }

    // Ensure dropdown doesn't go off the left edge
    if (finalLeft < 16) {
      finalLeft = 16;
    }

    // Calculate available space below and above textarea
    const availableSpaceBelow = window.innerHeight - textareaRect.bottom - 32;
    const availableSpaceAbove = textareaRect.top - 32;

    // If not enough space below, position above
    if (
      availableSpaceBelow < minDropdownHeight &&
      availableSpaceAbove > availableSpaceBelow
    ) {
      const actualHeight =
        dropdownRef.current?.getBoundingClientRect().height ||
        minDropdownHeight;
      finalTop = textareaRect.top - actualHeight - 4;
      maxHeight = Math.min(
        maxDropdownHeight,
        Math.max(availableSpaceAbove, minDropdownHeight)
      );
    } else {
      maxHeight = Math.min(
        maxDropdownHeight,
        Math.max(availableSpaceBelow, minDropdownHeight)
      );
    }

    return { top: finalTop, left: finalLeft, maxHeight };
  };

  // Update dropdown position when results change
  useEffect(() => {
    if (showDropdown && dropdownRef.current) {
      setTimeout(() => {
        const newPosition = getDropdownPosition();
        if (dropdownRef.current) {
          dropdownRef.current.style.top = `${newPosition.top}px`;
          dropdownRef.current.style.left = `${newPosition.left}px`;
          dropdownRef.current.style.maxHeight = `${newPosition.maxHeight}px`;
        }
      }, 0);
    }
  }, [searchResults.length, showDropdown]);

  const dropdownPosition = getDropdownPosition();

  return (
    <div
      className={`relative ${className?.includes('flex-1') ? 'flex-1' : ''}`}
    >
      <AutoExpandingTextarea
        ref={textareaRef}
        value={value}
        onChange={handleChange}
        onKeyDown={handleKeyDown}
        placeholder={placeholder}
        rows={rows}
        disabled={disabled}
        className={className}
        maxRows={maxRows}
      />

      {showDropdown &&
        createPortal(
          <div
            ref={dropdownRef}
            className="fixed bg-background border border-border rounded-md shadow-lg overflow-y-auto min-w-64"
            style={{
              top: dropdownPosition.top,
              left: dropdownPosition.left,
              maxHeight: dropdownPosition.maxHeight,
              zIndex: 10000,
            }}
          >
            {isLoading ? (
              <div className="p-2 text-sm text-muted-foreground">
                Searching...
              </div>
            ) : searchResults.length === 0 ? (
              <div className="p-2 text-sm text-muted-foreground">
                No files found
              </div>
            ) : (
              <div className="py-1">
                {searchResults.map((file, index) => (
                  <div
                    key={file.path}
                    className={`px-3 py-2 cursor-pointer text-sm ${
                      index === selectedIndex
                        ? 'bg-blue-50 text-blue-900'
                        : 'hover:bg-muted'
                    }`}
                    onClick={() => selectFile(file)}
                  >
                    <div className="font-medium truncate">{file.name}</div>
                    <div className="text-xs text-muted-foreground truncate">
                      {file.path}
                    </div>
                  </div>
                ))}
              </div>
            )}
          </div>,
          document.body
        )}
    </div>
  );
}
</file>

<file path="frontend/src/components/ui/select.tsx">
'use client';

import * as React from 'react';
import * as SelectPrimitive from '@radix-ui/react-select';
import { Check, ChevronDown, ChevronUp } from 'lucide-react';

import { cn } from '@/lib/utils';

const Select = SelectPrimitive.Root;

const SelectGroup = SelectPrimitive.Group;

const SelectValue = SelectPrimitive.Value;

const SelectTrigger = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Trigger>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Trigger
    ref={ref}
    className={cn(
      'flex h-10 w-full items-center justify-between border border-input px-3 py-2 text-sm ring-offset-background data-[placeholder]:text-muted-foreground focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50 [&>span]:line-clamp-1',
      className
    )}
    {...props}
  >
    {children}
    <SelectPrimitive.Icon asChild>
      <ChevronDown className="h-4 w-4 opacity-50" />
    </SelectPrimitive.Icon>
  </SelectPrimitive.Trigger>
));
SelectTrigger.displayName = SelectPrimitive.Trigger.displayName;

const SelectScrollUpButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollUpButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollUpButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollUpButton
    ref={ref}
    className={cn(
      'flex cursor-default items-center justify-center py-1',
      className
    )}
    {...props}
  >
    <ChevronUp className="h-4 w-4" />
  </SelectPrimitive.ScrollUpButton>
));
SelectScrollUpButton.displayName = SelectPrimitive.ScrollUpButton.displayName;

const SelectScrollDownButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollDownButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollDownButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollDownButton
    ref={ref}
    className={cn(
      'flex cursor-default items-center justify-center py-1',
      className
    )}
    {...props}
  >
    <ChevronDown className="h-4 w-4" />
  </SelectPrimitive.ScrollDownButton>
));
SelectScrollDownButton.displayName =
  SelectPrimitive.ScrollDownButton.displayName;

const SelectContent = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Content>
>(({ className, children, position = 'popper', ...props }, ref) => (
  <SelectPrimitive.Portal>
    <SelectPrimitive.Content
      ref={ref}
      className={cn(
        'relative z-[10000] max-h-[--radix-select-content-available-height] min-w-[8rem] overflow-y-auto overflow-x-hidden border bg-primary text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 origin-[--radix-select-content-transform-origin]',
        position === 'popper' &&
          'data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1',
        className
      )}
      position={position}
      {...props}
    >
      <SelectScrollUpButton />
      <SelectPrimitive.Viewport
        className={cn(
          'p-1',
          position === 'popper' &&
            'h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)]'
        )}
      >
        {children}
      </SelectPrimitive.Viewport>
      <SelectScrollDownButton />
    </SelectPrimitive.Content>
  </SelectPrimitive.Portal>
));
SelectContent.displayName = SelectPrimitive.Content.displayName;

const SelectLabel = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Label>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Label
    ref={ref}
    className={cn('py-1.5 pl-8 pr-2 text-sm font-semibold', className)}
    {...props}
  />
));
SelectLabel.displayName = SelectPrimitive.Label.displayName;

const SelectItem = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Item>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Item
    ref={ref}
    className={cn(
      'relative flex w-full cursor-default select-none items-center py-1.5 pl-8 pr-2 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50',
      className
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <SelectPrimitive.ItemIndicator>
        <Check className="h-4 w-4" />
      </SelectPrimitive.ItemIndicator>
    </span>

    <SelectPrimitive.ItemText>{children}</SelectPrimitive.ItemText>
  </SelectPrimitive.Item>
));
SelectItem.displayName = SelectPrimitive.Item.displayName;

const SelectSeparator = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Separator
    ref={ref}
    className={cn('-mx-1 my-1 h-px bg-muted', className)}
    {...props}
  />
));
SelectSeparator.displayName = SelectPrimitive.Separator.displayName;

export {
  Select,
  SelectGroup,
  SelectValue,
  SelectTrigger,
  SelectContent,
  SelectLabel,
  SelectItem,
  SelectSeparator,
  SelectScrollUpButton,
  SelectScrollDownButton,
};
</file>

<file path="frontend/src/components/ui/tabs.tsx">
import * as React from 'react';
import * as TabsPrimitive from '@radix-ui/react-tabs';

import { cn } from '@/lib/utils';

const Tabs = TabsPrimitive.Root;

const TabsList = React.forwardRef<
  React.ElementRef<typeof TabsPrimitive.List>,
  React.ComponentPropsWithoutRef<typeof TabsPrimitive.List>
>(({ className, ...props }, ref) => (
  <TabsPrimitive.List
    ref={ref}
    className={cn(
      'inline-flex h-10 items-center justify-center rounded-md bg-muted p-1 text-muted-foreground',
      className
    )}
    {...props}
  />
));
TabsList.displayName = TabsPrimitive.List.displayName;

const TabsTrigger = React.forwardRef<
  React.ElementRef<typeof TabsPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof TabsPrimitive.Trigger>
>(({ className, ...props }, ref) => (
  <TabsPrimitive.Trigger
    ref={ref}
    className={cn(
      'inline-flex items-center justify-center whitespace-nowrap rounded-sm px-3 py-1.5 text-sm font-medium ring-offset-background transition-all focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 data-[state=active]:bg-background data-[state=active]:text-foreground data-[state=active]:shadow-sm',
      className
    )}
    {...props}
  />
));
TabsTrigger.displayName = TabsPrimitive.Trigger.displayName;

const TabsContent = React.forwardRef<
  React.ElementRef<typeof TabsPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof TabsPrimitive.Content>
>(({ className, ...props }, ref) => (
  <TabsPrimitive.Content
    ref={ref}
    className={cn(
      'mt-2 ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2',
      className
    )}
    {...props}
  />
));
TabsContent.displayName = TabsPrimitive.Content.displayName;

export { Tabs, TabsList, TabsTrigger, TabsContent };
</file>

<file path="frontend/src/components/ui/textarea.tsx">
import * as React from 'react';

import { cn } from '@/lib/utils';

const Textarea = React.forwardRef<
  HTMLTextAreaElement,
  React.ComponentProps<'textarea'>
>(({ className, ...props }, ref) => {
  return (
    <textarea
      className={cn(
        'flex min-h-[80px] w-full bg-transparent border px-3 py-2 text-sm ring-offset-background focus-visible:outline-none disabled:cursor-not-allowed disabled:opacity-50',
        className
      )}
      ref={ref}
      {...props}
    />
  );
});
Textarea.displayName = 'Textarea';

export { Textarea };
</file>

<file path="frontend/src/components/ui/tooltip.tsx">
import * as React from 'react';
import * as TooltipPrimitive from '@radix-ui/react-tooltip';

import { cn } from '@/lib/utils';

const TooltipProvider = TooltipPrimitive.Provider;

const Tooltip = TooltipPrimitive.Root;

const TooltipTrigger = TooltipPrimitive.Trigger;

const TooltipContent = React.forwardRef<
  React.ElementRef<typeof TooltipPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof TooltipPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <TooltipPrimitive.Content
    ref={ref}
    sideOffset={sideOffset}
    className={cn(
      'z-50 overflow-hidden rounded-md border bg-popover px-3 py-1.5 text-sm text-popover-foreground shadow-md animate-in fade-in-0 zoom-in-95 data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=closed]:zoom-out-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2 origin-[--radix-tooltip-content-transform-origin]',
      className
    )}
    {...props}
  />
));
TooltipContent.displayName = TooltipPrimitive.Content.displayName;

export { Tooltip, TooltipTrigger, TooltipContent, TooltipProvider };
</file>

<file path="frontend/src/components/config-provider.tsx">
import {
  createContext,
  ReactNode,
  useCallback,
  useContext,
  useEffect,
  useMemo,
  useState,
} from 'react';
import {
  type Config,
  type Environment,
  type UserSystemInfo,
  type BaseAgentCapability,
  CheckTokenResponse,
} from 'shared/types';
import type { ExecutorConfig } from 'shared/types';
import { configApi, githubAuthApi } from '../lib/api';
import { updateLanguageFromConfig } from '../i18n/config';

interface UserSystemState {
  config: Config | null;
  environment: Environment | null;
  profiles: Record<string, ExecutorConfig> | null;
  capabilities: Record<string, BaseAgentCapability[]> | null;
}

interface UserSystemContextType {
  // Full system state
  system: UserSystemState;

  // Hot path - config helpers (most frequently used)
  config: Config | null;
  updateConfig: (updates: Partial<Config>) => void;
  updateAndSaveConfig: (updates: Partial<Config>) => Promise<boolean>;
  saveConfig: () => Promise<boolean>;

  // System data access
  environment: Environment | null;
  profiles: Record<string, ExecutorConfig> | null;
  capabilities: Record<string, BaseAgentCapability[]> | null;
  setEnvironment: (env: Environment | null) => void;
  setProfiles: (profiles: Record<string, ExecutorConfig> | null) => void;
  setCapabilities: (caps: Record<string, BaseAgentCapability[]> | null) => void;

  // Reload system data
  reloadSystem: () => Promise<void>;

  // State
  loading: boolean;
  githubTokenInvalid: boolean;
}

const UserSystemContext = createContext<UserSystemContextType | undefined>(
  undefined
);

interface UserSystemProviderProps {
  children: ReactNode;
}

export function UserSystemProvider({ children }: UserSystemProviderProps) {
  // Split state for performance - independent re-renders
  const [config, setConfig] = useState<Config | null>(null);
  const [environment, setEnvironment] = useState<Environment | null>(null);
  const [profiles, setProfiles] = useState<Record<
    string,
    ExecutorConfig
  > | null>(null);
  const [capabilities, setCapabilities] = useState<Record<
    string,
    BaseAgentCapability[]
  > | null>(null);
  const [loading, setLoading] = useState(true);
  const [githubTokenInvalid, setGithubTokenInvalid] = useState(false);

  useEffect(() => {
    const loadUserSystem = async () => {
      try {
        const userSystemInfo: UserSystemInfo = await configApi.getConfig();
        setConfig(userSystemInfo.config);
        setEnvironment(userSystemInfo.environment);
        setProfiles(
          userSystemInfo.executors as Record<string, ExecutorConfig> | null
        );
        setCapabilities(
          (userSystemInfo.capabilities || null) as Record<
            string,
            BaseAgentCapability[]
          > | null
        );
      } catch (err) {
        console.error('Error loading user system:', err);
      } finally {
        setLoading(false);
      }
    };

    loadUserSystem();
  }, []);

  // Sync language with i18n when config changes
  useEffect(() => {
    if (config?.language) {
      updateLanguageFromConfig(config.language);
    }
  }, [config?.language]);

  // Check GitHub token validity after config loads
  useEffect(() => {
    if (loading) return;
    const checkToken = async () => {
      const valid = await githubAuthApi.checkGithubToken();
      if (valid === undefined) {
        // Network/server error: do not update githubTokenInvalid
        return;
      }
      switch (valid) {
        case CheckTokenResponse.VALID:
          setGithubTokenInvalid(false);
          break;
        case CheckTokenResponse.INVALID:
          setGithubTokenInvalid(true);
          break;
      }
    };
    checkToken();
  }, [loading]);

  const updateConfig = useCallback((updates: Partial<Config>) => {
    setConfig((prev) => (prev ? { ...prev, ...updates } : null));
  }, []);

  const saveConfig = useCallback(async (): Promise<boolean> => {
    if (!config) return false;
    try {
      await configApi.saveConfig(config);
      return true;
    } catch (err) {
      console.error('Error saving config:', err);
      return false;
    }
  }, [config]);

  const updateAndSaveConfig = useCallback(
    async (updates: Partial<Config>): Promise<boolean> => {
      setLoading(true);
      const newConfig: Config | null = config
        ? { ...config, ...updates }
        : null;
      try {
        if (!newConfig) return false;
        const saved = await configApi.saveConfig(newConfig);
        setConfig(saved);
        return true;
      } catch (err) {
        console.error('Error saving config:', err);
        return false;
      } finally {
        setLoading(false);
      }
    },
    [config]
  );

  const reloadSystem = useCallback(async () => {
    setLoading(true);
    try {
      const userSystemInfo: UserSystemInfo = await configApi.getConfig();
      setConfig(userSystemInfo.config);
      setEnvironment(userSystemInfo.environment);
      setProfiles(
        userSystemInfo.executors as Record<string, ExecutorConfig> | null
      );
      setCapabilities(
        (userSystemInfo.capabilities || null) as Record<
          string,
          BaseAgentCapability[]
        > | null
      );
    } catch (err) {
      console.error('Error reloading user system:', err);
    } finally {
      setLoading(false);
    }
  }, []);

  // Memoize context value to prevent unnecessary re-renders
  const value = useMemo<UserSystemContextType>(
    () => ({
      system: { config, environment, profiles, capabilities },
      config,
      environment,
      profiles,
      capabilities,
      updateConfig,
      saveConfig,
      updateAndSaveConfig,
      setEnvironment,
      setProfiles,
      setCapabilities,
      reloadSystem,
      loading,
      githubTokenInvalid,
    }),
    [
      config,
      environment,
      profiles,
      capabilities,
      updateConfig,
      saveConfig,
      updateAndSaveConfig,
      reloadSystem,
      loading,
      githubTokenInvalid,
    ]
  );

  return (
    <UserSystemContext.Provider value={value}>
      {children}
    </UserSystemContext.Provider>
  );
}

export function useUserSystem() {
  const context = useContext(UserSystemContext);
  if (context === undefined) {
    throw new Error('useUserSystem must be used within a UserSystemProvider');
  }
  return context;
}
</file>

<file path="frontend/src/components/DevBanner.tsx">
import { AlertTriangle } from 'lucide-react';

export function DevBanner() {
  // Only show in development mode
  if (import.meta.env.MODE !== 'development') {
    return null;
  }

  return (
    <div className="bg-orange-500 text-white text-center py-2 px-4 text-sm font-medium border-b border-orange-600">
      <div className="flex items-center justify-center gap-2">
        <AlertTriangle className="h-4 w-4" />
        <span>Development Mode - This is a development build</span>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/diff-view-switch.tsx">
import { Columns, FileText } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { cn } from '@/lib/utils';
import { useDiffViewMode, useDiffViewStore } from '@/stores/useDiffViewStore';

type Props = {
  className?: string;
  size?: 'xs' | 'sm';
};

/**
 * Segmented switch for Inline vs Split diff modes.
 * - Left segment: Inline (Unified)
 * - Right segment: Split
 * Uses global Zustand store so changing here updates all diffs.
 */
export default function DiffViewSwitch({ className, size = 'xs' }: Props) {
  const mode = useDiffViewMode();
  const setMode = useDiffViewStore((s) => s.setMode);

  const isUnified = mode === 'unified';

  return (
    <div
      className={cn(
        'inline-flex rounded-md border border-input overflow-hidden',
        className
      )}
      role="group"
      aria-label="Diff view mode"
    >
      <Button
        variant={isUnified ? 'default' : 'outline'}
        size={size}
        className={cn(
          'rounded-none rounded-l-md h-6',
          !isUnified && 'bg-background',
          'gap-1',
          // Highlight the inner divider when right side is active
          !isUnified && 'border-r-foreground'
        )}
        aria-pressed={isUnified}
        onClick={() => setMode('unified')}
      >
        <FileText className="h-3 w-3" />
        <span className="text-[11px]">Inline</span>
      </Button>
      <Button
        variant={!isUnified ? 'default' : 'outline'}
        size={size}
        className={cn(
          'rounded-none rounded-r-md -ml-px h-6',
          isUnified && 'bg-background',
          'gap-1',
          // Ensure inner divider reflects active left side
          isUnified && 'border-l-foreground'
        )}
        aria-pressed={!isUnified}
        onClick={() => setMode('split')}
      >
        <Columns className="h-3 w-3" />
        <span className="text-[11px]">Split</span>
      </Button>
    </div>
  );
}
</file>

<file path="frontend/src/components/DiffCard.tsx">
import { Diff } from 'shared/types';
import { DiffModeEnum, DiffView, SplitSide } from '@git-diff-view/react';
import { generateDiffFile, type DiffFile } from '@git-diff-view/file';
import { useMemo } from 'react';
import { useUserSystem } from '@/components/config-provider';
import { getHighLightLanguageFromPath } from '@/utils/extToLanguage';
import { getActualTheme } from '@/utils/theme';
import { stripLineEnding } from '@/utils/string';
import { Button } from '@/components/ui/button';
import {
  ChevronRight,
  ChevronUp,
  Trash2,
  ArrowLeftRight,
  FilePlus2,
  PencilLine,
  Copy,
  Key,
  ExternalLink,
  MessageSquare,
} from 'lucide-react';
import '@/styles/diff-style-overrides.css';
import { attemptsApi } from '@/lib/api';
import type { TaskAttempt } from 'shared/types';
import { useReview, type ReviewDraft } from '@/contexts/ReviewProvider';
import { CommentWidgetLine } from '@/components/diff/CommentWidgetLine';
import { ReviewCommentRenderer } from '@/components/diff/ReviewCommentRenderer';
import { useDiffViewMode } from '@/stores/useDiffViewStore';
import { useProject } from '@/contexts/project-context';

type Props = {
  diff: Diff;
  expanded: boolean;
  onToggle: () => void;
  selectedAttempt: TaskAttempt | null;
};

function labelAndIcon(diff: Diff) {
  const c = diff.change;
  if (c === 'deleted') return { label: 'Deleted', Icon: Trash2 };
  if (c === 'renamed') return { label: 'Renamed', Icon: ArrowLeftRight };
  if (c === 'added')
    return { label: undefined as string | undefined, Icon: FilePlus2 };
  if (c === 'copied') return { label: 'Copied', Icon: Copy };
  if (c === 'permissionChange')
    return { label: 'Permission Changed', Icon: Key };
  return { label: undefined as string | undefined, Icon: PencilLine };
}

function readPlainLine(
  diffFile: DiffFile | null,
  lineNumber: number,
  side: SplitSide
) {
  if (!diffFile) return undefined;
  try {
    const rawLine =
      side === SplitSide.old
        ? diffFile.getOldPlainLine(lineNumber)
        : diffFile.getNewPlainLine(lineNumber);
    if (rawLine?.value === undefined) return undefined;
    return stripLineEnding(rawLine.value);
  } catch (error) {
    console.error('Failed to read line content for review comment', error);
    return undefined;
  }
}

export default function DiffCard({
  diff,
  expanded,
  onToggle,
  selectedAttempt,
}: Props) {
  const { config } = useUserSystem();
  const theme = getActualTheme(config?.theme);
  const { comments, drafts, setDraft } = useReview();
  const globalMode = useDiffViewMode();
  const { projectId } = useProject();

  const oldName = diff.oldPath || undefined;
  const newName = diff.newPath || oldName || 'unknown';
  const oldLang =
    getHighLightLanguageFromPath(oldName || newName || '') || 'plaintext';
  const newLang =
    getHighLightLanguageFromPath(newName || oldName || '') || 'plaintext';
  const { label, Icon } = labelAndIcon(diff);
  const isOmitted = !!diff.contentOmitted;

  // Build a diff from raw contents so the viewer can expand beyond hunks
  const oldContentSafe = diff.oldContent || '';
  const newContentSafe = diff.newContent || '';
  const isContentEqual = oldContentSafe === newContentSafe;

  const diffFile = useMemo(() => {
    if (isContentEqual || isOmitted) return null;
    try {
      const oldFileName = oldName || newName || 'unknown';
      const newFileName = newName || oldName || 'unknown';
      const file = generateDiffFile(
        oldFileName,
        oldContentSafe,
        newFileName,
        newContentSafe,
        oldLang,
        newLang
      );
      file.initRaw();
      return file;
    } catch (e) {
      console.error('Failed to build diff for view', e);
      return null;
    }
  }, [
    isContentEqual,
    isOmitted,
    oldName,
    newName,
    oldLang,
    newLang,
    oldContentSafe,
    newContentSafe,
  ]);

  const add = isOmitted
    ? (diff.additions ?? 0)
    : (diffFile?.additionLength ?? 0);
  const del = isOmitted
    ? (diff.deletions ?? 0)
    : (diffFile?.deletionLength ?? 0);

  // Review functionality
  const filePath = newName || oldName || 'unknown';
  const commentsForFile = useMemo(
    () => comments.filter((c) => c.filePath === filePath),
    [comments, filePath]
  );

  // Transform comments to git-diff-view extendData format
  const extendData = useMemo(() => {
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    const oldFileData: Record<string, { data: any }> = {};
    const newFileData: Record<string, { data: any }> = {};

    commentsForFile.forEach((comment) => {
      const lineKey = String(comment.lineNumber);
      if (comment.side === SplitSide.old) {
        oldFileData[lineKey] = { data: comment };
      } else {
        newFileData[lineKey] = { data: comment };
      }
    });

    return {
      oldFile: oldFileData,
      newFile: newFileData,
    };
  }, [commentsForFile]);

  const handleAddWidgetClick = (lineNumber: number, side: SplitSide) => {
    const widgetKey = `${filePath}-${side}-${lineNumber}`;
    const codeLine = readPlainLine(diffFile, lineNumber, side);
    const draft: ReviewDraft = {
      filePath,
      side,
      lineNumber,
      text: '',
      ...(codeLine !== undefined ? { codeLine } : {}),
    };
    setDraft(widgetKey, draft);
  };

  const renderWidgetLine = (props: any) => {
    const widgetKey = `${filePath}-${props.side}-${props.lineNumber}`;
    const draft = drafts[widgetKey];
    if (!draft) return null;

    return (
      <CommentWidgetLine
        draft={draft}
        widgetKey={widgetKey}
        onSave={props.onClose}
        onCancel={props.onClose}
        projectId={projectId}
      />
    );
  };

  const renderExtendLine = (lineData: any) => {
    return (
      <ReviewCommentRenderer comment={lineData.data} projectId={projectId} />
    );
  };

  // Title row
  const title = (
    <p
      className="text-xs font-mono overflow-x-auto flex-1"
      style={{ color: 'hsl(var(--muted-foreground) / 0.7)' }}
    >
      <Icon className="h-3 w-3 inline mr-2" aria-hidden />
      {label && <span className="mr-2">{label}</span>}
      {diff.change === 'renamed' && oldName ? (
        <span className="inline-flex items-center gap-2">
          <span>{oldName}</span>
          <span aria-hidden>→</span>
          <span>{newName}</span>
        </span>
      ) : (
        <span>{newName}</span>
      )}
      <span className="ml-3" style={{ color: 'hsl(var(--console-success))' }}>
        +{add}
      </span>
      <span className="ml-2" style={{ color: 'hsl(var(--console-error))' }}>
        -{del}
      </span>
      {commentsForFile.length > 0 && (
        <span className="ml-3 inline-flex items-center gap-1 px-2 py-0.5 text-xs bg-primary/10 text-primary rounded">
          <MessageSquare className="h-3 w-3" />
          {commentsForFile.length}
        </span>
      )}
    </p>
  );

  const handleOpenInIDE = async () => {
    if (!selectedAttempt?.id) return;
    try {
      const openPath = newName || oldName;
      await attemptsApi.openEditor(
        selectedAttempt.id,
        undefined,
        openPath || undefined
      );
    } catch (err) {
      console.error('Failed to open file in IDE:', err);
    }
  };

  const expandable = true;

  return (
    <div className="my-4 border">
      <div className="flex items-center px-4 py-2">
        {expandable && (
          <Button
            variant="ghost"
            size="sm"
            onClick={onToggle}
            className="h-6 w-6 p-0 mr-2"
            title={expanded ? 'Collapse' : 'Expand'}
            aria-expanded={expanded}
          >
            {expanded ? (
              <ChevronUp className="h-3 w-3" />
            ) : (
              <ChevronRight className="h-3 w-3" />
            )}
          </Button>
        )}
        {title}
        <Button
          variant="ghost"
          size="sm"
          onClick={(e) => {
            e.stopPropagation();
            handleOpenInIDE();
          }}
          className="h-6 w-6 p-0 ml-2"
          title="Open in IDE"
          disabled={diff.change === 'deleted'}
        >
          <ExternalLink className="h-3 w-3" aria-hidden />
        </Button>
      </div>

      {expanded && diffFile && (
        <div>
          <DiffView
            diffFile={diffFile}
            diffViewWrap={false}
            diffViewTheme={theme}
            diffViewHighlight
            diffViewMode={
              globalMode === 'split' ? DiffModeEnum.Split : DiffModeEnum.Unified
            }
            diffViewFontSize={12}
            diffViewAddWidget
            onAddWidgetClick={handleAddWidgetClick}
            renderWidgetLine={renderWidgetLine}
            extendData={extendData}
            renderExtendLine={renderExtendLine}
          />
        </div>
      )}
      {expanded && !diffFile && (
        <div
          className="px-4 pb-4 text-xs font-mono"
          style={{ color: 'hsl(var(--muted-foreground) / 0.9)' }}
        >
          {isOmitted
            ? 'Content omitted due to file size. Open in editor to view.'
            : isContentEqual
              ? diff.change === 'renamed'
                ? 'File renamed with no content changes.'
                : diff.change === 'permissionChange'
                  ? 'File permission changed.'
                  : 'No content changes to display.'
              : 'Failed to render diff for this file.'}
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/ExecutorConfigForm.tsx">
import { useMemo, useEffect, useState } from 'react';
import Form from '@rjsf/core';
import { RJSFValidationError } from '@rjsf/utils';
import validator from '@rjsf/validator-ajv8';

import { Alert, AlertDescription } from '@/components/ui/alert';
import { Card, CardContent } from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import { Loader2 } from 'lucide-react';
import { shadcnTheme } from './rjsf';
// Using custom shadcn/ui widgets instead of @rjsf/shadcn theme

type ExecutorType =
  | 'AMP'
  | 'CLAUDE_CODE'
  | 'GEMINI'
  | 'CODEX'
  | 'CURSOR'
  | 'OPENCODE'
  | 'QWEN_CODE';

interface ExecutorConfigFormProps {
  executor: ExecutorType;
  value: any;
  onSubmit?: (formData: any) => void;
  onChange?: (formData: any) => void;
  onSave?: (formData: any) => Promise<void>;
  disabled?: boolean;
  isSaving?: boolean;
  isDirty?: boolean;
}

import schemas from 'virtual:executor-schemas';

export function ExecutorConfigForm({
  executor,
  value,
  onSubmit,
  onChange,
  onSave,
  disabled = false,
  isSaving = false,
  isDirty = false,
}: ExecutorConfigFormProps) {
  const [formData, setFormData] = useState(value || {});
  const [validationErrors, setValidationErrors] = useState<
    RJSFValidationError[]
  >([]);

  const schema = useMemo(() => {
    return schemas[executor];
  }, [executor]);

  useEffect(() => {
    setFormData(value || {});
    setValidationErrors([]);
  }, [value, executor]);

  const handleChange = ({ formData: newFormData }: any) => {
    setFormData(newFormData);
    if (onChange) {
      onChange(newFormData);
    }
  };

  const handleSubmit = async ({ formData: submitData }: any) => {
    setValidationErrors([]);
    if (onSave) {
      await onSave(submitData);
    } else if (onSubmit) {
      onSubmit(submitData);
    }
  };

  const handleError = (errors: RJSFValidationError[]) => {
    setValidationErrors(errors);
  };

  if (!schema) {
    return (
      <Alert variant="destructive">
        <AlertDescription>
          Schema not found for executor type: {executor}
        </AlertDescription>
      </Alert>
    );
  }

  return (
    <div className="space-y-8">
      <Card>
        <CardContent className="p-0">
          <Form
            schema={schema}
            formData={formData}
            onChange={handleChange}
            onSubmit={handleSubmit}
            onError={handleError}
            validator={validator}
            disabled={disabled}
            liveValidate
            showErrorList={false}
            widgets={shadcnTheme.widgets}
            templates={shadcnTheme.templates}
          >
            {onSave && (
              <div className="flex justify-end pt-4">
                <Button
                  type="submit"
                  disabled={!isDirty || validationErrors.length > 0 || isSaving}
                >
                  {isSaving && (
                    <Loader2 className="mr-2 h-4 w-4 animate-spin" />
                  )}
                  Save Configuration
                </Button>
              </div>
            )}
          </Form>
        </CardContent>
      </Card>

      {validationErrors.length > 0 && (
        <Alert variant="destructive">
          <AlertDescription>
            <ul className="list-disc list-inside space-y-1">
              {validationErrors.map((error, index) => (
                <li key={index}>
                  {error.property}: {error.message}
                </li>
              ))}
            </ul>
          </AlertDescription>
        </Alert>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/logo.tsx">
import { useTheme } from '@/components/theme-provider';
import { useEffect, useState } from 'react';

export function Logo({ className = '' }: { className?: string }) {
  const { theme } = useTheme();
  const [isDark, setIsDark] = useState(false);

  useEffect(() => {
    const updateTheme = () => {
      if (theme === 'LIGHT') {
        setIsDark(false);
      } else if (theme === 'SYSTEM') {
        // System theme
        setIsDark(window.matchMedia('(prefers-color-scheme: dark)').matches);
      } else {
        // All other themes (dark, purple, green, blue, orange, red) have dark backgrounds
        setIsDark(true);
      }
    };

    updateTheme();

    // Listen for system theme changes when using system theme
    if (theme === 'SYSTEM') {
      const mediaQuery = window.matchMedia('(prefers-color-scheme: dark)');
      mediaQuery.addEventListener('change', updateTheme);
      return () => mediaQuery.removeEventListener('change', updateTheme);
    }
  }, [theme]);

  const fillColor = isDark ? '#ffffff' : '#000000';

  return (
    <svg
      width="140"
      viewBox="0 0 604 74"
      fill={fillColor}
      xmlns="http://www.w3.org/2000/svg"
      className={className}
    >
      <path d="M0 13.6035V0.00976562H7.20117V13.6035H0ZM7.20703 13.6035V0.00976562H14.4082V13.6035H7.20703ZM18.5215 13.6035V6.42578H14.4141V5.56445H19.3828V13.6035H18.5215ZM16.6465 8.30078H14.4141V7.43945H17.5078V13.6035H16.6465V8.30078ZM43.2422 13.6035V0.00976562H50.4434V13.6035H43.2422ZM50.4492 13.6035V0.00976562H57.6504V13.6035H50.4492ZM61.7637 13.6035V6.42578H57.6562V5.56445H62.625V13.6035H61.7637ZM59.8887 8.30078H57.6562V7.43945H60.75V13.6035H59.8887V8.30078ZM64.8633 13.6035V0.00976562H72.0645V13.6035H64.8633ZM72.0703 13.6035V0.00976562H79.2715V13.6035H72.0703ZM83.3848 13.6035V6.42578H79.2773V5.56445H84.2461V13.6035H83.3848ZM81.5098 8.30078H79.2773V7.43945H82.3711V13.6035H81.5098V8.30078ZM86.4844 13.6035V0.00976562H93.6855V13.6035H86.4844ZM93.6914 13.6035V0.00976562H100.893V13.6035H93.6914ZM100.898 13.6035V0.00976562H108.1V13.6035H100.898ZM108.105 13.6035V0.00976562H115.307V13.6035H108.105ZM115.312 13.6035V0.00976562H122.514V13.6035H115.312ZM122.52 13.6035V0.00976562H129.721V13.6035H122.52ZM133.834 13.6035V6.42578H129.727V5.56445H134.695V13.6035H133.834ZM131.959 8.30078H129.727V7.43945H132.82V13.6035H131.959V8.30078ZM144.141 13.6035V0.00976562H151.342V13.6035H144.141ZM151.348 13.6035V0.00976562H158.549V13.6035H151.348ZM158.555 13.6035V0.00976562H165.756V13.6035H158.555ZM165.762 13.6035V0.00976562H172.963V13.6035H165.762ZM172.969 13.6035V0.00976562H180.17V13.6035H172.969ZM180.176 13.6035V0.00976562H187.377V13.6035H180.176ZM187.383 13.6035V0.00976562H194.584V13.6035H187.383ZM198.697 13.6035V6.42578H194.59V5.56445H199.559V13.6035H198.697ZM196.822 8.30078H194.59V7.43945H197.684V13.6035H196.822V8.30078ZM230.625 13.6035V0.00976562H237.826V13.6035H230.625ZM237.832 13.6035V0.00976562H245.033V13.6035H237.832ZM249.146 13.6035V6.42578H245.039V5.56445H250.008V13.6035H249.146ZM247.271 8.30078H245.039V7.43945H248.133V13.6035H247.271V8.30078ZM266.66 13.6035V0.00976562H273.861V13.6035H266.66ZM273.867 13.6035V0.00976562H281.068V13.6035H273.867ZM285.182 13.6035V6.42578H281.074V5.56445H286.043V13.6035H285.182ZM283.307 8.30078H281.074V7.43945H284.168V13.6035H283.307V8.30078ZM295.488 13.6035V0.00976562H302.689V13.6035H295.488ZM302.695 13.6035V0.00976562H309.896V13.6035H302.695ZM309.902 13.6035V0.00976562H317.104V13.6035H309.902ZM317.109 13.6035V0.00976562H324.311V13.6035H317.109ZM324.316 13.6035V0.00976562H331.518V13.6035H324.316ZM335.631 13.6035V6.42578H331.523V5.56445H336.492V13.6035H335.631ZM333.756 8.30078H331.523V7.43945H334.617V13.6035H333.756V8.30078ZM345.938 13.6035V0.00976562H353.139V13.6035H345.938ZM353.145 13.6035V0.00976562H360.346V13.6035H353.145ZM360.352 13.6035V0.00976562H367.553V13.6035H360.352ZM371.666 13.6035V6.42578H367.559V5.56445H372.527V13.6035H371.666ZM369.791 8.30078H367.559V7.43945H370.652V13.6035H369.791V8.30078ZM396.387 13.6035V0.00976562H403.588V13.6035H396.387ZM403.594 13.6035V0.00976562H410.795V13.6035H403.594ZM414.908 13.6035V6.42578H410.801V5.56445H415.77V13.6035H414.908ZM413.033 8.30078H410.801V7.43945H413.895V13.6035H413.033V8.30078ZM418.008 13.6035V0.00976562H425.209V13.6035H418.008ZM425.215 13.6035V0.00976562H432.416V13.6035H425.215ZM432.422 13.6035V0.00976562H439.623V13.6035H432.422ZM439.629 13.6035V0.00976562H446.83V13.6035H439.629ZM446.836 13.6035V0.00976562H454.037V13.6035H446.836ZM454.043 13.6035V0.00976562H461.244V13.6035H454.043ZM465.357 13.6035V6.42578H461.25V5.56445H466.219V13.6035H465.357ZM463.482 8.30078H461.25V7.43945H464.344V13.6035H463.482V8.30078ZM482.871 13.6035V0.00976562H490.072V13.6035H482.871ZM490.078 13.6035V0.00976562H497.279V13.6035H490.078ZM497.285 13.6035V0.00976562H504.486V13.6035H497.285ZM504.492 13.6035V0.00976562H511.693V13.6035H504.492ZM511.699 13.6035V0.00976562H518.9V13.6035H511.699ZM523.014 13.6035V6.42578H518.906V5.56445H523.875V13.6035H523.014ZM521.139 8.30078H518.906V7.43945H522V13.6035H521.139V8.30078ZM533.32 13.6035V0.00976562H540.521V13.6035H533.32ZM540.527 13.6035V0.00976562H547.729V13.6035H540.527ZM547.734 13.6035V0.00976562H554.936V13.6035H547.734ZM559.049 13.6035V6.42578H554.941V5.56445H559.91V13.6035H559.049ZM557.174 8.30078H554.941V7.43945H558.035V13.6035H557.174V8.30078ZM583.77 13.6035V0.00976562H590.971V13.6035H583.77ZM590.977 13.6035V0.00976562H598.178V13.6035H590.977ZM602.291 13.6035V6.42578H598.184V5.56445H603.152V13.6035H602.291ZM600.416 8.30078H598.184V7.43945H601.277V13.6035H600.416V8.30078ZM0 26.6035V13.0098H7.20117V26.6035H0ZM7.20703 26.6035V13.0098H14.4082V26.6035H7.20703ZM18.5215 26.6035V13.0098H19.3828V26.6035H18.5215ZM16.6465 26.6035V13.0098H17.5078V26.6035H16.6465ZM43.2422 26.6035V13.0098H50.4434V26.6035H43.2422ZM50.4492 26.6035V13.0098H57.6504V26.6035H50.4492ZM61.7637 26.6035V13.0098H62.625V26.6035H61.7637ZM59.8887 26.6035V13.0098H60.75V26.6035H59.8887ZM64.8633 26.6035V13.0098H72.0645V26.6035H64.8633ZM72.0703 26.6035V13.0098H79.2715V26.6035H72.0703ZM83.3848 26.6035V13.0098H84.2461V26.6035H83.3848ZM81.5098 26.6035V13.0098H82.3711V26.6035H81.5098ZM86.4844 26.6035V13.0098H93.6855V26.6035H86.4844ZM93.6914 26.6035V13.0098H100.893V26.6035H93.6914ZM103.992 26.6035H103.131V18.5645H108.1V19.4258H103.992V26.6035ZM105.867 21.3008V26.6035H105.006V20.4395H108.1V21.3008H105.867ZM115.307 19.4258H108.105V18.5645H115.307V19.4258ZM115.307 21.3008H108.105V20.4395H115.307V21.3008ZM122.514 19.4258H115.312V18.5645H122.514V19.4258ZM122.514 21.3008H115.312V20.4395H122.514V21.3008ZM122.52 26.6035V13.0098H129.721V26.6035H122.52ZM129.727 26.6035V13.0098H136.928V26.6035H129.727ZM141.041 26.6035V19.4258H136.934V18.5645H141.902V26.6035H141.041ZM139.166 21.3008H136.934V20.4395H140.027V26.6035H139.166V21.3008ZM144.141 26.6035V13.0098H151.342V26.6035H144.141ZM151.348 26.6035V13.0098H158.549V26.6035H151.348ZM161.648 26.6035H160.787V18.5645H165.756V19.4258H161.648V26.6035ZM163.523 21.3008V26.6035H162.662V20.4395H165.756V21.3008H163.523ZM172.963 19.4258H165.762V18.5645H172.963V19.4258ZM172.963 21.3008H165.762V20.4395H172.963V21.3008ZM180.17 19.4258H172.969V18.5645H180.17V19.4258ZM180.17 21.3008H172.969V20.4395H180.17V21.3008ZM187.377 19.4258H180.176V18.5645H187.377V19.4258ZM187.377 21.3008H180.176V20.4395H187.377V21.3008ZM194.584 19.4258H187.383V18.5645H194.584V19.4258ZM194.584 21.3008H187.383V20.4395H194.584V21.3008ZM198.697 13.0098H199.559V21.3008H194.59V20.4395H198.697V13.0098ZM196.822 18.5645V13.0098H197.684V19.4258H194.59V18.5645H196.822ZM230.625 26.6035V13.0098H237.826V26.6035H230.625ZM237.832 26.6035V13.0098H245.033V26.6035H237.832ZM249.146 26.6035V13.0098H250.008V26.6035H249.146ZM247.271 26.6035V13.0098H248.133V26.6035H247.271ZM259.453 26.6035V13.0098H266.654V26.6035H259.453ZM266.66 26.6035V13.0098H273.861V26.6035H266.66ZM276.961 26.6035H276.1V18.5645H281.068V19.4258H276.961V26.6035ZM278.836 21.3008V26.6035H277.975V20.4395H281.068V21.3008H278.836ZM285.182 13.0098H286.043V21.3008H281.074V20.4395H285.182V13.0098ZM283.307 18.5645V13.0098H284.168V19.4258H281.074V18.5645H283.307ZM288.281 26.6035V13.0098H295.482V26.6035H288.281ZM295.488 26.6035V13.0098H302.689V26.6035H295.488ZM305.789 26.6035H304.928V18.5645H309.896V19.4258H305.789V26.6035ZM307.664 21.3008V26.6035H306.803V20.4395H309.896V21.3008H307.664ZM317.104 19.4258H309.902V18.5645H317.104V19.4258ZM317.104 21.3008H309.902V20.4395H317.104V21.3008ZM324.311 19.4258H317.109V18.5645H324.311V19.4258ZM324.311 21.3008H317.109V20.4395H324.311V21.3008ZM324.316 26.6035V13.0098H331.518V26.6035H324.316ZM331.523 26.6035V13.0098H338.725V26.6035H331.523ZM342.838 26.6035V19.4258H338.73V18.5645H343.699V26.6035H342.838ZM340.963 21.3008H338.73V20.4395H341.824V26.6035H340.963V21.3008ZM345.938 26.6035V13.0098H353.139V26.6035H345.938ZM353.145 26.6035V13.0098H360.346V26.6035H353.145ZM360.352 26.6035V13.0098H367.553V26.6035H360.352ZM367.559 26.6035V13.0098H374.76V26.6035H367.559ZM378.873 26.6035V19.4258H374.766V18.5645H379.734V26.6035H378.873ZM376.998 21.3008H374.766V20.4395H377.859V26.6035H376.998V21.3008ZM396.387 26.6035V13.0098H403.588V26.6035H396.387ZM403.594 26.6035V13.0098H410.795V26.6035H403.594ZM414.908 26.6035V13.0098H415.77V26.6035H414.908ZM413.033 26.6035V13.0098H413.895V26.6035H413.033ZM418.008 26.6035V13.0098H425.209V26.6035H418.008ZM425.215 26.6035V13.0098H432.416V26.6035H425.215ZM435.516 26.6035H434.654V18.5645H439.623V19.4258H435.516V26.6035ZM437.391 21.3008V26.6035H436.529V20.4395H439.623V21.3008H437.391ZM446.83 19.4258H439.629V18.5645H446.83V19.4258ZM446.83 21.3008H439.629V20.4395H446.83V21.3008ZM454.037 19.4258H446.836V18.5645H454.037V19.4258ZM454.037 21.3008H446.836V20.4395H454.037V21.3008ZM454.043 26.6035V13.0098H461.244V26.6035H454.043ZM461.25 26.6035V13.0098H468.451V26.6035H461.25ZM472.564 26.6035V19.4258H468.457V18.5645H473.426V26.6035H472.564ZM470.689 21.3008H468.457V20.4395H471.551V26.6035H470.689V21.3008ZM475.664 26.6035V13.0098H482.865V26.6035H475.664ZM482.871 26.6035V13.0098H490.072V26.6035H482.871ZM493.172 26.6035H492.311V18.5645H497.279V19.4258H493.172V26.6035ZM495.047 21.3008V26.6035H494.186V20.4395H497.279V21.3008H495.047ZM504.486 19.4258H497.285V18.5645H504.486V19.4258ZM504.486 21.3008H497.285V20.4395H504.486V21.3008ZM511.693 19.4258H504.492V18.5645H511.693V19.4258ZM511.693 21.3008H504.492V20.4395H511.693V21.3008ZM511.699 26.6035V13.0098H518.9V26.6035H511.699ZM518.906 26.6035V13.0098H526.107V26.6035H518.906ZM530.221 26.6035V19.4258H526.113V18.5645H531.082V26.6035H530.221ZM528.346 21.3008H526.113V20.4395H529.207V26.6035H528.346V21.3008ZM533.32 26.6035V13.0098H540.521V26.6035H533.32ZM540.527 26.6035V13.0098H547.729V26.6035H540.527ZM547.734 26.6035V13.0098H554.936V26.6035H547.734ZM554.941 26.6035V13.0098H562.143V26.6035H554.941ZM566.256 26.6035V19.4258H562.148V18.5645H567.117V26.6035H566.256ZM564.381 21.3008H562.148V20.4395H565.242V26.6035H564.381V21.3008ZM583.77 26.6035V13.0098H590.971V26.6035H583.77ZM590.977 26.6035V13.0098H598.178V26.6035H590.977ZM602.291 26.6035V13.0098H603.152V26.6035H602.291ZM600.416 26.6035V13.0098H601.277V26.6035H600.416ZM0 39.6035V26.0098H7.20117V39.6035H0ZM7.20703 39.6035V26.0098H14.4082V39.6035H7.20703ZM18.5215 39.6035V26.0098H19.3828V39.6035H18.5215ZM16.6465 39.6035V26.0098H17.5078V39.6035H16.6465ZM43.2422 39.6035V26.0098H50.4434V39.6035H43.2422ZM50.4492 39.6035V26.0098H57.6504V39.6035H50.4492ZM61.7637 39.6035V26.0098H62.625V39.6035H61.7637ZM59.8887 39.6035V26.0098H60.75V39.6035H59.8887ZM64.8633 39.6035V26.0098H72.0645V39.6035H64.8633ZM72.0703 39.6035V26.0098H79.2715V39.6035H72.0703ZM83.3848 39.6035V26.0098H84.2461V39.6035H83.3848ZM81.5098 39.6035V26.0098H82.3711V39.6035H81.5098ZM86.4844 39.6035V26.0098H93.6855V39.6035H86.4844ZM93.6914 39.6035V26.0098H100.893V39.6035H93.6914ZM100.898 39.6035V26.0098H108.1V39.6035H100.898ZM108.105 39.6035V26.0098H115.307V39.6035H108.105ZM115.312 39.6035V26.0098H122.514V39.6035H115.312ZM122.52 39.6035V26.0098H129.721V39.6035H122.52ZM132.82 39.6035H131.959V31.5645H136.928V32.4258H132.82V39.6035ZM134.695 34.3008V39.6035H133.834V33.4395H136.928V34.3008H134.695ZM141.041 26.0098H141.902V34.3008H136.934V33.4395H141.041V26.0098ZM139.166 31.5645V26.0098H140.027V32.4258H136.934V31.5645H139.166ZM144.141 39.6035V26.0098H151.342V39.6035H144.141ZM151.348 39.6035V26.0098H158.549V39.6035H151.348ZM158.555 39.6035V26.0098H165.756V39.6035H158.555ZM165.762 39.6035V26.0098H172.963V39.6035H165.762ZM172.969 39.6035V26.0098H180.17V39.6035H172.969ZM184.283 39.6035V32.4258H180.176V31.5645H185.145V39.6035H184.283ZM182.408 34.3008H180.176V33.4395H183.27V39.6035H182.408V34.3008ZM187.383 39.6035V26.0098H194.584V39.6035H187.383ZM194.59 39.6035V26.0098H201.791V39.6035H194.59ZM201.797 39.6035V26.0098H208.998V39.6035H201.797ZM209.004 39.6035V26.0098H216.205V39.6035H209.004ZM216.211 39.6035V26.0098H223.412V39.6035H216.211ZM227.525 39.6035V32.4258H223.418V31.5645H228.387V39.6035H227.525ZM225.65 34.3008H223.418V33.4395H226.512V39.6035H225.65V34.3008ZM230.625 39.6035V26.0098H237.826V39.6035H230.625ZM237.832 39.6035V26.0098H245.033V39.6035H237.832ZM245.039 39.6035V26.0098H252.24V39.6035H245.039ZM252.246 39.6035V26.0098H259.447V39.6035H252.246ZM259.453 39.6035V26.0098H266.654V39.6035H259.453ZM269.754 39.6035H268.893V31.5645H273.861V32.4258H269.754V39.6035ZM271.629 34.3008V39.6035H270.768V33.4395H273.861V34.3008H271.629ZM277.975 26.0098H278.836V34.3008H273.867V33.4395H277.975V26.0098ZM276.1 31.5645V26.0098H276.961V32.4258H273.867V31.5645H276.1ZM288.281 39.6035V26.0098H295.482V39.6035H288.281ZM295.488 39.6035V26.0098H302.689V39.6035H295.488ZM302.695 39.6035V26.0098H309.896V39.6035H302.695ZM309.902 39.6035V26.0098H317.104V39.6035H309.902ZM317.109 39.6035V26.0098H324.311V39.6035H317.109ZM324.316 39.6035V26.0098H331.518V39.6035H324.316ZM331.523 39.6035V26.0098H338.725V39.6035H331.523ZM342.838 39.6035V26.0098H343.699V39.6035H342.838ZM340.963 39.6035V26.0098H341.824V39.6035H340.963ZM345.938 39.6035V26.0098H353.139V39.6035H345.938ZM353.145 39.6035V26.0098H360.346V39.6035H353.145ZM363.445 39.6035H362.584V31.5645H367.553V32.4258H363.445V39.6035ZM365.32 34.3008V39.6035H364.459V33.4395H367.553V34.3008H365.32ZM367.559 39.6035V26.0098H374.76V39.6035H367.559ZM374.766 39.6035V26.0098H381.967V39.6035H374.766ZM386.08 39.6035V32.4258H381.973V31.5645H386.941V39.6035H386.08ZM384.205 34.3008H381.973V33.4395H385.066V39.6035H384.205V34.3008ZM396.387 39.6035V26.0098H403.588V39.6035H396.387ZM403.594 39.6035V26.0098H410.795V39.6035H403.594ZM414.908 39.6035V26.0098H415.77V39.6035H414.908ZM413.033 39.6035V26.0098H413.895V39.6035H413.033ZM418.008 39.6035V26.0098H425.209V39.6035H418.008ZM425.215 39.6035V26.0098H432.416V39.6035H425.215ZM432.422 39.6035V26.0098H439.623V39.6035H432.422ZM439.629 39.6035V26.0098H446.83V39.6035H439.629ZM446.836 39.6035V26.0098H454.037V39.6035H446.836ZM454.043 39.6035V26.0098H461.244V39.6035H454.043ZM464.344 39.6035H463.482V31.5645H468.451V32.4258H464.344V39.6035ZM466.219 34.3008V39.6035H465.357V33.4395H468.451V34.3008H466.219ZM472.564 26.0098H473.426V34.3008H468.457V33.4395H472.564V26.0098ZM470.689 31.5645V26.0098H471.551V32.4258H468.457V31.5645H470.689ZM475.664 39.6035V26.0098H482.865V39.6035H475.664ZM482.871 39.6035V26.0098H490.072V39.6035H482.871ZM490.078 39.6035V26.0098H497.279V39.6035H490.078ZM497.285 39.6035V26.0098H504.486V39.6035H497.285ZM504.492 39.6035V26.0098H511.693V39.6035H504.492ZM511.699 39.6035V26.0098H518.9V39.6035H511.699ZM518.906 39.6035V26.0098H526.107V39.6035H518.906ZM530.221 39.6035V26.0098H531.082V39.6035H530.221ZM528.346 39.6035V26.0098H529.207V39.6035H528.346ZM533.32 39.6035V26.0098H540.521V39.6035H533.32ZM540.527 39.6035V26.0098H547.729V39.6035H540.527ZM550.828 39.6035H549.967V31.5645H554.936V32.4258H550.828V39.6035ZM552.703 34.3008V39.6035H551.842V33.4395H554.936V34.3008H552.703ZM554.941 39.6035V26.0098H562.143V39.6035H554.941ZM562.148 39.6035V26.0098H569.35V39.6035H562.148ZM573.463 39.6035V32.4258H569.355V31.5645H574.324V39.6035H573.463ZM571.588 34.3008H569.355V33.4395H572.449V39.6035H571.588V34.3008ZM583.77 39.6035V26.0098H590.971V39.6035H583.77ZM590.977 39.6035V26.0098H598.178V39.6035H590.977ZM602.291 39.6035V26.0098H603.152V39.6035H602.291ZM600.416 39.6035V26.0098H601.277V39.6035H600.416ZM3.09375 39.0098V46.4395H7.20117V47.3008H2.23242V39.0098H3.09375ZM4.96875 44.5645H7.20117V45.4258H4.10742V39.0098H4.96875V44.5645ZM7.20703 52.6035V39.0098H14.4082V52.6035H7.20703ZM14.4141 52.6035V39.0098H21.6152V52.6035H14.4141ZM25.7285 52.6035V45.4258H21.6211V44.5645H26.5898V52.6035H25.7285ZM23.8535 47.3008H21.6211V46.4395H24.7148V52.6035H23.8535V47.3008ZM36.0352 52.6035V39.0098H43.2363V52.6035H36.0352ZM43.2422 52.6035V39.0098H50.4434V52.6035H43.2422ZM53.543 52.6035H52.6816V44.5645H57.6504V45.4258H53.543V52.6035ZM55.418 47.3008V52.6035H54.5566V46.4395H57.6504V47.3008H55.418ZM61.7637 39.0098H62.625V47.3008H57.6562V46.4395H61.7637V39.0098ZM59.8887 44.5645V39.0098H60.75V45.4258H57.6562V44.5645H59.8887ZM64.8633 52.6035V39.0098H72.0645V52.6035H64.8633ZM72.0703 52.6035V39.0098H79.2715V52.6035H72.0703ZM83.3848 52.6035V39.0098H84.2461V52.6035H83.3848ZM81.5098 52.6035V39.0098H82.3711V52.6035H81.5098ZM86.4844 52.6035V39.0098H93.6855V52.6035H86.4844ZM93.6914 52.6035V39.0098H100.893V52.6035H93.6914ZM103.992 52.6035H103.131V44.5645H108.1V45.4258H103.992V52.6035ZM105.867 47.3008V52.6035H105.006V46.4395H108.1V47.3008H105.867ZM115.307 45.4258H108.105V44.5645H115.307V45.4258ZM115.307 47.3008H108.105V46.4395H115.307V47.3008ZM122.514 45.4258H115.312V44.5645H122.514V45.4258ZM122.514 47.3008H115.312V46.4395H122.514V47.3008ZM122.52 52.6035V39.0098H129.721V52.6035H122.52ZM129.727 52.6035V39.0098H136.928V52.6035H129.727ZM141.041 52.6035V45.4258H136.934V44.5645H141.902V52.6035H141.041ZM139.166 47.3008H136.934V46.4395H140.027V52.6035H139.166V47.3008ZM144.141 52.6035V39.0098H151.342V52.6035H144.141ZM151.348 52.6035V39.0098H158.549V52.6035H151.348ZM161.648 52.6035H160.787V44.5645H165.756V45.4258H161.648V52.6035ZM163.523 47.3008V52.6035H162.662V46.4395H165.756V47.3008H163.523ZM172.963 45.4258H165.762V44.5645H172.963V45.4258ZM172.963 47.3008H165.762V46.4395H172.963V47.3008ZM180.17 45.4258H172.969V44.5645H180.17V45.4258ZM180.17 47.3008H172.969V46.4395H180.17V47.3008ZM184.283 39.0098H185.145V47.3008H180.176V46.4395H184.283V39.0098ZM182.408 44.5645V39.0098H183.27V45.4258H180.176V44.5645H182.408ZM190.477 39.0098V46.4395H194.584V47.3008H189.615V39.0098H190.477ZM192.352 44.5645H194.584V45.4258H191.49V39.0098H192.352V44.5645ZM201.791 45.4258H194.59V44.5645H201.791V45.4258ZM201.791 47.3008H194.59V46.4395H201.791V47.3008ZM208.998 45.4258H201.797V44.5645H208.998V45.4258ZM208.998 47.3008H201.797V46.4395H208.998V47.3008ZM216.205 45.4258H209.004V44.5645H216.205V45.4258ZM216.205 47.3008H209.004V46.4395H216.205V47.3008ZM223.412 45.4258H216.211V44.5645H223.412V45.4258ZM223.412 47.3008H216.211V46.4395H223.412V47.3008ZM227.525 39.0098H228.387V47.3008H223.418V46.4395H227.525V39.0098ZM225.65 44.5645V39.0098H226.512V45.4258H223.418V44.5645H225.65ZM230.625 52.6035V39.0098H237.826V52.6035H230.625ZM237.832 52.6035V39.0098H245.033V52.6035H237.832ZM248.133 52.6035H247.271V44.5645H252.24V45.4258H248.133V52.6035ZM250.008 47.3008V52.6035H249.146V46.4395H252.24V47.3008H250.008ZM259.447 45.4258H252.246V44.5645H259.447V45.4258ZM259.447 47.3008H252.246V46.4395H259.447V47.3008ZM259.453 52.6035V39.0098H266.654V52.6035H259.453ZM266.66 52.6035V39.0098H273.861V52.6035H266.66ZM277.975 52.6035V45.4258H273.867V44.5645H278.836V52.6035H277.975ZM276.1 47.3008H273.867V46.4395H276.961V52.6035H276.1V47.3008ZM288.281 52.6035V39.0098H295.482V52.6035H288.281ZM295.488 52.6035V39.0098H302.689V52.6035H295.488ZM305.789 52.6035H304.928V44.5645H309.896V45.4258H305.789V52.6035ZM307.664 47.3008V52.6035H306.803V46.4395H309.896V47.3008H307.664ZM317.104 45.4258H309.902V44.5645H317.104V45.4258ZM317.104 47.3008H309.902V46.4395H317.104V47.3008ZM324.311 45.4258H317.109V44.5645H324.311V45.4258ZM324.311 47.3008H317.109V46.4395H324.311V47.3008ZM324.316 52.6035V39.0098H331.518V52.6035H324.316ZM331.523 52.6035V39.0098H338.725V52.6035H331.523ZM342.838 52.6035V39.0098H343.699V52.6035H342.838ZM340.963 52.6035V39.0098H341.824V52.6035H340.963ZM345.938 52.6035V39.0098H353.139V52.6035H345.938ZM353.145 52.6035V39.0098H360.346V52.6035H353.145ZM364.459 52.6035V39.0098H365.32V52.6035H364.459ZM362.584 52.6035V39.0098H363.445V52.6035H362.584ZM370.652 39.0098V46.4395H374.76V47.3008H369.791V39.0098H370.652ZM372.527 44.5645H374.76V45.4258H371.666V39.0098H372.527V44.5645ZM374.766 52.6035V39.0098H381.967V52.6035H374.766ZM381.973 52.6035V39.0098H389.174V52.6035H381.973ZM393.287 52.6035V45.4258H389.18V44.5645H394.148V52.6035H393.287ZM391.412 47.3008H389.18V46.4395H392.273V52.6035H391.412V47.3008ZM396.387 52.6035V39.0098H403.588V52.6035H396.387ZM403.594 52.6035V39.0098H410.795V52.6035H403.594ZM414.908 52.6035V39.0098H415.77V52.6035H414.908ZM413.033 52.6035V39.0098H413.895V52.6035H413.033ZM418.008 52.6035V39.0098H425.209V52.6035H418.008ZM425.215 52.6035V39.0098H432.416V52.6035H425.215ZM435.516 52.6035H434.654V44.5645H439.623V45.4258H435.516V52.6035ZM437.391 47.3008V52.6035H436.529V46.4395H439.623V47.3008H437.391ZM446.83 45.4258H439.629V44.5645H446.83V45.4258ZM446.83 47.3008H439.629V46.4395H446.83V47.3008ZM454.037 45.4258H446.836V44.5645H454.037V45.4258ZM454.037 47.3008H446.836V46.4395H454.037V47.3008ZM454.043 52.6035V39.0098H461.244V52.6035H454.043ZM461.25 52.6035V39.0098H468.451V52.6035H461.25ZM472.564 52.6035V45.4258H468.457V44.5645H473.426V52.6035H472.564ZM470.689 47.3008H468.457V46.4395H471.551V52.6035H470.689V47.3008ZM475.664 52.6035V39.0098H482.865V52.6035H475.664ZM482.871 52.6035V39.0098H490.072V52.6035H482.871ZM493.172 52.6035H492.311V44.5645H497.279V45.4258H493.172V52.6035ZM495.047 47.3008V52.6035H494.186V46.4395H497.279V47.3008H495.047ZM504.486 45.4258H497.285V44.5645H504.486V45.4258ZM504.486 47.3008H497.285V46.4395H504.486V47.3008ZM511.693 45.4258H504.492V44.5645H511.693V45.4258ZM511.693 47.3008H504.492V46.4395H511.693V47.3008ZM511.699 52.6035V39.0098H518.9V52.6035H511.699ZM518.906 52.6035V39.0098H526.107V52.6035H518.906ZM530.221 52.6035V39.0098H531.082V52.6035H530.221ZM528.346 52.6035V39.0098H529.207V52.6035H528.346ZM533.32 52.6035V39.0098H540.521V52.6035H533.32ZM540.527 52.6035V39.0098H547.729V52.6035H540.527ZM551.842 52.6035V39.0098H552.703V52.6035H551.842ZM549.967 52.6035V39.0098H550.828V52.6035H549.967ZM558.035 39.0098V46.4395H562.143V47.3008H557.174V39.0098H558.035ZM559.91 44.5645H562.143V45.4258H559.049V39.0098H559.91V44.5645ZM562.148 52.6035V39.0098H569.35V52.6035H562.148ZM569.355 52.6035V39.0098H576.557V52.6035H569.355ZM580.67 52.6035V45.4258H576.562V44.5645H581.531V52.6035H580.67ZM578.795 47.3008H576.562V46.4395H579.656V52.6035H578.795V47.3008ZM583.77 52.6035V39.0098H590.971V52.6035H583.77ZM590.977 52.6035V39.0098H598.178V52.6035H590.977ZM602.291 52.6035V39.0098H603.152V52.6035H602.291ZM600.416 52.6035V39.0098H601.277V52.6035H600.416ZM10.3008 52.0098V59.4395H14.4082V60.3008H9.43945V52.0098H10.3008ZM12.1758 57.5645H14.4082V58.4258H11.3145V52.0098H12.1758V57.5645ZM14.4141 65.6035V52.0098H21.6152V65.6035H14.4141ZM21.6211 65.6035V52.0098H28.8223V65.6035H21.6211ZM28.8281 65.6035V52.0098H36.0293V65.6035H28.8281ZM36.0352 65.6035V52.0098H43.2363V65.6035H36.0352ZM46.3359 65.6035H45.4746V57.5645H50.4434V58.4258H46.3359V65.6035ZM48.2109 60.3008V65.6035H47.3496V59.4395H50.4434V60.3008H48.2109ZM54.5566 52.0098H55.418V60.3008H50.4492V59.4395H54.5566V52.0098ZM52.6816 57.5645V52.0098H53.543V58.4258H50.4492V57.5645H52.6816ZM64.8633 65.6035V52.0098H72.0645V65.6035H64.8633ZM72.0703 65.6035V52.0098H79.2715V65.6035H72.0703ZM83.3848 65.6035V52.0098H84.2461V65.6035H83.3848ZM81.5098 65.6035V52.0098H82.3711V65.6035H81.5098ZM86.4844 65.6035V52.0098H93.6855V65.6035H86.4844ZM93.6914 65.6035V52.0098H100.893V65.6035H93.6914ZM100.898 65.6035V52.0098H108.1V65.6035H100.898ZM108.105 65.6035V52.0098H115.307V65.6035H108.105ZM115.312 65.6035V52.0098H122.514V65.6035H115.312ZM122.52 65.6035V52.0098H129.721V65.6035H122.52ZM132.82 65.6035H131.959V57.5645H136.928V58.4258H132.82V65.6035ZM134.695 60.3008V65.6035H133.834V59.4395H136.928V60.3008H134.695ZM141.041 52.0098H141.902V60.3008H136.934V59.4395H141.041V52.0098ZM139.166 57.5645V52.0098H140.027V58.4258H136.934V57.5645H139.166ZM144.141 65.6035V52.0098H151.342V65.6035H144.141ZM151.348 65.6035V52.0098H158.549V65.6035H151.348ZM158.555 65.6035V52.0098H165.756V65.6035H158.555ZM165.762 65.6035V52.0098H172.963V65.6035H165.762ZM172.969 65.6035V52.0098H180.17V65.6035H172.969ZM180.176 65.6035V52.0098H187.377V65.6035H180.176ZM187.383 65.6035V52.0098H194.584V65.6035H187.383ZM198.697 65.6035V58.4258H194.59V57.5645H199.559V65.6035H198.697ZM196.822 60.3008H194.59V59.4395H197.684V65.6035H196.822V60.3008ZM230.625 65.6035V52.0098H237.826V65.6035H230.625ZM237.832 65.6035V52.0098H245.033V65.6035H237.832ZM249.146 65.6035V52.0098H250.008V65.6035H249.146ZM247.271 65.6035V52.0098H248.133V65.6035H247.271ZM266.66 65.6035V52.0098H273.861V65.6035H266.66ZM273.867 65.6035V52.0098H281.068V65.6035H273.867ZM285.182 65.6035V58.4258H281.074V57.5645H286.043V65.6035H285.182ZM283.307 60.3008H281.074V59.4395H284.168V65.6035H283.307V60.3008ZM288.281 65.6035V52.0098H295.482V65.6035H288.281ZM295.488 65.6035V52.0098H302.689V65.6035H295.488ZM306.803 65.6035V52.0098H307.664V65.6035H306.803ZM304.928 65.6035V52.0098H305.789V65.6035H304.928ZM324.316 65.6035V52.0098H331.518V65.6035H324.316ZM331.523 65.6035V52.0098H338.725V65.6035H331.523ZM342.838 65.6035V52.0098H343.699V65.6035H342.838ZM340.963 65.6035V52.0098H341.824V65.6035H340.963ZM345.938 65.6035V52.0098H353.139V65.6035H345.938ZM353.145 65.6035V52.0098H360.346V65.6035H353.145ZM364.459 65.6035V52.0098H365.32V65.6035H364.459ZM362.584 65.6035V52.0098H363.445V65.6035H362.584ZM377.859 52.0098V59.4395H381.967V60.3008H376.998V52.0098H377.859ZM379.734 57.5645H381.967V58.4258H378.873V52.0098H379.734V57.5645ZM381.973 65.6035V52.0098H389.174V65.6035H381.973ZM389.18 65.6035V52.0098H396.381V65.6035H389.18ZM396.387 65.6035V52.0098H403.588V65.6035H396.387ZM403.594 65.6035V52.0098H410.795V65.6035H403.594ZM414.908 65.6035V52.0098H415.77V65.6035H414.908ZM413.033 65.6035V52.0098H413.895V65.6035H413.033ZM418.008 65.6035V52.0098H425.209V65.6035H418.008ZM425.215 65.6035V52.0098H432.416V65.6035H425.215ZM432.422 65.6035V52.0098H439.623V65.6035H432.422ZM439.629 65.6035V52.0098H446.83V65.6035H439.629ZM446.836 65.6035V52.0098H454.037V65.6035H446.836ZM454.043 65.6035V52.0098H461.244V65.6035H454.043ZM464.344 65.6035H463.482V57.5645H468.451V58.4258H464.344V65.6035ZM466.219 60.3008V65.6035H465.357V59.4395H468.451V60.3008H466.219ZM472.564 52.0098H473.426V60.3008H468.457V59.4395H472.564V52.0098ZM470.689 57.5645V52.0098H471.551V58.4258H468.457V57.5645H470.689ZM475.664 65.6035V52.0098H482.865V65.6035H475.664ZM482.871 65.6035V52.0098H490.072V65.6035H482.871ZM494.186 65.6035V52.0098H495.047V65.6035H494.186ZM492.311 65.6035V52.0098H493.172V65.6035H492.311ZM511.699 65.6035V52.0098H518.9V65.6035H511.699ZM518.906 65.6035V52.0098H526.107V65.6035H518.906ZM530.221 65.6035V52.0098H531.082V65.6035H530.221ZM528.346 65.6035V52.0098H529.207V65.6035H528.346ZM533.32 65.6035V52.0098H540.521V65.6035H533.32ZM540.527 65.6035V52.0098H547.729V65.6035H540.527ZM551.842 65.6035V52.0098H552.703V65.6035H551.842ZM549.967 65.6035V52.0098H550.828V65.6035H549.967ZM565.242 52.0098V59.4395H569.35V60.3008H564.381V52.0098H565.242ZM567.117 57.5645H569.35V58.4258H566.256V52.0098H567.117V57.5645ZM569.355 65.6035V52.0098H576.557V65.6035H569.355ZM576.562 65.6035V52.0098H583.764V65.6035H576.562ZM583.77 65.6035V52.0098H590.971V65.6035H583.77ZM590.977 65.6035V52.0098H598.178V65.6035H590.977ZM602.291 65.6035V52.0098H603.152V65.6035H602.291ZM600.416 65.6035V52.0098H601.277V65.6035H600.416ZM17.5078 65.0098V72.4395H21.6152V73.3008H16.6465V65.0098H17.5078ZM19.3828 70.5645H21.6152V71.4258H18.5215V65.0098H19.3828V70.5645ZM28.8223 71.4258H21.6211V70.5645H28.8223V71.4258ZM28.8223 73.3008H21.6211V72.4395H28.8223V73.3008ZM36.0293 71.4258H28.8281V70.5645H36.0293V71.4258ZM36.0293 73.3008H28.8281V72.4395H36.0293V73.3008ZM43.2363 71.4258H36.0352V70.5645H43.2363V71.4258ZM43.2363 73.3008H36.0352V72.4395H43.2363V73.3008ZM47.3496 65.0098H48.2109V73.3008H43.2422V72.4395H47.3496V65.0098ZM45.4746 70.5645V65.0098H46.3359V71.4258H43.2422V70.5645H45.4746ZM67.957 65.0098V72.4395H72.0645V73.3008H67.0957V65.0098H67.957ZM69.832 70.5645H72.0645V71.4258H68.9707V65.0098H69.832V70.5645ZM79.2715 71.4258H72.0703V70.5645H79.2715V71.4258ZM79.2715 73.3008H72.0703V72.4395H79.2715V73.3008ZM83.3848 65.0098H84.2461V73.3008H79.2773V72.4395H83.3848V65.0098ZM81.5098 70.5645V65.0098H82.3711V71.4258H79.2773V70.5645H81.5098ZM89.5781 65.0098V72.4395H93.6855V73.3008H88.7168V65.0098H89.5781ZM91.4531 70.5645H93.6855V71.4258H90.5918V65.0098H91.4531V70.5645ZM100.893 71.4258H93.6914V70.5645H100.893V71.4258ZM100.893 73.3008H93.6914V72.4395H100.893V73.3008ZM108.1 71.4258H100.898V70.5645H108.1V71.4258ZM108.1 73.3008H100.898V72.4395H108.1V73.3008ZM115.307 71.4258H108.105V70.5645H115.307V71.4258ZM115.307 73.3008H108.105V72.4395H115.307V73.3008ZM122.514 71.4258H115.312V70.5645H122.514V71.4258ZM122.514 73.3008H115.312V72.4395H122.514V73.3008ZM129.721 71.4258H122.52V70.5645H129.721V71.4258ZM129.721 73.3008H122.52V72.4395H129.721V73.3008ZM133.834 65.0098H134.695V73.3008H129.727V72.4395H133.834V65.0098ZM131.959 70.5645V65.0098H132.82V71.4258H129.727V70.5645H131.959ZM147.234 65.0098V72.4395H151.342V73.3008H146.373V65.0098H147.234ZM149.109 70.5645H151.342V71.4258H148.248V65.0098H149.109V70.5645ZM158.549 71.4258H151.348V70.5645H158.549V71.4258ZM158.549 73.3008H151.348V72.4395H158.549V73.3008ZM165.756 71.4258H158.555V70.5645H165.756V71.4258ZM165.756 73.3008H158.555V72.4395H165.756V73.3008ZM172.963 71.4258H165.762V70.5645H172.963V71.4258ZM172.963 73.3008H165.762V72.4395H172.963V73.3008ZM180.17 71.4258H172.969V70.5645H180.17V71.4258ZM180.17 73.3008H172.969V72.4395H180.17V73.3008ZM187.377 71.4258H180.176V70.5645H187.377V71.4258ZM187.377 73.3008H180.176V72.4395H187.377V73.3008ZM194.584 71.4258H187.383V70.5645H194.584V71.4258ZM194.584 73.3008H187.383V72.4395H194.584V73.3008ZM198.697 65.0098H199.559V73.3008H194.59V72.4395H198.697V65.0098ZM196.822 70.5645V65.0098H197.684V71.4258H194.59V70.5645H196.822ZM233.719 65.0098V72.4395H237.826V73.3008H232.857V65.0098H233.719ZM235.594 70.5645H237.826V71.4258H234.732V65.0098H235.594V70.5645ZM245.033 71.4258H237.832V70.5645H245.033V71.4258ZM245.033 73.3008H237.832V72.4395H245.033V73.3008ZM249.146 65.0098H250.008V73.3008H245.039V72.4395H249.146V65.0098ZM247.271 70.5645V65.0098H248.133V71.4258H245.039V70.5645H247.271ZM269.754 65.0098V72.4395H273.861V73.3008H268.893V65.0098H269.754ZM271.629 70.5645H273.861V71.4258H270.768V65.0098H271.629V70.5645ZM281.068 71.4258H273.867V70.5645H281.068V71.4258ZM281.068 73.3008H273.867V72.4395H281.068V73.3008ZM285.182 65.0098H286.043V73.3008H281.074V72.4395H285.182V65.0098ZM283.307 70.5645V65.0098H284.168V71.4258H281.074V70.5645H283.307ZM291.375 65.0098V72.4395H295.482V73.3008H290.514V65.0098H291.375ZM293.25 70.5645H295.482V71.4258H292.389V65.0098H293.25V70.5645ZM302.689 71.4258H295.488V70.5645H302.689V71.4258ZM302.689 73.3008H295.488V72.4395H302.689V73.3008ZM306.803 65.0098H307.664V73.3008H302.695V72.4395H306.803V65.0098ZM304.928 70.5645V65.0098H305.789V71.4258H302.695V70.5645H304.928ZM327.41 65.0098V72.4395H331.518V73.3008H326.549V65.0098H327.41ZM329.285 70.5645H331.518V71.4258H328.424V65.0098H329.285V70.5645ZM338.725 71.4258H331.523V70.5645H338.725V71.4258ZM338.725 73.3008H331.523V72.4395H338.725V73.3008ZM342.838 65.0098H343.699V73.3008H338.73V72.4395H342.838V65.0098ZM340.963 70.5645V65.0098H341.824V71.4258H338.73V70.5645H340.963ZM349.031 65.0098V72.4395H353.139V73.3008H348.17V65.0098H349.031ZM350.906 70.5645H353.139V71.4258H350.045V65.0098H350.906V70.5645ZM360.346 71.4258H353.145V70.5645H360.346V71.4258ZM360.346 73.3008H353.145V72.4395H360.346V73.3008ZM364.459 65.0098H365.32V73.3008H360.352V72.4395H364.459V65.0098ZM362.584 70.5645V65.0098H363.445V71.4258H360.352V70.5645H362.584ZM385.066 65.0098V72.4395H389.174V73.3008H384.205V65.0098H385.066ZM386.941 70.5645H389.174V71.4258H386.08V65.0098H386.941V70.5645ZM396.381 71.4258H389.18V70.5645H396.381V71.4258ZM396.381 73.3008H389.18V72.4395H396.381V73.3008ZM403.588 71.4258H396.387V70.5645H403.588V71.4258ZM403.588 73.3008H396.387V72.4395H403.588V73.3008ZM410.795 71.4258H403.594V70.5645H410.795V71.4258ZM410.795 73.3008H403.594V72.4395H410.795V73.3008ZM414.908 65.0098H415.77V73.3008H410.801V72.4395H414.908V65.0098ZM413.033 70.5645V65.0098H413.895V71.4258H410.801V70.5645H413.033ZM421.102 65.0098V72.4395H425.209V73.3008H420.24V65.0098H421.102ZM422.977 70.5645H425.209V71.4258H422.115V65.0098H422.977V70.5645ZM432.416 71.4258H425.215V70.5645H432.416V71.4258ZM432.416 73.3008H425.215V72.4395H432.416V73.3008ZM439.623 71.4258H432.422V70.5645H439.623V71.4258ZM439.623 73.3008H432.422V72.4395H439.623V73.3008ZM446.83 71.4258H439.629V70.5645H446.83V71.4258ZM446.83 73.3008H439.629V72.4395H446.83V73.3008ZM454.037 71.4258H446.836V70.5645H454.037V71.4258ZM454.037 73.3008H446.836V72.4395H454.037V73.3008ZM461.244 71.4258H454.043V70.5645H461.244V71.4258ZM461.244 73.3008H454.043V72.4395H461.244V73.3008ZM465.357 65.0098H466.219V73.3008H461.25V72.4395H465.357V65.0098ZM463.482 70.5645V65.0098H464.344V71.4258H461.25V70.5645H463.482ZM478.758 65.0098V72.4395H482.865V73.3008H477.896V65.0098H478.758ZM480.633 70.5645H482.865V71.4258H479.771V65.0098H480.633V70.5645ZM490.072 71.4258H482.871V70.5645H490.072V71.4258ZM490.072 73.3008H482.871V72.4395H490.072V73.3008ZM494.186 65.0098H495.047V73.3008H490.078V72.4395H494.186V65.0098ZM492.311 70.5645V65.0098H493.172V71.4258H490.078V70.5645H492.311ZM514.793 65.0098V72.4395H518.9V73.3008H513.932V65.0098H514.793ZM516.668 70.5645H518.9V71.4258H515.807V65.0098H516.668V70.5645ZM526.107 71.4258H518.906V70.5645H526.107V71.4258ZM526.107 73.3008H518.906V72.4395H526.107V73.3008ZM530.221 65.0098H531.082V73.3008H526.113V72.4395H530.221V65.0098ZM528.346 70.5645V65.0098H529.207V71.4258H526.113V70.5645H528.346ZM536.414 65.0098V72.4395H540.521V73.3008H535.553V65.0098H536.414ZM538.289 70.5645H540.521V71.4258H537.428V65.0098H538.289V70.5645ZM547.729 71.4258H540.527V70.5645H547.729V71.4258ZM547.729 73.3008H540.527V72.4395H547.729V73.3008ZM551.842 65.0098H552.703V73.3008H547.734V72.4395H551.842V65.0098ZM549.967 70.5645V65.0098H550.828V71.4258H547.734V70.5645H549.967ZM572.449 65.0098V72.4395H576.557V73.3008H571.588V65.0098H572.449ZM574.324 70.5645H576.557V71.4258H573.463V65.0098H574.324V70.5645ZM583.764 71.4258H576.562V70.5645H583.764V71.4258ZM583.764 73.3008H576.562V72.4395H583.764V73.3008ZM590.971 71.4258H583.77V70.5645H590.971V71.4258ZM590.971 73.3008H583.77V72.4395H590.971V73.3008ZM598.178 71.4258H590.977V70.5645H598.178V71.4258ZM598.178 73.3008H590.977V72.4395H598.178V73.3008ZM602.291 65.0098H603.152V73.3008H598.184V72.4395H602.291V65.0098ZM600.416 70.5645V65.0098H601.277V71.4258H598.184V70.5645H600.416Z" />
    </svg>
  );
}
</file>

<file path="frontend/src/components/search-bar.tsx">
import * as React from 'react';
import { Search } from 'lucide-react';
import { Input } from '@/components/ui/input';
import { cn } from '@/lib/utils';
import { Project } from 'shared/types';

interface SearchBarProps {
  className?: string;
  value?: string;
  onChange?: (value: string) => void;
  disabled?: boolean;
  onClear?: () => void;
  project: Project | null;
}

export function SearchBar({
  className,
  value = '',
  onChange,
  disabled = false,
  onClear,
  project,
}: SearchBarProps) {
  const inputRef = React.useRef<HTMLInputElement>(null);

  React.useEffect(() => {
    function onKeyDown(e: KeyboardEvent) {
      if ((e.metaKey || e.ctrlKey) && e.key.toLowerCase() === 's') {
        e.preventDefault();
        inputRef.current?.focus();
      }

      if (e.key === 'Escape' && document.activeElement === inputRef.current) {
        e.preventDefault();
        onClear?.();
        inputRef.current?.blur();
      }
    }

    window.addEventListener('keydown', onKeyDown);
    return () => window.removeEventListener('keydown', onKeyDown);
  }, [onClear]);

  if (disabled) {
    return null;
  }

  return (
    <div className={cn('relative w-64 sm:w-72', className)}>
      <Search className="absolute left-2.5 top-1/2 -translate-y-1/2 h-4 w-4 text-muted-foreground" />
      <Input
        ref={inputRef}
        value={value}
        onChange={(e) => onChange?.(e.target.value)}
        disabled={disabled}
        placeholder={project ? `Search ${project.name}...` : 'Search...'}
        className="pl-8 pr-14 h-8 bg-muted"
      />
      <kbd className="absolute right-2.5 top-1/2 -translate-y-1/2 pointer-events-none select-none font-mono text-[10px] text-muted-foreground rounded border bg-muted px-1 py-0.5">
        ⌘S
      </kbd>
    </div>
  );
}
</file>

<file path="frontend/src/components/TaskTemplateManager.tsx">
import { useState, useEffect, useCallback } from 'react';
import { Button } from '@/components/ui/button';
import { Plus, Edit2, Trash2, Loader2 } from 'lucide-react';
import { templatesApi } from '@/lib/api';
import { showTaskTemplateEdit } from '@/lib/modals';
import type { TaskTemplate } from 'shared/types';

interface TaskTemplateManagerProps {
  projectId?: string;
  isGlobal?: boolean;
}

export function TaskTemplateManager({
  projectId,
  isGlobal = false,
}: TaskTemplateManagerProps) {
  const [templates, setTemplates] = useState<TaskTemplate[]>([]);
  const [loading, setLoading] = useState(true);

  const fetchTemplates = useCallback(async () => {
    setLoading(true);
    try {
      const data = isGlobal
        ? await templatesApi.listGlobal()
        : projectId
          ? await templatesApi.listByProject(projectId)
          : [];

      // Filter to show only templates for this specific scope
      const filtered = data.filter((template) =>
        isGlobal
          ? template.project_id === null
          : template.project_id === projectId
      );

      setTemplates(filtered);
    } catch (err) {
      console.error('Failed to fetch templates:', err);
    } finally {
      setLoading(false);
    }
  }, [isGlobal, projectId]);

  useEffect(() => {
    fetchTemplates();
  }, [fetchTemplates]);

  const handleOpenDialog = useCallback(
    async (template?: TaskTemplate) => {
      try {
        const result = await showTaskTemplateEdit({
          template: template || null,
          projectId,
          isGlobal,
        });

        if (result === 'saved') {
          await fetchTemplates();
        }
      } catch (error) {
        // User cancelled - do nothing
      }
    },
    [projectId, isGlobal, fetchTemplates]
  );

  const handleDelete = useCallback(
    async (template: TaskTemplate) => {
      if (
        !confirm(
          `Are you sure you want to delete the template "${template.template_name}"?`
        )
      ) {
        return;
      }

      try {
        await templatesApi.delete(template.id);
        await fetchTemplates();
      } catch (err) {
        console.error('Failed to delete template:', err);
      }
    },
    [fetchTemplates]
  );

  if (loading) {
    return (
      <div className="flex items-center justify-center py-8">
        <Loader2 className="h-8 w-8 animate-spin" />
      </div>
    );
  }

  return (
    <div className="space-y-4">
      <div className="flex justify-between items-center">
        <h3 className="text-lg font-semibold">
          {isGlobal ? 'Global Task Templates' : 'Project Task Templates'}
        </h3>
        <Button onClick={() => handleOpenDialog()} size="sm">
          <Plus className="h-4 w-4 mr-2" />
          Add Template
        </Button>
      </div>

      {templates.length === 0 ? (
        <div className="text-center py-8 text-muted-foreground">
          No templates yet. Create your first template to get started.
        </div>
      ) : (
        <div className="border rounded-lg overflow-hidden">
          <div className="max-h-[400px] overflow-auto">
            <table className="w-full">
              <thead className="border-b bg-muted/50 sticky top-0">
                <tr>
                  <th className="text-left p-2 text-sm font-medium">
                    Template Name
                  </th>
                  <th className="text-left p-2 text-sm font-medium">Title</th>
                  <th className="text-left p-2 text-sm font-medium">
                    Description
                  </th>
                  <th className="text-right p-2 text-sm font-medium">
                    Actions
                  </th>
                </tr>
              </thead>
              <tbody>
                {templates.map((template) => (
                  <tr
                    key={template.id}
                    className="border-b hover:bg-muted/30 transition-colors"
                  >
                    <td className="p-2 text-sm font-medium">
                      {template.template_name}
                    </td>
                    <td className="p-2 text-sm">{template.title}</td>
                    <td className="p-2 text-sm">
                      <div
                        className="max-w-[200px] truncate"
                        title={template.description || ''}
                      >
                        {template.description || (
                          <span className="text-muted-foreground">-</span>
                        )}
                      </div>
                    </td>
                    <td className="p-2">
                      <div className="flex justify-end gap-1">
                        <Button
                          variant="ghost"
                          size="icon"
                          className="h-7 w-7"
                          onClick={() => handleOpenDialog(template)}
                          title="Edit template"
                        >
                          <Edit2 className="h-3 w-3" />
                        </Button>
                        <Button
                          variant="ghost"
                          size="icon"
                          className="h-7 w-7"
                          onClick={() => handleDelete(template)}
                          title="Delete template"
                        >
                          <Trash2 className="h-3 w-3" />
                        </Button>
                      </div>
                    </td>
                  </tr>
                ))}
              </tbody>
            </table>
          </div>
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/theme-provider.tsx">
import React, { createContext, useContext, useEffect, useState } from 'react';
import { ThemeMode } from 'shared/types';

type ThemeProviderProps = {
  children: React.ReactNode;
  initialTheme?: ThemeMode;
};

type ThemeProviderState = {
  theme: ThemeMode;
  setTheme: (theme: ThemeMode) => void;
};

const initialState: ThemeProviderState = {
  theme: ThemeMode.SYSTEM,
  setTheme: () => null,
};

const ThemeProviderContext = createContext<ThemeProviderState>(initialState);

export function ThemeProvider({
  children,
  initialTheme = ThemeMode.SYSTEM,
  ...props
}: ThemeProviderProps) {
  const [theme, setThemeState] = useState<ThemeMode>(initialTheme);

  // Update theme when initialTheme changes
  useEffect(() => {
    setThemeState(initialTheme);
  }, [initialTheme]);

  useEffect(() => {
    const root = window.document.documentElement;

    root.classList.remove(
      'light',
      'dark',
      'purple',
      'green',
      'blue',
      'orange',
      'red'
    );

    if (theme === ThemeMode.SYSTEM) {
      const systemTheme = window.matchMedia('(prefers-color-scheme: dark)')
        .matches
        ? 'dark'
        : 'light';

      root.classList.add(systemTheme);
      return;
    }

    root.classList.add(theme.toLowerCase());
  }, [theme]);

  const setTheme = (newTheme: ThemeMode) => {
    setThemeState(newTheme);
  };

  const value = {
    theme,
    setTheme,
  };

  return (
    <ThemeProviderContext.Provider {...props} value={value}>
      {children}
    </ThemeProviderContext.Provider>
  );
}

export const useTheme = () => {
  const context = useContext(ThemeProviderContext);

  if (context === undefined)
    throw new Error('useTheme must be used within a ThemeProvider');

  return context;
};
</file>

<file path="frontend/src/constants/processes.ts">
import type {
  ExecutionProcessRunReason,
  ExecutionProcessStatus,
  ExecutionProcess,
} from 'shared/types';

// Process run reasons
export const PROCESS_RUN_REASONS = {
  SETUP_SCRIPT: 'setupscript' as ExecutionProcessRunReason,
  CLEANUP_SCRIPT: 'cleanupscript' as ExecutionProcessRunReason,
  CODING_AGENT: 'codingagent' as ExecutionProcessRunReason,
  DEV_SERVER: 'devserver' as ExecutionProcessRunReason,
} as const;

// Process statuses
export const PROCESS_STATUSES = {
  RUNNING: 'running' as ExecutionProcessStatus,
  COMPLETED: 'completed' as ExecutionProcessStatus,
  FAILED: 'failed' as ExecutionProcessStatus,
  KILLED: 'killed' as ExecutionProcessStatus,
} as const;

// Helper functions
export const isAutoCollapsibleProcess = (
  runReason: ExecutionProcessRunReason
): boolean => {
  return (
    runReason === PROCESS_RUN_REASONS.SETUP_SCRIPT ||
    runReason === PROCESS_RUN_REASONS.CLEANUP_SCRIPT
  );
};

export const isCodingAgent = (
  runReason: ExecutionProcessRunReason
): boolean => {
  return runReason === PROCESS_RUN_REASONS.CODING_AGENT;
};

export const isProcessCompleted = (status: ExecutionProcessStatus): boolean => {
  return (
    status === PROCESS_STATUSES.COMPLETED || status === PROCESS_STATUSES.FAILED
  );
};

export const shouldShowInLogs = (
  runReason: ExecutionProcessRunReason
): boolean => {
  return runReason !== PROCESS_RUN_REASONS.DEV_SERVER;
};

export const getLatestCodingAgent = (
  processes: ExecutionProcess[]
): string | null => {
  const codingAgents = processes.filter((p) => isCodingAgent(p.run_reason));
  if (codingAgents.length === 0) return null;

  return codingAgents.sort((a, b) =>
    a.started_at === b.started_at
      ? a.id.localeCompare(b.id) // tie-break for same timestamp
      : new Date(b.started_at).getTime() - new Date(a.started_at).getTime()
  )[0].id;
};
</file>

<file path="frontend/src/contexts/EntriesContext.tsx">
import {
  createContext,
  useContext,
  useState,
  useMemo,
  useCallback,
  ReactNode,
} from 'react';
import type { PatchTypeWithKey } from '@/hooks/useConversationHistory';

interface EntriesContextType {
  entries: PatchTypeWithKey[];
  setEntries: (entries: PatchTypeWithKey[]) => void;
  reset: () => void;
}

const EntriesContext = createContext<EntriesContextType | null>(null);

interface EntriesProviderProps {
  children: ReactNode;
}

export const EntriesProvider = ({ children }: EntriesProviderProps) => {
  const [entries, setEntriesState] = useState<PatchTypeWithKey[]>([]);

  const setEntries = useCallback((newEntries: PatchTypeWithKey[]) => {
    setEntriesState(newEntries);
  }, []);

  const reset = useCallback(() => {
    setEntriesState([]);
  }, []);

  const value = useMemo(
    () => ({
      entries,
      setEntries,
      reset,
    }),
    [entries, setEntries, reset]
  );

  return (
    <EntriesContext.Provider value={value}>{children}</EntriesContext.Provider>
  );
};

export const useEntries = (): EntriesContextType => {
  const context = useContext(EntriesContext);
  if (!context) {
    throw new Error('useEntries must be used within an EntriesProvider');
  }
  return context;
};
</file>

<file path="frontend/src/contexts/ProcessSelectionContext.tsx">
import {
  createContext,
  useContext,
  useState,
  useMemo,
  useCallback,
  ReactNode,
} from 'react';
import { useTabNavigation } from './TabNavigationContext';

interface ProcessSelectionContextType {
  selectedProcessId: string | null;
  setSelectedProcessId: (id: string | null) => void;
  jumpToProcess: (processId: string) => void;
}

const ProcessSelectionContext =
  createContext<ProcessSelectionContextType | null>(null);

interface ProcessSelectionProviderProps {
  children: ReactNode;
}

export function ProcessSelectionProvider({
  children,
}: ProcessSelectionProviderProps) {
  const { setActiveTab } = useTabNavigation();
  const [selectedProcessId, setSelectedProcessId] = useState<string | null>(
    null
  );

  const jumpToProcess = useCallback(
    (processId: string) => {
      setSelectedProcessId(processId);
      setActiveTab('processes');
    },
    [setActiveTab]
  );

  const value = useMemo(
    () => ({
      selectedProcessId,
      setSelectedProcessId,
      jumpToProcess,
    }),
    [selectedProcessId, setSelectedProcessId, jumpToProcess]
  );

  return (
    <ProcessSelectionContext.Provider value={value}>
      {children}
    </ProcessSelectionContext.Provider>
  );
}

export const useProcessSelection = () => {
  const context = useContext(ProcessSelectionContext);
  if (!context) {
    throw new Error(
      'useProcessSelection must be used within ProcessSelectionProvider'
    );
  }
  return context;
};
</file>

<file path="frontend/src/contexts/project-context.tsx">
import { createContext, useContext, ReactNode, useMemo } from 'react';
import { useLocation } from 'react-router-dom';
import { useQuery } from '@tanstack/react-query';
import { projectsApi } from '@/lib/api';
import type { Project } from 'shared/types';

interface ProjectContextValue {
  projectId: string | undefined;
  project: Project | undefined;
  isLoading: boolean;
  error: Error | null;
  isError: boolean;
}

const ProjectContext = createContext<ProjectContextValue | null>(null);

interface ProjectProviderProps {
  children: ReactNode;
}

export function ProjectProvider({ children }: ProjectProviderProps) {
  const location = useLocation();

  // Extract projectId from current route path
  const projectId = useMemo(() => {
    const match = location.pathname.match(/^\/projects\/([^/]+)/);
    return match ? match[1] : undefined;
  }, [location.pathname]);

  const query = useQuery({
    queryKey: ['project', projectId],
    queryFn: () => projectsApi.getById(projectId!),
    enabled: !!projectId,
    staleTime: 5 * 60 * 1000, // 5 minutes
  });

  const value = useMemo(
    () => ({
      projectId,
      project: query.data,
      isLoading: query.isLoading,
      error: query.error,
      isError: query.isError,
    }),
    [projectId, query.data, query.isLoading, query.error, query.isError]
  );

  return (
    <ProjectContext.Provider value={value}>{children}</ProjectContext.Provider>
  );
}

export function useProject(): ProjectContextValue {
  const context = useContext(ProjectContext);
  if (!context) {
    throw new Error('useProject must be used within a ProjectProvider');
  }
  return context;
}
</file>

<file path="frontend/src/contexts/ReviewProvider.tsx">
import { SplitSide } from '@git-diff-view/react';
import { createContext, useContext, useState, ReactNode } from 'react';

export interface ReviewComment {
  id: string;
  filePath: string;
  lineNumber: number;
  side: SplitSide;
  text: string;
  codeLine?: string;
}

export interface ReviewDraft {
  filePath: string;
  side: SplitSide;
  lineNumber: number;
  text: string;
  codeLine?: string;
}

interface ReviewContextType {
  comments: ReviewComment[];
  drafts: Record<string, ReviewDraft>;
  addComment: (comment: Omit<ReviewComment, 'id'>) => void;
  updateComment: (id: string, text: string) => void;
  deleteComment: (id: string) => void;
  clearComments: () => void;
  setDraft: (key: string, draft: ReviewDraft | null) => void;
  generateReviewMarkdown: () => string;
}

const ReviewContext = createContext<ReviewContextType | null>(null);

export function useReview() {
  const context = useContext(ReviewContext);
  if (!context) {
    throw new Error('useReview must be used within a ReviewProvider');
  }
  return context;
}

export function ReviewProvider({ children }: { children: ReactNode }) {
  const [comments, setComments] = useState<ReviewComment[]>([]);
  const [drafts, setDrafts] = useState<Record<string, ReviewDraft>>({});

  const addComment = (comment: Omit<ReviewComment, 'id'>) => {
    const newComment: ReviewComment = {
      ...comment,
      id: crypto.randomUUID(),
    };
    setComments((prev) => [...prev, newComment]);
  };

  const updateComment = (id: string, text: string) => {
    setComments((prev) =>
      prev.map((comment) =>
        comment.id === id ? { ...comment, text } : comment
      )
    );
  };

  const deleteComment = (id: string) => {
    setComments((prev) => prev.filter((comment) => comment.id !== id));
  };

  const clearComments = () => {
    setComments([]);
    setDrafts({});
  };

  const setDraft = (key: string, draft: ReviewDraft | null) => {
    setDrafts((prev) => {
      if (draft === null) {
        const newDrafts = { ...prev };
        delete newDrafts[key];
        return newDrafts;
      }
      return { ...prev, [key]: draft };
    });
  };

  const generateReviewMarkdown = () => {
    if (comments.length === 0) return '';

    const commentsNum = comments.length;

    const header = `## Review Comments (${commentsNum})\n\n`;
    const formatCodeLine = (line?: string) => {
      if (!line) return '';
      if (line.includes('`')) {
        return `\`\`\`\n${line}\n\`\`\``;
      }
      return `\`${line}\``;
    };

    const commentsMd = comments
      .map((comment) => {
        const codeLine = formatCodeLine(comment.codeLine);
        // Format file paths in comment body with backticks
        const bodyWithFormattedPaths = comment.text
          .trim()
          .replace(/([/\\]?[\w.-]+(?:[/\\][\w.-]+)+)/g, '`$1`');
        if (codeLine) {
          return `**${comment.filePath}** (Line ${comment.lineNumber})\n${codeLine}\n\n> ${bodyWithFormattedPaths}\n`;
        }
        return `**${comment.filePath}** (Line ${comment.lineNumber})\n\n> ${bodyWithFormattedPaths}\n`;
      })
      .join('\n');

    return header + commentsMd;
  };

  return (
    <ReviewContext.Provider
      value={{
        comments,
        drafts,
        addComment,
        updateComment,
        deleteComment,
        clearComments,
        setDraft,
        generateReviewMarkdown,
      }}
    >
      {children}
    </ReviewContext.Provider>
  );
}
</file>

<file path="frontend/src/contexts/search-context.tsx">
import {
  createContext,
  useContext,
  useState,
  useEffect,
  ReactNode,
} from 'react';
import { useLocation, useParams } from 'react-router-dom';

interface SearchState {
  query: string;
  setQuery: (query: string) => void;
  active: boolean;
  clear: () => void;
}

const SearchContext = createContext<SearchState | null>(null);

interface SearchProviderProps {
  children: ReactNode;
}

export function SearchProvider({ children }: SearchProviderProps) {
  const [query, setQuery] = useState('');
  const location = useLocation();
  const { projectId } = useParams<{ projectId: string }>();

  // Check if we're on a tasks route
  const isTasksRoute = /^\/projects\/[^/]+\/tasks/.test(location.pathname);

  // Clear search when leaving tasks pages
  useEffect(() => {
    if (!isTasksRoute && query !== '') {
      setQuery('');
    }
  }, [isTasksRoute, query]);

  // Clear search when project changes
  useEffect(() => {
    setQuery('');
  }, [projectId]);

  const clear = () => setQuery('');

  const value: SearchState = {
    query,
    setQuery,
    active: isTasksRoute,
    clear,
  };

  return (
    <SearchContext.Provider value={value}>{children}</SearchContext.Provider>
  );
}

export function useSearch(): SearchState {
  const context = useContext(SearchContext);
  if (!context) {
    throw new Error('useSearch must be used within a SearchProvider');
  }
  return context;
}
</file>

<file path="frontend/src/contexts/TabNavigationContext.tsx">
import { createContext, useContext } from 'react';
import type { TabType } from '@/types/tabs';

interface TabNavContextType {
  activeTab: TabType;
  setActiveTab: (tab: TabType) => void;
}

export const TabNavContext = createContext<TabNavContextType | null>(null);

export const useTabNavigation = () => {
  const context = useContext(TabNavContext);
  if (!context) {
    throw new Error('useTabNavigation must be used within TabNavContext');
  }
  return context;
};
</file>

<file path="frontend/src/hooks/follow-up/useDefaultVariant.ts">
import { useEffect, useMemo, useState } from 'react';
import type {
  ExecutorAction,
  ExecutorConfig,
  ExecutionProcess,
  ExecutorProfileId,
} from 'shared/types';
import { useVariantCyclingShortcut } from '@/lib/keyboard-shortcuts';

type Args = {
  processes: ExecutionProcess[];
  profiles?: Record<string, ExecutorConfig> | null;
};

export function useDefaultVariant({ processes, profiles }: Args) {
  const latestProfileId = useMemo<ExecutorProfileId | null>(() => {
    if (!processes?.length) return null;

    // Walk processes from newest to oldest and extract the first executor_profile_id
    // from either the action itself or its next_action (when current is a ScriptRequest).
    const extractProfile = (
      action: ExecutorAction | null
    ): ExecutorProfileId | null => {
      let curr: ExecutorAction | null = action;
      while (curr) {
        const typ = curr.typ;
        switch (typ.type) {
          case 'CodingAgentInitialRequest':
          case 'CodingAgentFollowUpRequest':
            return typ.executor_profile_id;
          case 'ScriptRequest':
            curr = curr.next_action;
            continue;
        }
      }
      return null;
    };
    return (
      processes
        .slice()
        .reverse()
        .map((p) => extractProfile(p.executor_action ?? null))
        .find((pid) => pid !== null) ?? null
    );
  }, [processes]);

  const defaultFollowUpVariant = latestProfileId?.variant ?? null;

  const [selectedVariant, setSelectedVariant] = useState<string | null>(
    defaultFollowUpVariant
  );
  useEffect(
    () => setSelectedVariant(defaultFollowUpVariant),
    [defaultFollowUpVariant]
  );

  const currentProfile = useMemo(() => {
    if (!latestProfileId) return null;
    return profiles?.[latestProfileId.executor] ?? null;
  }, [latestProfileId, profiles]);

  useVariantCyclingShortcut({
    currentProfile,
    selectedVariant,
    setSelectedVariant,
  });

  return { selectedVariant, setSelectedVariant, currentProfile } as const;
}
</file>

<file path="frontend/src/hooks/follow-up/useDraftAutosave.ts">
import { useEffect, useRef, useState } from 'react';
import { attemptsApi, type UpdateFollowUpDraftRequest } from '@/lib/api';
import type { FollowUpDraft } from 'shared/types';

export type SaveStatus = 'idle' | 'saving' | 'saved' | 'offline' | 'sent';

type DraftData = Pick<FollowUpDraft, 'prompt' | 'variant' | 'image_ids'>;

type Args = {
  attemptId?: string;
  serverDraft: FollowUpDraft | null;
  current: DraftData;
  isQueuedUI: boolean;
  isDraftSending: boolean;
  isQueuing: boolean;
  isUnqueuing: boolean;
  suppressNextSaveRef: React.MutableRefObject<boolean>;
  lastServerVersionRef: React.MutableRefObject<number>;
  forceNextApplyRef: React.MutableRefObject<boolean>;
};

export function useDraftAutosave({
  attemptId,
  serverDraft,
  current,
  isQueuedUI,
  isDraftSending,
  isQueuing,
  isUnqueuing,
  suppressNextSaveRef,
  lastServerVersionRef,
  forceNextApplyRef,
}: Args) {
  const [isSaving, setIsSaving] = useState(false);
  const [saveStatus, setSaveStatus] = useState<SaveStatus>('idle');
  // Presentation timers moved to FollowUpStatusRow; keep only raw status.

  // debounced save
  const lastSentRef = useRef<string>('');
  const saveTimeoutRef = useRef<number | undefined>(undefined);
  useEffect(() => {
    if (!attemptId) return;
    if (isDraftSending) return;
    if (isQueuing || isUnqueuing) return;
    if (suppressNextSaveRef.current) {
      suppressNextSaveRef.current = false;
      return;
    }
    if (isQueuedUI) return;

    const saveDraft = async () => {
      const payload: Partial<UpdateFollowUpDraftRequest> = {};
      if (serverDraft && current.prompt !== (serverDraft.prompt || ''))
        payload.prompt = current.prompt || '';
      if ((serverDraft?.variant ?? null) !== (current.variant ?? null))
        payload.variant = (current.variant ?? null) as string | null;
      const currentIds = (current.image_ids as string[] | null) ?? [];
      const serverIds = (serverDraft?.image_ids as string[] | undefined) ?? [];
      const idsEqual =
        currentIds.length === serverIds.length &&
        currentIds.every((id, i) => id === serverIds[i]);
      if (!idsEqual) payload.image_ids = currentIds;
      const keys = Object.keys(payload);
      if (keys.length === 0) return;
      const payloadKey = JSON.stringify(payload);
      if (payloadKey === lastSentRef.current) return;
      lastSentRef.current = payloadKey;
      try {
        setIsSaving(true);
        setSaveStatus(navigator.onLine ? 'saving' : 'offline');
        await attemptsApi.saveFollowUpDraft(
          attemptId,
          payload as UpdateFollowUpDraftRequest
        );
        setSaveStatus('saved');
      } catch {
        try {
          // Fetch latest server draft to ensure stream catches up,
          // and force next apply to override local edits when it arrives.
          await attemptsApi.getFollowUpDraft(attemptId);
          suppressNextSaveRef.current = true;
          forceNextApplyRef.current = true;
        } catch {
          /* ignore */
        }
        setSaveStatus(navigator.onLine ? 'idle' : 'offline');
      } finally {
        setIsSaving(false);
      }
    };
    if (saveTimeoutRef.current) window.clearTimeout(saveTimeoutRef.current);
    saveTimeoutRef.current = window.setTimeout(saveDraft, 400);
    return () => {
      if (saveTimeoutRef.current) window.clearTimeout(saveTimeoutRef.current);
    };
  }, [
    attemptId,
    serverDraft?.prompt,
    serverDraft?.variant,
    serverDraft?.image_ids,
    current.prompt,
    current.variant,
    current.image_ids,
    isQueuedUI,
    isDraftSending,
    isQueuing,
    isUnqueuing,
    suppressNextSaveRef,
    lastServerVersionRef,
  ]);

  return { isSaving, saveStatus } as const;
}
</file>

<file path="frontend/src/hooks/follow-up/useDraftEdits.ts">
import { useEffect, useRef, useState, useCallback } from 'react';
import type { FollowUpDraft, ImageResponse } from 'shared/types';
import { imagesApi } from '@/lib/api';

type Args = {
  draft: FollowUpDraft | null;
  lastServerVersionRef: React.MutableRefObject<number>;
  suppressNextSaveRef: React.MutableRefObject<boolean>;
  forceNextApplyRef: React.MutableRefObject<boolean>;
  taskId: string;
};

export function useDraftEdits({
  draft,
  lastServerVersionRef,
  suppressNextSaveRef,
  forceNextApplyRef,
  taskId,
}: Args) {
  const [message, setMessageInner] = useState('');
  const [images, setImages] = useState<ImageResponse[]>([]);
  const [newlyUploadedImageIds, setNewlyUploadedImageIds] = useState<string[]>(
    []
  );

  const localDirtyRef = useRef<boolean>(false);
  const imagesDirtyRef = useRef<boolean>(false);

  useEffect(() => {
    if (!draft) return;
    const incomingVersion = Number(draft.version ?? 0n);

    if (incomingVersion === lastServerVersionRef.current) return;
    suppressNextSaveRef.current = true;
    const isInitial = lastServerVersionRef.current === -1;
    const shouldForce = forceNextApplyRef.current;
    const allowApply = isInitial || shouldForce || !localDirtyRef.current;
    if (allowApply && incomingVersion >= lastServerVersionRef.current) {
      setMessageInner(draft.prompt || '');
      localDirtyRef.current = false;
      lastServerVersionRef.current = incomingVersion;
      if (shouldForce) forceNextApplyRef.current = false;
    } else if (incomingVersion > lastServerVersionRef.current) {
      // Skip applying server changes while user is editing; still advance version to avoid loops
      lastServerVersionRef.current = incomingVersion;
    }
  }, [draft]);

  // Sync images from server when not locally dirty
  useEffect(() => {
    if (!draft) return;
    const serverIds = (draft.image_ids || []) as string[];
    const wantIds = new Set(serverIds);
    const haveIds = new Set(images.map((img) => img.id));
    const equal =
      haveIds.size === wantIds.size &&
      Array.from(haveIds).every((id) => wantIds.has(id));

    if (equal) {
      imagesDirtyRef.current = false;
      return;
    }

    if (imagesDirtyRef.current) return;

    imagesApi
      .getTaskImages(taskId)
      .then((all) => {
        const next = all.filter((img) => wantIds.has(img.id));
        setImages(next);
        setNewlyUploadedImageIds([]);
      })
      .catch(() => void 0);
  }, [draft?.image_ids, taskId, images]);

  const handleImageUploaded = useCallback((image: ImageResponse) => {
    imagesDirtyRef.current = true;
    setImages((prev) => [...prev, image]);
    setNewlyUploadedImageIds((prev) => [...prev, image.id]);
  }, []);

  const clearImagesAndUploads = useCallback(() => {
    imagesDirtyRef.current = false;
    setImages([]);
    setNewlyUploadedImageIds([]);
  }, []);

  return {
    message,
    setMessage: (v: React.SetStateAction<string>) => {
      localDirtyRef.current = true;
      if (typeof v === 'function') {
        setMessageInner((prev) => (v as (prev: string) => string)(prev));
      } else {
        setMessageInner(v);
      }
    },
    images,
    setImages,
    newlyUploadedImageIds,
    handleImageUploaded,
    clearImagesAndUploads,
  } as const;
}
</file>

<file path="frontend/src/hooks/follow-up/useDraftQueue.ts">
import { useCallback } from 'react';
import { attemptsApi, type UpdateFollowUpDraftRequest } from '@/lib/api';
import type { FollowUpDraft, ImageResponse } from 'shared/types';

type Args = {
  attemptId?: string;
  draft: FollowUpDraft | null;
  message: string;
  selectedVariant: string | null;
  images: ImageResponse[];
  suppressNextSaveRef: React.MutableRefObject<boolean>;
  lastServerVersionRef: React.MutableRefObject<number>;
};

export function useDraftQueue({
  attemptId,
  draft,
  message,
  selectedVariant,
  images,
  suppressNextSaveRef,
  lastServerVersionRef,
}: Args) {
  const onQueue = useCallback(async (): Promise<boolean> => {
    if (!attemptId) return false;
    if (draft?.queued) return true;
    if (message.trim().length === 0) return false;
    try {
      const immediatePayload: Partial<UpdateFollowUpDraftRequest> = {
        prompt: message,
      };
      if ((draft?.variant ?? null) !== (selectedVariant ?? null))
        immediatePayload.variant = (selectedVariant ?? null) as string | null;
      const currentIds = images.map((img) => img.id);
      const serverIds = (draft?.image_ids as string[] | undefined) ?? [];
      const idsEqual =
        currentIds.length === serverIds.length &&
        currentIds.every((id, i) => id === serverIds[i]);
      if (!idsEqual) immediatePayload.image_ids = currentIds;
      suppressNextSaveRef.current = true;
      await attemptsApi.saveFollowUpDraft(
        attemptId,
        immediatePayload as UpdateFollowUpDraftRequest
      );
      try {
        const resp = await attemptsApi.setFollowUpQueue(attemptId, true);
        if (resp?.version !== undefined && resp?.version !== null) {
          lastServerVersionRef.current = Number(resp.version ?? 0n);
        }
        return !!resp?.queued;
      } catch {
        /* adopt server on failure */
        const latest = await attemptsApi.getFollowUpDraft(attemptId);
        suppressNextSaveRef.current = true;
        if (latest.version !== undefined && latest.version !== null) {
          lastServerVersionRef.current = Number(latest.version ?? 0n);
        }
        return !!latest?.queued;
      }
    } finally {
      // presentation-only state handled by caller
    }
    return false;
  }, [
    attemptId,
    draft?.variant,
    draft?.image_ids,
    images,
    message,
    selectedVariant,
    suppressNextSaveRef,
    lastServerVersionRef,
  ]);

  const onUnqueue = useCallback(async (): Promise<boolean> => {
    if (!attemptId) return false;
    try {
      suppressNextSaveRef.current = true;
      try {
        const resp = await attemptsApi.setFollowUpQueue(attemptId, false);
        if (resp?.version !== undefined && resp?.version !== null) {
          lastServerVersionRef.current = Number(resp.version ?? 0n);
        }
        return !!resp && !resp.queued;
      } catch {
        const latest = await attemptsApi.getFollowUpDraft(attemptId);
        suppressNextSaveRef.current = true;
        if (latest.version !== undefined && latest.version !== null) {
          lastServerVersionRef.current = Number(latest.version ?? 0n);
        }
        return !!latest && !latest.queued;
      }
    } finally {
      // presentation-only state handled by caller
    }
    return false;
  }, [attemptId, suppressNextSaveRef, lastServerVersionRef]);

  return { onQueue, onUnqueue } as const;
}
</file>

<file path="frontend/src/hooks/follow-up/useDraftStream.ts">
import { useCallback, useEffect, useRef, useState } from 'react';
import { useJsonPatchWsStream } from '@/hooks/useJsonPatchWsStream';
import { attemptsApi } from '@/lib/api';
import type { FollowUpDraft } from 'shared/types';
import { inIframe } from '@/vscode/bridge';

type DraftStreamState = { follow_up_draft: FollowUpDraft };

export function useDraftStream(attemptId?: string) {
  const [draft, setDraft] = useState<FollowUpDraft | null>(null);
  const [isDraftLoaded, setIsDraftLoaded] = useState(false);
  const lastServerVersionRef = useRef<number>(-1);
  const suppressNextSaveRef = useRef<boolean>(false);
  const forceNextApplyRef = useRef<boolean>(false);

  const endpoint = attemptId
    ? `/api/task-attempts/${attemptId}/follow-up-draft/stream/ws`
    : undefined;

  const makeInitial = useCallback(
    (): DraftStreamState => ({
      follow_up_draft: {
        id: '',
        task_attempt_id: attemptId || '',
        prompt: '',
        queued: false,
        sending: false,
        variant: null,
        image_ids: [],
        version: 0n,
        created_at: new Date().toISOString(),
        updated_at: new Date().toISOString(),
      },
    }),
    [attemptId]
  );

  const { data, isConnected, error } = useJsonPatchWsStream<DraftStreamState>(
    endpoint,
    !!endpoint,
    makeInitial
  );

  // Quick initial draft loading from REST
  useEffect(() => {
    let cancelled = false;
    const hydrate = async () => {
      if (!attemptId) return;
      try {
        const d = await attemptsApi.getFollowUpDraft(attemptId);
        if (cancelled) return;
        suppressNextSaveRef.current = true;
        setDraft({
          id: 'rest',
          task_attempt_id: d.task_attempt_id,
          prompt: d.prompt || '',
          queued: !!d.queued,
          sending: false,
          variant: (d.variant ?? null) as string | null,
          image_ids: (d.image_ids ?? []) as string[],
          version: (d.version ?? 0n) as unknown as bigint,
          created_at: new Date().toISOString(),
          updated_at: new Date().toISOString(),
        });
        if (!isDraftLoaded) setIsDraftLoaded(true);
      } catch {
        // ignore, rely on stream
      }
    };
    hydrate();
    return () => {
      cancelled = true;
    };
  }, [attemptId, isDraftLoaded]);

  // Handle stream updates
  useEffect(() => {
    if (!data) return;
    const d = data.follow_up_draft;
    if (d.id === '') return;
    const incomingVersion = Number(d.version ?? 0n);
    if (incomingVersion === lastServerVersionRef.current) {
      if (!isDraftLoaded) setIsDraftLoaded(true);
      return;
    }
    suppressNextSaveRef.current = true;
    // Let consumers decide whether to apply or ignore based on local dirty/forceApply.
    setDraft(d);
    if (!isDraftLoaded) setIsDraftLoaded(true);
  }, [data, isDraftLoaded]);

  // VSCode iframe poll fallback
  const pollTimerRef = useRef<number | undefined>(undefined);
  useEffect(() => {
    if (!attemptId) return;
    const shouldPoll = inIframe() && (!isConnected || !!error);
    if (!shouldPoll) {
      if (pollTimerRef.current) window.clearInterval(pollTimerRef.current);
      pollTimerRef.current = undefined;
      return;
    }
    const pollOnce = async () => {
      try {
        const d = await attemptsApi.getFollowUpDraft(attemptId);
        const incomingVersion = Number((d as FollowUpDraft).version ?? 0n);
        if (incomingVersion !== lastServerVersionRef.current) {
          suppressNextSaveRef.current = true;
          setDraft({
            id: 'rest',
            task_attempt_id: d.task_attempt_id,
            prompt: d.prompt || '',
            queued: !!d.queued,
            sending: false,
            variant: (d.variant ?? null) as string | null,
            image_ids: (d.image_ids ?? []) as string[],
            version: (d.version ?? 0n) as unknown as bigint,
            created_at: new Date().toISOString(),
            updated_at: new Date().toISOString(),
          });
          if (!isDraftLoaded) setIsDraftLoaded(true);
        }
      } catch {
        // ignore
      }
    };
    pollOnce();
    pollTimerRef.current = window.setInterval(pollOnce, 1000);
    return () => {
      if (pollTimerRef.current) window.clearInterval(pollTimerRef.current);
      pollTimerRef.current = undefined;
    };
  }, [attemptId, isConnected, error, isDraftLoaded]);

  return {
    draft,
    isDraftLoaded,
    isConnected,
    error,
    lastServerVersionRef,
    suppressNextSaveRef,
    forceNextApplyRef,
  } as const;
}
</file>

<file path="frontend/src/hooks/follow-up/useFollowUpSend.ts">
import { useCallback, useState } from 'react';
import { attemptsApi } from '@/lib/api';
import type { ImageResponse } from 'shared/types';

type Args = {
  attemptId?: string;
  message: string;
  reviewMarkdown: string;
  selectedVariant: string | null;
  images: ImageResponse[];
  newlyUploadedImageIds: string[];
  clearComments: () => void;
  jumpToLogsTab: () => void;
  onAfterSendCleanup: () => void;
  setMessage: (v: string) => void;
};

export function useFollowUpSend({
  attemptId,
  message,
  reviewMarkdown,
  selectedVariant,
  images,
  newlyUploadedImageIds,
  clearComments,
  jumpToLogsTab,
  onAfterSendCleanup,
  setMessage,
}: Args) {
  const [isSendingFollowUp, setIsSendingFollowUp] = useState(false);
  const [followUpError, setFollowUpError] = useState<string | null>(null);

  const onSendFollowUp = useCallback(async () => {
    if (!attemptId) return;
    const extraMessage = message.trim();
    const finalPrompt = [reviewMarkdown, extraMessage]
      .filter(Boolean)
      .join('\n\n');
    if (!finalPrompt) return;
    try {
      setIsSendingFollowUp(true);
      setFollowUpError(null);
      const image_ids =
        newlyUploadedImageIds.length > 0
          ? newlyUploadedImageIds
          : images.length > 0
            ? images.map((img) => img.id)
            : null;
      await attemptsApi.followUp(attemptId, {
        prompt: finalPrompt,
        variant: selectedVariant,
        image_ids,
      });
      setMessage('');
      clearComments();
      onAfterSendCleanup();
      jumpToLogsTab();
    } catch (error: unknown) {
      const err = error as { message?: string };
      setFollowUpError(
        `Failed to start follow-up execution: ${err.message ?? 'Unknown error'}`
      );
    } finally {
      setIsSendingFollowUp(false);
    }
  }, [
    attemptId,
    message,
    reviewMarkdown,
    newlyUploadedImageIds,
    images,
    selectedVariant,
    clearComments,
    jumpToLogsTab,
    onAfterSendCleanup,
    setMessage,
  ]);

  return {
    isSendingFollowUp,
    followUpError,
    setFollowUpError,
    onSendFollowUp,
  } as const;
}
</file>

<file path="frontend/src/hooks/index.ts">
export { useBranchStatus } from './useBranchStatus';
export { useAttemptExecution } from './useAttemptExecution';
export { useOpenInEditor } from './useOpenInEditor';
export { useDevServer } from './useDevServer';
export { useRebase } from './useRebase';
export { useCreatePR } from './useCreatePR';
export { useMerge } from './useMerge';
export { usePush } from './usePush';
export { useProjectBranches } from './useProjectBranches';
</file>

<file path="frontend/src/hooks/useAttemptBranch.ts">
import { useQuery } from '@tanstack/react-query';
import { attemptsApi } from '@/lib/api';

export function useAttemptBranch(attemptId?: string) {
  const query = useQuery({
    queryKey: ['attemptBranch', attemptId],
    queryFn: async () => {
      const attempt = await attemptsApi.get(attemptId!);
      return attempt.branch ?? null;
    },
    enabled: !!attemptId,
  });

  return {
    branch: query.data ?? null,
    isLoading: query.isLoading,
    refetch: query.refetch,
  } as const;
}
</file>

<file path="frontend/src/hooks/useAttemptCreation.ts">
import { useMutation, useQueryClient } from '@tanstack/react-query';
import { useParams } from 'react-router-dom';
import { attemptsApi } from '@/lib/api';
import { useTaskViewManager } from '@/hooks/useTaskViewManager';
import type { TaskAttempt } from 'shared/types';
import type { ExecutorProfileId } from 'shared/types';

export function useAttemptCreation(taskId: string) {
  const queryClient = useQueryClient();
  const { projectId } = useParams<{ projectId: string }>();
  const { navigateToAttempt } = useTaskViewManager();

  const mutation = useMutation({
    mutationFn: ({
      profile,
      baseBranch,
    }: {
      profile: ExecutorProfileId;
      baseBranch: string;
    }) =>
      attemptsApi.create({
        task_id: taskId,
        executor_profile_id: profile,
        base_branch: baseBranch,
      }),
    onSuccess: (newAttempt: TaskAttempt) => {
      // Optimistically add to cache to prevent UI flicker
      queryClient.setQueryData(
        ['taskAttempts', taskId],
        (old: TaskAttempt[] = []) => [newAttempt, ...old]
      );

      // Navigate to new attempt (triggers polling switch)
      if (projectId) {
        navigateToAttempt(projectId, taskId, newAttempt.id);
      }
    },
  });

  return {
    createAttempt: mutation.mutateAsync,
    isCreating: mutation.isPending,
    error: mutation.error,
  };
}
</file>

<file path="frontend/src/hooks/useAttemptExecution.ts">
import { useMemo, useCallback } from 'react';
import { useQuery, useQueries, useQueryClient } from '@tanstack/react-query';
import { attemptsApi, executionProcessesApi } from '@/lib/api';
import { useTaskStopping } from '@/stores/useTaskDetailsUiStore';
import type { AttemptData } from '@/lib/types';
import type { ExecutionProcess } from 'shared/types';

export function useAttemptExecution(attemptId?: string, taskId?: string) {
  const queryClient = useQueryClient();
  const { isStopping, setIsStopping } = useTaskStopping(taskId || '');

  // Main execution processes query with polling
  const {
    data: executionData,
    isLoading: processesLoading,
    isFetching: processesFetching,
    refetch,
  } = useQuery({
    queryKey: ['executionProcesses', attemptId],
    queryFn: () => executionProcessesApi.getExecutionProcesses(attemptId!),
    enabled: !!attemptId,
    refetchInterval: 5000,
    select: (data) => ({
      processes: data,
      isAttemptRunning: data.some(
        (process: ExecutionProcess) =>
          (process.run_reason === 'codingagent' ||
            process.run_reason === 'setupscript' ||
            process.run_reason === 'cleanupscript') &&
          process.status === 'running'
      ),
    }),
  });

  // Get setup script processes that need detailed info
  const setupProcesses = useMemo(() => {
    if (!executionData?.processes) return [];
    return executionData.processes.filter(
      (p) => p.run_reason === 'setupscript'
    );
  }, [executionData?.processes]);

  // Fetch details for setup processes
  const processDetailQueries = useQueries({
    queries: setupProcesses.map((process) => ({
      queryKey: ['processDetails', process.id],
      queryFn: () => executionProcessesApi.getDetails(process.id),
      enabled: !!process.id,
    })),
  });

  // Build attempt data combining processes and details
  const attemptData: AttemptData = useMemo(() => {
    if (!executionData?.processes) {
      return { processes: [], runningProcessDetails: {} };
    }

    // Build runningProcessDetails from the detail queries
    const runningProcessDetails: Record<string, ExecutionProcess> = {};

    setupProcesses.forEach((process, index) => {
      const detailQuery = processDetailQueries[index];
      if (detailQuery?.data) {
        runningProcessDetails[process.id] = detailQuery.data;
      }
    });

    return {
      processes: executionData.processes,
      runningProcessDetails,
    };
  }, [executionData?.processes, setupProcesses, processDetailQueries]);

  // Stop execution function
  const stopExecution = useCallback(async () => {
    if (!attemptId || !executionData?.isAttemptRunning || isStopping) return;

    try {
      setIsStopping(true);
      await attemptsApi.stop(attemptId);

      // Invalidate queries to refresh data
      await queryClient.invalidateQueries({
        queryKey: ['executionProcesses', attemptId],
      });
    } catch (error) {
      console.error('Failed to stop executions:', error);
      throw error;
    } finally {
      setIsStopping(false);
    }
  }, [
    attemptId,
    executionData?.isAttemptRunning,
    isStopping,
    setIsStopping,
    queryClient,
  ]);

  const isLoading =
    processesLoading || processDetailQueries.some((q) => q.isLoading);
  const isFetching =
    processesFetching || processDetailQueries.some((q) => q.isFetching);

  return {
    // Data
    processes: executionData?.processes || [],
    attemptData,
    runningProcessDetails: attemptData.runningProcessDetails,

    // Status
    isAttemptRunning: executionData?.isAttemptRunning ?? false,
    isLoading,
    isFetching,

    // Actions
    stopExecution,
    isStopping,
    refetch,
  };
}
</file>

<file path="frontend/src/hooks/useBranchStatus.ts">
import { useQuery } from '@tanstack/react-query';
import { attemptsApi } from '@/lib/api';

export function useBranchStatus(attemptId?: string) {
  return useQuery({
    queryKey: ['branchStatus', attemptId],
    queryFn: () => attemptsApi.getBranchStatus(attemptId!),
    enabled: !!attemptId,
    // Poll faster to promptly reflect rebase/abort transitions
    refetchInterval: 5000,
  });
}
</file>

<file path="frontend/src/hooks/useConversationHistory.ts">
// useConversationHistory.ts
import {
  CommandExitStatus,
  ExecutionProcess,
  ExecutorAction,
  NormalizedEntry,
  PatchType,
  TaskAttempt,
} from 'shared/types';
import { useExecutionProcesses } from './useExecutionProcesses';
import { useEffect, useMemo, useRef } from 'react';
import { streamJsonPatchEntries } from '@/utils/streamJsonPatchEntries';

export type PatchTypeWithKey = PatchType & {
  patchKey: string;
  executionProcessId: string;
};

export type AddEntryType = 'initial' | 'running' | 'historic';

export type OnEntriesUpdated = (
  newEntries: PatchTypeWithKey[],
  addType: AddEntryType,
  loading: boolean
) => void;

type ExecutionProcessStaticInfo = {
  id: string;
  created_at: string;
  updated_at: string;
  executor_action: ExecutorAction;
};

type ExecutionProcessState = {
  executionProcess: ExecutionProcessStaticInfo;
  entries: PatchTypeWithKey[];
};

type ExecutionProcessStateStore = Record<string, ExecutionProcessState>;

interface UseConversationHistoryParams {
  attempt: TaskAttempt;
  onEntriesUpdated: OnEntriesUpdated;
}

interface UseConversationHistoryResult {}

const MIN_INITIAL_ENTRIES = 10;
const REMAINING_BATCH_SIZE = 50;

export const useConversationHistory = ({
  attempt,
  onEntriesUpdated,
}: UseConversationHistoryParams): UseConversationHistoryResult => {
  const { executionProcesses: executionProcessesRaw } = useExecutionProcesses(
    attempt.id
  );
  const executionProcesses = useRef<ExecutionProcess[]>(executionProcessesRaw);
  const displayedExecutionProcesses = useRef<ExecutionProcessStateStore>({});
  const loadedInitialEntries = useRef(false);
  const lastRunningProcessId = useRef<string | null>(null);
  const onEntriesUpdatedRef = useRef<OnEntriesUpdated | null>(null);
  useEffect(() => {
    onEntriesUpdatedRef.current = onEntriesUpdated;
  }, [onEntriesUpdated]);

  // Keep executionProcesses up to date
  useEffect(() => {
    executionProcesses.current = executionProcessesRaw;
  }, [executionProcessesRaw]);

  const loadEntriesForHistoricExecutionProcess = (
    executionProcess: ExecutionProcess
  ) => {
    let url = '';
    if (executionProcess.executor_action.typ.type === 'ScriptRequest') {
      url = `/api/execution-processes/${executionProcess.id}/raw-logs/ws`;
    } else {
      url = `/api/execution-processes/${executionProcess.id}/normalized-logs/ws`;
    }

    return new Promise<PatchType[]>((resolve) => {
      const controller = streamJsonPatchEntries<PatchType>(url, {
        onFinished: (allEntries) => {
          controller.close();
          resolve(allEntries);
        },
        onError: (err) => {
          console.warn!(
            `Error loading entries for historic execution process ${executionProcess.id}`,
            err
          );
          controller.close();
          resolve([]);
        },
      });
    });
  };

  const getLiveExecutionProcess = (
    executionProcessId: string
  ): ExecutionProcess | undefined => {
    return executionProcesses?.current.find(
      (executionProcess) => executionProcess.id === executionProcessId
    );
  };

  // This emits its own events as they are streamed
  const loadRunningAndEmit = (
    executionProcess: ExecutionProcess
  ): Promise<void> => {
    return new Promise((resolve, reject) => {
      let url = '';
      if (executionProcess.executor_action.typ.type === 'ScriptRequest') {
        url = `/api/execution-processes/${executionProcess.id}/raw-logs/ws`;
      } else {
        url = `/api/execution-processes/${executionProcess.id}/normalized-logs/ws`;
      }
      const controller = streamJsonPatchEntries<PatchType>(url, {
        onEntries(entries) {
          const patchesWithKey = entries.map((entry, index) =>
            patchWithKey(entry, executionProcess.id, index)
          );
          const localEntries = displayedExecutionProcesses.current;
          localEntries[executionProcess.id] = {
            executionProcess,
            entries: patchesWithKey,
          };
          displayedExecutionProcesses.current = localEntries;
          emitEntries(localEntries, 'running', false);
        },
        onFinished: () => {
          emitEntries(displayedExecutionProcesses.current, 'running', false);
          controller.close();
          resolve();
        },
        onError: () => {
          controller.close();
          reject();
        },
      });
    });
  };

  // Sometimes it can take a few seconds for the stream to start, wrap the loadRunningAndEmit method
  const loadRunningAndEmitWithBackoff = async (
    executionProcess: ExecutionProcess
  ) => {
    for (let i = 0; i < 20; i++) {
      try {
        await loadRunningAndEmit(executionProcess);
        break;
      } catch (_) {
        await new Promise((resolve) => setTimeout(resolve, 500));
      }
    }
  };

  const getRunningExecutionProcesses = (): ExecutionProcess | null => {
    // Filter for running processes, excluding dev server and other non-agent processes
    const runningProcesses = executionProcesses?.current.filter(
      (p) => p.status === 'running' && p.run_reason !== 'devserver'
    );
    // Only throw error if there are multiple agent processes running
    if (runningProcesses.length > 1) {
      throw new Error('More than one running execution process found');
    }
    return runningProcesses[0] || null;
  };

  const flattenEntries = (
    executionProcessState: ExecutionProcessStateStore
  ): PatchTypeWithKey[] => {
    return Object.values(executionProcessState)
      .filter(
        (p) =>
          p.executionProcess.executor_action.typ.type ===
            'CodingAgentFollowUpRequest' ||
          p.executionProcess.executor_action.typ.type ===
            'CodingAgentInitialRequest'
      )
      .sort(
        (a, b) =>
          new Date(
            a.executionProcess.created_at as unknown as string
          ).getTime() -
          new Date(b.executionProcess.created_at as unknown as string).getTime()
      )
      .flatMap((p) => p.entries);
  };

  const loadingPatch: PatchTypeWithKey = {
    type: 'NORMALIZED_ENTRY',
    content: {
      entry_type: {
        type: 'loading',
      },
      content: '',
      timestamp: null,
    },
    patchKey: 'loading',
    executionProcessId: '',
  };

  const flattenEntriesForEmit = (
    executionProcessState: ExecutionProcessStateStore
  ): PatchTypeWithKey[] => {
    // Create user messages + tool calls for setup/cleanup scripts
    const allEntries = Object.values(executionProcessState)
      .sort(
        (a, b) =>
          new Date(
            a.executionProcess.created_at as unknown as string
          ).getTime() -
          new Date(b.executionProcess.created_at as unknown as string).getTime()
      )
      .flatMap((p) => {
        const entries: PatchTypeWithKey[] = [];
        if (
          p.executionProcess.executor_action.typ.type ===
            'CodingAgentInitialRequest' ||
          p.executionProcess.executor_action.typ.type ===
            'CodingAgentFollowUpRequest'
        ) {
          // New user message
          const userNormalizedEntry: NormalizedEntry = {
            entry_type: {
              type: 'user_message',
            },
            content: p.executionProcess.executor_action.typ.prompt,
            timestamp: null,
          };
          const userPatch: PatchType = {
            type: 'NORMALIZED_ENTRY',
            content: userNormalizedEntry,
          };
          const userPatchTypeWithKey = patchWithKey(
            userPatch,
            p.executionProcess.id,
            'user'
          );
          entries.push(userPatchTypeWithKey);

          // Remove all coding agent added user messages, replace with our custom one
          const entriesExcludingUser = p.entries.filter(
            (e) =>
              e.type !== 'NORMALIZED_ENTRY' ||
              e.content.entry_type.type !== 'user_message'
          );
          entries.push(...entriesExcludingUser);
          if (
            getLiveExecutionProcess(p.executionProcess.id)?.status === 'running'
          ) {
            entries.push(loadingPatch);
          }
        } else if (
          p.executionProcess.executor_action.typ.type === 'ScriptRequest'
        ) {
          // Add setup and cleanup script as a tool call
          let toolName = '';
          switch (p.executionProcess.executor_action.typ.context) {
            case 'SetupScript':
              toolName = 'Setup Script';
              break;
            case 'CleanupScript':
              toolName = 'Cleanup Script';
              break;
            default:
              return [];
          }

          const executionProcess = getLiveExecutionProcess(
            p.executionProcess.id
          );

          const exit_status: CommandExitStatus | null =
            executionProcess?.status === 'running'
              ? null
              : {
                  type: 'exit_code',
                  code: Number(executionProcess?.exit_code) || 0,
                };
          const output = p.entries.map((line) => line.content).join('\n');

          const toolNormalizedEntry: NormalizedEntry = {
            entry_type: {
              type: 'tool_use',
              tool_name: toolName,
              action_type: {
                action: 'command_run',
                command: p.executionProcess.executor_action.typ.script,
                result: {
                  output,
                  exit_status,
                },
              },
            },
            content: toolName,
            timestamp: null,
          };
          const toolPatch: PatchType = {
            type: 'NORMALIZED_ENTRY',
            content: toolNormalizedEntry,
          };
          const toolPatchWithKey: PatchTypeWithKey = patchWithKey(
            toolPatch,
            p.executionProcess.id,
            0
          );

          entries.push(toolPatchWithKey);
        }

        return entries;
      });

    return allEntries;
  };

  const patchWithKey = (
    patch: PatchType,
    executionProcessId: string,
    index: number | 'user'
  ) => {
    return {
      ...patch,
      patchKey: `${executionProcessId}:${index}`,
      executionProcessId,
    };
  };

  const loadInitialEntries = async (): Promise<ExecutionProcessStateStore> => {
    const localDisplayedExecutionProcesses: ExecutionProcessStateStore = {};

    if (!executionProcesses?.current) return localDisplayedExecutionProcesses;

    for (const executionProcess of [...executionProcesses.current].reverse()) {
      if (executionProcess.status === 'running') continue;

      const entries =
        await loadEntriesForHistoricExecutionProcess(executionProcess);
      const entriesWithKey = entries.map((e, idx) =>
        patchWithKey(e, executionProcess.id, idx)
      );

      localDisplayedExecutionProcesses[executionProcess.id] = {
        executionProcess,
        entries: entriesWithKey,
      };

      if (
        flattenEntries(localDisplayedExecutionProcesses).length >
        MIN_INITIAL_ENTRIES
      ) {
        break;
      }
    }

    return localDisplayedExecutionProcesses;
  };

  const loadRemainingEntriesInBatches = async (
    batchSize: number
  ): Promise<ExecutionProcessStateStore | null> => {
    const local = displayedExecutionProcesses.current; // keep ref if intentional
    if (!executionProcesses?.current) return null;

    let anyUpdated = false;
    for (const executionProcess of [...executionProcesses.current].reverse()) {
      if (local[executionProcess.id] || executionProcess.status === 'running')
        continue;

      const entries =
        await loadEntriesForHistoricExecutionProcess(executionProcess);
      const entriesWithKey = entries.map((e, idx) =>
        patchWithKey(e, executionProcess.id, idx)
      );

      local[executionProcess.id] = {
        executionProcess,
        entries: entriesWithKey,
      };
      if (flattenEntries(local).length > batchSize) {
        anyUpdated = true;
        break;
      }
      anyUpdated = true;
    }
    return anyUpdated ? local : null;
  };

  const emitEntries = (
    executionProcessState: ExecutionProcessStateStore,
    addEntryType: AddEntryType,
    loading: boolean
  ) => {
    // Flatten entries in chronological order of process start
    const entries = flattenEntriesForEmit(executionProcessState);
    onEntriesUpdatedRef.current?.(entries, addEntryType, loading);
  };

  // Stable key for dependency arrays when process list changes
  const idListKey = useMemo(
    () => executionProcessesRaw?.map((p) => p.id).join(','),
    [executionProcessesRaw]
  );

  // Initial load when attempt changes
  useEffect(() => {
    let cancelled = false;
    (async () => {
      // Waiting for execution processes to load
      if (
        executionProcesses?.current.length === 0 ||
        loadedInitialEntries.current
      )
        return;

      // Initial entries
      const allInitialEntries = await loadInitialEntries();
      if (cancelled) return;
      displayedExecutionProcesses.current = allInitialEntries;
      emitEntries(allInitialEntries, 'initial', false);
      loadedInitialEntries.current = true;

      // Then load the remaining in batches
      let updatedEntries;
      while (
        !cancelled &&
        (updatedEntries =
          await loadRemainingEntriesInBatches(REMAINING_BATCH_SIZE))
      ) {
        if (cancelled) return;
        displayedExecutionProcesses.current = updatedEntries;
      }
      await new Promise((resolve) => setTimeout(resolve, 100));
      emitEntries(displayedExecutionProcesses.current, 'historic', false);
    })();
    return () => {
      cancelled = true;
    };
  }, [attempt.id, idListKey]); // include idListKey so new processes trigger reload

  // Running processes
  useEffect(() => {
    const runningProcess = getRunningExecutionProcesses();
    if (runningProcess && lastRunningProcessId.current !== runningProcess.id) {
      lastRunningProcessId.current = runningProcess.id;
      loadRunningAndEmitWithBackoff(runningProcess);
    }
  }, [attempt.id, idListKey]);

  // If an execution process is removed, remove it from the state
  useEffect(() => {
    if (!executionProcessesRaw) return;

    const removedProcessIds = Object.keys(
      displayedExecutionProcesses.current
    ).filter((id) => !executionProcessesRaw.some((p) => p.id === id));

    removedProcessIds.forEach((id) => {
      delete displayedExecutionProcesses.current[id];
    });
  }, [attempt.id, idListKey]);

  // Reset state when attempt changes
  useEffect(() => {
    displayedExecutionProcesses.current = {};
    loadedInitialEntries.current = false;
    lastRunningProcessId.current = null;
    // Emit blank entries
    emitEntries(displayedExecutionProcesses.current, 'initial', true);
  }, [attempt.id]);

  return {};
};
</file>

<file path="frontend/src/hooks/useCreatePR.ts">
import { useMutation, useQueryClient } from '@tanstack/react-query';
import { attemptsApi, type Result } from '@/lib/api';
import type { CreateGitHubPrRequest, GitHubServiceError } from 'shared/types';

export function useCreatePR(
  attemptId: string | undefined,
  onSuccess?: (prUrl?: string) => void,
  onError?: (err: unknown) => void
) {
  const queryClient = useQueryClient();

  return useMutation<
    Result<string, GitHubServiceError>,
    Error,
    CreateGitHubPrRequest
  >({
    mutationFn: async (prData: CreateGitHubPrRequest) => {
      if (!attemptId)
        return { success: false, error: undefined, message: 'No attempt ID' };
      return attemptsApi.createPR(attemptId, prData);
    },
    onSuccess: (result) => {
      if (result.success) {
        queryClient.invalidateQueries({
          queryKey: ['branchStatus', attemptId],
        });
        onSuccess?.(result.data);
      } else {
        throw (
          result.error || new Error(result.message || 'Failed to create PR')
        );
      }
    },
    onError: (err) => {
      console.error('Failed to create PR:', err);
      onError?.(err);
    },
  });
}
</file>

<file path="frontend/src/hooks/useDevServer.ts">
import { useMemo } from 'react';
import { useMutation, useQueryClient } from '@tanstack/react-query';
import { attemptsApi, executionProcessesApi } from '@/lib/api';
import { useAttemptExecution } from '@/hooks/useAttemptExecution';
import type { ExecutionProcess } from 'shared/types';

interface UseDevServerOptions {
  onStartSuccess?: () => void;
  onStartError?: (err: unknown) => void;
  onStopSuccess?: () => void;
  onStopError?: (err: unknown) => void;
}

export function useDevServer(
  attemptId: string | undefined,
  options?: UseDevServerOptions
) {
  const queryClient = useQueryClient();
  const { attemptData } = useAttemptExecution(attemptId);

  // Find running dev server process
  const runningDevServer = useMemo<ExecutionProcess | undefined>(() => {
    return attemptData.processes.find(
      (process) =>
        process.run_reason === 'devserver' && process.status === 'running'
    );
  }, [attemptData.processes]);

  // Find latest dev server process (for logs viewing)
  const latestDevServerProcess = useMemo<ExecutionProcess | undefined>(() => {
    return [...attemptData.processes]
      .filter((process) => process.run_reason === 'devserver')
      .sort(
        (a, b) =>
          new Date(b.started_at).getTime() - new Date(a.started_at).getTime()
      )[0];
  }, [attemptData.processes]);

  // Start mutation
  const startMutation = useMutation({
    mutationKey: ['startDevServer', attemptId],
    mutationFn: async () => {
      if (!attemptId) return;
      await attemptsApi.startDevServer(attemptId);
    },
    onSuccess: async () => {
      await queryClient.invalidateQueries({
        queryKey: ['executionProcesses', attemptId],
      });
      options?.onStartSuccess?.();
    },
    onError: (err) => {
      console.error('Failed to start dev server:', err);
      options?.onStartError?.(err);
    },
  });

  // Stop mutation
  const stopMutation = useMutation({
    mutationKey: ['stopDevServer', runningDevServer?.id],
    mutationFn: async () => {
      if (!runningDevServer) return;
      await executionProcessesApi.stopExecutionProcess(runningDevServer.id);
    },
    onSuccess: async () => {
      await Promise.all([
        queryClient.invalidateQueries({
          queryKey: ['executionProcesses', attemptId],
        }),
        runningDevServer
          ? queryClient.invalidateQueries({
              queryKey: ['processDetails', runningDevServer.id],
            })
          : Promise.resolve(),
      ]);
      options?.onStopSuccess?.();
    },
    onError: (err) => {
      console.error('Failed to stop dev server:', err);
      options?.onStopError?.(err);
    },
  });

  return {
    start: startMutation.mutate,
    stop: stopMutation.mutate,
    isStarting: startMutation.isPending,
    isStopping: stopMutation.isPending,
    runningDevServer,
    latestDevServerProcess,
  };
}
</file>

<file path="frontend/src/hooks/useDiffEntries.ts">
import { useMemo } from 'react';
import { useDiffStream } from './useDiffStream';
import type { Diff, PatchType } from 'shared/types';

interface UseDiffEntriesResult {
  diffs: Diff[];
  isConnected: boolean;
  error: string | null;
}

export const useDiffEntries = (
  attemptId: string | null,
  enabled: boolean
): UseDiffEntriesResult => {
  const { data, isConnected, error } = useDiffStream(attemptId, enabled);

  const diffs = useMemo(() => {
    if (!data) return [];
    return Object.values(data.entries)
      .filter(
        (e): e is Extract<PatchType, { type: 'DIFF' }> => e?.type === 'DIFF'
      )
      .map((e) => e.content);
  }, [data]);

  return { diffs, isConnected, error };
};
</file>

<file path="frontend/src/hooks/useDiffStream.ts">
import { useCallback } from 'react';
import type { PatchType } from 'shared/types';
import { useJsonPatchStream } from './useJsonPatchStream';

interface DiffState {
  entries: Record<string, PatchType>;
}

interface UseDiffStreamResult {
  data: DiffState | undefined;
  isConnected: boolean;
  error: string | null;
}

export const useDiffStream = (
  attemptId: string | null,
  enabled: boolean
): UseDiffStreamResult => {
  const endpoint = attemptId
    ? `/api/task-attempts/${attemptId}/diff`
    : undefined;

  const initialData = useCallback(
    (): DiffState => ({
      entries: {},
    }),
    []
  );

  const { data, isConnected, error } = useJsonPatchStream(
    endpoint,
    enabled && !!attemptId,
    initialData
    // No need for injectInitialEntry or deduplicatePatches for diffs
  );

  return { data, isConnected, error };
};
</file>

<file path="frontend/src/hooks/useDiffSummary.ts">
import { useDiffEntries } from '@/hooks/useDiffEntries';
import { getHighLightLanguageFromPath } from '@/utils/extToLanguage';
import { generateDiffFile } from '@git-diff-view/file';
import { useMemo } from 'react';

export function useDiffSummary(attemptId: string | null) {
  const { diffs, error, isConnected } = useDiffEntries(attemptId, true);

  const { fileCount, added, deleted } = useMemo(() => {
    if (!attemptId || diffs.length === 0) {
      return { fileCount: 0, added: 0, deleted: 0 };
    }

    return diffs.reduce(
      (acc, d) => {
        try {
          if (d.contentOmitted) {
            acc.added += d.additions ?? 0;
            acc.deleted += d.deletions ?? 0;
            return acc;
          }
          const oldName = d.oldPath || d.newPath || 'old';
          const newName = d.newPath || d.oldPath || 'new';
          const oldContent = d.oldContent || '';
          const newContent = d.newContent || '';
          const oldLang = getHighLightLanguageFromPath(oldName) || 'plaintext';
          const newLang = getHighLightLanguageFromPath(newName) || 'plaintext';

          const file = generateDiffFile(
            oldName,
            oldContent,
            newName,
            newContent,
            oldLang,
            newLang
          );
          file.initRaw();
          acc.added += file.additionLength ?? 0;
          acc.deleted += file.deletionLength ?? 0;
        } catch (e) {
          console.error('Failed to compute totals for diff', e);
        }
        return acc;
      },
      { fileCount: diffs.length, added: 0, deleted: 0 }
    );
  }, [attemptId, diffs]);

  return { fileCount, added, deleted, isConnected, error };
}
</file>

<file path="frontend/src/hooks/useEventSourceManager.ts">
import { useEffect, useState, useRef } from 'react';
import { applyPatch } from 'rfc6902';
import type { ExecutionProcess } from 'shared/types';
import type { ProcessStartPayload } from '@/types/logs';

interface ProcessData {
  [processId: string]: any;
}

interface UseEventSourceManagerParams {
  processes: ExecutionProcess[];
  enabled: boolean;
  getEndpoint: (process: ExecutionProcess) => string;
  initialData?: any;
}

interface UseEventSourceManagerResult {
  processData: ProcessData;
  isConnected: boolean;
  error: string | null;
}

export const useEventSourceManager = ({
  processes,
  enabled,
  getEndpoint,
  initialData = null,
}: UseEventSourceManagerParams): UseEventSourceManagerResult => {
  const [processData, setProcessData] = useState<ProcessData>({});
  const [isConnected, setIsConnected] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const eventSourcesRef = useRef<Map<string, EventSource>>(new Map());
  const processDataRef = useRef<ProcessData>({});
  const processedEntriesRef = useRef<Map<string, Set<number>>>(new Map());
  const processesRef = useRef<ExecutionProcess[]>([]);
  const enabledRef = useRef<boolean>(enabled);
  const getEndpointRef = useRef(getEndpoint);
  const retryCountsRef = useRef<Map<string, number>>(new Map());
  const retryTimersRef = useRef<Map<string, ReturnType<typeof setTimeout>>>(
    new Map()
  );

  // Keep latest values in refs for retry handlers
  useEffect(() => {
    processesRef.current = processes;
  }, [processes]);
  useEffect(() => {
    enabledRef.current = enabled;
  }, [enabled]);
  useEffect(() => {
    getEndpointRef.current = getEndpoint;
  }, [getEndpoint]);

  useEffect(() => {
    if (!enabled || !processes.length) {
      // Close all connections and reset state
      eventSourcesRef.current.forEach((es) => es.close());
      eventSourcesRef.current.clear();
      setProcessData({});
      setIsConnected(false);
      setError(null);
      processDataRef.current = {};
      processedEntriesRef.current.clear();
      return;
    }

    const currentIds = new Set(processes.map((p) => p.id));

    // Remove old connections
    eventSourcesRef.current.forEach((es, id) => {
      if (!currentIds.has(id)) {
        es.close();
        eventSourcesRef.current.delete(id);
        delete processDataRef.current[id];
        processedEntriesRef.current.delete(id);
      }
    });

    // Helper to open an EventSource with auto-retry on transient failures (e.g., race before store is ready)
    const openEventSource = (process: ExecutionProcess) => {
      // If disabled or process no longer present, don't connect
      if (!enabledRef.current) return;
      if (!processesRef.current.find((p) => p.id === process.id)) return;

      const endpoint = getEndpointRef.current(process);

      // Reinitialize process data on each (re)connect to avoid duplicating history
      processDataRef.current[process.id] = initialData
        ? structuredClone(initialData)
        : { entries: [] };
      processedEntriesRef.current.delete(process.id);

      // Inject process start marker as the first entry (client-side only)
      const processStartPayload: ProcessStartPayload = {
        processId: process.id,
        runReason: process.run_reason,
        startedAt: process.started_at,
        status: process.status,
      };
      const processStartEntry = {
        type: 'PROCESS_START' as const,
        content: processStartPayload,
      };
      processDataRef.current[process.id].entries.push(processStartEntry);

      const eventSource = new EventSource(endpoint);

      eventSource.onopen = () => {
        setError(null);
        setIsConnected(true);
        retryCountsRef.current.set(process.id, 0);
      };

      eventSource.addEventListener('json_patch', (event) => {
        try {
          const patches = JSON.parse(event.data);

          if (!processedEntriesRef.current.has(process.id)) {
            processedEntriesRef.current.set(process.id, new Set());
          }
          applyPatch(processDataRef.current[process.id], patches);
          setProcessData({ ...processDataRef.current });
        } catch (err) {
          console.error('Failed to apply JSON patch:', err);
          setError('Failed to process log update');
        }
      });

      eventSource.addEventListener('finished', () => {
        eventSource.close();
        eventSourcesRef.current.delete(process.id);
        retryCountsRef.current.delete(process.id);
        const t = retryTimersRef.current.get(process.id);
        if (t) {
          clearTimeout(t);
          retryTimersRef.current.delete(process.id);
        }
        setIsConnected(eventSourcesRef.current.size > 0);
      });

      eventSource.onerror = () => {
        setError('Connection failed');
        eventSource.close();
        eventSourcesRef.current.delete(process.id);

        const nextAttempt = (retryCountsRef.current.get(process.id) || 0) + 1;
        retryCountsRef.current.set(process.id, nextAttempt);

        const maxAttempts = 6;
        if (
          nextAttempt <= maxAttempts &&
          enabledRef.current &&
          processesRef.current.find((p) => p.id === process.id)
        ) {
          const delay = Math.min(1500, 250 * 2 ** (nextAttempt - 1));
          const timer = setTimeout(() => openEventSource(process), delay);
          const prevTimer = retryTimersRef.current.get(process.id);
          if (prevTimer) clearTimeout(prevTimer);
          retryTimersRef.current.set(process.id, timer);
        } else {
          setIsConnected(eventSourcesRef.current.size > 0);
        }
      };

      eventSourcesRef.current.set(process.id, eventSource);
    };

    // Add new connections
    processes.forEach((process) => {
      if (eventSourcesRef.current.has(process.id)) return;
      openEventSource(process);
    });

    setIsConnected(eventSourcesRef.current.size > 0);

    return () => {
      // Cleanup all event sources and any pending retry timers
      eventSourcesRef.current.forEach((es) => es.close());
      eventSourcesRef.current.clear();
      retryTimersRef.current.forEach((t) => clearTimeout(t));
      retryTimersRef.current.clear();
    };
  }, [processes, enabled, getEndpoint, initialData]);

  return { processData, isConnected, error };
};
</file>

<file path="frontend/src/hooks/useExecutionProcesses.ts">
import { useCallback } from 'react';
import { useJsonPatchWsStream } from './useJsonPatchWsStream';
import type { ExecutionProcess } from 'shared/types';

type ExecutionProcessState = {
  execution_processes: Record<string, ExecutionProcess>;
};

interface UseExecutionProcessesResult {
  executionProcesses: ExecutionProcess[];
  executionProcessesById: Record<string, ExecutionProcess>;
  isLoading: boolean;
  isConnected: boolean;
  error: string | null;
}

/**
 * Stream execution processes for a task attempt via WebSocket (JSON Patch) and expose as array + map.
 * Server sends initial snapshot: replace /execution_processes with an object keyed by id.
 * Live updates arrive at /execution_processes/<id> via add/replace/remove operations.
 */
export const useExecutionProcesses = (
  taskAttemptId: string,
  opts?: { showSoftDeleted?: boolean }
): UseExecutionProcessesResult => {
  const showSoftDeleted = opts?.showSoftDeleted;
  const params = new URLSearchParams({ task_attempt_id: taskAttemptId });
  if (typeof showSoftDeleted === 'boolean') {
    params.set('show_soft_deleted', String(showSoftDeleted));
  }
  const endpoint = `/api/execution-processes/stream/ws?${params.toString()}`;

  const initialData = useCallback(
    (): ExecutionProcessState => ({ execution_processes: {} }),
    []
  );

  const { data, isConnected, error } =
    useJsonPatchWsStream<ExecutionProcessState>(
      endpoint,
      !!taskAttemptId,
      initialData
    );

  const executionProcessesById = data?.execution_processes ?? {};
  const executionProcesses = Object.values(executionProcessesById).sort(
    (a, b) =>
      new Date(a.created_at as unknown as string).getTime() -
      new Date(b.created_at as unknown as string).getTime()
  );
  const isLoading = !data && !error; // until first snapshot

  return {
    executionProcesses,
    executionProcessesById,
    isLoading,
    isConnected,
    error,
  };
};
</file>

<file path="frontend/src/hooks/useJsonPatchStream.ts">
import { useEffect, useState, useRef } from 'react';
import { applyPatch } from 'rfc6902';
import type { Operation } from 'rfc6902';

interface UseJsonPatchStreamOptions<T> {
  /**
   * Called once when the stream starts to inject initial data
   */
  injectInitialEntry?: (data: T) => void;
  /**
   * Filter/deduplicate patches before applying them
   */
  deduplicatePatches?: (patches: Operation[]) => Operation[];
}

interface UseJsonPatchStreamResult<T> {
  data: T | undefined;
  isConnected: boolean;
  error: string | null;
}

/**
 * Generic hook for consuming SSE streams that send JSON patches
 */
export const useJsonPatchStream = <T>(
  endpoint: string | undefined,
  enabled: boolean,
  initialData: () => T,
  options: UseJsonPatchStreamOptions<T> = {}
): UseJsonPatchStreamResult<T> => {
  const [data, setData] = useState<T | undefined>(undefined);
  const [isConnected, setIsConnected] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const eventSourceRef = useRef<EventSource | null>(null);
  const dataRef = useRef<T | undefined>(undefined);
  const retryTimerRef = useRef<number | null>(null);
  const retryAttemptsRef = useRef<number>(0);
  const [retryNonce, setRetryNonce] = useState(0);

  function scheduleReconnect() {
    if (retryTimerRef.current) return; // already scheduled
    // Exponential backoff with cap: 1s, 2s, 4s, 8s (max), then stay at 8s
    const attempt = retryAttemptsRef.current;
    const delay = Math.min(8000, 1000 * Math.pow(2, attempt));
    retryTimerRef.current = window.setTimeout(() => {
      retryTimerRef.current = null;
      setRetryNonce((n) => n + 1);
    }, delay);
  }

  useEffect(() => {
    if (!enabled || !endpoint) {
      // Close connection and reset state
      if (eventSourceRef.current) {
        eventSourceRef.current.close();
        eventSourceRef.current = null;
      }
      if (retryTimerRef.current) {
        window.clearTimeout(retryTimerRef.current);
        retryTimerRef.current = null;
      }
      retryAttemptsRef.current = 0;
      setData(undefined);
      setIsConnected(false);
      setError(null);
      dataRef.current = undefined;
      return;
    }

    // Initialize data
    if (!dataRef.current) {
      dataRef.current = initialData();

      // Inject initial entry if provided
      if (options.injectInitialEntry) {
        options.injectInitialEntry(dataRef.current);
      }

      setData({ ...dataRef.current });
    }

    // Create EventSource if it doesn't exist
    if (!eventSourceRef.current) {
      const eventSource = new EventSource(endpoint);

      eventSource.onopen = () => {
        setError(null);
        setIsConnected(true);
        // Reset backoff on successful connection
        retryAttemptsRef.current = 0;
        if (retryTimerRef.current) {
          window.clearTimeout(retryTimerRef.current);
          retryTimerRef.current = null;
        }
      };

      eventSource.addEventListener('json_patch', (event) => {
        try {
          const patches: Operation[] = JSON.parse(event.data);
          const filtered = options.deduplicatePatches
            ? options.deduplicatePatches(patches)
            : patches;

          if (!filtered.length || !dataRef.current) return;

          // Deep clone the current state before mutating it
          dataRef.current = structuredClone(dataRef.current);

          // Apply patch (mutates the clone in place)
          applyPatch(dataRef.current as any, filtered);

          // React re-render: dataRef.current is already a new object
          setData(dataRef.current);
        } catch (err) {
          console.error('Failed to apply JSON patch:', err);
          setError('Failed to process stream update');
        }
      });

      eventSource.addEventListener('finished', () => {
        eventSource.close();
        eventSourceRef.current = null;
        setIsConnected(false);
        // Treat finished as terminal and schedule reconnect; servers may rotate
        retryAttemptsRef.current += 1;
        scheduleReconnect();
      });

      eventSource.onerror = () => {
        setError('Connection failed');
        // Close and schedule reconnect
        try {
          eventSource.close();
        } catch {
          /* empty */
        }
        eventSourceRef.current = null;
        setIsConnected(false);
        retryAttemptsRef.current += 1;
        scheduleReconnect();
      };

      eventSourceRef.current = eventSource;
    }

    return () => {
      if (eventSourceRef.current) {
        eventSourceRef.current.close();
        eventSourceRef.current = null;
      }
      if (retryTimerRef.current) {
        window.clearTimeout(retryTimerRef.current);
        retryTimerRef.current = null;
      }
      dataRef.current = undefined;
      setData(undefined);
    };
  }, [
    endpoint,
    enabled,
    initialData,
    options.injectInitialEntry,
    options.deduplicatePatches,
    retryNonce,
  ]);

  return { data, isConnected, error };
};
</file>

<file path="frontend/src/hooks/useJsonPatchWsStream.ts">
import { useEffect, useState, useRef } from 'react';
import { applyPatch } from 'rfc6902';
import type { Operation } from 'rfc6902';

type WsJsonPatchMsg = { JsonPatch: Operation[] };
type WsFinishedMsg = { finished: boolean };
type WsMsg = WsJsonPatchMsg | WsFinishedMsg;

interface UseJsonPatchStreamOptions<T> {
  /**
   * Called once when the stream starts to inject initial data
   */
  injectInitialEntry?: (data: T) => void;
  /**
   * Filter/deduplicate patches before applying them
   */
  deduplicatePatches?: (patches: Operation[]) => Operation[];
}

interface UseJsonPatchStreamResult<T> {
  data: T | undefined;
  isConnected: boolean;
  error: string | null;
}

/**
 * Generic hook for consuming WebSocket streams that send JSON messages with patches
 */
export const useJsonPatchWsStream = <T>(
  endpoint: string | undefined,
  enabled: boolean,
  initialData: () => T,
  options: UseJsonPatchStreamOptions<T> = {}
): UseJsonPatchStreamResult<T> => {
  const [data, setData] = useState<T | undefined>(undefined);
  const [isConnected, setIsConnected] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const wsRef = useRef<WebSocket | null>(null);
  const dataRef = useRef<T | undefined>(undefined);
  const retryTimerRef = useRef<number | null>(null);
  const retryAttemptsRef = useRef<number>(0);
  const [retryNonce, setRetryNonce] = useState(0);

  function scheduleReconnect() {
    if (retryTimerRef.current) return; // already scheduled
    // Exponential backoff with cap: 1s, 2s, 4s, 8s (max), then stay at 8s
    const attempt = retryAttemptsRef.current;
    const delay = Math.min(8000, 1000 * Math.pow(2, attempt));
    retryTimerRef.current = window.setTimeout(() => {
      retryTimerRef.current = null;
      setRetryNonce((n) => n + 1);
    }, delay);
  }

  useEffect(() => {
    if (!enabled || !endpoint) {
      // Close connection and reset state
      if (wsRef.current) {
        wsRef.current.close();
        wsRef.current = null;
      }
      if (retryTimerRef.current) {
        window.clearTimeout(retryTimerRef.current);
        retryTimerRef.current = null;
      }
      retryAttemptsRef.current = 0;
      setData(undefined);
      setIsConnected(false);
      setError(null);
      dataRef.current = undefined;
      return;
    }

    // Initialize data
    if (!dataRef.current) {
      dataRef.current = initialData();

      // Inject initial entry if provided
      if (options.injectInitialEntry) {
        options.injectInitialEntry(dataRef.current);
      }

      setData({ ...dataRef.current });
    }

    // Create WebSocket if it doesn't exist
    if (!wsRef.current) {
      // Convert HTTP endpoint to WebSocket endpoint
      const wsEndpoint = endpoint.replace(/^http/, 'ws');
      const ws = new WebSocket(wsEndpoint);

      ws.onopen = () => {
        setError(null);
        setIsConnected(true);
        // Reset backoff on successful connection
        retryAttemptsRef.current = 0;
        if (retryTimerRef.current) {
          window.clearTimeout(retryTimerRef.current);
          retryTimerRef.current = null;
        }
      };

      ws.onmessage = (event) => {
        try {
          const msg: WsMsg = JSON.parse(event.data);

          // Handle JsonPatch messages (same as SSE json_patch event)
          if ('JsonPatch' in msg) {
            const patches: Operation[] = msg.JsonPatch;
            const filtered = options.deduplicatePatches
              ? options.deduplicatePatches(patches)
              : patches;

            if (!filtered.length || !dataRef.current) return;

            // Deep clone the current state before mutating it
            dataRef.current = structuredClone(dataRef.current);

            // Apply patch (mutates the clone in place)
            applyPatch(dataRef.current as any, filtered);

            // React re-render: dataRef.current is already a new object
            setData(dataRef.current);
          }

          // Handle finished messages ({finished: true})
          if ('finished' in msg) {
            ws.close();
            wsRef.current = null;
            setIsConnected(false);
            // Treat finished as terminal and schedule reconnect; servers may rotate
            retryAttemptsRef.current += 1;
            scheduleReconnect();
          }
        } catch (err) {
          console.error('Failed to process WebSocket message:', err);
          setError('Failed to process stream update');
        }
      };

      ws.onerror = () => {
        setError('Connection failed');
      };

      ws.onclose = () => {
        setIsConnected(false);
        wsRef.current = null;
        retryAttemptsRef.current += 1;
        scheduleReconnect();
      };

      wsRef.current = ws;
    }

    return () => {
      if (wsRef.current) {
        wsRef.current.close();
        wsRef.current = null;
      }
      if (retryTimerRef.current) {
        window.clearTimeout(retryTimerRef.current);
        retryTimerRef.current = null;
      }
      dataRef.current = undefined;
      setData(undefined);
    };
  }, [
    endpoint,
    enabled,
    initialData,
    options.injectInitialEntry,
    options.deduplicatePatches,
    retryNonce,
  ]);

  return { data, isConnected, error };
};
</file>

<file path="frontend/src/hooks/useLogStream.ts">
import { useEffect, useState, useRef } from 'react';
import type { PatchType } from 'shared/types';

type LogEntry = Extract<PatchType, { type: 'STDOUT' } | { type: 'STDERR' }>;

interface UseLogStreamResult {
  logs: LogEntry[];
  error: string | null;
}

export const useLogStream = (processId: string): UseLogStreamResult => {
  const [logs, setLogs] = useState<LogEntry[]>([]);
  const [error, setError] = useState<string | null>(null);
  const wsRef = useRef<WebSocket | null>(null);
  const retryCountRef = useRef<number>(0);
  const retryTimerRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const isIntentionallyClosed = useRef<boolean>(false);

  useEffect(() => {
    if (!processId) {
      return;
    }

    // Clear logs when process changes
    setLogs([]);
    setError(null);

    const open = () => {
      const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
      const host = window.location.host;
      const ws = new WebSocket(
        `${protocol}//${host}/api/execution-processes/${processId}/raw-logs/ws`
      );
      wsRef.current = ws;
      isIntentionallyClosed.current = false;

      ws.onopen = () => {
        setError(null);
        // Reset logs on new connection since server replays history
        setLogs([]);
        retryCountRef.current = 0;
      };

      const addLogEntry = (entry: LogEntry) => {
        setLogs((prev) => [...prev, entry]);
      };

      // Handle WebSocket messages
      ws.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);

          // Handle different message types based on LogMsg enum
          if ('JsonPatch' in data) {
            const patches = data.JsonPatch;
            patches.forEach((patch: any) => {
              const value = patch?.value;
              if (!value || !value.type) return;

              switch (value.type) {
                case 'STDOUT':
                case 'STDERR':
                  addLogEntry({ type: value.type, content: value.content });
                  break;
                // Ignore other patch types (NORMALIZED_ENTRY, DIFF, etc.)
                default:
                  break;
              }
            });
          } else if (data.finished === true) {
            isIntentionallyClosed.current = true;
            ws.close();
          }
        } catch (e) {
          console.error('Failed to parse message:', e);
        }
      };

      ws.onerror = () => {
        setError('Connection failed');
      };

      ws.onclose = (event) => {
        // Only retry if the close was not intentional and not a normal closure
        if (!isIntentionallyClosed.current && event.code !== 1000) {
          const next = retryCountRef.current + 1;
          retryCountRef.current = next;
          if (next <= 6) {
            const delay = Math.min(1500, 250 * 2 ** (next - 1));
            retryTimerRef.current = setTimeout(() => open(), delay);
          }
        }
      };
    };

    open();

    return () => {
      if (wsRef.current) {
        isIntentionallyClosed.current = true;
        wsRef.current.close();
        wsRef.current = null;
      }
      if (retryTimerRef.current) {
        clearTimeout(retryTimerRef.current);
        retryTimerRef.current = null;
      }
    };
  }, [processId]);

  return { logs, error };
};
</file>

<file path="frontend/src/hooks/useMerge.ts">
import { useMutation, useQueryClient } from '@tanstack/react-query';
import { attemptsApi } from '@/lib/api';

export function useMerge(
  attemptId?: string,
  onSuccess?: () => void,
  onError?: (err: unknown) => void
) {
  const queryClient = useQueryClient();

  return useMutation({
    mutationFn: () => {
      if (!attemptId) return Promise.resolve();
      return attemptsApi.merge(attemptId);
    },
    onSuccess: () => {
      // Refresh attempt-specific branch information
      queryClient.invalidateQueries({ queryKey: ['branchStatus', attemptId] });

      // If a merge can change the list of branches shown elsewhere
      queryClient.invalidateQueries({ queryKey: ['projectBranches'] });

      onSuccess?.();
    },
    onError: (err) => {
      console.error('Failed to merge:', err);
      onError?.(err);
    },
  });
}
</file>

<file path="frontend/src/hooks/useOpenInEditor.ts">
import { useCallback } from 'react';
import { attemptsApi } from '@/lib/api';
import NiceModal from '@ebay/nice-modal-react';
import type { EditorType, TaskAttempt } from 'shared/types';

export function useOpenInEditor(
  attempt: TaskAttempt | null,
  onShowEditorDialog?: () => void
) {
  return useCallback(
    async (editorType?: EditorType) => {
      if (!attempt) return;

      try {
        const result = await attemptsApi.openEditor(attempt.id, editorType);

        if (result === undefined && !editorType) {
          if (onShowEditorDialog) {
            onShowEditorDialog();
          } else {
            NiceModal.show('editor-selection', { selectedAttempt: attempt });
          }
        }
      } catch (err) {
        console.error('Failed to open editor:', err);
        if (!editorType) {
          if (onShowEditorDialog) {
            onShowEditorDialog();
          } else {
            NiceModal.show('editor-selection', { selectedAttempt: attempt });
          }
        }
      }
    },
    [attempt, onShowEditorDialog]
  );
}
</file>

<file path="frontend/src/hooks/useOpenProjectInEditor.ts">
import { useCallback } from 'react';
import { projectsApi } from '@/lib/api';
import NiceModal from '@ebay/nice-modal-react';
import type { EditorType, Project } from 'shared/types';

export function useOpenProjectInEditor(
  project: Project | null,
  onShowEditorDialog?: () => void
) {
  return useCallback(
    async (editorType?: EditorType) => {
      if (!project) return;

      try {
        await projectsApi.openEditor(project.id, editorType);
      } catch (err) {
        console.error('Failed to open project in editor:', err);
        if (!editorType) {
          if (onShowEditorDialog) {
            onShowEditorDialog();
          } else {
            NiceModal.show('project-editor-selection', {
              selectedProject: project,
            });
          }
        }
      }
    },
    [project, onShowEditorDialog]
  );
}
</file>

<file path="frontend/src/hooks/usePinnedTodos.ts">
import { useMemo } from 'react';
import type { TodoItem } from 'shared/types';
import type { PatchTypeWithKey } from '@/hooks/useConversationHistory';

interface UsePinnedTodosResult {
  todos: TodoItem[];
  lastUpdated: string | null;
}

/**
 * Hook that extracts and maintains the latest TODO state from normalized conversation entries.
 * Filters for TodoManagement ActionType entries and returns the most recent todo list.
 */
export const usePinnedTodos = (
  entries: PatchTypeWithKey[]
): UsePinnedTodosResult => {
  return useMemo(() => {
    let latestTodos: TodoItem[] = [];
    let lastUpdatedTime: string | null = null;

    for (const entry of entries) {
      if (entry.type === 'NORMALIZED_ENTRY' && entry.content) {
        const normalizedEntry = entry.content as any;

        if (
          normalizedEntry.entry_type?.type === 'tool_use' &&
          normalizedEntry.entry_type?.action_type?.action === 'todo_management'
        ) {
          const actionType = normalizedEntry.entry_type.action_type;
          const partialTodos = actionType.todos || [];
          const currentTimestamp =
            normalizedEntry.timestamp || new Date().toISOString();

          // Only update latestTodos if we have meaningful content OR this is our first entry
          const hasMeaningfulTodos =
            partialTodos.length > 0 &&
            partialTodos.every(
              (todo: TodoItem) =>
                todo.content && todo.content.trim().length > 0 && todo.status
            );
          const isNewerThanLatest =
            !lastUpdatedTime || currentTimestamp >= lastUpdatedTime;

          if (
            hasMeaningfulTodos ||
            (isNewerThanLatest && latestTodos.length === 0)
          ) {
            latestTodos = partialTodos;
            lastUpdatedTime = currentTimestamp;
          }
        }
      }
    }

    return {
      todos: latestTodos,
      lastUpdated: lastUpdatedTime,
    };
  }, [entries]);
};
</file>

<file path="frontend/src/hooks/useProcessRetry.ts">
// hooks/useProcessRetry.ts
import { useCallback, useMemo, useState } from 'react';
import { useAttemptExecution } from '@/hooks/useAttemptExecution';
import { useBranchStatus } from '@/hooks/useBranchStatus';
import { showModal } from '@/lib/modals';
import {
  shouldShowInLogs,
  isCodingAgent,
  PROCESS_RUN_REASONS,
} from '@/constants/processes';
import type { ExecutionProcess, TaskAttempt } from 'shared/types';
import type {
  ExecutorActionType,
  CodingAgentInitialRequest,
  CodingAgentFollowUpRequest,
} from 'shared/types';

function isCodingAgentActionType(
  t: ExecutorActionType
): t is
  | ({ type: 'CodingAgentInitialRequest' } & CodingAgentInitialRequest)
  | ({ type: 'CodingAgentFollowUpRequest' } & CodingAgentFollowUpRequest) {
  return (
    t.type === 'CodingAgentInitialRequest' ||
    t.type === 'CodingAgentFollowUpRequest'
  );
}

/**
 * Reusable hook to retry a process given its executionProcessId and a new prompt.
 * Handles:
 *  - Preventing retry while anything is running (or that process is already running)
 *  - Optional worktree reset (via modal)
 *  - Variant extraction for coding-agent processes
 *  - Refetching attempt + branch data after replace
 */
export function useProcessRetry(attempt: TaskAttempt | undefined) {
  const attemptId = attempt?.id;

  // Fetch attempt + branch state the same way your component did
  const { attemptData, refetch: refetchAttempt } =
    useAttemptExecution(attemptId);
  const { data: branchStatus, refetch: refetchBranch } =
    useBranchStatus(attemptId);

  const [busy, setBusy] = useState(false);

  // Any process running at all?
  const anyRunning = useMemo(
    () => (attemptData.processes || []).some((p) => p.status === 'running'),
    [attemptData.processes?.map((p) => p.status).join(',')]
  );

  // Convenience lookups
  const getProcessById = useCallback(
    (pid: string): ExecutionProcess | undefined =>
      (attemptData.processes || []).find((p) => p.id === pid),
    [attemptData.processes]
  );

  /**
   * Returns whether a process is currently allowed to retry, and why not.
   * Useful if you want to gray out buttons in any component.
   */
  const getRetryDisabledState = useCallback(
    (pid: string) => {
      const proc = getProcessById(pid);
      const isRunningProc = proc?.status === 'running';
      const disabled = busy || anyRunning || isRunningProc;
      let reason: string | undefined;
      if (isRunningProc) reason = 'Finish or stop this run to retry.';
      else if (anyRunning) reason = 'Cannot retry while a process is running.';
      else if (busy) reason = 'Retry in progress.';
      return { disabled, reason };
    },
    [busy, anyRunning, getProcessById]
  );

  /**
   * Primary entrypoint: retry a process with a new prompt.
   */
  const retryProcess = useCallback(
    async (executionProcessId: string, newPrompt: string) => {
      if (!attemptId) return;

      const proc = getProcessById(executionProcessId);
      if (!proc) return;

      // Respect current disabled state
      const { disabled } = getRetryDisabledState(executionProcessId);
      if (disabled) return;

      type WithBefore = { before_head_commit?: string | null };
      const before =
        (proc as WithBefore | undefined)?.before_head_commit || null;

      // Try to gather comparison info (best-effort)
      let targetSubject: string | null = null;
      let commitsToReset: number | null = null;
      let isLinear: boolean | null = null;

      if (before) {
        try {
          const { commitsApi } = await import('@/lib/api');
          const info = await commitsApi.getInfo(attemptId, before);
          targetSubject = info.subject;
          const cmp = await commitsApi.compareToHead(attemptId, before);
          commitsToReset = cmp.is_linear ? cmp.ahead_from_head : null;
          isLinear = cmp.is_linear;
        } catch {
          // ignore best-effort enrichments
        }
      }

      const head = branchStatus?.head_oid || null;
      const dirty = !!branchStatus?.has_uncommitted_changes;
      const needReset = !!(before && (before !== head || dirty));
      const canGitReset = needReset && !dirty;

      // Compute “later processes” context for the dialog
      const procs = (attemptData.processes || []).filter(
        (p) => !p.dropped && shouldShowInLogs(p.run_reason)
      );
      const idx = procs.findIndex((p) => p.id === executionProcessId);
      const later = idx >= 0 ? procs.slice(idx + 1) : [];
      const laterCount = later.length;
      const laterCoding = later.filter((p) =>
        isCodingAgent(p.run_reason)
      ).length;
      const laterSetup = later.filter(
        (p) => p.run_reason === PROCESS_RUN_REASONS.SETUP_SCRIPT
      ).length;
      const laterCleanup = later.filter(
        (p) => p.run_reason === PROCESS_RUN_REASONS.CLEANUP_SCRIPT
      ).length;

      // Ask user for confirmation / reset options
      let modalResult:
        | {
            action: 'confirmed' | 'canceled';
            performGitReset?: boolean;
            forceWhenDirty?: boolean;
          }
        | undefined;

      try {
        modalResult = await showModal<
          typeof modalResult extends infer T
            ? T extends object
              ? T
              : never
            : never
        >('restore-logs', {
          targetSha: before,
          targetSubject,
          commitsToReset,
          isLinear,
          laterCount,
          laterCoding,
          laterSetup,
          laterCleanup,
          needGitReset: needReset,
          canGitReset,
          hasRisk: dirty,
          uncommittedCount: branchStatus?.uncommitted_count ?? 0,
          untrackedCount: branchStatus?.untracked_count ?? 0,
          // Defaults
          initialWorktreeResetOn: true,
          initialForceReset: false,
        });
      } catch {
        // user closed dialog
        return;
      }

      if (!modalResult || modalResult.action !== 'confirmed') return;

      let variant: string | null = null;

      const typ = proc?.executor_action?.typ; // type: ExecutorActionType

      if (typ && isCodingAgentActionType(typ)) {
        // executor_profile_id is ExecutorProfileId -> has `variant: string | null`
        variant = typ.executor_profile_id.variant;
      }

      // Perform the replacement
      try {
        setBusy(true);
        const { attemptsApi } = await import('@/lib/api');
        await attemptsApi.replaceProcess(attemptId, {
          process_id: executionProcessId,
          prompt: newPrompt,
          variant,
          perform_git_reset: modalResult.performGitReset ?? true,
          force_when_dirty: modalResult.forceWhenDirty ?? false,
        });

        // Refresh local caches
        await refetchAttempt();
        await refetchBranch();
      } finally {
        setBusy(false);
      }
    },
    [
      attemptId,
      attemptData.processes,
      branchStatus?.head_oid,
      branchStatus?.has_uncommitted_changes,
      branchStatus?.uncommitted_count,
      branchStatus?.untracked_count,
      getProcessById,
      getRetryDisabledState,
      refetchAttempt,
      refetchBranch,
    ]
  );

  return {
    retryProcess,
    busy,
    anyRunning,
    /** Helpful for buttons/tooltips */
    getRetryDisabledState,
  };
}

export type UseProcessRetryReturn = ReturnType<typeof useProcessRetry>;
</file>

<file path="frontend/src/hooks/useProfiles.ts">
import { useMemo } from 'react';
import { useQuery, useMutation, useQueryClient } from '@tanstack/react-query';
import { profilesApi } from '@/lib/api';

export type UseProfilesReturn = {
  // data
  profilesContent: string;
  parsedProfiles: any | null;
  profilesPath: string;

  // status
  isLoading: boolean;
  isError: boolean;
  error: unknown;
  isSaving: boolean;

  // actions
  refetch: () => void;
  save: (content: string) => Promise<void>;
  saveParsed: (obj: unknown) => Promise<void>;
};

export function useProfiles(): UseProfilesReturn {
  const queryClient = useQueryClient();

  const { data, isLoading, isError, error, refetch } = useQuery({
    queryKey: ['profiles'],
    queryFn: () => profilesApi.load(),
    staleTime: 1000 * 60, // 1 minute cache
  });

  const { mutateAsync: saveMutation, isPending: isSaving } = useMutation({
    mutationFn: (content: string) => profilesApi.save(content),
    onSuccess: (_, content) => {
      // Optimistically update cache with new content
      queryClient.setQueryData(['profiles'], (old: any) =>
        old ? { ...old, content } : old
      );
    },
  });

  const save = async (content: string): Promise<void> => {
    await saveMutation(content);
  };

  const parsedProfiles = useMemo(() => {
    if (!data?.content) return null;
    try {
      return JSON.parse(data.content);
    } catch {
      return null;
    }
  }, [data?.content]);

  const saveParsed = async (obj: unknown) => {
    await save(JSON.stringify(obj, null, 2));
  };

  return {
    profilesContent: data?.content ?? '',
    parsedProfiles,
    profilesPath: data?.path ?? '',
    isLoading,
    isError,
    error,
    isSaving,
    refetch,
    save,
    saveParsed,
  };
}
</file>

<file path="frontend/src/hooks/useProjectBranches.ts">
import { useQuery } from '@tanstack/react-query';
import { projectsApi } from '@/lib/api';

export function useProjectBranches(projectId?: string) {
  return useQuery({
    queryKey: ['projectBranches', projectId],
    queryFn: () => projectsApi.getBranches(projectId!),
    enabled: !!projectId,
    staleTime: 30_000,
    refetchOnWindowFocus: false,
  });
}
</file>

<file path="frontend/src/hooks/useProjectTasks.ts">
import { useCallback } from 'react';
import { useJsonPatchWsStream } from './useJsonPatchWsStream';
import type { TaskWithAttemptStatus } from 'shared/types';

type TasksState = {
  tasks: Record<string, TaskWithAttemptStatus>;
};

interface UseProjectTasksResult {
  tasks: TaskWithAttemptStatus[];
  tasksById: Record<string, TaskWithAttemptStatus>;
  isLoading: boolean;
  isConnected: boolean;
  error: string | null;
}

/**
 * Stream tasks for a project via WebSocket (JSON Patch) and expose as array + map.
 * Server sends initial snapshot: replace /tasks with an object keyed by id.
 * Live updates arrive at /tasks/<id> via add/replace/remove operations.
 */
export const useProjectTasks = (projectId: string): UseProjectTasksResult => {
  const endpoint = `/api/tasks/stream/ws?project_id=${encodeURIComponent(projectId)}`;

  const initialData = useCallback((): TasksState => ({ tasks: {} }), []);

  const { data, isConnected, error } = useJsonPatchWsStream(
    endpoint,
    !!projectId,
    initialData
  );

  const tasksById = data?.tasks ?? {};
  const tasks = Object.values(tasksById).sort(
    (a, b) =>
      new Date(b.created_at as unknown as string).getTime() -
      new Date(a.created_at as unknown as string).getTime()
  );
  const isLoading = !data && !error; // until first snapshot

  return { tasks, tasksById, isLoading, isConnected, error };
};
</file>

<file path="frontend/src/hooks/usePush.ts">
import { useMutation, useQueryClient } from '@tanstack/react-query';
import { attemptsApi } from '@/lib/api';

export function usePush(
  attemptId?: string,
  onSuccess?: () => void,
  onError?: (err: unknown) => void
) {
  const queryClient = useQueryClient();

  return useMutation({
    mutationFn: () => {
      if (!attemptId) return Promise.resolve();
      return attemptsApi.push(attemptId);
    },
    onSuccess: () => {
      // A push only affects remote status; invalidate the same branchStatus
      queryClient.invalidateQueries({ queryKey: ['branchStatus', attemptId] });
      onSuccess?.();
    },
    onError: (err) => {
      console.error('Failed to push:', err);
      onError?.(err);
    },
  });
}
</file>

<file path="frontend/src/hooks/useRebase.ts">
import { useMutation, useQueryClient } from '@tanstack/react-query';
import { attemptsApi, Result } from '@/lib/api';
import type { GitOperationError } from 'shared/types';
import type { RebaseTaskAttemptRequest } from 'shared/types';

export function useRebase(
  attemptId: string | undefined,
  projectId: string | undefined,
  onSuccess?: () => void,
  onError?: (err: Result<void, GitOperationError>) => void
) {
  const queryClient = useQueryClient();

  return useMutation<void, Result<void, GitOperationError>, string | undefined>(
    {
      mutationFn: (newBaseBranch?: string) => {
        if (!attemptId) return Promise.resolve();

        const data: RebaseTaskAttemptRequest = {
          new_base_branch: newBaseBranch || null,
        };
        return attemptsApi.rebase(attemptId, data).then((res) => {
          if (!res.success) {
            // Propagate typed failure Result for caller to handle (no manual ApiError construction)
            return Promise.reject(res);
          }
        });
      },
      onSuccess: () => {
        // Refresh branch status immediately
        queryClient.invalidateQueries({
          queryKey: ['branchStatus', attemptId],
        });

        // Refresh branch list used by PR dialog
        if (projectId) {
          queryClient.invalidateQueries({
            queryKey: ['projectBranches', projectId],
          });
        }

        onSuccess?.();
      },
      onError: (err: Result<void, GitOperationError>) => {
        console.error('Failed to rebase:', err);
        // Even on failure (likely conflicts), re-fetch branch status immediately to show rebase-in-progress
        queryClient.invalidateQueries({
          queryKey: ['branchStatus', attemptId],
        });
        onError?.(err);
      },
    }
  );
}
</file>

<file path="frontend/src/hooks/useTaskMutations.ts">
import { useMutation, useQueryClient } from '@tanstack/react-query';
import { tasksApi } from '@/lib/api';
import { useTaskViewManager } from '@/hooks/useTaskViewManager';
import type {
  CreateTask,
  CreateAndStartTaskRequest,
  Task,
  TaskWithAttemptStatus,
  UpdateTask,
} from 'shared/types';

export function useTaskMutations(projectId?: string) {
  const queryClient = useQueryClient();
  const { navigateToTask } = useTaskViewManager();

  const invalidateQueries = (taskId?: string) => {
    queryClient.invalidateQueries({ queryKey: ['tasks', projectId] });
    if (taskId) {
      queryClient.invalidateQueries({ queryKey: ['task', taskId] });
    }
  };

  const createTask = useMutation({
    mutationFn: (data: CreateTask) => tasksApi.create(data),
    onSuccess: (createdTask: Task) => {
      invalidateQueries();
      if (projectId) {
        navigateToTask(projectId, createdTask.id);
      }
    },
    onError: (err) => {
      console.error('Failed to create task:', err);
    },
  });

  const createAndStart = useMutation({
    mutationFn: (data: CreateAndStartTaskRequest) =>
      tasksApi.createAndStart(data),
    onSuccess: (createdTask: TaskWithAttemptStatus) => {
      invalidateQueries();
      if (projectId) {
        navigateToTask(projectId, createdTask.id);
      }
    },
    onError: (err) => {
      console.error('Failed to create and start task:', err);
    },
  });

  const updateTask = useMutation({
    mutationFn: ({ taskId, data }: { taskId: string; data: UpdateTask }) =>
      tasksApi.update(taskId, data),
    onSuccess: (updatedTask: Task) => {
      invalidateQueries(updatedTask.id);
    },
    onError: (err) => {
      console.error('Failed to update task:', err);
    },
  });

  return {
    createTask,
    createAndStart,
    updateTask,
  };
}
</file>

<file path="frontend/src/hooks/useTaskViewManager.ts">
import { useCallback } from 'react';
import { useLocation, useNavigate } from 'react-router-dom';

interface NavigateOptions {
  attemptId?: string;
  fullscreen?: boolean;
  replace?: boolean;
  state?: unknown;
}

/**
 * Centralised hook for task routing and fullscreen controls
 * Exposes navigation helpers alongside fullscreen state/toggles
 */
export function useTaskViewManager() {
  const navigate = useNavigate();
  const location = useLocation();

  const isFullscreen = location.pathname.endsWith('/full');

  const toggleFullscreen = useCallback(
    (fullscreen: boolean) => {
      const currentPath = location.pathname;
      let targetPath: string;

      if (fullscreen) {
        targetPath = currentPath.endsWith('/full')
          ? currentPath
          : `${currentPath}/full`;
      } else {
        targetPath = currentPath.endsWith('/full')
          ? currentPath.slice(0, -5)
          : currentPath;
      }

      navigate(targetPath, { replace: true });
    },
    [location.pathname, navigate]
  );

  const buildTaskUrl = useCallback(
    (projectId: string, taskId: string, options?: NavigateOptions) => {
      const baseUrl = `/projects/${projectId}/tasks/${taskId}`;
      const attemptUrl = options?.attemptId
        ? `/attempts/${options.attemptId}`
        : '';
      const fullscreenSuffix =
        (options?.fullscreen ?? isFullscreen) ? '/full' : '';

      return `${baseUrl}${attemptUrl}${fullscreenSuffix}`;
    },
    [isFullscreen]
  );

  const navigateToTask = useCallback(
    (projectId: string, taskId: string, options?: NavigateOptions) => {
      const targetUrl = buildTaskUrl(projectId, taskId, options);

      navigate(targetUrl, {
        replace: options?.replace ?? true,
        state: options?.state,
      });
    },
    [buildTaskUrl, navigate]
  );

  const navigateToAttempt = useCallback(
    (
      projectId: string,
      taskId: string,
      attemptId: string,
      options?: Omit<NavigateOptions, 'attemptId'>
    ) => {
      navigateToTask(projectId, taskId, {
        ...options,
        attemptId,
      });
    },
    [navigateToTask]
  );

  return {
    isFullscreen,
    toggleFullscreen,
    buildTaskUrl,
    navigateToTask,
    navigateToAttempt,
  };
}
</file>

<file path="frontend/src/i18n/locales/en/common.json">
{
  "buttons": {
    "save": "Save",
    "cancel": "Cancel",
    "delete": "Delete",
    "edit": "Edit",
    "create": "Create",
    "continue": "Continue",
    "reset": "Reset",
    "manage": "Manage",
    "connect": "Connect",
    "disconnect": "Disconnect"
  },
  "states": {
    "loading": "Loading...",
    "saving": "Saving...",
    "error": "Error",
    "success": "Success"
  },
  "language": {
    "en": "English",
    "ja": "日本語",
    "browserDefault": "Browser Default"
  }
}
</file>

<file path="frontend/src/i18n/locales/en/projects.json">
{
  "title": "Projects",
  "subtitle": "Manage your projects and track their progress",
  "createProject": "Create Project",
  "loading": "Loading projects...",
  "errors": {
    "fetchFailed": "Failed to fetch projects"
  },
  "empty": {
    "title": "No projects yet",
    "description": "Get started by creating your first project.",
    "createFirst": "Create your first project"
  }
}
</file>

<file path="frontend/src/i18n/locales/en/settings.json">
{
  "settings": {
    "layout": {
      "nav": {
        "title": "Settings",
        "general": "General",
        "generalDesc": "Theme, notifications, and preferences",
        "agents": "Agents",
        "agentsDesc": "Coding agent configurations",
        "mcp": "MCP Servers",
        "mcpDesc": "Model Context Protocol servers"
      }
    },
    "general": {
      "loading": "Loading settings...",
      "loadError": "Failed to load configuration.",
      "save": {
        "button": "Save Settings",
        "success": "✓ Settings saved successfully!",
        "error": "Failed to save configuration"
      },
      "appearance": {
        "title": "Appearance",
        "description": "Customize how the application looks and feels.",
        "theme": {
          "label": "Theme",
          "placeholder": "Select theme",
          "helper": "Choose your preferred color scheme."
        },
        "language": {
          "label": "Language",
          "placeholder": "Select language",
          "helper": "Choose your preferred language. Browser Default follows your system language."
        }
      },
      "taskExecution": {
        "title": "Task Execution",
        "description": "Configure how tasks are executed and processed.",
        "executor": {
          "label": "Default Agent Configuration",
          "placeholder": "Select profile",
          "helper": "Choose the default agent configuration to use when creating a task attempt."
        },
        "variant": "DEFAULT"
      },
      "editor": {
        "title": "Editor",
        "description": "Configure your code editing experience.",
        "type": {
          "label": "Editor Type",
          "placeholder": "Select editor",
          "helper": "Choose your preferred code editor interface."
        }
      },
      "github": {
        "title": "GitHub Integration",
        "connected": "Connected as {{username}}",
        "connectButton": "Connect GitHub Account",
        "manage": "Manage",
        "disconnect": "Disconnect",
        "helper": "Connect your GitHub account to access private repositories and enable advanced Git operations."
      },
      "notifications": {
        "title": "Notifications",
        "description": "Control when and how you receive notifications.",
        "sound": {
          "label": "Sound Notifications",
          "helper": "Play a sound when task attempts finish running.",
          "fileLabel": "Sound",
          "filePlaceholder": "Select sound",
          "fileHelper": "Choose the sound to play when tasks complete. Click the volume button to preview."
        },
        "push": {
          "label": "Push Notifications",
          "helper": "Show system notifications when task attempts finish running."
        }
      },
      "privacy": {
        "title": "Privacy",
        "description": "Help improve Vibe-Kanban by sharing anonymous usage data.",
        "telemetry": {
          "label": "Enable Telemetry",
          "helper": "Enables anonymous usage events tracking to help improve the application. No prompts or project information are collected."
        }
      },
      "taskTemplates": {
        "title": "Task Templates",
        "description": "Manage global task templates that can be used across all projects."
      },
      "safety": {
        "title": "Safety & Disclaimers",
        "description": "Reset acknowledgments for safety warnings and onboarding.",
        "disclaimer": {
          "title": "Disclaimer Acknowledgment",
          "description": "Reset the safety disclaimer.",
          "button": "Reset"
        },
        "onboarding": {
          "title": "Onboarding",
          "description": "Reset the onboarding flow.",
          "button": "Reset"
        }
      }
    },
    "agents": {
      "title": "Coding Agent Configurations",
      "description": "Customize the behavior of coding agents with different configurations.",
      "loading": "Loading agent configurations...",
      "save": {
        "button": "Save Agent Configurations",
        "success": "✓ Executor configurations saved successfully!"
      },
      "editor": {
        "formLabel": "Edit JSON",
        "agentLabel": "Agent",
        "agentPlaceholder": "Select executor type",
        "configLabel": "Configuration",
        "configPlaceholder": "Select configuration",
        "createNew": "Create new...",
        "deleteTitle": "Cannot delete the last configuration",
        "deleteButton": "Delete {{name}}",
        "deleteText": "Delete",
        "jsonLabel": "Agent Configuration (JSON)",
        "jsonPlaceholder": "Loading profiles...",
        "jsonLoading": "Loading...",
        "pathLabel": "Configuration file location:"
      },
      "errors": {
        "deleteFailed": "Failed to delete configuration. Please try again.",
        "saveFailed": "Failed to save agent configurations. Please try again.",
        "saveConfigFailed": "Failed to save configuration. Please try again."
      }
    },
    "mcp": {
      "title": "MCP Server Configuration",
      "description": "Configure Model Context Protocol servers to extend coding agent capabilities with custom tools and resources.",
      "loading": "Loading MCP configuration...",
      "applying": "Applying configuration...",
      "labels": {
        "agent": "Agent",
        "agentPlaceholder": "Select executor",
        "agentHelper": "Choose which agent to configure MCP servers for.",
        "serverConfig": "Server Configuration (JSON)",
        "popularServers": "Popular servers",
        "serverHelper": "Click a card to insert that MCP Server into the JSON above.",
        "saveLocation": "Changes will be saved to:"
      },
      "loading": {
        "jsonEditor": "Loading...",
        "configuration": "Loading current MCP server configuration..."
      },
      "errors": {
        "loadFailed": "Failed to load configuration.",
        "invalidJson": "Invalid JSON format",
        "validationError": "Validation error",
        "saveFailed": "Failed to save MCP servers",
        "applyFailed": "Failed to apply MCP server configuration",
        "addServerFailed": "Failed to add preconfigured server",
        "mcpError": "MCP Configuration Error: {{error}}",
        "notSupported": "MCP Not Supported",
        "supportMessage": "To use MCP servers, please select a different executor that supports MCP (Claude, Amp, Gemini, Codex, or Opencode) above."
      },
      "save": {
        "button": "Save MCP Configuration",
        "success": "Settings Saved!",
        "successMessage": "✓ MCP configuration saved successfully!",
        "loading": "Loading current MCP server configuration..."
      }
    }
  }
}
</file>

<file path="frontend/src/i18n/locales/ja/common.json">
{
  "buttons": {
    "save": "保存",
    "cancel": "キャンセル",
    "delete": "削除",
    "edit": "編集",
    "create": "作成",
    "continue": "続行",
    "reset": "リセット",
    "manage": "管理",
    "connect": "接続",
    "disconnect": "切断"
  },
  "states": {
    "loading": "読み込み中...",
    "saving": "保存中...",
    "error": "エラー",
    "success": "成功"
  },
  "language": {
    "en": "English",
    "ja": "日本語",
    "browserDefault": "ブラウザ設定"
  }
}
</file>

<file path="frontend/src/i18n/locales/ja/projects.json">
{
  "title": "プロジェクト",
  "subtitle": "プロジェクトを管理し、進捗を追跡します",
  "createProject": "プロジェクトを作成",
  "loading": "プロジェクトを読み込み中...",
  "errors": {
    "fetchFailed": "プロジェクトの取得に失敗しました"
  },
  "empty": {
    "title": "プロジェクトがありません",
    "description": "最初のプロジェクトを作成して始めましょう。",
    "createFirst": "最初のプロジェクトを作成"
  }
}
</file>

<file path="frontend/src/i18n/locales/ja/settings.json">
{
  "settings": {
    "layout": {
      "nav": {
        "title": "設定",
        "general": "一般",
        "generalDesc": "テーマ、通知、および設定",
        "agents": "エージェント",
        "agentsDesc": "コーディングエージェントの設定",
        "mcp": "MCPサーバー",
        "mcpDesc": "モデルコンテキストプロトコルサーバー"
      }
    },
    "general": {
      "loading": "設定を読み込み中...",
      "loadError": "設定の読み込みに失敗しました。",
      "save": {
        "button": "設定を保存",
        "success": "✓ 設定が正常に保存されました！",
        "error": "設定の保存に失敗しました"
      },
      "appearance": {
        "title": "外観",
        "description": "アプリケーションの見た目と操作感をカスタマイズします。",
        "theme": {
          "label": "テーマ",
          "placeholder": "テーマを選択",
          "helper": "お好みの色スキームを選択してください。"
        },
        "language": {
          "label": "言語",
          "placeholder": "言語を選択",
          "helper": "お好みの言語を選択してください。ブラウザ設定では、システム言語に従います。"
        }
      },
      "taskExecution": {
        "title": "タスク実行",
        "description": "タスクの実行と処理方法を設定します。",
        "executor": {
          "label": "デフォルトエージェント設定",
          "placeholder": "プロファイルを選択",
          "helper": "タスク試行を作成する際に使用するデフォルトエージェント設定を選択してください。"
        },
        "variant": "デフォルト"
      },
      "editor": {
        "title": "エディター",
        "description": "コード編集体験を設定します。",
        "type": {
          "label": "エディタータイプ",
          "placeholder": "エディターを選択",
          "helper": "お好みのコードエディターインターフェースを選択してください。"
        }
      },
      "github": {
        "title": "GitHub連携",
        "connected": "{{username}}として接続中",
        "connectButton": "GitHubアカウントを接続",
        "manage": "管理",
        "disconnect": "切断",
        "helper": "GitHubアカウントを接続して、プライベートリポジトリへのアクセスと高度なGit操作を有効にします。"
      },
      "notifications": {
        "title": "通知",
        "description": "通知を受け取るタイミングと方法を制御します。",
        "sound": {
          "label": "音声通知",
          "helper": "タスク試行の実行が完了したときに音を再生します。",
          "fileLabel": "音声",
          "filePlaceholder": "音声を選択",
          "fileHelper": "タスク完了時に再生する音声を選択してください。音量ボタンをクリックしてプレビューできます。"
        },
        "push": {
          "label": "プッシュ通知",
          "helper": "タスク試行の実行が完了したときにシステム通知を表示します。"
        }
      },
      "privacy": {
        "title": "プライバシー",
        "description": "匿名の使用データを共有してVibe-Kanbanの改善にご協力ください。",
        "telemetry": {
          "label": "テレメトリを有効化",
          "helper": "アプリケーションの改善に役立つ匿名の使用イベント追跡を有効にします。プロンプトやプロジェクト情報は収集されません。"
        }
      },
      "taskTemplates": {
        "title": "タスクテンプレート",
        "description": "すべてのプロジェクトで使用できるグローバルタスクテンプレートを管理します。"
      },
      "safety": {
        "title": "安全性と免責事項",
        "description": "安全警告とオンボーディングの承認をリセットします。",
        "disclaimer": {
          "title": "免責事項の承認",
          "description": "安全免責事項をリセットします。",
          "button": "リセット"
        },
        "onboarding": {
          "title": "オンボーディング",
          "description": "オンボーディングフローをリセットします。",
          "button": "リセット"
        }
      }
    },
    "agents": {
      "title": "コーディングエージェント設定",
      "description": "コーディングエージェントの動作を異なる設定でカスタマイズします。",
      "loading": "エージェント設定を読み込み中...",
      "save": {
        "button": "エージェント設定を保存",
        "success": "✓ 実行設定が正常に保存されました！"
      },
      "editor": {
        "formLabel": "JSONを編集",
        "agentLabel": "エージェント",
        "agentPlaceholder": "実行タイプを選択",
        "configLabel": "設定",
        "configPlaceholder": "設定を選択",
        "createNew": "新規作成...",
        "deleteTitle": "最後の設定は削除できません",
        "deleteButton": "{{name}}を削除",
        "deleteText": "削除",
        "jsonLabel": "エージェント設定（JSON）",
        "jsonPlaceholder": "プロファイルを読み込み中...",
        "jsonLoading": "読み込み中...",
        "pathLabel": "設定ファイルの場所："
      },
      "errors": {
        "deleteFailed": "設定の削除に失敗しました。もう一度お試しください。",
        "saveFailed": "エージェント設定の保存に失敗しました。もう一度お試しください。",
        "saveConfigFailed": "設定の保存に失敗しました。もう一度お試しください。"
      }
    },
    "mcp": {
      "title": "MCPサーバー設定",
      "description": "モデルコンテキストプロトコルサーバーを設定して、コーディングエージェントの機能をカスタムツールとリソースで拡張します。",
      "loading": "MCP設定を読み込み中...",
      "applying": "設定を適用中...",
      "labels": {
        "agent": "エージェント",
        "agentPlaceholder": "実行器を選択",
        "agentHelper": "MCPサーバーを設定するエージェントを選択してください。",
        "serverConfig": "サーバー設定（JSON）",
        "popularServers": "人気サーバー",
        "serverHelper": "カードをクリックして、そのMCPサーバーを上記のJSONに挿入します。",
        "saveLocation": "変更は次の場所に保存されます："
      },
      "loading": {
        "jsonEditor": "読み込み中...",
        "configuration": "現在のMCPサーバー設定を読み込み中..."
      },
      "errors": {
        "loadFailed": "設定の読み込みに失敗しました。",
        "invalidJson": "無効なJSON形式です",
        "validationError": "検証エラー",
        "saveFailed": "MCPサーバーの保存に失敗しました",
        "applyFailed": "MCPサーバー設定の適用に失敗しました",
        "addServerFailed": "事前設定サーバーの追加に失敗しました",
        "mcpError": "MCP設定エラー：{{error}}",
        "notSupported": "MCPはサポートされていません",
        "supportMessage": "MCPサーバーを使用するには、MCP（Claude、Amp、Gemini、Codex、またはOpencode）をサポートする別の実行器を上記で選択してください。"
      },
      "save": {
        "button": "MCP設定を保存",
        "success": "設定が保存されました！",
        "successMessage": "✓ MCP設定が正常に保存されました！",
        "loading": "現在のMCPサーバー設定を読み込み中..."
      }
    }
  }
}
</file>

<file path="frontend/src/i18n/config.ts">
import i18n from 'i18next';
import { initReactI18next } from 'react-i18next';
import LanguageDetector from 'i18next-browser-languagedetector';

// Import translation files
import enCommon from './locales/en/common.json';
import enSettings from './locales/en/settings.json';
import enProjects from './locales/en/projects.json';
import jaCommon from './locales/ja/common.json';
import jaSettings from './locales/ja/settings.json';
import jaProjects from './locales/ja/projects.json';

const resources = {
  en: {
    common: enCommon,
    settings: enSettings,
    projects: enProjects,
  },
  ja: {
    common: jaCommon,
    settings: jaSettings,
    projects: jaProjects,
  },
};

i18n
  .use(LanguageDetector)
  .use(initReactI18next)
  .init({
    resources,
    fallbackLng: 'en',
    defaultNS: 'common',
    debug: import.meta.env.DEV,

    interpolation: {
      escapeValue: false, // React already escapes
    },

    react: {
      useSuspense: false, // Avoid suspense for now to simplify initial setup
    },

    detection: {
      order: ['navigator', 'htmlTag'],
      caches: [], // Disable localStorage cache - we'll handle this via config
    },
  });

// Debug logging in development
if (import.meta.env.DEV) {
  console.log('i18n initialized:', i18n.isInitialized);
  console.log('i18n language:', i18n.language);
  console.log('i18n namespaces:', i18n.options.ns);
  console.log('Common bundle loaded:', i18n.hasResourceBundle('en', 'common'));
}

// Function to update language from config
export const updateLanguageFromConfig = (configLanguage: string) => {
  if (configLanguage === 'BROWSER') {
    // Use browser detection
    const detected = i18n.services.languageDetector?.detect();
    const detectedLang = Array.isArray(detected) ? detected[0] : detected;
    i18n.changeLanguage(detectedLang || 'en');
  } else {
    // Use explicit language selection
    const langCode = configLanguage.toLowerCase();
    i18n.changeLanguage(langCode);
  }
};

export default i18n;
</file>

<file path="frontend/src/i18n/index.ts">
import './config';
export { default } from './config';
</file>

<file path="frontend/src/lib/api.ts">
// Import all necessary types from shared types

import {
  ApiResponse,
  BranchStatus,
  CheckTokenResponse,
  Config,
  CommitInfo,
  CreateFollowUpAttempt,
  CreateGitHubPrRequest,
  CreateTask,
  CreateAndStartTaskRequest,
  CreateTaskAttemptBody,
  CreateTaskTemplate,
  DeviceFlowStartResponse,
  DevicePollStatus,
  DirectoryListResponse,
  DirectoryEntry,
  EditorType,
  ExecutionProcess,
  GitBranch,
  Project,
  CreateProject,
  RebaseTaskAttemptRequest,
  RepositoryInfo,
  SearchResult,
  Task,
  TaskAttempt,
  TaskRelationships,
  TaskTemplate,
  TaskWithAttemptStatus,
  UpdateProject,
  UpdateTask,
  UpdateTaskTemplate,
  UserSystemInfo,
  GitHubServiceError,
  McpServerQuery,
  UpdateMcpServersBody,
  GetMcpServerResponse,
  ImageResponse,
  FollowUpDraftResponse,
  UpdateFollowUpDraftRequest,
  GitOperationError,
} from 'shared/types';

// Re-export types for convenience
export type { RepositoryInfo } from 'shared/types';
export type {
  FollowUpDraftResponse,
  UpdateFollowUpDraftRequest,
} from 'shared/types';

export class ApiError<E = unknown> extends Error {
  public status?: number;
  public error_data?: E;

  constructor(
    message: string,
    public statusCode?: number,
    public response?: Response,
    error_data?: E
  ) {
    super(message);
    this.name = 'ApiError';
    this.status = statusCode;
    this.error_data = error_data;
  }
}

export const makeRequest = async (url: string, options: RequestInit = {}) => {
  const headers = {
    'Content-Type': 'application/json',
    ...(options.headers || {}),
  };

  return fetch(url, {
    ...options,
    headers,
  });
};

export interface FollowUpResponse {
  message: string;
  actual_attempt_id: string;
  created_new_attempt: boolean;
}

export type Ok<T> = { success: true; data: T };
export type Err<E> = { success: false; error: E | undefined; message?: string };

// Result type for endpoints that need typed errors
export type Result<T, E> = Ok<T> | Err<E>;

// Special handler for Result-returning endpoints
const handleApiResponseAsResult = async <T, E>(
  response: Response
): Promise<Result<T, E>> => {
  if (!response.ok) {
    // HTTP error - no structured error data
    let errorMessage = `Request failed with status ${response.status}`;

    try {
      const errorData = await response.json();
      if (errorData.message) {
        errorMessage = errorData.message;
      }
    } catch {
      errorMessage = response.statusText || errorMessage;
    }

    return {
      success: false,
      error: undefined,
      message: errorMessage,
    };
  }

  const result: ApiResponse<T, E> = await response.json();

  if (!result.success) {
    return {
      success: false,
      error: result.error_data || undefined,
      message: result.message || undefined,
    };
  }

  return { success: true, data: result.data as T };
};

const handleApiResponse = async <T, E = T>(response: Response): Promise<T> => {
  if (!response.ok) {
    let errorMessage = `Request failed with status ${response.status}`;

    try {
      const errorData = await response.json();
      if (errorData.message) {
        errorMessage = errorData.message;
      }
    } catch {
      // Fallback to status text if JSON parsing fails
      errorMessage = response.statusText || errorMessage;
    }

    console.error('[API Error]', {
      message: errorMessage,
      status: response.status,
      response,
      endpoint: response.url,
      timestamp: new Date().toISOString(),
    });
    throw new ApiError<E>(errorMessage, response.status, response);
  }

  const result: ApiResponse<T, E> = await response.json();

  if (!result.success) {
    // Check for error_data first (structured errors), then fall back to message
    if (result.error_data) {
      console.error('[API Error with data]', {
        error_data: result.error_data,
        message: result.message,
        status: response.status,
        response,
        endpoint: response.url,
        timestamp: new Date().toISOString(),
      });
      // Throw a properly typed error with the error data
      throw new ApiError<E>(
        result.message || 'API request failed',
        response.status,
        response,
        result.error_data
      );
    }

    console.error('[API Error]', {
      message: result.message || 'API request failed',
      status: response.status,
      response,
      endpoint: response.url,
      timestamp: new Date().toISOString(),
    });
    throw new ApiError<E>(
      result.message || 'API request failed',
      response.status,
      response
    );
  }

  return result.data as T;
};

// Project Management APIs
export const projectsApi = {
  getAll: async (): Promise<Project[]> => {
    const response = await makeRequest('/api/projects');
    return handleApiResponse<Project[]>(response);
  },

  getById: async (id: string): Promise<Project> => {
    const response = await makeRequest(`/api/projects/${id}`);
    return handleApiResponse<Project>(response);
  },

  create: async (data: CreateProject): Promise<Project> => {
    const response = await makeRequest('/api/projects', {
      method: 'POST',
      body: JSON.stringify(data),
    });
    return handleApiResponse<Project>(response);
  },

  update: async (id: string, data: UpdateProject): Promise<Project> => {
    const response = await makeRequest(`/api/projects/${id}`, {
      method: 'PUT',
      body: JSON.stringify(data),
    });
    return handleApiResponse<Project>(response);
  },

  delete: async (id: string): Promise<void> => {
    const response = await makeRequest(`/api/projects/${id}`, {
      method: 'DELETE',
    });
    return handleApiResponse<void>(response);
  },

  openEditor: async (id: string, editorType?: EditorType): Promise<void> => {
    const requestBody: any = {};
    if (editorType) requestBody.editor_type = editorType;

    const response = await makeRequest(`/api/projects/${id}/open-editor`, {
      method: 'POST',
      body: JSON.stringify(
        Object.keys(requestBody).length > 0 ? requestBody : null
      ),
    });
    return handleApiResponse<void>(response);
  },

  getBranches: async (id: string): Promise<GitBranch[]> => {
    const response = await makeRequest(`/api/projects/${id}/branches`);
    return handleApiResponse<GitBranch[]>(response);
  },

  searchFiles: async (
    id: string,
    query: string,
    mode?: string,
    options?: RequestInit
  ): Promise<SearchResult[]> => {
    const modeParam = mode ? `&mode=${encodeURIComponent(mode)}` : '';
    const response = await makeRequest(
      `/api/projects/${id}/search?q=${encodeURIComponent(query)}${modeParam}`,
      options
    );
    return handleApiResponse<SearchResult[]>(response);
  },
};

// Task Management APIs
export const tasksApi = {
  getAll: async (projectId: string): Promise<TaskWithAttemptStatus[]> => {
    const response = await makeRequest(`/api/tasks?project_id=${projectId}`);
    return handleApiResponse<TaskWithAttemptStatus[]>(response);
  },

  getById: async (taskId: string): Promise<Task> => {
    const response = await makeRequest(`/api/tasks/${taskId}`);
    return handleApiResponse<Task>(response);
  },

  create: async (data: CreateTask): Promise<Task> => {
    const response = await makeRequest(`/api/tasks`, {
      method: 'POST',
      body: JSON.stringify(data),
    });
    return handleApiResponse<Task>(response);
  },

  createAndStart: async (
    data: CreateAndStartTaskRequest
  ): Promise<TaskWithAttemptStatus> => {
    const response = await makeRequest(`/api/tasks/create-and-start`, {
      method: 'POST',
      body: JSON.stringify(data),
    });
    return handleApiResponse<TaskWithAttemptStatus>(response);
  },

  update: async (taskId: string, data: UpdateTask): Promise<Task> => {
    const response = await makeRequest(`/api/tasks/${taskId}`, {
      method: 'PUT',
      body: JSON.stringify(data),
    });
    return handleApiResponse<Task>(response);
  },

  delete: async (taskId: string): Promise<void> => {
    const response = await makeRequest(`/api/tasks/${taskId}`, {
      method: 'DELETE',
    });
    return handleApiResponse<void>(response);
  },
};

// Task Attempts APIs
export const attemptsApi = {
  getChildren: async (attemptId: string): Promise<TaskRelationships> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/children`
    );
    return handleApiResponse<TaskRelationships>(response);
  },

  getAll: async (taskId: string): Promise<TaskAttempt[]> => {
    const response = await makeRequest(`/api/task-attempts?task_id=${taskId}`);
    return handleApiResponse<TaskAttempt[]>(response);
  },

  get: async (attemptId: string): Promise<TaskAttempt> => {
    const response = await makeRequest(`/api/task-attempts/${attemptId}`);
    return handleApiResponse<TaskAttempt>(response);
  },

  create: async (data: CreateTaskAttemptBody): Promise<TaskAttempt> => {
    const response = await makeRequest(`/api/task-attempts`, {
      method: 'POST',
      body: JSON.stringify(data),
    });
    return handleApiResponse<TaskAttempt>(response);
  },

  stop: async (attemptId: string): Promise<void> => {
    const response = await makeRequest(`/api/task-attempts/${attemptId}/stop`, {
      method: 'POST',
    });
    return handleApiResponse<void>(response);
  },

  replaceProcess: async (
    attemptId: string,
    data: {
      process_id: string;
      prompt: string;
      variant?: string | null;
      force_when_dirty?: boolean;
      perform_git_reset?: boolean;
    }
  ): Promise<unknown> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/replace-process`,
      {
        method: 'POST',
        body: JSON.stringify(data),
      }
    );
    return handleApiResponse(response);
  },

  followUp: async (
    attemptId: string,
    data: CreateFollowUpAttempt
  ): Promise<void> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/follow-up`,
      {
        method: 'POST',
        body: JSON.stringify(data),
      }
    );
    return handleApiResponse<void>(response);
  },

  getFollowUpDraft: async (
    attemptId: string
  ): Promise<FollowUpDraftResponse> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/follow-up-draft`
    );
    return handleApiResponse<FollowUpDraftResponse>(response);
  },

  saveFollowUpDraft: async (
    attemptId: string,
    data: UpdateFollowUpDraftRequest
  ): Promise<FollowUpDraftResponse> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/follow-up-draft`,
      {
        // Server expects PUT for saving/updating the draft
        method: 'PUT',
        body: JSON.stringify(data),
      }
    );
    return handleApiResponse<FollowUpDraftResponse>(response);
  },

  setFollowUpQueue: async (
    attemptId: string,
    queued: boolean,
    expectedQueued?: boolean,
    expectedVersion?: number
  ): Promise<FollowUpDraftResponse> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/follow-up-draft/queue`,
      {
        method: 'POST',
        body: JSON.stringify({
          queued,
          expected_queued: expectedQueued,
          expected_version: expectedVersion,
        }),
      }
    );
    return handleApiResponse<FollowUpDraftResponse>(response);
  },

  deleteFile: async (
    attemptId: string,
    fileToDelete: string
  ): Promise<void> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/delete-file?file_path=${encodeURIComponent(
        fileToDelete
      )}`,
      {
        method: 'POST',
      }
    );
    return handleApiResponse<void>(response);
  },

  openEditor: async (
    attemptId: string,
    editorType?: EditorType,
    filePath?: string
  ): Promise<void> => {
    const requestBody: { editor_type?: EditorType; file_path?: string } = {};
    if (editorType) requestBody.editor_type = editorType;
    if (filePath) requestBody.file_path = filePath;

    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/open-editor`,
      {
        method: 'POST',
        body: JSON.stringify(
          Object.keys(requestBody).length > 0 ? requestBody : null
        ),
      }
    );
    return handleApiResponse<void>(response);
  },

  getBranchStatus: async (attemptId: string): Promise<BranchStatus> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/branch-status`
    );
    return handleApiResponse<BranchStatus>(response);
  },

  merge: async (attemptId: string): Promise<void> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/merge`,
      {
        method: 'POST',
      }
    );
    return handleApiResponse<void>(response);
  },

  push: async (attemptId: string): Promise<void> => {
    const response = await makeRequest(`/api/task-attempts/${attemptId}/push`, {
      method: 'POST',
    });
    return handleApiResponse<void>(response);
  },

  rebase: async (
    attemptId: string,
    data: RebaseTaskAttemptRequest
  ): Promise<Result<void, GitOperationError>> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/rebase`,
      {
        method: 'POST',
        body: JSON.stringify(data),
      }
    );
    return handleApiResponseAsResult<void, GitOperationError>(response);
  },

  abortConflicts: async (attemptId: string): Promise<void> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/conflicts/abort`,
      {
        method: 'POST',
      }
    );
    return handleApiResponse<void>(response);
  },

  createPR: async (
    attemptId: string,
    data: CreateGitHubPrRequest
  ): Promise<Result<string, GitHubServiceError>> => {
    const response = await makeRequest(`/api/task-attempts/${attemptId}/pr`, {
      method: 'POST',
      body: JSON.stringify(data),
    });
    return handleApiResponseAsResult<string, GitHubServiceError>(response);
  },

  startDevServer: async (attemptId: string): Promise<void> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/start-dev-server`,
      {
        method: 'POST',
      }
    );
    return handleApiResponse<void>(response);
  },
};

// Extra helpers
export const commitsApi = {
  getInfo: async (attemptId: string, sha: string): Promise<CommitInfo> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/commit-info?sha=${encodeURIComponent(
        sha
      )}`
    );
    return handleApiResponse<CommitInfo>(response);
  },
  compareToHead: async (
    attemptId: string,
    sha: string
  ): Promise<{
    head_oid: string;
    target_oid: string;
    ahead_from_head: number;
    behind_from_head: number;
    is_linear: boolean;
  }> => {
    const response = await makeRequest(
      `/api/task-attempts/${attemptId}/commit-compare?sha=${encodeURIComponent(
        sha
      )}`
    );
    return handleApiResponse(response);
  },
};

// Execution Process APIs
export const executionProcessesApi = {
  getExecutionProcesses: async (
    attemptId: string
  ): Promise<ExecutionProcess[]> => {
    const response = await makeRequest(
      `/api/execution-processes?task_attempt_id=${attemptId}`
    );
    return handleApiResponse<ExecutionProcess[]>(response);
  },

  getDetails: async (processId: string): Promise<ExecutionProcess> => {
    const response = await makeRequest(`/api/execution-processes/${processId}`);
    return handleApiResponse<ExecutionProcess>(response);
  },

  stopExecutionProcess: async (processId: string): Promise<void> => {
    const response = await makeRequest(
      `/api/execution-processes/${processId}/stop`,
      {
        method: 'POST',
      }
    );
    return handleApiResponse<void>(response);
  },
};

// File System APIs
export const fileSystemApi = {
  list: async (path?: string): Promise<DirectoryListResponse> => {
    const queryParam = path ? `?path=${encodeURIComponent(path)}` : '';
    const response = await makeRequest(
      `/api/filesystem/directory${queryParam}`
    );
    return handleApiResponse<DirectoryListResponse>(response);
  },

  listGitRepos: async (path?: string): Promise<DirectoryEntry[]> => {
    const queryParam = path ? `?path=${encodeURIComponent(path)}` : '';
    const response = await makeRequest(
      `/api/filesystem/git-repos${queryParam}`
    );
    return handleApiResponse<DirectoryEntry[]>(response);
  },
};

// Config APIs (backwards compatible)
export const configApi = {
  getConfig: async (): Promise<UserSystemInfo> => {
    const response = await makeRequest('/api/info');
    return handleApiResponse<UserSystemInfo>(response);
  },
  saveConfig: async (config: Config): Promise<Config> => {
    const response = await makeRequest('/api/config', {
      method: 'PUT',
      body: JSON.stringify(config),
    });
    return handleApiResponse<Config>(response);
  },
};

// GitHub Device Auth APIs
export const githubAuthApi = {
  checkGithubToken: async (): Promise<CheckTokenResponse> => {
    const response = await makeRequest('/api/auth/github/check');
    return handleApiResponse<CheckTokenResponse>(response);
  },
  start: async (): Promise<DeviceFlowStartResponse> => {
    const response = await makeRequest('/api/auth/github/device/start', {
      method: 'POST',
    });
    return handleApiResponse<DeviceFlowStartResponse>(response);
  },
  poll: async (): Promise<DevicePollStatus> => {
    const response = await makeRequest('/api/auth/github/device/poll', {
      method: 'POST',
    });
    return handleApiResponse<DevicePollStatus>(response);
  },
};

// GitHub APIs (only available in cloud mode)
export const githubApi = {
  listRepositories: async (page: number = 1): Promise<RepositoryInfo[]> => {
    const response = await makeRequest(`/api/github/repositories?page=${page}`);
    return handleApiResponse<RepositoryInfo[]>(response);
  },
  // createProjectFromRepository: async (
  //   data: CreateProjectFromGitHub
  // ): Promise<Project> => {
  //   const response = await makeRequest('/api/projects/from-github', {
  //     method: 'POST',
  //     body: JSON.stringify(data, (_key, value) =>
  //       typeof value === 'bigint' ? Number(value) : value
  //     ),
  //   });
  //   return handleApiResponse<Project>(response);
  // },
};

// Task Templates APIs
export const templatesApi = {
  list: async (): Promise<TaskTemplate[]> => {
    const response = await makeRequest('/api/templates');
    return handleApiResponse<TaskTemplate[]>(response);
  },

  listGlobal: async (): Promise<TaskTemplate[]> => {
    const response = await makeRequest('/api/templates?global=true');
    return handleApiResponse<TaskTemplate[]>(response);
  },

  listByProject: async (projectId: string): Promise<TaskTemplate[]> => {
    const response = await makeRequest(
      `/api/templates?project_id=${projectId}`
    );
    return handleApiResponse<TaskTemplate[]>(response);
  },

  get: async (templateId: string): Promise<TaskTemplate> => {
    const response = await makeRequest(`/api/templates/${templateId}`);
    return handleApiResponse<TaskTemplate>(response);
  },

  create: async (data: CreateTaskTemplate): Promise<TaskTemplate> => {
    const response = await makeRequest('/api/templates', {
      method: 'POST',
      body: JSON.stringify(data),
    });
    return handleApiResponse<TaskTemplate>(response);
  },

  update: async (
    templateId: string,
    data: UpdateTaskTemplate
  ): Promise<TaskTemplate> => {
    const response = await makeRequest(`/api/templates/${templateId}`, {
      method: 'PUT',
      body: JSON.stringify(data),
    });
    return handleApiResponse<TaskTemplate>(response);
  },

  delete: async (templateId: string): Promise<void> => {
    const response = await makeRequest(`/api/templates/${templateId}`, {
      method: 'DELETE',
    });
    return handleApiResponse<void>(response);
  },
};

// MCP Servers APIs
export const mcpServersApi = {
  load: async (query: McpServerQuery): Promise<GetMcpServerResponse> => {
    const params = new URLSearchParams(query);
    const response = await makeRequest(`/api/mcp-config?${params.toString()}`);
    return handleApiResponse<GetMcpServerResponse>(response);
  },
  save: async (
    query: McpServerQuery,
    data: UpdateMcpServersBody
  ): Promise<void> => {
    const params = new URLSearchParams(query);
    // params.set('profile', profile);
    const response = await makeRequest(`/api/mcp-config?${params.toString()}`, {
      method: 'POST',
      body: JSON.stringify(data),
    });
    if (!response.ok) {
      const errorData = await response.json();
      console.error('[API Error] Failed to save MCP servers', {
        message: errorData.message,
        status: response.status,
        response,
        timestamp: new Date().toISOString(),
      });
      throw new ApiError(
        errorData.message || 'Failed to save MCP servers',
        response.status,
        response
      );
    }
  },
};

// Profiles API
export const profilesApi = {
  load: async (): Promise<{ content: string; path: string }> => {
    const response = await makeRequest('/api/profiles');
    return handleApiResponse<{ content: string; path: string }>(response);
  },
  save: async (content: string): Promise<string> => {
    const response = await makeRequest('/api/profiles', {
      method: 'PUT',
      body: content,
      headers: {
        'Content-Type': 'application/json',
      },
    });
    return handleApiResponse<string>(response);
  },
};

// Images API
export const imagesApi = {
  upload: async (file: File): Promise<ImageResponse> => {
    const formData = new FormData();
    formData.append('image', file);

    const response = await fetch('/api/images/upload', {
      method: 'POST',
      body: formData,
      credentials: 'include',
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new ApiError(
        `Failed to upload image: ${errorText}`,
        response.status,
        response
      );
    }

    return handleApiResponse<ImageResponse>(response);
  },

  delete: async (imageId: string): Promise<void> => {
    const response = await makeRequest(`/api/images/${imageId}`, {
      method: 'DELETE',
    });
    return handleApiResponse<void>(response);
  },

  getTaskImages: async (taskId: string): Promise<ImageResponse[]> => {
    const response = await makeRequest(`/api/images/task/${taskId}`);
    return handleApiResponse<ImageResponse[]>(response);
  },

  getImageUrl: (imageId: string): string => {
    return `/api/images/${imageId}/file`;
  },
};
</file>

<file path="frontend/src/lib/conflicts.ts">
import type { ConflictOp } from 'shared/types';

export function displayConflictOpLabel(op?: ConflictOp | null): string {
  switch (op) {
    case 'merge':
      return 'Merge';
    case 'cherry_pick':
      return 'Cherry-pick';
    case 'revert':
      return 'Revert';
    case 'rebase':
    default:
      return 'Rebase';
  }
}

export function formatConflictHeader(
  op: ConflictOp | null | undefined,
  sourceBranch: string,
  baseBranch?: string
): string {
  switch (op) {
    case 'merge':
      return `Merge conflicts while merging into '${sourceBranch}'.`;
    case 'cherry_pick':
      return `Cherry-pick conflicts on '${sourceBranch}'.`;
    case 'revert':
      return `Revert conflicts on '${sourceBranch}'.`;
    case 'rebase':
    default:
      return `Rebase conflicts while rebasing '${sourceBranch}' onto '${baseBranch ?? 'base branch'}'.`;
  }
}

export function buildResolveConflictsInstructions(
  sourceBranch: string | null,
  baseBranch: string | undefined,
  conflictedFiles: string[],
  op?: ConflictOp | null
): string {
  const source = sourceBranch || 'current attempt branch';
  const base = baseBranch ?? 'base branch';
  const filesList = conflictedFiles.slice(0, 12);
  const filesBlock = filesList.length
    ? `\n\nFiles with conflicts:\n${filesList.map((f) => `- ${f}`).join('\n')}`
    : '';

  const opTitle = displayConflictOpLabel(op);
  const header = formatConflictHeader(op, source, base);

  return (
    `${header}` +
    filesBlock +
    `\n\nPlease resolve each file carefully. When continuing, ensure the ${opTitle.toLowerCase()} does not hang (set \`GIT_EDITOR=true\` or use a non-interactive editor).`
  );
}
</file>

<file path="frontend/src/lib/keyboard-shortcuts.ts">
import { useCallback, useEffect } from 'react';
import { useLocation, useNavigate } from 'react-router-dom';
import type { ExecutorConfig } from 'shared/types';

// Define available keyboard shortcuts
export interface KeyboardShortcut {
  key: string;
  description: string;
  action: (context?: KeyboardShortcutContext) => void;
  requiresModifier?: boolean;
  disabled?: boolean;
}

export interface KeyboardShortcutContext {
  navigate?: ReturnType<typeof useNavigate>;
  closeDialog?: () => void;
  onC?: () => void;
  currentPath?: string;
  hasOpenDialog?: boolean;
  location?: ReturnType<typeof useLocation>;
  stopExecution?: () => void;
  newAttempt?: () => void;
  onEnter?: () => void;
  ignoreEscape?: boolean;
}

// Centralized shortcut definitions
export const createKeyboardShortcuts = (
  context: KeyboardShortcutContext
): Record<string, KeyboardShortcut> => ({
  Escape: {
    key: 'Escape',
    description: 'Go back or close dialog',
    action: () => {
      if (context.ignoreEscape) {
        return;
      }

      // If there's an open dialog, close it
      if (context.hasOpenDialog && context.closeDialog) {
        context.closeDialog();
        return;
      }

      // Otherwise, navigate back
      if (context.navigate) {
        const currentPath =
          context.currentPath || context.location?.pathname || '/';

        // Navigate back based on current path
        if (
          currentPath.includes('/tasks/') &&
          !currentPath.endsWith('/tasks')
        ) {
          // From task details, go back to project tasks
          const projectPath = currentPath.split('/tasks/')[0] + '/tasks';
          context.navigate(projectPath);
        } else if (
          currentPath.includes('/projects/') &&
          currentPath.includes('/tasks')
        ) {
          // From project tasks, go back to projects
          context.navigate('/projects');
        } else if (currentPath !== '/' && currentPath !== '/projects') {
          // Default: go to projects page
          context.navigate('/projects');
        }
      }
    },
  },
  Enter: {
    key: 'Enter',
    description: 'Enter or submit',
    action: () => {
      if (context.onEnter) {
        context.onEnter();
      }
    },
  },
  KeyC: {
    key: 'c',
    description: 'Create new task',
    action: () => {
      if (context.onC) {
        context.onC();
      }
    },
  },
  KeyS: {
    key: 's',
    description: 'Stop all executions',
    action: () => {
      context.stopExecution && context.stopExecution();
    },
  },
  KeyN: {
    key: 'n',
    description: 'Create new task attempt',
    action: () => {
      context.newAttempt && context.newAttempt();
    },
  },
});

// Hook to register global keyboard shortcuts
export function useKeyboardShortcuts(context: KeyboardShortcutContext) {
  const shortcuts = createKeyboardShortcuts(context);

  const handleKeyDown = useCallback(
    (event: KeyboardEvent) => {
      // Don't trigger shortcuts when typing in input fields
      const target = event.target as HTMLElement;
      if (
        target.tagName === 'INPUT' ||
        target.tagName === 'TEXTAREA' ||
        target.isContentEditable
      ) {
        return;
      }

      // Don't trigger shortcuts when modifier keys are pressed (except for specific shortcuts)
      if (event.ctrlKey || event.metaKey || event.altKey) {
        return;
      }

      const shortcut = shortcuts[event.code] || shortcuts[event.key];

      if (shortcut && !shortcut.disabled) {
        event.preventDefault();
        shortcut.action(context);
      }
    },
    [shortcuts, context]
  );

  useEffect(() => {
    document.addEventListener('keydown', handleKeyDown);
    return () => document.removeEventListener('keydown', handleKeyDown);
  }, [handleKeyDown]);

  return shortcuts;
}

// Hook for dialog-specific keyboard shortcuts
export function useDialogKeyboardShortcuts(onClose: () => void) {
  useEffect(() => {
    const handleKeyDown = (event: KeyboardEvent) => {
      if (event.key === 'Escape') {
        event.preventDefault();
        onClose();
      }
    };

    document.addEventListener('keydown', handleKeyDown);
    return () => document.removeEventListener('keydown', handleKeyDown);
  }, [onClose]);
}

// Kanban board keyboard navigation hook
export function useKanbanKeyboardNavigation({
  focusedTaskId,
  setFocusedTaskId,
  focusedStatus,
  setFocusedStatus,
  groupedTasks,
  filteredTasks,
  allTaskStatuses,
  onViewTaskDetails,
  preserveIndexOnColumnSwitch = false,
}: {
  focusedTaskId: string | null;
  setFocusedTaskId: (id: string | null) => void;
  focusedStatus: string | null;
  setFocusedStatus: (status: string | null) => void;
  groupedTasks: Record<string, any[]>;
  filteredTasks: unknown[];
  allTaskStatuses: string[];
  onViewTaskDetails?: (task: any) => void;
  preserveIndexOnColumnSwitch?: boolean;
}) {
  useEffect(() => {
    function handleKeyDown(e: KeyboardEvent) {
      // Don't handle if typing in input, textarea, or select
      const tag = (e.target as HTMLElement)?.tagName;
      if (
        tag === 'INPUT' ||
        tag === 'TEXTAREA' ||
        tag === 'SELECT' ||
        (e.target as HTMLElement)?.isContentEditable
      )
        return;
      if (!focusedTaskId || !focusedStatus) return;
      const currentColumn = groupedTasks[focusedStatus];
      const currentIndex = currentColumn.findIndex(
        (t: any) => t.id === focusedTaskId
      );
      let newStatus = focusedStatus;
      let newTaskId = focusedTaskId;
      if (e.key === 'ArrowDown') {
        if (currentIndex < currentColumn.length - 1) {
          newTaskId = currentColumn[currentIndex + 1].id;
        }
      } else if (e.key === 'ArrowUp') {
        if (currentIndex > 0) {
          newTaskId = currentColumn[currentIndex - 1].id;
        }
      } else if (e.key === 'ArrowRight') {
        let colIdx = allTaskStatuses.indexOf(focusedStatus);
        while (colIdx < allTaskStatuses.length - 1) {
          colIdx++;
          const nextStatus = allTaskStatuses[colIdx];
          if (groupedTasks[nextStatus] && groupedTasks[nextStatus].length > 0) {
            newStatus = nextStatus;
            if (preserveIndexOnColumnSwitch) {
              const nextCol = groupedTasks[nextStatus];
              const idx = Math.min(currentIndex, nextCol.length - 1);
              newTaskId = nextCol[idx].id;
            } else {
              newTaskId = groupedTasks[nextStatus][0].id;
            }
            break;
          }
        }
      } else if (e.key === 'ArrowLeft') {
        let colIdx = allTaskStatuses.indexOf(focusedStatus);
        while (colIdx > 0) {
          colIdx--;
          const prevStatus = allTaskStatuses[colIdx];
          if (groupedTasks[prevStatus] && groupedTasks[prevStatus].length > 0) {
            newStatus = prevStatus;
            if (preserveIndexOnColumnSwitch) {
              const prevCol = groupedTasks[prevStatus];
              const idx = Math.min(currentIndex, prevCol.length - 1);
              newTaskId = prevCol[idx].id;
            } else {
              newTaskId = groupedTasks[prevStatus][0].id;
            }
            break;
          }
        }
      } else if ((e.key === 'Enter' || e.key === ' ') && onViewTaskDetails) {
        const task = filteredTasks.find((t: any) => t.id === focusedTaskId);
        if (task) {
          onViewTaskDetails(task);
        }
      } else {
        return;
      }
      e.preventDefault();
      setFocusedTaskId(newTaskId);
      setFocusedStatus(newStatus);
    }

    window.addEventListener('keydown', handleKeyDown);
    return () => window.removeEventListener('keydown', handleKeyDown);
  }, [
    focusedTaskId,
    focusedStatus,
    groupedTasks,
    filteredTasks,
    onViewTaskDetails,
    allTaskStatuses,
    setFocusedTaskId,
    setFocusedStatus,
    preserveIndexOnColumnSwitch,
  ]);
}

// Hook for cycling through profile variants with Left Shift + Tab
export function useVariantCyclingShortcut({
  currentProfile,
  selectedVariant,
  setSelectedVariant,
}: {
  currentProfile: ExecutorConfig | null | undefined;
  selectedVariant: string | null;
  setSelectedVariant: (variant: string | null) => void;
}) {
  useEffect(() => {
    if (!currentProfile || Object.keys(currentProfile).length === 0) {
      return;
    }

    const handleKeyDown = (e: KeyboardEvent) => {
      // Check for Left Shift + Tab
      if (e.shiftKey && e.key === 'Tab') {
        e.preventDefault();

        // Build the variant cycle: variant1 → variant2 → ... → variant1
        const variants = currentProfile;
        const variantLabels = Object.keys(variants);

        // Find current index and cycle to next
        const currentIndex = variantLabels.findIndex(
          (v) => v === selectedVariant
        );
        const nextIndex = (currentIndex + 1) % variantLabels.length;
        const nextVariant = variantLabels[nextIndex];

        setSelectedVariant(nextVariant);
      }
    };

    document.addEventListener('keydown', handleKeyDown);
    return () => document.removeEventListener('keydown', handleKeyDown);
  }, [currentProfile, selectedVariant, setSelectedVariant]);
}
</file>

<file path="frontend/src/lib/mcp-strategies.ts">
import { McpConfig } from 'shared/types';

export class McpConfigStrategyGeneral {
  static createFullConfig(cfg: McpConfig): Record<string, any> {
    // create a template with servers filled in at cfg.servers
    const fullConfig = JSON.parse(JSON.stringify(cfg.template));
    let current = fullConfig;
    for (let i = 0; i < cfg.servers_path.length - 1; i++) {
      const key = cfg.servers_path[i];
      if (!current[key]) {
        current[key] = {};
      }
      current = current[key];
    }
    if (cfg.servers_path.length > 0) {
      const lastKey = cfg.servers_path[cfg.servers_path.length - 1];
      current[lastKey] = cfg.servers;
    }
    return fullConfig;
  }
  static validateFullConfig(
    mcp_config: McpConfig,
    full_config: Record<string, any>
  ): void {
    // Validate using the schema path
    let current = full_config;
    for (const key of mcp_config.servers_path) {
      current = current?.[key];
      if (current === undefined) {
        throw new Error(
          `Missing required field at path: ${mcp_config.servers_path.join('.')}`
        );
      }
    }
    if (typeof current !== 'object') {
      throw new Error('Servers configuration must be an object');
    }
  }
  static extractServersForApi(
    mcp_config: McpConfig,
    full_config: Record<string, any>
  ): Record<string, any> {
    // Extract the servers object based on the path
    let current = full_config;
    for (const key of mcp_config.servers_path) {
      current = current?.[key];
      if (current === undefined) {
        throw new Error(
          `Missing required field at path: ${mcp_config.servers_path.join('.')}`
        );
      }
    }
    return current;
  }

  static addPreconfiguredToConfig(
    mcp_config: McpConfig,
    existingConfig: Record<string, any>,
    serverKey: string
  ): Record<string, any> {
    const preconf = mcp_config.preconfigured as Record<string, any>;
    if (!preconf || typeof preconf !== 'object' || !(serverKey in preconf)) {
      throw new Error(`Unknown preconfigured server '${serverKey}'`);
    }

    const updated = JSON.parse(JSON.stringify(existingConfig || {}));
    let current = updated;

    for (let i = 0; i < mcp_config.servers_path.length - 1; i++) {
      const key = mcp_config.servers_path[i];
      if (!current[key] || typeof current[key] !== 'object') current[key] = {};
      current = current[key];
    }

    const lastKey = mcp_config.servers_path[mcp_config.servers_path.length - 1];
    if (!current[lastKey] || typeof current[lastKey] !== 'object')
      current[lastKey] = {};

    current[lastKey][serverKey] = preconf[serverKey];

    return updated;
  }
}
</file>

<file path="frontend/src/lib/modals.ts">
import NiceModal from '@ebay/nice-modal-react';
import type {
  FolderPickerDialogProps,
  TaskTemplateEditDialogProps,
  TaskTemplateEditResult,
  ProjectFormDialogProps,
  ProjectFormDialogResult,
} from '@/components/dialogs';

/**
 * Typed wrapper around NiceModal.show with better TypeScript support
 * @param modal - Modal ID (string) or component reference
 * @param props - Props to pass to the modal
 * @returns Promise that resolves with the modal's result
 */
export function showModal<T = void>(
  modal: string,
  props: Record<string, unknown> = {}
): Promise<T> {
  return NiceModal.show<T>(modal, props) as Promise<T>;
}

/**
 * Show folder picker dialog
 * @param props - Props for folder picker
 * @returns Promise that resolves with selected path or null if cancelled
 */
export function showFolderPicker(
  props: FolderPickerDialogProps = {}
): Promise<string | null> {
  return showModal<string | null>(
    'folder-picker',
    props as Record<string, unknown>
  );
}

/**
 * Show task template edit dialog
 * @param props - Props for template edit dialog
 * @returns Promise that resolves with 'saved' or 'canceled'
 */
export function showTaskTemplateEdit(
  props: TaskTemplateEditDialogProps
): Promise<TaskTemplateEditResult> {
  return showModal<TaskTemplateEditResult>(
    'task-template-edit',
    props as Record<string, unknown>
  );
}

/**
 * Show project form dialog
 * @param props - Props for project form dialog
 * @returns Promise that resolves with 'saved' or 'canceled'
 */
export function showProjectForm(
  props: ProjectFormDialogProps = {}
): Promise<ProjectFormDialogResult> {
  return showModal<ProjectFormDialogResult>(
    'project-form',
    props as Record<string, unknown>
  );
}

/**
 * Hide a modal by ID
 */
export function hideModal(modal: string): void {
  NiceModal.hide(modal);
}

/**
 * Remove a modal by ID
 */
export function removeModal(modal: string): void {
  NiceModal.remove(modal);
}

/**
 * Hide all currently visible modals
 */
export function hideAllModals(): void {
  // NiceModal doesn't have a direct hideAll, so we'll implement as needed
  console.log('Hide all modals - implement as needed');
}

/**
 * Common modal result types for standardization
 */
export type ConfirmResult = 'confirmed' | 'canceled';
export type DeleteResult = 'deleted' | 'canceled';
export type SaveResult = 'saved' | 'canceled';

/**
 * Error handling utility for modal operations
 */
export function getErrorMessage(error: unknown): string {
  if (error instanceof Error) {
    return error.message;
  }
  if (typeof error === 'string') {
    return error;
  }
  return 'An unknown error occurred';
}
</file>

<file path="frontend/src/lib/openTaskForm.ts">
import NiceModal from '@ebay/nice-modal-react';
import type { TaskFormDialogProps } from '@/components/dialogs/tasks/TaskFormDialog';

/**
 * Open the task form dialog programmatically
 * This replaces the previous TaskFormDialogContainer pattern
 */
export function openTaskForm(props: TaskFormDialogProps) {
  return NiceModal.show('task-form', props);
}
</file>

<file path="frontend/src/lib/responsive-config.ts">
/**
 * Centralized responsive configuration for TaskDetailsPanel
 * Adjust these values to change when the panel switches between overlay and side-by-side modes
 */

// The breakpoint at which we switch from overlay to side-by-side mode
// Change this value to adjust when the panel switches to side-by-side mode:
// 'sm' = 640px, 'md' = 768px, 'lg' = 1024px, 'xl' = 1280px, '2xl' = 1536px
export const PANEL_SIDE_BY_SIDE_BREAKPOINT = 'xl' as const;

// Panel widths for different screen sizes (in overlay mode)
export const PANEL_WIDTHS = {
  base: 'w-full', // < 640px
  sm: 'sm:w-[560px]', // 640px+
  md: 'md:w-[600px]', // 768px+
  lg: 'lg:w-[650px]', // 1024px+ (smaller to start transitioning)
  xl: 'xl:w-[750px]', // 1280px+
  '2xl': '2xl:w-[800px]', // 1536px+ (side-by-side mode)
} as const;

// Generate classes for TaskDetailsPanel
export const getTaskPanelClasses = (forceFullScreen: boolean) => {
  const overlayClasses = forceFullScreen
    ? 'w-full'
    : [
        PANEL_WIDTHS.base,
        PANEL_WIDTHS.sm,
        PANEL_WIDTHS.md,
        PANEL_WIDTHS.lg,
        PANEL_WIDTHS.xl,
      ].join(' ');

  const sideBySideClasses = forceFullScreen
    ? ''
    : [
        `${PANEL_SIDE_BY_SIDE_BREAKPOINT}:relative`,
        `${PANEL_SIDE_BY_SIDE_BREAKPOINT}:inset-auto`,
        `${PANEL_SIDE_BY_SIDE_BREAKPOINT}:z-auto`,
        `${PANEL_SIDE_BY_SIDE_BREAKPOINT}:h-full`,
        `${PANEL_SIDE_BY_SIDE_BREAKPOINT}:w-[800px]`,
      ].join(' ');

  return `fixed flex flex-col items-center inset-y-0 right-0 z-50 ${overlayClasses} ${sideBySideClasses} bg-diagonal-lines shadow-lg overflow-hidden `;
};

export const getTaskPanelInnerClasses = () => {
  return `flex-1 flex flex-col min-h-0 w-full max-w-[1400px] bg-muted border-x`;
};

// Generate classes for backdrop (only show in overlay mode)
export const getBackdropClasses = (forceFullScreen: boolean) => {
  return `fixed inset-0 z-40 bg-background/80 backdrop-blur-sm ${PANEL_SIDE_BY_SIDE_BREAKPOINT}:hidden ${forceFullScreen ? '' : 'hidden'}`;
};

// Generate classes for main container (always column layout, side-by-side moved to inner wrapper)
export const getMainContainerClasses = (
  isPanelOpen: boolean,
  forceFullScreen: boolean
) => {
  const overlayClasses =
    isPanelOpen && forceFullScreen ? 'w-full h-full' : 'h-full flex flex-col';

  return `${overlayClasses}`;
};

// Generate classes for kanban section
export const getKanbanSectionClasses = (
  isPanelOpen: boolean,
  forceFullScreen: boolean
) => {
  const baseClasses = 'h-full w-full';

  if (!isPanelOpen) return baseClasses;

  // const overlayClasses = 'w-full opacity-50 pointer-events-none';
  const sideBySideClasses =
    isPanelOpen && forceFullScreen
      ? ''
      : [
          `${PANEL_SIDE_BY_SIDE_BREAKPOINT}:flex-1`,
          `${PANEL_SIDE_BY_SIDE_BREAKPOINT}:min-w-0`,
          `${PANEL_SIDE_BY_SIDE_BREAKPOINT}:h-full`,
          `${PANEL_SIDE_BY_SIDE_BREAKPOINT}:overflow-y-auto`,
          `${PANEL_SIDE_BY_SIDE_BREAKPOINT}:opacity-100`,
          `${PANEL_SIDE_BY_SIDE_BREAKPOINT}:pointer-events-auto`,
        ].join(' ');

  // return `${overlayClasses} ${sideBySideClasses}`;
  return `${baseClasses} ${sideBySideClasses}`;
};
</file>

<file path="frontend/src/lib/types.ts">
import { ExecutionProcess } from 'shared/types';

export type AttemptData = {
  processes: ExecutionProcess[];
  runningProcessDetails: Record<string, ExecutionProcess>;
};

export interface ConversationEntryDisplayType {
  entry: any;
  processId: string;
  processPrompt?: string;
  processStatus: string;
  processIsRunning: boolean;
  process: any;
  isFirstInProcess: boolean;
  processIndex: number;
  entryIndex: number;
}
</file>

<file path="frontend/src/lib/utils.ts">
import { type ClassValue, clsx } from 'clsx';
import { twMerge } from 'tailwind-merge';

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs));
}
</file>

<file path="frontend/src/pages/settings/AgentSettings.tsx">
import { useEffect, useState } from 'react';
import { useTranslation } from 'react-i18next';
import {
  Card,
  CardContent,
  CardDescription,
  CardHeader,
  CardTitle,
} from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from '@/components/ui/select';
import { Label } from '@/components/ui/label';
import { Alert, AlertDescription } from '@/components/ui/alert';
import { Checkbox } from '@/components/ui/checkbox';
import { JSONEditor } from '@/components/ui/json-editor';
import { Loader2 } from 'lucide-react';

import { ExecutorConfigForm } from '@/components/ExecutorConfigForm';
import { useProfiles } from '@/hooks/useProfiles';
import { useUserSystem } from '@/components/config-provider';
import { showModal } from '@/lib/modals';

export function AgentSettings() {
  const { t } = useTranslation('settings');
  // Use profiles hook for server state
  const {
    profilesContent: serverProfilesContent,
    profilesPath,
    isLoading: profilesLoading,
    isSaving: profilesSaving,
    error: profilesError,
    save: saveProfiles,
  } = useProfiles();

  const { reloadSystem } = useUserSystem();

  // Local editor state (draft that may differ from server)
  const [localProfilesContent, setLocalProfilesContent] = useState('');
  const [profilesSuccess, setProfilesSuccess] = useState(false);
  const [saveError, setSaveError] = useState<string | null>(null);

  // Form-based editor state
  const [useFormEditor, setUseFormEditor] = useState(true);
  const [selectedExecutorType, setSelectedExecutorType] =
    useState<string>('CLAUDE_CODE');
  const [selectedConfiguration, setSelectedConfiguration] =
    useState<string>('DEFAULT');
  const [localParsedProfiles, setLocalParsedProfiles] = useState<any>(null);
  const [isDirty, setIsDirty] = useState(false);

  // Sync server state to local state when not dirty
  useEffect(() => {
    if (!isDirty && serverProfilesContent) {
      setLocalProfilesContent(serverProfilesContent);
      // Parse JSON inside effect to avoid object dependency
      try {
        const parsed = JSON.parse(serverProfilesContent);
        setLocalParsedProfiles(parsed);
      } catch (err) {
        console.error('Failed to parse profiles JSON:', err);
        setLocalParsedProfiles(null);
      }
    }
  }, [serverProfilesContent, isDirty]);

  // Sync raw profiles with parsed profiles
  const syncRawProfiles = (profiles: unknown) => {
    setLocalProfilesContent(JSON.stringify(profiles, null, 2));
  };

  // Mark profiles as dirty
  const markDirty = (nextProfiles: unknown) => {
    setLocalParsedProfiles(nextProfiles);
    syncRawProfiles(nextProfiles);
    setIsDirty(true);
  };

  // Open create dialog
  const openCreateDialog = async () => {
    try {
      const result = await showModal<{
        action: 'created' | 'canceled';
        configName?: string;
        cloneFrom?: string | null;
      }>('create-configuration', {
        executorType: selectedExecutorType,
        existingConfigs: Object.keys(
          localParsedProfiles?.executors?.[selectedExecutorType] || {}
        ),
      });

      if (result.action === 'created' && result.configName) {
        createConfiguration(
          selectedExecutorType,
          result.configName,
          result.cloneFrom
        );
      }
    } catch (error) {
      // User cancelled - do nothing
    }
  };

  // Create new configuration
  const createConfiguration = (
    executorType: string,
    configName: string,
    baseConfig?: string | null
  ) => {
    if (!localParsedProfiles || !localParsedProfiles.executors) return;

    const base =
      baseConfig &&
      localParsedProfiles.executors[executorType]?.[baseConfig]?.[executorType]
        ? localParsedProfiles.executors[executorType][baseConfig][executorType]
        : {};

    const updatedProfiles = {
      ...localParsedProfiles,
      executors: {
        ...localParsedProfiles.executors,
        [executorType]: {
          ...localParsedProfiles.executors[executorType],
          [configName]: {
            [executorType]: base,
          },
        },
      },
    };

    markDirty(updatedProfiles);
    setSelectedConfiguration(configName);
  };

  // Open delete dialog
  const openDeleteDialog = async (configName: string) => {
    try {
      const result = await showModal<'deleted' | 'canceled'>(
        'delete-configuration',
        {
          configName,
          executorType: selectedExecutorType,
        }
      );

      if (result === 'deleted') {
        await handleDeleteConfiguration(configName);
      }
    } catch (error) {
      // User cancelled - do nothing
    }
  };

  // Handle delete configuration
  const handleDeleteConfiguration = async (configToDelete: string) => {
    if (!localParsedProfiles) {
      return;
    }

    // Clear any previous errors
    setSaveError(null);

    try {
      // Validate that the configuration exists
      if (
        !localParsedProfiles.executors[selectedExecutorType]?.[configToDelete]
      ) {
        return;
      }

      // Check if this is the last configuration
      const currentConfigs = Object.keys(
        localParsedProfiles.executors[selectedExecutorType] || {}
      );
      if (currentConfigs.length <= 1) {
        return;
      }

      // Remove the configuration from the executor
      const remainingConfigs = {
        ...localParsedProfiles.executors[selectedExecutorType],
      };
      delete remainingConfigs[configToDelete];

      const updatedProfiles = {
        ...localParsedProfiles,
        executors: {
          ...localParsedProfiles.executors,
          [selectedExecutorType]: remainingConfigs,
        },
      };

      // If no configurations left, create a blank DEFAULT (should not happen due to check above)
      if (Object.keys(remainingConfigs).length === 0) {
        updatedProfiles.executors[selectedExecutorType] = {
          DEFAULT: { [selectedExecutorType]: {} },
        };
      }

      try {
        // Save using hook
        await saveProfiles(JSON.stringify(updatedProfiles, null, 2));

        // Update local state and reset dirty flag
        setLocalParsedProfiles(updatedProfiles);
        setLocalProfilesContent(JSON.stringify(updatedProfiles, null, 2));
        setIsDirty(false);

        // Select the next available configuration
        const nextConfigs = Object.keys(
          updatedProfiles.executors[selectedExecutorType]
        );
        const nextSelected = nextConfigs[0] || 'DEFAULT';
        setSelectedConfiguration(nextSelected);

        // Show success
        setProfilesSuccess(true);
        setTimeout(() => setProfilesSuccess(false), 3000);

        // Refresh global system so deleted configs are removed elsewhere
        reloadSystem();
      } catch (saveError: unknown) {
        console.error('Failed to save deletion to backend:', saveError);
        setSaveError(t('settings.agents.errors.deleteFailed'));
      }
    } catch (error) {
      console.error('Error deleting configuration:', error);
    }
  };

  const handleProfilesChange = (value: string) => {
    setLocalProfilesContent(value);
    setIsDirty(true);

    // Validate JSON on change
    if (value.trim()) {
      try {
        const parsed = JSON.parse(value);
        setLocalParsedProfiles(parsed);
      } catch (err) {
        // Invalid JSON, keep local content but clear parsed
        setLocalParsedProfiles(null);
      }
    }
  };

  const handleSaveProfiles = async () => {
    // Clear any previous errors
    setSaveError(null);

    try {
      const contentToSave =
        useFormEditor && localParsedProfiles
          ? JSON.stringify(localParsedProfiles, null, 2)
          : localProfilesContent;

      await saveProfiles(contentToSave);
      setProfilesSuccess(true);
      setIsDirty(false);
      setTimeout(() => setProfilesSuccess(false), 3000);

      // Update the local content if using form editor
      if (useFormEditor && localParsedProfiles) {
        setLocalProfilesContent(contentToSave);
      }

      // Refresh global system so new profiles are available elsewhere
      reloadSystem();
    } catch (err: unknown) {
      console.error('Failed to save profiles:', err);
      setSaveError(t('settings.agents.errors.saveFailed'));
    }
  };

  const handleExecutorConfigChange = (
    executorType: string,
    configuration: string,
    formData: unknown
  ) => {
    if (!localParsedProfiles || !localParsedProfiles.executors) return;

    // Update the parsed profiles with the new config
    const updatedProfiles = {
      ...localParsedProfiles,
      executors: {
        ...localParsedProfiles.executors,
        [executorType]: {
          ...localParsedProfiles.executors[executorType],
          [configuration]: {
            [executorType]: formData,
          },
        },
      },
    };

    markDirty(updatedProfiles);
  };

  const handleExecutorConfigSave = async (formData: unknown) => {
    if (!localParsedProfiles || !localParsedProfiles.executors) return;

    // Clear any previous errors
    setSaveError(null);

    // Update the parsed profiles with the saved config
    const updatedProfiles = {
      ...localParsedProfiles,
      executors: {
        ...localParsedProfiles.executors,
        [selectedExecutorType]: {
          ...localParsedProfiles.executors[selectedExecutorType],
          [selectedConfiguration]: {
            [selectedExecutorType]: formData,
          },
        },
      },
    };

    // Update state
    setLocalParsedProfiles(updatedProfiles);

    // Save the updated profiles directly
    try {
      const contentToSave = JSON.stringify(updatedProfiles, null, 2);

      await saveProfiles(contentToSave);
      setProfilesSuccess(true);
      setIsDirty(false);
      setTimeout(() => setProfilesSuccess(false), 3000);

      // Update the local content as well
      setLocalProfilesContent(contentToSave);

      // Refresh global system so new profiles are available elsewhere
      reloadSystem();
    } catch (err: unknown) {
      console.error('Failed to save profiles:', err);
      setSaveError(t('settings.agents.errors.saveConfigFailed'));
    }
  };

  if (profilesLoading) {
    return (
      <div className="flex items-center justify-center py-8">
        <Loader2 className="h-8 w-8 animate-spin" />
        <span className="ml-2">{t('settings.agents.loading')}</span>
      </div>
    );
  }

  return (
    <div className="space-y-6">
      {!!profilesError && (
        <Alert variant="destructive">
          <AlertDescription>
            {profilesError instanceof Error
              ? profilesError.message
              : String(profilesError)}
          </AlertDescription>
        </Alert>
      )}

      {profilesSuccess && (
        <Alert className="border-green-200 bg-green-50 text-green-800 dark:border-green-800 dark:bg-green-950 dark:text-green-200">
          <AlertDescription className="font-medium">
            {t('settings.agents.save.success')}
          </AlertDescription>
        </Alert>
      )}

      {saveError && (
        <Alert variant="destructive">
          <AlertDescription>{saveError}</AlertDescription>
        </Alert>
      )}

      <Card>
        <CardHeader>
          <CardTitle>{t('settings.agents.title')}</CardTitle>
          <CardDescription>{t('settings.agents.description')}</CardDescription>
        </CardHeader>
        <CardContent className="space-y-4">
          {/* Editor type toggle */}
          <div className="flex items-center space-x-2">
            <Checkbox
              id="use-form-editor"
              checked={!useFormEditor}
              onCheckedChange={(checked) => setUseFormEditor(!checked)}
              disabled={profilesLoading || !localParsedProfiles}
            />
            <Label htmlFor="use-form-editor">
              {t('settings.agents.editor.formLabel')}
            </Label>
          </div>

          {useFormEditor &&
          localParsedProfiles &&
          localParsedProfiles.executors ? (
            // Form-based editor
            <div className="space-y-4">
              <div className="grid grid-cols-2 gap-4">
                <div className="space-y-2">
                  <Label htmlFor="executor-type">
                    {t('settings.agents.editor.agentLabel')}
                  </Label>
                  <Select
                    value={selectedExecutorType}
                    onValueChange={(value) => {
                      setSelectedExecutorType(value);
                      // Reset configuration selection when executor type changes
                      setSelectedConfiguration('DEFAULT');
                    }}
                  >
                    <SelectTrigger id="executor-type">
                      <SelectValue
                        placeholder={t(
                          'settings.agents.editor.agentPlaceholder'
                        )}
                      />
                    </SelectTrigger>
                    <SelectContent>
                      {Object.keys(localParsedProfiles.executors).map(
                        (type) => (
                          <SelectItem key={type} value={type}>
                            {type}
                          </SelectItem>
                        )
                      )}
                    </SelectContent>
                  </Select>
                </div>

                <div className="space-y-2">
                  <Label htmlFor="configuration">
                    {t('settings.agents.editor.configLabel')}
                  </Label>
                  <div className="flex gap-2">
                    <Select
                      value={selectedConfiguration}
                      onValueChange={(value) => {
                        if (value === '__create__') {
                          openCreateDialog();
                        } else {
                          setSelectedConfiguration(value);
                        }
                      }}
                      disabled={
                        !localParsedProfiles.executors[selectedExecutorType]
                      }
                    >
                      <SelectTrigger id="configuration">
                        <SelectValue
                          placeholder={t(
                            'settings.agents.editor.configPlaceholder'
                          )}
                        />
                      </SelectTrigger>
                      <SelectContent>
                        {Object.keys(
                          localParsedProfiles.executors[selectedExecutorType] ||
                            {}
                        ).map((configuration) => (
                          <SelectItem key={configuration} value={configuration}>
                            {configuration}
                          </SelectItem>
                        ))}
                        <SelectItem value="__create__">
                          {t('settings.agents.editor.createNew')}
                        </SelectItem>
                      </SelectContent>
                    </Select>
                    <Button
                      variant="destructive"
                      size="sm"
                      className="h-10"
                      onClick={() => openDeleteDialog(selectedConfiguration)}
                      disabled={
                        profilesSaving ||
                        !localParsedProfiles.executors[selectedExecutorType] ||
                        Object.keys(
                          localParsedProfiles.executors[selectedExecutorType] ||
                            {}
                        ).length <= 1
                      }
                      title={
                        Object.keys(
                          localParsedProfiles.executors[selectedExecutorType] ||
                            {}
                        ).length <= 1
                          ? t('settings.agents.editor.deleteTitle')
                          : t('settings.agents.editor.deleteButton', {
                              name: selectedConfiguration,
                            })
                      }
                    >
                      {t('settings.agents.editor.deleteText')}
                    </Button>
                  </div>
                </div>
              </div>

              {localParsedProfiles.executors[selectedExecutorType]?.[
                selectedConfiguration
              ]?.[selectedExecutorType] && (
                <ExecutorConfigForm
                  executor={selectedExecutorType as any}
                  value={
                    localParsedProfiles.executors[selectedExecutorType][
                      selectedConfiguration
                    ][selectedExecutorType] || {}
                  }
                  onChange={(formData) =>
                    handleExecutorConfigChange(
                      selectedExecutorType,
                      selectedConfiguration,
                      formData
                    )
                  }
                  onSave={handleExecutorConfigSave}
                  disabled={profilesSaving}
                  isSaving={profilesSaving}
                  isDirty={isDirty}
                />
              )}
            </div>
          ) : (
            // Raw JSON editor
            <div className="space-y-4">
              <div className="space-y-2">
                <Label htmlFor="profiles-editor">
                  {t('settings.agents.editor.jsonLabel')}
                </Label>
                <JSONEditor
                  id="profiles-editor"
                  placeholder={t('settings.agents.editor.jsonPlaceholder')}
                  value={
                    profilesLoading
                      ? t('settings.agents.editor.jsonLoading')
                      : localProfilesContent
                  }
                  onChange={handleProfilesChange}
                  disabled={profilesLoading}
                  minHeight={300}
                />
              </div>

              {!profilesError && profilesPath && (
                <div className="space-y-2">
                  <p className="text-sm text-muted-foreground">
                    <span className="font-medium">
                      {t('settings.agents.editor.pathLabel')}
                    </span>{' '}
                    <span className="font-mono text-xs">{profilesPath}</span>
                  </p>
                </div>
              )}
            </div>
          )}
        </CardContent>
      </Card>

      {/* Sticky Save bar (used for both editors) */}
      <div className="sticky bottom-0 z-10 bg-background/80 backdrop-blur-sm border-t py-4">
        <div className="flex justify-end">
          <Button
            onClick={handleSaveProfiles}
            disabled={!isDirty || profilesSaving || !!profilesError}
          >
            {profilesSaving && (
              <Loader2 className="mr-2 h-4 w-4 animate-spin" />
            )}
            {t('settings.agents.save.button')}
          </Button>
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/pages/settings/GeneralSettings.tsx">
import { useCallback, useState } from 'react';
import { useTranslation } from 'react-i18next';
import {
  Card,
  CardContent,
  CardDescription,
  CardHeader,
  CardTitle,
} from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from '@/components/ui/select';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import { Label } from '@/components/ui/label';
import { Alert, AlertDescription } from '@/components/ui/alert';
import { Checkbox } from '@/components/ui/checkbox';
import { ChevronDown, Key, Loader2, Volume2 } from 'lucide-react';
import {
  BaseCodingAgent,
  EditorType,
  ExecutorProfileId,
  SoundFile,
  ThemeMode,
  UiLanguage,
} from 'shared/types';

import { toPrettyCase } from '@/utils/string';
import { useTheme } from '@/components/theme-provider';
import { useUserSystem } from '@/components/config-provider';
import { TaskTemplateManager } from '@/components/TaskTemplateManager';
import NiceModal from '@ebay/nice-modal-react';

export function GeneralSettings() {
  const { t } = useTranslation(['settings', 'common']);
  const {
    config,
    updateConfig,
    saveConfig,
    loading,
    updateAndSaveConfig,
    profiles,
  } = useUserSystem();
  const [saving, setSaving] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [success, setSuccess] = useState(false);
  const { setTheme } = useTheme();

  const playSound = async (soundFile: SoundFile) => {
    const audio = new Audio(`/api/sounds/${soundFile}`);
    try {
      await audio.play();
    } catch (err) {
      console.error('Failed to play sound:', err);
    }
  };

  const handleSave = async () => {
    if (!config) return;

    setSaving(true);
    setError(null);
    setSuccess(false);

    try {
      const success = await saveConfig();

      if (success) {
        setSuccess(true);
        setTheme(config.theme);
        setTimeout(() => setSuccess(false), 3000);
      } else {
        setError('Failed to save configuration');
      }
    } catch (err) {
      setError('Failed to save configuration');
      console.error('Error saving config:', err);
    } finally {
      setSaving(false);
    }
  };

  const resetDisclaimer = async () => {
    if (!config) return;
    updateAndSaveConfig({ disclaimer_acknowledged: false });
  };

  const resetOnboarding = async () => {
    if (!config) return;
    updateAndSaveConfig({ onboarding_acknowledged: false });
  };

  const isAuthenticated = !!(
    config?.github?.username && config?.github?.oauth_token
  );

  const handleLogout = useCallback(async () => {
    if (!config) return;
    updateAndSaveConfig({
      github: {
        ...config.github,
        oauth_token: null,
        username: null,
        primary_email: null,
      },
    });
  }, [config, updateAndSaveConfig]);

  if (loading) {
    return (
      <div className="flex items-center justify-center py-8">
        <Loader2 className="h-8 w-8 animate-spin" />
        <span className="ml-2">{t('settings.general.loading')}</span>
      </div>
    );
  }

  if (!config) {
    return (
      <div className="py-8">
        <Alert variant="destructive">
          <AlertDescription>{t('settings.general.loadError')}</AlertDescription>
        </Alert>
      </div>
    );
  }

  return (
    <div className="space-y-6">
      {error && (
        <Alert variant="destructive">
          <AlertDescription>{error}</AlertDescription>
        </Alert>
      )}

      {success && (
        <Alert className="border-green-200 bg-green-50 text-green-800 dark:border-green-800 dark:bg-green-950 dark:text-green-200">
          <AlertDescription className="font-medium">
            {t('settings.general.save.success')}
          </AlertDescription>
        </Alert>
      )}

      <Card>
        <CardHeader>
          <CardTitle>{t('settings.general.appearance.title')}</CardTitle>
          <CardDescription>
            {t('settings.general.appearance.description')}
          </CardDescription>
        </CardHeader>
        <CardContent className="space-y-4">
          <div className="space-y-2">
            <Label htmlFor="theme">
              {t('settings.general.appearance.theme.label')}
            </Label>
            <Select
              value={config.theme}
              onValueChange={(value: ThemeMode) =>
                updateConfig({ theme: value })
              }
            >
              <SelectTrigger id="theme">
                <SelectValue
                  placeholder={t(
                    'settings.general.appearance.theme.placeholder'
                  )}
                />
              </SelectTrigger>
              <SelectContent>
                {Object.values(ThemeMode).map((theme) => (
                  <SelectItem key={theme} value={theme}>
                    {toPrettyCase(theme)}
                  </SelectItem>
                ))}
              </SelectContent>
            </Select>
            <p className="text-sm text-muted-foreground">
              {t('settings.general.appearance.theme.helper')}
            </p>
          </div>

          <div className="space-y-2">
            <Label htmlFor="language">
              {t('settings.general.appearance.language.label')}
            </Label>
            <Select
              value={config.language}
              onValueChange={(value: UiLanguage) =>
                updateConfig({ language: value })
              }
            >
              <SelectTrigger id="language">
                <SelectValue
                  placeholder={t(
                    'settings.general.appearance.language.placeholder'
                  )}
                />
              </SelectTrigger>
              <SelectContent>
                <SelectItem value="BROWSER">
                  {t('language.browserDefault', {
                    ns: 'common',
                    defaultValue: 'Browser Default',
                  })}
                </SelectItem>
                <SelectItem value="EN">
                  {t('language.en', { ns: 'common', defaultValue: 'English' })}
                </SelectItem>
                <SelectItem value="JA">
                  {t('language.ja', { ns: 'common', defaultValue: '日本語' })}
                </SelectItem>
              </SelectContent>
            </Select>
            <p className="text-sm text-muted-foreground">
              {t('settings.general.appearance.language.helper')}
            </p>
          </div>
        </CardContent>
      </Card>

      <Card>
        <CardHeader>
          <CardTitle>{t('settings.general.taskExecution.title')}</CardTitle>
          <CardDescription>
            {t('settings.general.taskExecution.description')}
          </CardDescription>
        </CardHeader>
        <CardContent className="space-y-4">
          <div className="space-y-2">
            <Label htmlFor="executor">
              {t('settings.general.taskExecution.executor.label')}
            </Label>
            <div className="grid grid-cols-2 gap-2">
              <Select
                value={config.executor_profile?.executor ?? ''}
                onValueChange={(value: string) => {
                  const variants = profiles?.[value];
                  const keepCurrentVariant =
                    variants &&
                    config.executor_profile?.variant &&
                    variants[config.executor_profile.variant];

                  const newProfile: ExecutorProfileId = {
                    executor: value as BaseCodingAgent,
                    variant: keepCurrentVariant
                      ? config.executor_profile!.variant
                      : null,
                  };
                  updateConfig({
                    executor_profile: newProfile,
                  });
                }}
                disabled={!profiles}
              >
                <SelectTrigger id="executor">
                  <SelectValue placeholder="Select profile" />
                </SelectTrigger>
                <SelectContent>
                  {profiles &&
                    Object.entries(profiles)
                      .sort((a, b) => a[0].localeCompare(b[0]))
                      .map(([profileKey]) => (
                        <SelectItem key={profileKey} value={profileKey}>
                          {profileKey}
                        </SelectItem>
                      ))}
                </SelectContent>
              </Select>

              {/* Show variant selector if selected profile has variants */}
              {(() => {
                const currentProfileVariant = config.executor_profile;
                const selectedProfile =
                  profiles?.[currentProfileVariant?.executor || ''];
                const hasVariants =
                  selectedProfile && Object.keys(selectedProfile).length > 0;

                if (hasVariants) {
                  return (
                    <DropdownMenu>
                      <DropdownMenuTrigger asChild>
                        <Button
                          variant="outline"
                          className="w-full h-10 px-2 flex items-center justify-between"
                        >
                          <span className="text-sm truncate flex-1 text-left">
                            {currentProfileVariant?.variant || 'DEFAULT'}
                          </span>
                          <ChevronDown className="h-4 w-4 ml-1 flex-shrink-0" />
                        </Button>
                      </DropdownMenuTrigger>
                      <DropdownMenuContent>
                        {Object.entries(selectedProfile).map(
                          ([variantLabel]) => (
                            <DropdownMenuItem
                              key={variantLabel}
                              onClick={() => {
                                const newProfile: ExecutorProfileId = {
                                  executor: currentProfileVariant!.executor,
                                  variant: variantLabel,
                                };
                                updateConfig({
                                  executor_profile: newProfile,
                                });
                              }}
                              className={
                                currentProfileVariant?.variant === variantLabel
                                  ? 'bg-accent'
                                  : ''
                              }
                            >
                              {variantLabel}
                            </DropdownMenuItem>
                          )
                        )}
                      </DropdownMenuContent>
                    </DropdownMenu>
                  );
                } else if (selectedProfile) {
                  // Show disabled button when profile exists but has no variants
                  return (
                    <Button
                      variant="outline"
                      className="w-full h-10 px-2 flex items-center justify-between"
                      disabled
                    >
                      <span className="text-sm truncate flex-1 text-left">
                        Default
                      </span>
                    </Button>
                  );
                }
                return null;
              })()}
            </div>
            <p className="text-sm text-muted-foreground">
              {t('settings.general.taskExecution.executor.helper')}
            </p>
          </div>
        </CardContent>
      </Card>

      <Card>
        <CardHeader>
          <CardTitle>{t('settings.general.editor.title')}</CardTitle>
          <CardDescription>
            {t('settings.general.editor.description')}
          </CardDescription>
        </CardHeader>
        <CardContent className="space-y-4">
          <div className="space-y-2">
            <Label htmlFor="editor-type">
              {t('settings.general.editor.type.label')}
            </Label>
            <Select
              value={config.editor.editor_type}
              onValueChange={(value: EditorType) =>
                updateConfig({
                  editor: { ...config.editor, editor_type: value },
                })
              }
            >
              <SelectTrigger id="editor-type">
                <SelectValue
                  placeholder={t('settings.general.editor.type.placeholder')}
                />
              </SelectTrigger>
              <SelectContent>
                {Object.values(EditorType).map((editor) => (
                  <SelectItem key={editor} value={editor}>
                    {toPrettyCase(editor)}
                  </SelectItem>
                ))}
              </SelectContent>
            </Select>
            <p className="text-sm text-muted-foreground">
              {t('settings.general.editor.type.helper')}
            </p>
          </div>
        </CardContent>
      </Card>

      <Card>
        <CardHeader>
          <CardTitle className="flex items-center gap-2">
            <Key className="h-5 w-5" />
            {t('settings.general.github.title')}
          </CardTitle>
        </CardHeader>
        <CardContent className="space-y-4">
          {isAuthenticated ? (
            <div className="space-y-4">
              <div className="flex items-center justify-between p-4 border rounded-lg">
                <div>
                  <p className="font-medium">
                    {t('settings.general.github.connected', {
                      username: config.github.username,
                    })}
                  </p>
                  {config.github.primary_email && (
                    <p className="text-sm text-muted-foreground">
                      {config.github.primary_email}
                    </p>
                  )}
                </div>
                <DropdownMenu>
                  <DropdownMenuTrigger asChild>
                    <Button variant="outline" size="sm">
                      {t('settings.general.github.manage')}{' '}
                      <ChevronDown className="ml-1 h-4 w-4" />
                    </Button>
                  </DropdownMenuTrigger>
                  <DropdownMenuContent align="end">
                    <DropdownMenuItem onClick={handleLogout}>
                      {t('settings.general.github.disconnect')}
                    </DropdownMenuItem>
                  </DropdownMenuContent>
                </DropdownMenu>
              </div>
            </div>
          ) : (
            <div className="space-y-4">
              <p className="text-sm text-muted-foreground">
                {t('settings.general.github.helper')}
              </p>
              <Button
                onClick={() =>
                  NiceModal.show('github-login').finally(() =>
                    NiceModal.hide('github-login')
                  )
                }
              >
                {t('settings.general.github.connectButton')}
              </Button>
            </div>
          )}
        </CardContent>
      </Card>

      <Card>
        <CardHeader>
          <CardTitle>{t('settings.general.notifications.title')}</CardTitle>
          <CardDescription>
            {t('settings.general.notifications.description')}
          </CardDescription>
        </CardHeader>
        <CardContent className="space-y-4">
          <div className="flex items-center space-x-2">
            <Checkbox
              id="sound-enabled"
              checked={config.notifications.sound_enabled}
              onCheckedChange={(checked: boolean) =>
                updateConfig({
                  notifications: {
                    ...config.notifications,
                    sound_enabled: checked,
                  },
                })
              }
            />
            <div className="space-y-0.5">
              <Label htmlFor="sound-enabled" className="cursor-pointer">
                {t('settings.general.notifications.sound.label')}
              </Label>
              <p className="text-sm text-muted-foreground">
                {t('settings.general.notifications.sound.helper')}
              </p>
            </div>
          </div>
          {config.notifications.sound_enabled && (
            <div className="ml-6 space-y-2">
              <Label htmlFor="sound-file">
                {t('settings.general.notifications.sound.fileLabel')}
              </Label>
              <div className="flex gap-2">
                <Select
                  value={config.notifications.sound_file}
                  onValueChange={(value: SoundFile) =>
                    updateConfig({
                      notifications: {
                        ...config.notifications,
                        sound_file: value,
                      },
                    })
                  }
                >
                  <SelectTrigger id="sound-file" className="flex-1">
                    <SelectValue
                      placeholder={t(
                        'settings.general.notifications.sound.filePlaceholder'
                      )}
                    />
                  </SelectTrigger>
                  <SelectContent>
                    {Object.values(SoundFile).map((soundFile) => (
                      <SelectItem key={soundFile} value={soundFile}>
                        {toPrettyCase(soundFile)}
                      </SelectItem>
                    ))}
                  </SelectContent>
                </Select>
                <Button
                  variant="outline"
                  size="sm"
                  onClick={() => playSound(config.notifications.sound_file)}
                  className="px-3"
                >
                  <Volume2 className="h-4 w-4" />
                </Button>
              </div>
              <p className="text-sm text-muted-foreground">
                {t('settings.general.notifications.sound.fileHelper')}
              </p>
            </div>
          )}
          <div className="flex items-center space-x-2">
            <Checkbox
              id="push-notifications"
              checked={config.notifications.push_enabled}
              onCheckedChange={(checked: boolean) =>
                updateConfig({
                  notifications: {
                    ...config.notifications,
                    push_enabled: checked,
                  },
                })
              }
            />
            <div className="space-y-0.5">
              <Label htmlFor="push-notifications" className="cursor-pointer">
                {t('settings.general.notifications.push.label')}
              </Label>
              <p className="text-sm text-muted-foreground">
                {t('settings.general.notifications.push.helper')}
              </p>
            </div>
          </div>
        </CardContent>
      </Card>

      <Card>
        <CardHeader>
          <CardTitle>{t('settings.general.privacy.title')}</CardTitle>
          <CardDescription>
            {t('settings.general.privacy.description')}
          </CardDescription>
        </CardHeader>
        <CardContent className="space-y-4">
          <div className="flex items-center space-x-2">
            <Checkbox
              id="analytics-enabled"
              checked={config.analytics_enabled ?? false}
              onCheckedChange={(checked: boolean) =>
                updateConfig({ analytics_enabled: checked })
              }
            />
            <div className="space-y-0.5">
              <Label htmlFor="analytics-enabled" className="cursor-pointer">
                {t('settings.general.privacy.telemetry.label')}
              </Label>
              <p className="text-sm text-muted-foreground">
                {t('settings.general.privacy.telemetry.helper')}
              </p>
            </div>
          </div>
        </CardContent>
      </Card>

      <Card>
        <CardHeader>
          <CardTitle>{t('settings.general.taskTemplates.title')}</CardTitle>
          <CardDescription>
            {t('settings.general.taskTemplates.description')}
          </CardDescription>
        </CardHeader>
        <CardContent>
          <TaskTemplateManager isGlobal={true} />
        </CardContent>
      </Card>

      <Card>
        <CardHeader>
          <CardTitle>{t('settings.general.safety.title')}</CardTitle>
          <CardDescription>
            {t('settings.general.safety.description')}
          </CardDescription>
        </CardHeader>
        <CardContent className="space-y-4">
          <div className="flex items-center justify-between">
            <div>
              <p className="font-medium">
                {t('settings.general.safety.disclaimer.title')}
              </p>
              <p className="text-sm text-muted-foreground">
                {t('settings.general.safety.disclaimer.description')}
              </p>
            </div>
            <Button variant="outline" onClick={resetDisclaimer}>
              {t('settings.general.safety.disclaimer.button')}
            </Button>
          </div>
          <div className="flex items-center justify-between">
            <div>
              <p className="font-medium">
                {t('settings.general.safety.onboarding.title')}
              </p>
              <p className="text-sm text-muted-foreground">
                {t('settings.general.safety.onboarding.description')}
              </p>
            </div>
            <Button variant="outline" onClick={resetOnboarding}>
              {t('settings.general.safety.onboarding.button')}
            </Button>
          </div>
        </CardContent>
      </Card>

      {/* Sticky Save Button */}
      <div className="sticky bottom-0 z-10 bg-background/80 backdrop-blur-sm border-t py-4">
        <div className="flex justify-end">
          <Button onClick={handleSave} disabled={saving}>
            {saving && <Loader2 className="mr-2 h-4 w-4 animate-spin" />}
            {t('settings.general.save.button')}
          </Button>
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/pages/settings/index.ts">
export { SettingsLayout } from './SettingsLayout';
export { GeneralSettings } from './GeneralSettings';
export { AgentSettings } from './AgentSettings';
export { McpSettings } from './McpSettings';
</file>

<file path="frontend/src/pages/settings/McpSettings.tsx">
import { useEffect, useState } from 'react';
import { useTranslation } from 'react-i18next';
import {
  Card,
  CardContent,
  CardDescription,
  CardHeader,
  CardTitle,
} from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from '@/components/ui/select';
import {
  Carousel,
  CarouselContent,
  CarouselItem,
  CarouselNext,
  CarouselPrevious,
} from '@/components/ui/carousel';
import { Label } from '@/components/ui/label';
import { Alert, AlertDescription } from '@/components/ui/alert';
import { JSONEditor } from '@/components/ui/json-editor';
import { Loader2 } from 'lucide-react';
import type { BaseCodingAgent, ExecutorConfig } from 'shared/types';
import { McpConfig } from 'shared/types';
import { useUserSystem } from '@/components/config-provider';
import { mcpServersApi } from '@/lib/api';
import { McpConfigStrategyGeneral } from '@/lib/mcp-strategies';

export function McpSettings() {
  const { t } = useTranslation('settings');
  const { config, profiles } = useUserSystem();
  const [mcpServers, setMcpServers] = useState('{}');
  const [mcpConfig, setMcpConfig] = useState<McpConfig | null>(null);
  const [mcpError, setMcpError] = useState<string | null>(null);
  const [mcpLoading, setMcpLoading] = useState(true);
  const [selectedProfile, setSelectedProfile] = useState<ExecutorConfig | null>(
    null
  );
  const [mcpApplying, setMcpApplying] = useState(false);
  const [mcpConfigPath, setMcpConfigPath] = useState<string>('');
  const [success, setSuccess] = useState(false);

  // Initialize selected profile when config loads
  useEffect(() => {
    if (config?.executor_profile && profiles && !selectedProfile) {
      // Find the current profile
      const currentProfile = profiles[config.executor_profile.executor];
      if (currentProfile) {
        setSelectedProfile(currentProfile);
      } else if (Object.keys(profiles).length > 0) {
        // Default to first profile if current profile not found
        setSelectedProfile(Object.values(profiles)[0]);
      }
    }
  }, [config?.executor_profile, profiles, selectedProfile]);

  // Load existing MCP configuration when selected profile changes
  useEffect(() => {
    const loadMcpServersForProfile = async (profile: ExecutorConfig) => {
      // Reset state when loading
      setMcpLoading(true);
      setMcpError(null);
      // Set default empty config based on agent type using strategy
      setMcpConfigPath('');

      try {
        // Load MCP servers for the selected profile/agent
        // Find the key for this profile
        const profileKey = profiles
          ? Object.keys(profiles).find((key) => profiles[key] === profile)
          : null;
        if (!profileKey) {
          throw new Error('Profile key not found');
        }

        const result = await mcpServersApi.load({
          executor: profileKey as BaseCodingAgent,
        });
        // Store the McpConfig from backend
        setMcpConfig(result.mcp_config);
        // Create the full configuration structure using the schema
        const fullConfig = McpConfigStrategyGeneral.createFullConfig(
          result.mcp_config
        );
        const configJson = JSON.stringify(fullConfig, null, 2);
        setMcpServers(configJson);
        setMcpConfigPath(result.config_path);
      } catch (err: any) {
        if (err?.message && err.message.includes('does not support MCP')) {
          setMcpError(err.message);
        } else {
          console.error('Error loading MCP servers:', err);
        }
      } finally {
        setMcpLoading(false);
      }
    };

    // Load MCP servers for the selected profile
    if (selectedProfile) {
      loadMcpServersForProfile(selectedProfile);
    }
  }, [selectedProfile]);

  const handleMcpServersChange = (value: string) => {
    setMcpServers(value);
    setMcpError(null);

    // Validate JSON on change
    if (value.trim() && mcpConfig) {
      try {
        const parsedConfig = JSON.parse(value);
        // Validate using the schema path from backend
        McpConfigStrategyGeneral.validateFullConfig(mcpConfig, parsedConfig);
      } catch (err) {
        if (err instanceof SyntaxError) {
          setMcpError(t('settings.mcp.errors.invalidJson'));
        } else {
          setMcpError(
            err instanceof Error
              ? err.message
              : t('settings.mcp.errors.validationError')
          );
        }
      }
    }
  };

  const handleApplyMcpServers = async () => {
    if (!selectedProfile || !mcpConfig) return;

    setMcpApplying(true);
    setMcpError(null);

    try {
      // Validate and save MCP configuration
      if (mcpServers.trim()) {
        try {
          const fullConfig = JSON.parse(mcpServers);
          McpConfigStrategyGeneral.validateFullConfig(mcpConfig, fullConfig);
          const mcpServersConfig =
            McpConfigStrategyGeneral.extractServersForApi(
              mcpConfig,
              fullConfig
            );

          // Find the key for the selected profile
          const selectedProfileKey = profiles
            ? Object.keys(profiles).find(
                (key) => profiles[key] === selectedProfile
              )
            : null;
          if (!selectedProfileKey) {
            throw new Error('Selected profile key not found');
          }

          await mcpServersApi.save(
            {
              executor: selectedProfileKey as BaseCodingAgent,
            },
            { servers: mcpServersConfig }
          );

          // Show success feedback
          setSuccess(true);
          setTimeout(() => setSuccess(false), 3000);
        } catch (mcpErr) {
          if (mcpErr instanceof SyntaxError) {
            setMcpError(t('settings.mcp.errors.invalidJson'));
          } else {
            setMcpError(
              mcpErr instanceof Error
                ? mcpErr.message
                : t('settings.mcp.errors.saveFailed')
            );
          }
        }
      }
    } catch (err) {
      setMcpError(t('settings.mcp.errors.applyFailed'));
      console.error('Error applying MCP servers:', err);
    } finally {
      setMcpApplying(false);
    }
  };

  const addServer = (key: string) => {
    try {
      const existing = mcpServers.trim() ? JSON.parse(mcpServers) : {};
      const updated = McpConfigStrategyGeneral.addPreconfiguredToConfig(
        mcpConfig!,
        existing,
        key
      );
      setMcpServers(JSON.stringify(updated, null, 2));
      setMcpError(null);
    } catch (err) {
      console.error(err);
      setMcpError(
        err instanceof Error
          ? err.message
          : t('settings.mcp.errors.addServerFailed')
      );
    }
  };

  const preconfigured = (mcpConfig?.preconfigured ?? {}) as Record<string, any>;
  const meta = (preconfigured.meta ?? {}) as Record<
    string,
    { name?: string; description?: string; url?: string; icon?: string }
  >;
  const servers = Object.fromEntries(
    Object.entries(preconfigured).filter(([k]) => k !== 'meta')
  ) as Record<string, any>;
  const getMetaFor = (key: string) => meta[key] || {};

  if (!config) {
    return (
      <div className="py-8">
        <Alert variant="destructive">
          <AlertDescription>
            {t('settings.mcp.errors.loadFailed')}
          </AlertDescription>
        </Alert>
      </div>
    );
  }

  return (
    <div className="space-y-6">
      {mcpError && (
        <Alert variant="destructive">
          <AlertDescription>
            {t('settings.mcp.errors.mcpError', { error: mcpError })}
          </AlertDescription>
        </Alert>
      )}

      {success && (
        <Alert className="border-green-200 bg-green-50 text-green-800 dark:border-green-800 dark:bg-green-950 dark:text-green-200">
          <AlertDescription className="font-medium">
            {t('settings.mcp.save.successMessage')}
          </AlertDescription>
        </Alert>
      )}

      <Card>
        <CardHeader>
          <CardTitle>{t('settings.mcp.title')}</CardTitle>
          <CardDescription>{t('settings.mcp.description')}</CardDescription>
        </CardHeader>
        <CardContent className="space-y-4">
          <div className="space-y-2">
            <Label htmlFor="mcp-executor">
              {t('settings.mcp.labels.agent')}
            </Label>
            <Select
              value={
                selectedProfile
                  ? Object.keys(profiles || {}).find(
                      (key) => profiles![key] === selectedProfile
                    ) || ''
                  : ''
              }
              onValueChange={(value: string) => {
                const profile = profiles?.[value];
                if (profile) setSelectedProfile(profile);
              }}
            >
              <SelectTrigger id="mcp-executor">
                <SelectValue
                  placeholder={t('settings.mcp.labels.agentPlaceholder')}
                />
              </SelectTrigger>
              <SelectContent>
                {profiles &&
                  Object.entries(profiles)
                    .sort((a, b) => a[0].localeCompare(b[0]))
                    .map(([profileKey]) => (
                      <SelectItem key={profileKey} value={profileKey}>
                        {profileKey}
                      </SelectItem>
                    ))}
              </SelectContent>
            </Select>
            <p className="text-sm text-muted-foreground">
              {t('settings.mcp.labels.agentHelper')}
            </p>
          </div>

          {mcpError && mcpError.includes('does not support MCP') ? (
            <div className="rounded-lg border border-amber-200 bg-amber-50 p-4 dark:border-amber-800 dark:bg-amber-950">
              <div className="flex">
                <div className="ml-3">
                  <h3 className="text-sm font-medium text-amber-800 dark:text-amber-200">
                    {t('settings.mcp.errors.notSupported')}
                  </h3>
                  <div className="mt-2 text-sm text-amber-700 dark:text-amber-300">
                    <p>{mcpError}</p>
                    <p className="mt-1">
                      {t('settings.mcp.errors.supportMessage')}
                    </p>
                  </div>
                </div>
              </div>
            </div>
          ) : (
            <div className="space-y-2">
              <Label htmlFor="mcp-servers">
                {t('settings.mcp.labels.serverConfig')}
              </Label>
              <JSONEditor
                id="mcp-servers"
                placeholder={
                  mcpLoading
                    ? t('settings.mcp.save.loading')
                    : '{\n  "server-name": {\n    "type": "stdio",\n    "command": "your-command",\n    "args": ["arg1", "arg2"]\n  }\n}'
                }
                value={
                  mcpLoading ? t('settings.mcp.loading.jsonEditor') : mcpServers
                }
                onChange={handleMcpServersChange}
                disabled={mcpLoading}
                minHeight={300}
              />
              {mcpError && !mcpError.includes('does not support MCP') && (
                <p className="text-sm text-destructive dark:text-red-400">
                  {mcpError}
                </p>
              )}
              <div className="text-sm text-muted-foreground">
                {mcpLoading ? (
                  t('settings.mcp.loading.configuration')
                ) : (
                  <span>
                    {t('settings.mcp.labels.saveLocation')}
                    {mcpConfigPath && (
                      <span className="ml-2 font-mono text-xs">
                        {mcpConfigPath}
                      </span>
                    )}
                  </span>
                )}
              </div>

              {mcpConfig?.preconfigured &&
                typeof mcpConfig.preconfigured === 'object' && (
                  <div className="pt-4">
                    <Label>{t('settings.mcp.labels.popularServers')}</Label>
                    <p className="text-sm text-muted-foreground mb-2">
                      {t('settings.mcp.labels.serverHelper')}
                    </p>

                    <div className="relative overflow-hidden rounded-xl border bg-background">
                      <Carousel className="w-full px-4 py-3">
                        <CarouselContent className="gap-3 justify-center">
                          {Object.entries(servers).map(([key]) => {
                            const metaObj = getMetaFor(key) as {
                              name?: string;
                              description?: string;
                              url?: string;
                              icon?: string;
                            };
                            const name = metaObj.name || key;
                            const description =
                              metaObj.description || 'No description';
                            const icon = metaObj.icon
                              ? `/${metaObj.icon}`
                              : null;

                            return (
                              <CarouselItem
                                key={name}
                                className="sm:basis-1/3 lg:basis-1/4"
                              >
                                <button
                                  type="button"
                                  onClick={() => addServer(key)}
                                  aria-label={`Add ${name} to config`}
                                  className="group w-full text-left outline-none"
                                >
                                  <Card className="h-32 rounded-xl border hover:shadow-md transition">
                                    <CardHeader className="pb-0">
                                      <div className="flex items-center gap-3">
                                        <div className="w-6 h-6 rounded-lg border bg-muted grid place-items-center overflow-hidden">
                                          {icon ? (
                                            <img
                                              src={icon}
                                              alt=""
                                              className="w-full h-full object-cover"
                                            />
                                          ) : (
                                            <span className="font-semibold">
                                              {name.slice(0, 1).toUpperCase()}
                                            </span>
                                          )}
                                        </div>
                                        <CardTitle className="text-base font-medium truncate">
                                          {name}
                                        </CardTitle>
                                      </div>
                                    </CardHeader>

                                    <CardContent className="pt-2 px-4">
                                      <p className="text-sm text-muted-foreground line-clamp-3">
                                        {description}
                                      </p>
                                    </CardContent>
                                  </Card>
                                </button>
                              </CarouselItem>
                            );
                          })}
                        </CarouselContent>

                        <CarouselPrevious className="left-2 top-1/2 -translate-y-1/2 h-8 w-8 rounded-full border bg-background/80 shadow-sm backdrop-blur hover:bg-background" />
                        <CarouselNext className="right-2 top-1/2 -translate-y-1/2 h-8 w-8 rounded-full border bg-background/80 shadow-sm backdrop-blur hover:bg-background" />
                      </Carousel>
                    </div>
                  </div>
                )}
            </div>
          )}
        </CardContent>
      </Card>

      {/* Sticky Save Button */}
      <div className="sticky bottom-0 z-10 bg-background/80 backdrop-blur-sm border-t py-4">
        <div className="flex justify-end">
          <Button
            onClick={handleApplyMcpServers}
            disabled={mcpApplying || mcpLoading || !!mcpError || success}
            className={success ? 'bg-green-600 hover:bg-green-700' : ''}
          >
            {mcpApplying && <Loader2 className="mr-2 h-4 w-4 animate-spin" />}
            {success && <span className="mr-2">✓</span>}
            {success
              ? t('settings.mcp.save.success')
              : t('settings.mcp.save.button')}
          </Button>
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/pages/settings/SettingsLayout.tsx">
import { NavLink, Outlet } from 'react-router-dom';
import { useTranslation } from 'react-i18next';
import { Settings, Cpu, Server } from 'lucide-react';
import { cn } from '@/lib/utils';

const settingsNavigation = [
  {
    path: 'general',
    icon: Settings,
  },
  {
    path: 'agents',
    icon: Cpu,
  },
  {
    path: 'mcp',
    icon: Server,
  },
];

export function SettingsLayout() {
  const { t } = useTranslation('settings');

  return (
    <div className="container mx-auto px-4 py-8">
      <div className="flex flex-col lg:flex-row gap-8">
        {/* Sidebar Navigation */}
        <aside className="w-full lg:w-64 lg:shrink-0 lg:sticky lg:top-8 lg:h-fit lg:max-h-[calc(100vh-4rem)] lg:overflow-y-auto">
          <div className="space-y-1">
            <h2 className="px-3 py-2 text-lg font-semibold">
              {t('settings.layout.nav.title')}
            </h2>
            <nav className="space-y-1">
              {settingsNavigation.map((item) => {
                const Icon = item.icon;
                return (
                  <NavLink
                    key={item.path}
                    to={item.path}
                    end
                    className={({ isActive }) =>
                      cn(
                        'flex items-start gap-3 px-3 py-2 text-sm transition-colors',
                        'hover:text-accent-foreground',
                        isActive
                          ? 'text-primary-foreground'
                          : 'text-secondary-foreground'
                      )
                    }
                  >
                    <Icon className="h-4 w-4 mt-0.5 shrink-0" />
                    <div className="flex-1 min-w-0">
                      <div className="font-medium">
                        {t(`settings.layout.nav.${item.path}`)}
                      </div>
                      <div>{t(`settings.layout.nav.${item.path}Desc`)}</div>
                    </div>
                  </NavLink>
                );
              })}
            </nav>
          </div>
        </aside>

        {/* Main Content */}
        <main className="flex-1 min-w-0">
          <Outlet />
        </main>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/pages/project-tasks.tsx">
import { useCallback, useEffect, useState, useMemo } from 'react';
import { useNavigate, useParams } from 'react-router-dom';
import { Button } from '@/components/ui/button';
import { Card, CardContent } from '@/components/ui/card';
import { AlertTriangle, Plus } from 'lucide-react';
import { Loader } from '@/components/ui/loader';
import { projectsApi, tasksApi, attemptsApi } from '@/lib/api';
import { openTaskForm } from '@/lib/openTaskForm';
import { useKeyboardShortcuts } from '@/lib/keyboard-shortcuts';
import { useSearch } from '@/contexts/search-context';
import { useQuery } from '@tanstack/react-query';
import { useTaskViewManager } from '@/hooks/useTaskViewManager';

import {
  getKanbanSectionClasses,
  getMainContainerClasses,
} from '@/lib/responsive-config';

import TaskKanbanBoard from '@/components/tasks/TaskKanbanBoard';
import { TaskDetailsPanel } from '@/components/tasks/TaskDetailsPanel';
import type { TaskWithAttemptStatus, Project, TaskAttempt } from 'shared/types';
import type { DragEndEvent } from '@/components/ui/shadcn-io/kanban';
import { useProjectTasks } from '@/hooks/useProjectTasks';
import { Alert, AlertDescription, AlertTitle } from '@/components/ui/alert';
import NiceModal from '@ebay/nice-modal-react';

type Task = TaskWithAttemptStatus;

export function ProjectTasks() {
  const { projectId, taskId, attemptId } = useParams<{
    projectId: string;
    taskId?: string;
    attemptId?: string;
  }>();
  const navigate = useNavigate();

  const [project, setProject] = useState<Project | null>(null);
  const [error, setError] = useState<string | null>(null);
  // Helper functions to open task forms
  const handleCreateTask = () => {
    if (project?.id) {
      openTaskForm({ projectId: project.id });
    }
  };

  const handleEditTask = (task: Task) => {
    if (project?.id) {
      openTaskForm({ projectId: project.id, task });
    }
  };

  const handleDuplicateTask = (task: Task) => {
    if (project?.id) {
      openTaskForm({ projectId: project.id, initialTask: task });
    }
  };
  const { query: searchQuery } = useSearch();

  // Panel state
  const [selectedTask, setSelectedTask] = useState<Task | null>(null);
  const [isPanelOpen, setIsPanelOpen] = useState(false);

  // Fullscreen state using custom hook
  const { isFullscreen, navigateToTask, navigateToAttempt } =
    useTaskViewManager();

  // Attempts fetching (only when task is selected)
  const { data: attempts = [] } = useQuery({
    queryKey: ['taskAttempts', selectedTask?.id],
    queryFn: () => attemptsApi.getAll(selectedTask!.id),
    enabled: !!selectedTask?.id,
    refetchInterval: 5000,
  });

  // Selected attempt logic
  const selectedAttempt = useMemo(() => {
    if (!attempts.length) return null;
    if (attemptId) {
      const found = attempts.find((a) => a.id === attemptId);
      if (found) return found;
    }
    return attempts[0] || null; // Most recent fallback
  }, [attempts, attemptId]);

  // Navigation callback for attempt selection
  const setSelectedAttempt = useCallback(
    (attempt: TaskAttempt | null) => {
      if (!selectedTask) return;

      if (attempt) {
        navigateToAttempt(projectId!, selectedTask.id, attempt.id);
      } else {
        navigateToTask(projectId!, selectedTask.id);
      }
    },
    [navigateToTask, navigateToAttempt, projectId, selectedTask]
  );

  // Stream tasks for this project
  const {
    tasks,
    tasksById,
    isLoading,
    error: streamError,
  } = useProjectTasks(projectId || '');

  // Sync selectedTask with URL params and live task updates
  useEffect(() => {
    if (taskId) {
      const t = taskId ? tasksById[taskId] : undefined;
      if (t) {
        setSelectedTask(t);
        setIsPanelOpen(true);
      }
    } else {
      setSelectedTask(null);
      setIsPanelOpen(false);
    }
  }, [taskId, tasksById]);

  // Define task creation handler
  const handleCreateNewTask = useCallback(() => {
    handleCreateTask();
  }, [handleCreateTask]);

  // Full screen

  const fetchProject = useCallback(async () => {
    try {
      const result = await projectsApi.getById(projectId!);
      setProject(result);
    } catch (err) {
      setError('Failed to load project');
    }
  }, [projectId]);

  const handleClosePanel = useCallback(() => {
    // setIsPanelOpen(false);
    // setSelectedTask(null);
    // Remove task ID from URL when closing panel
    navigate(`/projects/${projectId}/tasks`, { replace: true });
  }, [projectId, navigate]);

  const handleDeleteTask = useCallback(
    (taskId: string) => {
      const task = tasksById[taskId];
      if (task) {
        NiceModal.show('delete-task-confirmation', {
          task,
          projectId: projectId!,
        })
          .then(() => {
            // Task was deleted, close panel if this task was selected
            if (selectedTask?.id === taskId) {
              handleClosePanel();
            }
          })
          .catch(() => {
            // Modal was cancelled - do nothing
          });
      }
    },
    [tasksById, projectId, selectedTask, handleClosePanel]
  );

  const handleEditTaskCallback = useCallback(
    (task: Task) => {
      handleEditTask(task);
    },
    [handleEditTask]
  );

  const handleDuplicateTaskCallback = useCallback(
    (task: Task) => {
      handleDuplicateTask(task);
    },
    [handleDuplicateTask]
  );

  const handleViewTaskDetails = useCallback(
    (task: Task, attemptIdToShow?: string, fullscreen?: boolean) => {
      if (attemptIdToShow) {
        navigateToAttempt(projectId!, task.id, attemptIdToShow, { fullscreen });
      } else {
        navigateToTask(projectId!, task.id, { fullscreen });
      }
    },
    [projectId, navigateToTask, navigateToAttempt]
  );

  const handleDragEnd = useCallback(
    async (event: DragEndEvent) => {
      const { active, over } = event;
      if (!over || !active.data.current) return;

      const draggedTaskId = active.id as string;
      const newStatus = over.id as Task['status'];
      const task = tasksById[draggedTaskId];
      if (!task || task.status === newStatus) return;

      try {
        await tasksApi.update(draggedTaskId, {
          title: task.title,
          description: task.description,
          status: newStatus,
          parent_task_attempt: task.parent_task_attempt,
          image_ids: null,
        });
        // UI will update via WebSocket stream
      } catch (err) {
        setError('Failed to update task status');
      }
    },
    [tasksById]
  );

  // Setup keyboard shortcuts
  useKeyboardShortcuts({
    navigate,
    currentPath: window.location.pathname,
    hasOpenDialog: false,
    closeDialog: () => {},
    onC: handleCreateNewTask,
  });

  // Initialize project when projectId changes
  useEffect(() => {
    if (projectId) {
      fetchProject();
    }
  }, [projectId, fetchProject]);

  // Remove legacy direct-navigation handler; live sync above covers this

  if (isLoading) {
    return <Loader message="Loading tasks..." size={32} className="py-8" />;
  }

  if (error) {
    return (
      <div className="p-4">
        <Alert>
          <AlertTitle className="flex items-center gap-2">
            <AlertTriangle size="16" />
            Error
          </AlertTitle>
          <AlertDescription>{error}</AlertDescription>
        </Alert>
      </div>
    );
  }

  return (
    <div
      className={`min-h-full ${getMainContainerClasses(isPanelOpen, isFullscreen)}`}
    >
      {streamError && (
        <Alert className="w-full z-30 xl:sticky xl:top-0">
          <AlertTitle className="flex items-center gap-2">
            <AlertTriangle size="16" />
            Reconnecting
          </AlertTitle>
          <AlertDescription>{streamError}</AlertDescription>
        </Alert>
      )}

      {/* Kanban + Panel Container - uses side-by-side layout on xl+ */}
      <div className="flex-1 min-h-0 xl:flex">
        {/* Left Column - Kanban Section */}
        <div className={getKanbanSectionClasses(isPanelOpen, isFullscreen)}>
          {tasks.length === 0 ? (
            <div className="max-w-7xl mx-auto mt-8">
              <Card>
                <CardContent className="text-center py-8">
                  <p className="text-muted-foreground">
                    No tasks found for this project.
                  </p>
                  <Button className="mt-4" onClick={handleCreateNewTask}>
                    <Plus className="h-4 w-4 mr-2" />
                    Create First Task
                  </Button>
                </CardContent>
              </Card>
            </div>
          ) : (
            <div className="w-full h-full overflow-x-auto">
              <TaskKanbanBoard
                tasks={tasks}
                searchQuery={searchQuery}
                onDragEnd={handleDragEnd}
                onEditTask={handleEditTaskCallback}
                onDeleteTask={handleDeleteTask}
                onDuplicateTask={handleDuplicateTaskCallback}
                onViewTaskDetails={handleViewTaskDetails}
                isPanelOpen={isPanelOpen}
              />
            </div>
          )}
        </div>

        {/* Right Column - Task Details Panel */}
        {isPanelOpen && (
          <TaskDetailsPanel
            task={selectedTask}
            projectHasDevScript={!!project?.dev_script}
            projectId={projectId!}
            onClose={handleClosePanel}
            onEditTask={handleEditTaskCallback}
            onDeleteTask={handleDeleteTask}
            onNavigateToTask={(taskId) => {
              const task = tasksById[taskId];
              if (task) {
                handleViewTaskDetails(task, undefined, true);
              }
            }}
            isFullScreen={isFullscreen}
            selectedAttempt={selectedAttempt}
            attempts={attempts}
            setSelectedAttempt={setSelectedAttempt}
            tasksById={tasksById}
          />
        )}
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/pages/projects.tsx">
import { useParams, useNavigate } from 'react-router-dom';
import { ProjectList } from '@/components/projects/project-list';
import { ProjectDetail } from '@/components/projects/project-detail';

export function Projects() {
  const { projectId } = useParams<{ projectId: string }>();
  const navigate = useNavigate();

  const handleBack = () => {
    navigate('/projects');
  };

  if (projectId) {
    return <ProjectDetail projectId={projectId} onBack={handleBack} />;
  }

  return <ProjectList />;
}
</file>

<file path="frontend/src/stores/useDiffViewStore.ts">
import { create } from 'zustand';

export type DiffViewMode = 'unified' | 'split';

type State = {
  mode: DiffViewMode;
  setMode: (mode: DiffViewMode) => void;
  toggle: () => void;
};

export const useDiffViewStore = create<State>((set) => ({
  mode: 'unified',
  setMode: (mode) => set({ mode }),
  toggle: () =>
    set((s) => ({ mode: s.mode === 'unified' ? 'split' : 'unified' })),
}));

export const useDiffViewMode = () => useDiffViewStore((s) => s.mode);
export const useToggleDiffViewMode = () => useDiffViewStore((s) => s.toggle);
</file>

<file path="frontend/src/stores/useExpandableStore.ts">
import { create } from 'zustand';

type State = {
  expanded: Record<string, boolean>;
  setKey: (key: string, value: boolean) => void;
  toggleKey: (key: string, fallback?: boolean) => void;
  clear: () => void;
};

export const useExpandableStore = create<State>((set) => ({
  expanded: {},
  setKey: (key, value) =>
    set((s) =>
      s.expanded[key] === value
        ? s
        : { expanded: { ...s.expanded, [key]: value } }
    ),
  toggleKey: (key, fallback = false) =>
    set((s) => {
      const next = !(s.expanded[key] ?? fallback);
      return { expanded: { ...s.expanded, [key]: next } };
    }),
  clear: () => set({ expanded: {} }),
}));

export function useExpandable(
  key: string,
  defaultValue = false
): [boolean, (next?: boolean) => void] {
  const expandedValue = useExpandableStore((s) => s.expanded[key]);
  const setKey = useExpandableStore((s) => s.setKey);
  const toggleKey = useExpandableStore((s) => s.toggleKey);

  const set = (next?: boolean) => {
    if (typeof next === 'boolean') setKey(key, next);
    else toggleKey(key, defaultValue);
  };

  return [expandedValue ?? defaultValue, set];
}
</file>

<file path="frontend/src/stores/useTaskDetailsUiStore.ts">
import { create } from 'zustand';

interface TaskUiState {
  loading: boolean;
  isStopping: boolean;
  deletingFiles: Set<string>;
  fileToDelete: string | null;
  // Additional UI state can be added here
}

interface UiStateMap {
  [taskId: string]: TaskUiState;
}

interface TaskDetailsUiStore {
  ui: UiStateMap;
  getUiState: (taskId: string) => TaskUiState;
  setUiState: (taskId: string, partial: Partial<TaskUiState>) => void;
  clearUiState: (taskId: string) => void;
}

const defaultUiState: TaskUiState = {
  loading: false,
  isStopping: false,
  deletingFiles: new Set(),
  fileToDelete: null,
};

export const useTaskDetailsUiStore = create<TaskDetailsUiStore>((set, get) => ({
  ui: {},

  getUiState: (taskId: string) => {
    return get().ui[taskId] ?? defaultUiState;
  },

  setUiState: (taskId: string, partial: Partial<TaskUiState>) => {
    set((state) => ({
      ui: {
        ...state.ui,
        [taskId]: {
          ...defaultUiState,
          ...state.ui[taskId],
          ...partial,
          // Handle Set immutability for deletingFiles
          deletingFiles: partial.deletingFiles
            ? new Set(partial.deletingFiles)
            : (state.ui[taskId]?.deletingFiles ?? new Set()),
        },
      },
    }));
  },

  clearUiState: (taskId: string) => {
    set((state) => {
      const newUi = { ...state.ui };
      delete newUi[taskId];
      return { ui: newUi };
    });
  },
}));

// Convenience hooks for specific UI state
export const useTaskLoading = (taskId: string) => {
  const { getUiState, setUiState } = useTaskDetailsUiStore();
  const { loading } = getUiState(taskId);

  return {
    loading,
    setLoading: (value: boolean) => setUiState(taskId, { loading: value }),
  };
};

export const useTaskStopping = (taskId: string) => {
  const { getUiState, setUiState } = useTaskDetailsUiStore();
  const { isStopping } = getUiState(taskId);

  return {
    isStopping,
    setIsStopping: (value: boolean) =>
      setUiState(taskId, { isStopping: value }),
  };
};

export const useTaskDeletingFiles = (taskId: string) => {
  const { getUiState, setUiState } = useTaskDetailsUiStore();
  const { deletingFiles, fileToDelete } = getUiState(taskId);

  return {
    deletingFiles,
    fileToDelete,
    setFileToDelete: (value: string | null) =>
      setUiState(taskId, { fileToDelete: value }),
    setDeletingFiles: (value: Set<string>) =>
      setUiState(taskId, { deletingFiles: value }),
  };
};
</file>

<file path="frontend/src/styles/diff-style-overrides.css">
.diff-tailwindcss-wrapper .container {
  width: 100%;
}

@media (min-width: 640px) {
  .diff-tailwindcss-wrapper .container {
    max-width: 640px;
  }
}

@media (min-width: 768px) {
  .diff-tailwindcss-wrapper .container {
    max-width: 768px;
  }
}

@media (min-width: 1024px) {
  .diff-tailwindcss-wrapper .container {
    max-width: 1024px;
  }
}

@media (min-width: 1280px) {
  .diff-tailwindcss-wrapper .container {
    max-width: 1280px;
  }
}

@media (min-width: 1536px) {
  .diff-tailwindcss-wrapper .container {
    max-width: 1536px;
  }
}

.diff-tailwindcss-wrapper .invisible {
  visibility: hidden;
}

.diff-tailwindcss-wrapper .absolute {
  position: absolute;
}

.diff-tailwindcss-wrapper .relative {
  position: relative;
}

.diff-tailwindcss-wrapper .sticky {
  position: sticky;
}

.diff-tailwindcss-wrapper .left-0 {
  left: 0px;
}

.diff-tailwindcss-wrapper .left-\[100\%\] {
  left: 100%;
}

.diff-tailwindcss-wrapper .right-\[100\%\] {
  right: 100%;
}

.diff-tailwindcss-wrapper .top-0 {
  top: 0px;
}

.diff-tailwindcss-wrapper .top-\[1px\] {
  top: 1px;
}

.diff-tailwindcss-wrapper .top-\[50\%\] {
  top: 50%;
}

.diff-tailwindcss-wrapper .z-\[1\] {
  z-index: 1;
}

.diff-tailwindcss-wrapper .ml-\[-1\.5em\] {
  margin-left: -1.5em;
}

.diff-tailwindcss-wrapper .block {
  display: block;
}

.diff-tailwindcss-wrapper .inline-block {
  display: inline-block;
}

.diff-tailwindcss-wrapper .flex {
  display: flex;
}

.diff-tailwindcss-wrapper .table {
  display: table;
}

.diff-tailwindcss-wrapper .hidden {
  display: none;
}

.diff-tailwindcss-wrapper .h-\[50\%\] {
  height: 50%;
}

.diff-tailwindcss-wrapper .h-full {
  height: 100%;
}

.diff-tailwindcss-wrapper .min-h-\[28px\] {
  min-height: 28px;
}

.diff-tailwindcss-wrapper .w-\[1\%\] {
  width: 1%;
}

.diff-tailwindcss-wrapper .w-\[1\.5em\] {
  width: 1.5em;
}

.diff-tailwindcss-wrapper .w-\[1\.5px\] {
  width: 1.5px;
}

.diff-tailwindcss-wrapper .w-\[10px\] {
  width: 10px;
}

.diff-tailwindcss-wrapper .w-\[1px\] {
  width: 1px;
}

.diff-tailwindcss-wrapper .w-\[50\%\] {
  width: 50%;
}

.diff-tailwindcss-wrapper .w-full {
  width: 100%;
}

.diff-tailwindcss-wrapper .w-max {
  width: -moz-max-content;
  width: max-content;
}

.diff-tailwindcss-wrapper .min-w-\[100px\] {
  min-width: 100px;
}

.diff-tailwindcss-wrapper .min-w-\[40px\] {
  min-width: 40px;
}

.diff-tailwindcss-wrapper .min-w-full {
  min-width: 100%;
}

.diff-tailwindcss-wrapper .flex-shrink-0 {
  flex-shrink: 0;
}

.diff-tailwindcss-wrapper .shrink-0 {
  flex-shrink: 0;
}

.diff-tailwindcss-wrapper .basis-\[50\%\] {
  flex-basis: 50%;
}

.diff-tailwindcss-wrapper .table-fixed {
  table-layout: fixed;
}

.diff-tailwindcss-wrapper .border-collapse {
  border-collapse: collapse;
}

.diff-tailwindcss-wrapper .border-spacing-0 {
  --tw-border-spacing-x: 0px;
  --tw-border-spacing-y: 0px;
  border-spacing: var(--tw-border-spacing-x) var(--tw-border-spacing-y);
}

.diff-tailwindcss-wrapper .origin-center {
  transform-origin: center;
}

.diff-tailwindcss-wrapper .translate-x-\[-50\%\] {
  --tw-translate-x: -50%;
  transform: translate(var(--tw-translate-x), var(--tw-translate-y))
    rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y))
    scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));
}

.diff-tailwindcss-wrapper .translate-x-\[50\%\] {
  --tw-translate-x: 50%;
  transform: translate(var(--tw-translate-x), var(--tw-translate-y))
    rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y))
    scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));
}

.diff-tailwindcss-wrapper .translate-y-\[-50\%\] {
  --tw-translate-y: -50%;
  transform: translate(var(--tw-translate-x), var(--tw-translate-y))
    rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y))
    scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));
}

.diff-tailwindcss-wrapper .cursor-pointer {
  cursor: pointer;
}

.diff-tailwindcss-wrapper .select-none {
  -webkit-user-select: none;
  -moz-user-select: none;
  user-select: none;
}

.diff-tailwindcss-wrapper .flex-col {
  flex-direction: column;
}

.diff-tailwindcss-wrapper .items-start {
  align-items: flex-start;
}

.diff-tailwindcss-wrapper .items-center {
  align-items: center;
}

.diff-tailwindcss-wrapper .justify-center {
  justify-content: center;
}

.diff-tailwindcss-wrapper .overflow-x-auto {
  overflow-x: auto;
}

.diff-tailwindcss-wrapper .overflow-y-hidden {
  overflow-y: hidden;
}

.diff-tailwindcss-wrapper .whitespace-nowrap {
  white-space: nowrap;
}

.diff-tailwindcss-wrapper .break-all {
  word-break: break-all;
}

.diff-tailwindcss-wrapper .rounded-\[0\.2em\] {
  border-radius: 0.2em;
}

.diff-tailwindcss-wrapper .rounded-\[2px\] {
  border-radius: 2px;
}

.diff-tailwindcss-wrapper .rounded-md {
  border-radius: 0.375rem;
}

.diff-tailwindcss-wrapper .border-l-\[1px\] {
  border-left-width: 1px;
}

.diff-tailwindcss-wrapper .fill-current {
  fill: currentColor;
}

.diff-tailwindcss-wrapper .p-0 {
  padding: 0px;
}

.diff-tailwindcss-wrapper .p-\[1px\] {
  padding: 1px;
}

.diff-tailwindcss-wrapper .px-\[10px\] {
  padding-left: 10px;
  padding-right: 10px;
}

.diff-tailwindcss-wrapper .py-\[2px\] {
  padding-top: 2px;
  padding-bottom: 2px;
}

.diff-tailwindcss-wrapper .py-\[6px\] {
  padding-top: 6px;
  padding-bottom: 6px;
}

.diff-tailwindcss-wrapper .pl-\[1\.5em\] {
  padding-left: 1.5em;
}

.diff-tailwindcss-wrapper .pl-\[10px\] {
  padding-left: 10px;
}

.diff-tailwindcss-wrapper .pl-\[2\.0em\] {
  padding-left: 2em;
}

.diff-tailwindcss-wrapper .pr-\[10px\] {
  padding-right: 10px;
}

.diff-tailwindcss-wrapper .text-right {
  text-align: right;
}

.diff-tailwindcss-wrapper .indent-\[0\.2em\] {
  text-indent: 0.2em;
}

.diff-tailwindcss-wrapper .align-top {
  vertical-align: top;
}

.diff-tailwindcss-wrapper .align-middle {
  vertical-align: middle;
}

.diff-tailwindcss-wrapper .text-\[1\.2em\] {
  font-size: 1.2em;
}

.diff-tailwindcss-wrapper .leading-\[1\.4\] {
  line-height: 1.4;
}

.diff-tailwindcss-wrapper .leading-\[1\.6\] {
  line-height: 1.6;
}

.diff-tailwindcss-wrapper .\!text-red-500 {
  --tw-text-opacity: 1 !important;
  color: rgb(239 68 68 / var(--tw-text-opacity, 1)) !important;
}

.diff-tailwindcss-wrapper .opacity-\[0\.5\] {
  opacity: 0.5;
}

.diff-tailwindcss-wrapper .filter {
  filter: var(--tw-blur) var(--tw-brightness) var(--tw-contrast)
    var(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate)
    var(--tw-sepia) var(--tw-drop-shadow);
}

.diff-tailwindcss-wrapper .transition-transform {
  transition-property: transform;
  transition-timing-function: cubic-bezier(0.4, 0, 0.2, 1);
  transition-duration: 150ms;
}

.diff-tailwindcss-wrapper * {
  box-sizing: border-box;
}

.diff-tailwindcss-wrapper .diff-style-root {
  --diff-border--: var(--border);
  --diff-add-content--: hsl(var(--console-success) / 0.2);
  --diff-del-content--: hsl(var(--console-error) / 0.2);
  --diff-add-lineNumber--: color-mix(
    in srgb,
    hsl(var(--console-success)) 20%,
    hsl(var(--background)) 80%
  );
  --diff-del-lineNumber--: hsl(var(--console-error) / 0.2);
  --diff-plain-content--: hsl(var(--muted));
  --diff-expand-content--: hsl(var(--muted));
  --diff-plain-lineNumber--: hsl(var(--muted));
  --diff-expand-lineNumber--: hsl(var(--muted));
  --diff-plain-lineNumber-color--: hsl(var(--muted-foreground) / 0.7);
  --diff-expand-lineNumber-color--: hsl(var(--muted-foreground) / 0.7);
  --diff-hunk-content--: hsl(var(--muted));
  --diff-hunk-lineNumber--: hsl(var(--muted));
  --diff-hunk-lineNumber-hover--: hsl(var(--muted-foreground) / 0.7);
  --diff-add-content-highlight--: hsl(var(--console-success) / 0.4);
  --diff-del-content-highlight--: hsl(var(--console-error) / 0.4);
  --diff-add-widget--: hsl(var(--muted-foreground) / 0.7);
  --diff-add-widget-color--: hsl(var(--muted));
  --diff-empty-content--: hsl(var(--background));
  --diff-hunk-content-color--: hsl(var(--muted-foreground) / 0.7);
}

.diff-tailwindcss-wrapper .diff-style-root .diff-line-syntax-raw *,
.diff-tailwindcss-wrapper .diff-line-syntax-raw * {
  color: var(--diff-view-light, inherit);
  font-weight: var(--diff-view-light-font-weight, inherit);
}

.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw * {
  color: var(--diff-view-dark, inherit);
  font-weight: var(--diff-view-dark-font-weight, inherit);
}

.diff-tailwindcss-wrapper table,
.diff-tailwindcss-wrapper tr,
.diff-tailwindcss-wrapper td {
  border-color: transparent;
  border-width: 0px;
  text-align: left;
}

.diff-tailwindcss-wrapper .diff-line-old-num,
.diff-tailwindcss-wrapper .diff-line-new-num,
.diff-tailwindcss-wrapper .diff-line-num {
  text-align: right;
}

.diff-tailwindcss-wrapper .diff-style-root tr {
  content-visibility: auto;
}

.diff-tailwindcss-wrapper .diff-add-widget-wrapper {
  transform-origin: center;
  transform: translateX(-50%) !important;
}

.diff-tailwindcss-wrapper .diff-line-old-content .diff-add-widget-wrapper,
.diff-tailwindcss-wrapper .diff-line-new-content .diff-add-widget-wrapper {
  transform: translateX(50%) !important;
}

.diff-tailwindcss-wrapper .diff-add-widget-wrapper:hover {
  transform: translateX(-50%) scale(1.1) !important;
}

.diff-tailwindcss-wrapper .diff-line-old-content .diff-add-widget-wrapper:hover,
.diff-tailwindcss-wrapper
  .diff-line-new-content
  .diff-add-widget-wrapper:hover {
  transform: translateX(50%) scale(1.1) !important;
}

.diff-tailwindcss-wrapper .diff-widget-tooltip {
  position: relative;
}

.diff-tailwindcss-wrapper .diff-add-widget,
.diff-tailwindcss-wrapper .diff-widget-tooltip {
  font-family: inherit;
  font-feature-settings: inherit;
  font-variation-settings: inherit;
  font-size: 100%;
  font-weight: inherit;
  line-height: inherit;
  letter-spacing: inherit;
  color: inherit;
  margin: 0;
  text-transform: none;
  border-width: 0px;
  background-color: transparent;
  background-image: none;
}

.diff-tailwindcss-wrapper .diff-widget-tooltip::after {
  display: none;
  box-sizing: border-box;
  background-color: #555555;
  position: absolute;
  content: attr(data-title);
  font-size: 11px;
  padding: 1px 2px;
  border-radius: 4px;
  overflow: hidden;
  top: 50%;
  white-space: nowrap;
  transform: translateY(-50%);
  left: calc(100% + 8px);
  color: #ffffff;
}

.diff-tailwindcss-wrapper .diff-widget-tooltip::before {
  display: none;
  box-sizing: border-box;
  content: '';
  position: absolute;
  top: 50%;
  left: calc(100% - 2px);
  transform: translateY(-50%);
  border: 6px solid transparent;
  border-right-color: #555555;
}

.diff-tailwindcss-wrapper .diff-widget-tooltip:hover {
  background-color: var(--diff-hunk-lineNumber-hover--);
  color: white;
}

.diff-tailwindcss-wrapper .diff-widget-tooltip:hover::before {
  display: block;
}

.diff-tailwindcss-wrapper .diff-widget-tooltip:hover::after {
  display: block;
}

.diff-line-extend-wrapper * {
  color: initial;
}

.diff-line-widget-wrapper * {
  color: initial;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw pre code.hljs {
  display: block;
  overflow-x: auto;
  padding: 1em;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw code.hljs {
  padding: 3px 5px;
}

/*!
  Theme: GitHub
  Description: Light theme as seen on github.com
  Author: github.com
  Maintainer: @Hirse
  Updated: 2021-05-15

  Outdated base version: https://github.com/primer/github-syntax-light
  Current colors taken from GitHub's CSS
*/
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs {
  color: #24292e;
  background: #ffffff;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-doctag,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-keyword,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-meta .hljs-keyword,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-template-tag,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-template-variable,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-type,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-variable.language_ {
  /* prettylights-syntax-keyword */
  color: #d73a49;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-title,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-title.class_,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-title.class_.inherited__,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-title.function_ {
  /* prettylights-syntax-entity */
  color: #6f42c1;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-attr,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-attribute,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-literal,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-meta,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-number,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-operator,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-variable,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-selector-attr,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-selector-class,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-selector-id {
  /* prettylights-syntax-constant */
  color: #005cc5;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-regexp,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-string,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-meta .hljs-string {
  /* prettylights-syntax-string */
  color: #032f62;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-built_in,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-symbol {
  /* prettylights-syntax-variable */
  color: #e36209;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-comment,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-code,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-formula {
  /* prettylights-syntax-comment */
  color: #6a737d;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-name,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-quote,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-selector-tag,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-selector-pseudo {
  /* prettylights-syntax-entity-tag */
  color: #22863a;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-subst {
  /* prettylights-syntax-storage-modifier-import */
  color: #24292e;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-section {
  /* prettylights-syntax-markup-heading */
  color: #005cc5;
  font-weight: bold;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-bullet {
  /* prettylights-syntax-markup-list */
  color: #735c0f;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-emphasis {
  /* prettylights-syntax-markup-italic */
  color: #24292e;
  font-style: italic;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-strong {
  /* prettylights-syntax-markup-bold */
  color: #24292e;
  font-weight: bold;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-addition {
  /* prettylights-syntax-markup-inserted */
  color: #22863a;
  background-color: #f0fff4;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-deletion {
  /* prettylights-syntax-markup-deleted */
  color: #b31d28;
  background-color: #ffeef0;
}

.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-char.escape_,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-link,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-params,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-property,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-punctuation,
.diff-tailwindcss-wrapper .diff-line-syntax-raw .hljs-tag {
  /* purposely ignored */
}

.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  pre
  code.hljs {
  display: block;
  overflow-x: auto;
  padding: 1em;
}

.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw code.hljs {
  padding: 3px 5px;
}

/*!
  Theme: GitHub Dark
  Description: Dark theme as seen on github.com
  Author: github.com
  Maintainer: @Hirse
  Updated: 2021-05-15

  Outdated base version: https://github.com/primer/github-syntax-dark
  Current colors taken from GitHub's CSS
*/
.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs {
  color: #c9d1d9;
  background: #0d1117;
}

.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-doctag,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-keyword,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-meta
  .hljs-keyword,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-template-tag,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-template-variable,
.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-type,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-variable.language_ {
  /* prettylights-syntax-keyword */
  color: #ff7b72;
}

.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-title,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-title.class_,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-title.class_.inherited__,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-title.function_ {
  /* prettylights-syntax-entity */
  color: #d2a8ff;
}

.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-attr,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-attribute,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-literal,
.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-meta,
.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-number,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-operator,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-variable,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-selector-attr,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-selector-class,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-selector-id {
  /* prettylights-syntax-constant */
  color: #79c0ff;
}

.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-regexp,
.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-string,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-meta
  .hljs-string {
  /* prettylights-syntax-string */
  color: #a5d6ff;
}

.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-built_in,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-symbol {
  /* prettylights-syntax-variable */
  color: #ffa657;
}

.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-comment,
.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-code,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-formula {
  /* prettylights-syntax-comment */
  color: #8b949e;
}

.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-name,
.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-quote,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-selector-tag,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-selector-pseudo {
  /* prettylights-syntax-entity-tag */
  color: #7ee787;
}

.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-subst {
  /* prettylights-syntax-storage-modifier-import */
  color: #c9d1d9;
}

.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-section {
  /* prettylights-syntax-markup-heading */
  color: #1f6feb;
  font-weight: bold;
}

.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-bullet {
  /* prettylights-syntax-markup-list */
  color: #f2cc60;
}

.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-emphasis {
  /* prettylights-syntax-markup-italic */
  color: #c9d1d9;
  font-style: italic;
}

.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-strong {
  /* prettylights-syntax-markup-bold */
  color: #c9d1d9;
  font-weight: bold;
}

.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-addition {
  /* prettylights-syntax-markup-inserted */
  color: #aff5b4;
  background-color: #033a16;
}

.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-deletion {
  /* prettylights-syntax-markup-deleted */
  color: #ffdcd7;
  background-color: #67060c;
}

.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-char.escape_,
.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-link,
.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-params,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-property,
.diff-tailwindcss-wrapper[data-theme='dark']
  .diff-line-syntax-raw
  .hljs-punctuation,
.diff-tailwindcss-wrapper[data-theme='dark'] .diff-line-syntax-raw .hljs-tag {
  /* purposely ignored */
}

.diff-tailwindcss-wrapper .hover\:scale-110:hover {
  --tw-scale-x: 1.1;
  --tw-scale-y: 1.1;
  transform: translate(var(--tw-translate-x), var(--tw-translate-y))
    rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y))
    scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));
}

.diff-tailwindcss-wrapper .group:hover .group-hover\:visible {
  visibility: visible;
}
</file>

<file path="frontend/src/styles/edit-diff-overrides.css">
/* Hide line numbers for replace (old/new) diffs rendered via DiffView */
.edit-diff-hide-nums .diff-line-old-num,
.edit-diff-hide-nums .diff-line-new-num,
.edit-diff-hide-nums .diff-line-num {
  display: none !important;
}

/* Ensure number gutters don't consume space when hidden */
.edit-diff-hide-nums .diff-line-old-num + .diff-line-old-content,
.edit-diff-hide-nums .diff-line-new-num + .diff-line-new-content,
.edit-diff-hide-nums .diff-line-num + .diff-line-content {
  padding-left: 0 !important;
}

.plain-file-content .diff-style-root {
  /* neutralize addition backgrounds */
  --diff-add-content--: hsl(var(--background));
  --diff-add-content-highlight--: hsl(var(--background));
}

.plain-file-content .diff-line-content-operator {
  display: none !important; /* hide leading '+' operator column */
}

.plain-file-content .diff-line-content-item {
  padding-left: 0 !important; /* remove indent left by operator column */
}

/* hide unified hunk header rows (e.g. @@ -1,+n @@) */
.plain-file-content .diff-line-hunk-content {
  display: none !important;
}
</file>

<file path="frontend/src/styles/index.css">
@import url('https://fonts.googleapis.com/css2?family=Chivo+Mono:ital,wght@0,100..900;1,100..900&family=Noto+Emoji:wght@300..700&display=swap');

@tailwind base;
@tailwind components;
@tailwind utilities;

/* 1) THEME TOKENS (underscored): defaults + classes control these */
@layer base {
  /* Light defaults */
  :root {
    --_background: 48 33% 97%;
    --_foreground: 222.2 84% 4.9%;
    --_primary: var(--_muted);
    --_primary-foreground: var(--_muted-foreground);
    --_secondary: var(--_muted);
    --_secondary-foreground: 215.4 16.3% 70.9%;
    --_muted: 0 0% 100%;
    --_muted-foreground: var(--_foreground);
    --_accent: var(--_background);
    --_accent-foreground: 222.2 84% 4.9%;
    --_destructive: 0 84.2% 60.2%;
    --_destructive-foreground: var(--_background);
    --_border: 214.3 31.8% 91.4%;
    --_input: var(--_border);
    --_ring: 222.2 84% 4.9%;
    --_radius: 0.5rem;

    /* Status (light) */
    --_success: 142.1 76.2% 36.3%;
    --_success-foreground: 138.5 76.5% 96.7%;
    --_warning: 32.2 95% 44.1%;
    --_warning-foreground: 26 83.3% 14.1%;
    --_info: 217.2 91.2% 59.8%;
    --_info-foreground: 222.2 84% 4.9%;
    --_neutral: 210 40% 96%;
    --_neutral-foreground: 222.2 84% 4.9%;

    /* Console (light) */
    --_console-background: 0 0% 100%;
    --_console-foreground: 222.2 84% 4.9%;
    --_console-success: 138 69% 45%;
    --_console-error: 5 100% 69%;
  }

  /* Dark defaults (used if no theme class but user prefers dark) */
  .dark {
    --_background: 60 2% 18%;
    --_foreground: 48 7% 95%;
    --_primary: var(--_muted);
    --_primary-foreground: var(--_muted-foreground);
    --_secondary: var(--_muted);
    --_secondary-foreground: 48 7% 73%;
    --_muted: 60 2% 20%;
    --_muted-foreground: var(--_foreground);
    --_accent: var(--_background);
    --_accent-foreground: 210 40% 98%;
    --_destructive: 0 62.8% 50.6%;
    --_destructive-foreground: var(--_background-foreground);
    --_border: 60 2% 25%;
    --_input: var(--_border);
    --_ring: 212.7 26.8% 83.9%;

    /* Status (dark) */
    --_success: 138.5 76.5% 47.7%;
    --_success-foreground: 138.5 76.5% 96.7%;
    --_warning: 32.2 95% 44.1%;
    --_warning-foreground: 26 83.3% 14.1%;
    --_info: 217.2 91.2% 59.8%;
    --_info-foreground: 222.2 84% 4.9%;
    --_neutral: 217.2 32.6% 17.5%;
    --_neutral-foreground: 210 40% 98%;

    /* Console (dark) */
    --_console-background: 0 0% 0%;
    --_console-foreground: 210 40% 98%;
    --_console-success: 138.5 76.5% 47.7%;
    --_console-error: 0 84.2% 60.2%;
  }
}

/* 2) PUBLIC TOKENS: prefer VS Code, else fall back to theme tokens */
@layer base {
  :root {
    --background: var(--vscode-editor-background, var(--_background));
    --foreground: var(--vscode-editor-foreground, var(--_foreground));

    --card: var(--muted);
    --card-foreground: var(--muted-foreground);
    --popover: var(--background);
    --popover-foreground: var(--foreground);

    --primary: var(--vscode-button-background, var(--_primary));
    --primary-foreground: var(
      --vscode-editor-foreground,
      var(--_primary-foreground)
    );
    --secondary: var(--vscode-input-background, var(--_secondary));
    --secondary-foreground: var(
      --vscode-input-foreground,
      var(--_secondary-foreground)
    );

    --muted: var(--vscode-editor-background, var(--_muted));
    --muted-foreground: var(
      --vscode-descriptionForeground,
      var(--_muted-foreground)
    );
    --accent: var(--vscode-focusBorder, var(--_accent));
    --accent-foreground: var(
      --vscode-editor-foreground,
      var(--_accent-foreground)
    );

    --destructive: var(--vscode-errorForeground, var(--_destructive));
    --destructive-foreground: var(
      --vscode-button-foreground,
      var(--_destructive-foreground)
    );

    --border: var(--vscode-input-background, var(--_border));
    --input: var(--vscode-input-background, var(--_input));
    --ring: var(--vscode-focusBorder, var(--_ring));

    --radius: var(--_radius);

    /* Status */
    --success: var(--vscode-testing-iconPassed, var(--_success));
    --success-foreground: var(
      --vscode-editor-foreground,
      var(--_success-foreground)
    );
    --warning: var(--vscode-testing-iconQueued, var(--_warning));
    --warning-foreground: var(
      --vscode-descriptionForeground,
      var(--_warning-foreground)
    );
    --info: var(--vscode-focusBorder, var(--_info));
    --info-foreground: var(--vscode-editor-foreground, var(--_info-foreground));
    --neutral: var(--vscode-input-background, var(--_neutral));
    --neutral-foreground: var(
      --vscode-editor-foreground,
      var(--_neutral-foreground)
    );

    /* Console/terminal */
    --console-background: var(
      --vscode-editor-background,
      var(--_console-background)
    );
    --console-foreground: var(
      --vscode-terminal-foreground,
      var(--_console-foreground)
    );
    --console-success: var(
      --vscode-testing-iconPassed,
      var(--_console-success)
    );
    --console-error: var(--vscode-terminal-ansiRed, var(--_console-error));
  }
}

/* 3) Usage */
@layer base {
  * {
    @apply border-border;
  }

  html,
  body,
  #root {
    @apply min-h-screen;
  }

  body {
    @apply bg-background text-foreground font-chivo-mono;
  }

  *:focus {
    @apply ring-inset;
  }
}

/* ANSI color classes for fancy-ansi */
@layer components {
  .ansi-red {
    @apply text-red-500;
  }

  .ansi-green {
    @apply text-green-500;
  }

  .ansi-yellow {
    @apply text-yellow-500;
  }

  .ansi-blue {
    @apply text-blue-500;
  }

  .ansi-magenta {
    @apply text-purple-500;
  }

  .ansi-cyan {
    @apply text-cyan-500;
  }

  .ansi-white {
    @apply text-white;
  }

  .ansi-black {
    @apply text-black;
  }

  .ansi-bright-red {
    @apply text-red-400;
  }

  .ansi-bright-green {
    @apply text-green-400;
  }

  .ansi-bright-yellow {
    @apply text-yellow-400;
  }

  .ansi-bright-blue {
    @apply text-blue-400;
  }

  .ansi-bright-magenta {
    @apply text-purple-400;
  }

  .ansi-bright-cyan {
    @apply text-cyan-400;
  }

  .ansi-bright-white {
    @apply text-gray-200;
  }

  .ansi-bright-black {
    @apply text-gray-700;
  }

  .ansi-bold {
    @apply font-bold;
  }

  .ansi-italic {
    @apply italic;
  }

  .ansi-underline {
    @apply underline;
  }
}
</file>

<file path="frontend/src/types/logs.ts">
import type { NormalizedEntry, ExecutorAction } from 'shared/types';

export interface UnifiedLogEntry {
  id: string;
  ts: number; // epoch-ms timestamp for sorting and react-window key
  processId: string;
  processName: string;
  channel: 'raw' | 'stdout' | 'stderr' | 'normalized' | 'process_start';
  payload: string | NormalizedEntry | ProcessStartPayload;
}

export interface ProcessStartPayload {
  processId: string;
  runReason: string;
  startedAt: string;
  status: string;
  action?: ExecutorAction;
}
</file>

<file path="frontend/src/types/modal-args.d.ts">
import { TaskAttempt } from 'shared/types';

// Extend nice-modal-react to provide type safety for modal arguments
declare module '@ebay/nice-modal-react' {
  interface ModalArgs {
    'github-login': void;
    'create-pr': {
      attempt: TaskAttempt;
      task: any; // Will be properly typed when we have the full task type
      projectId: string;
    };
  }
}

export {};
</file>

<file path="frontend/src/types/modals.ts">
import type { TaskAttempt, TaskWithAttemptStatus } from 'shared/types';
import type {
  ConfirmDialogProps,
  ProvidePatDialogProps,
  DeleteTaskConfirmationDialogProps,
  TaskFormDialogProps,
  EditorSelectionDialogProps,
} from '@/components/dialogs';

// Type definitions for nice-modal-react modal arguments
declare module '@ebay/nice-modal-react' {
  interface ModalArgs {
    // Existing modals
    'github-login': void;
    'create-pr': {
      attempt: TaskAttempt;
      task: TaskWithAttemptStatus;
      projectId: string;
    };

    // Generic modals
    confirm: ConfirmDialogProps;

    // App flow modals
    disclaimer: void;
    onboarding: void;
    'privacy-opt-in': void;
    'provide-pat': ProvidePatDialogProps;
    'release-notes': void;

    // Task-related modals
    'task-form': TaskFormDialogProps;
    'delete-task-confirmation': DeleteTaskConfirmationDialogProps;
    'editor-selection': EditorSelectionDialogProps;
  }
}

export {};
</file>

<file path="frontend/src/types/tabs.ts">
export type TabType = 'logs' | 'diffs' | 'processes';
</file>

<file path="frontend/src/types/virtual-executor-schemas.d.ts">
declare module 'virtual:executor-schemas' {
  import type { RJSFSchema } from '@rjsf/utils';
  import type { BaseCodingAgent } from '@/shared/types';

  const schemas: Record<BaseCodingAgent, RJSFSchema>;
  export { schemas };
  export default schemas;
}
</file>

<file path="frontend/src/utils/extToLanguage.ts">
/**
 * getHighlightLanguage(ext)
 * Returns the Highlight.js language id (or null if not mapped).
 *
 * @param {string} ext – File extension with or without the leading dot.
 * @example
 *   getHighlightLanguage('.py');   // "python"
 *   getHighlightLanguage('tsx');   // "tsx"
 */
const extToLang: Record<string, string> = {
  // Web & scripting
  js: 'javascript',
  mjs: 'javascript',
  cjs: 'javascript',
  ts: 'typescript',
  jsx: 'jsx',
  tsx: 'tsx',
  html: 'xml', // Highlight.js groups HTML/XML
  htm: 'xml',
  xml: 'xml',
  css: 'css',
  scss: 'scss',
  less: 'less',
  json: 'json',
  md: 'markdown',
  yml: 'yaml',
  yaml: 'yaml',
  sh: 'bash',
  bash: 'bash',
  zsh: 'bash',
  ps1: 'powershell',
  php: 'php',

  // Classic compiled
  c: 'c',
  h: 'c',
  cpp: 'cpp',
  cc: 'cpp',
  cxx: 'cpp',
  hpp: 'cpp',
  cs: 'csharp',
  java: 'java',
  kt: 'kotlin',
  scala: 'scala',
  go: 'go',
  rs: 'rust',
  swift: 'swift',
  dart: 'dart',

  // Others & fun stuff
  py: 'python',
  rb: 'ruby',
  pl: 'perl',
  lua: 'lua',
  r: 'r',
  sql: 'sql',
  tex: 'latex',
};

/**
 * Normalises the extension and looks it up.
 */
export function getHighlightLanguage(ext: string): string | null {
  ext = ext.toLowerCase();
  return extToLang[ext];
}

export function getHighLightLanguageFromPath(path: string): string | null {
  const ext = path.split('.').pop();
  return getHighlightLanguage(ext || '');
}
</file>

<file path="frontend/src/utils/script-placeholders.ts">
interface ScriptPlaceholders {
  setup: string;
  dev: string;
  cleanup: string;
}

interface ScriptPlaceholderStrategy {
  getPlaceholders(): ScriptPlaceholders;
}

class WindowsScriptPlaceholderStrategy implements ScriptPlaceholderStrategy {
  getPlaceholders(): ScriptPlaceholders {
    return {
      setup: `@echo off
npm install
REM Add any setup commands here...`,
      dev: `@echo off
npm run dev
REM Add dev server start command here...`,
      cleanup: `@echo off
REM Add cleanup commands here...
REM This runs after coding agent execution - only if changes were made`,
    };
  }
}

class UnixScriptPlaceholderStrategy implements ScriptPlaceholderStrategy {
  getPlaceholders(): ScriptPlaceholders {
    return {
      setup: `#!/bin/bash
npm install
# Add any setup commands here...`,
      dev: `#!/bin/bash
npm run dev
# Add dev server start command here...`,
      cleanup: `#!/bin/bash
# Add cleanup commands here...
# This runs after coding agent execution - only if changes were made`,
    };
  }
}

class ScriptPlaceholderContext {
  private strategy: ScriptPlaceholderStrategy;

  constructor(strategy: ScriptPlaceholderStrategy) {
    this.strategy = strategy;
  }

  setStrategy(strategy: ScriptPlaceholderStrategy): void {
    this.strategy = strategy;
  }

  getPlaceholders(): ScriptPlaceholders {
    return this.strategy.getPlaceholders();
  }
}

export function createScriptPlaceholderStrategy(
  osType: string
): ScriptPlaceholderStrategy {
  if (osType.toLowerCase().includes('windows')) {
    return new WindowsScriptPlaceholderStrategy();
  }
  return new UnixScriptPlaceholderStrategy();
}

export { ScriptPlaceholderContext, type ScriptPlaceholders };
</file>

<file path="frontend/src/utils/status-labels.ts">
import { TaskStatus } from 'shared/types';

export const statusLabels: Record<TaskStatus, string> = {
  todo: 'To Do',
  inprogress: 'In Progress',
  inreview: 'In Review',
  done: 'Done',
  cancelled: 'Cancelled',
};

export const statusBoardColors: Record<TaskStatus, string> = {
  todo: '--neutral-foreground',
  inprogress: '--info',
  inreview: '--warning',
  done: '--success',
  cancelled: '--destructive',
};
</file>

<file path="frontend/src/utils/streamJsonPatchEntries.ts">
// streamJsonPatchEntries.ts - WebSocket JSON patch streaming utility
import { applyPatch, type Operation } from 'rfc6902';

type PatchContainer<E = unknown> = { entries: E[] };

export interface StreamOptions<E = unknown> {
  initial?: PatchContainer<E>;
  /** called after each successful patch application */
  onEntries?: (entries: E[]) => void;
  onConnect?: () => void;
  onError?: (err: unknown) => void;
  /** called once when a "finished" event is received */
  onFinished?: (entries: E[]) => void;
}

interface StreamController<E = unknown> {
  /** Current entries array (immutable snapshot) */
  getEntries(): E[];
  /** Full { entries } snapshot */
  getSnapshot(): PatchContainer<E>;
  /** Best-effort connection state */
  isConnected(): boolean;
  /** Subscribe to updates; returns an unsubscribe function */
  onChange(cb: (entries: E[]) => void): () => void;
  /** Close the stream */
  close(): void;
}

/**
 * Connect to a WebSocket endpoint that emits JSON messages containing:
 *   {"JsonPatch": [{"op": "add", "path": "/entries/0", "value": {...}}, ...]}
 *   {"Finished": ""}
 *
 * Maintains an in-memory { entries: [] } snapshot and returns a controller.
 */
export function streamJsonPatchEntries<E = unknown>(
  url: string,
  opts: StreamOptions<E> = {}
): StreamController<E> {
  let connected = false;
  let snapshot: PatchContainer<E> = structuredClone(
    opts.initial ?? ({ entries: [] } as PatchContainer<E>)
  );

  const subscribers = new Set<(entries: E[]) => void>();
  if (opts.onEntries) subscribers.add(opts.onEntries);

  // Convert HTTP endpoint to WebSocket endpoint
  const wsUrl = url.replace(/^http/, 'ws');
  const ws = new WebSocket(wsUrl);

  const notify = () => {
    for (const cb of subscribers) {
      try {
        cb(snapshot.entries);
      } catch {
        /* swallow subscriber errors */
      }
    }
  };

  const handleMessage = (event: MessageEvent) => {
    try {
      const msg = JSON.parse(event.data);

      // Handle JsonPatch messages (from LogMsg::to_ws_message)
      if (msg.JsonPatch) {
        const raw = msg.JsonPatch as Operation[];
        const ops = dedupeOps(raw);

        // Apply to a working copy (applyPatch mutates)
        const next = structuredClone(snapshot);
        applyPatch(next as unknown as object, ops);

        snapshot = next;
        notify();
      }

      // Handle Finished messages
      if (msg.finished !== undefined) {
        opts.onFinished?.(snapshot.entries);
        ws.close();
      }
    } catch (err) {
      opts.onError?.(err);
    }
  };

  ws.addEventListener('open', () => {
    connected = true;
    opts.onConnect?.();
  });

  ws.addEventListener('message', handleMessage);

  ws.addEventListener('error', (err) => {
    connected = false;
    opts.onError?.(err);
  });

  ws.addEventListener('close', () => {
    connected = false;
  });

  return {
    getEntries(): E[] {
      return snapshot.entries;
    },
    getSnapshot(): PatchContainer<E> {
      return snapshot;
    },
    isConnected(): boolean {
      return connected;
    },
    onChange(cb: (entries: E[]) => void): () => void {
      subscribers.add(cb);
      // push current state immediately
      cb(snapshot.entries);
      return () => subscribers.delete(cb);
    },
    close(): void {
      ws.close();
      subscribers.clear();
      connected = false;
    },
  };
}

/**
 * Dedupe multiple ops that touch the same path within a single event.
 * Last write for a path wins, while preserving the overall left-to-right
 * order of the *kept* final operations.
 *
 * Example:
 *   add /entries/4, replace /entries/4  -> keep only the final replace
 */
function dedupeOps(ops: Operation[]): Operation[] {
  const lastIndexByPath = new Map<string, number>();
  ops.forEach((op, i) => lastIndexByPath.set(op.path, i));

  // Keep only the last op for each path, in ascending order of their final index
  const keptIndices = [...lastIndexByPath.values()].sort((a, b) => a - b);
  return keptIndices.map((i) => ops[i]!);
}
</file>

<file path="frontend/src/utils/string.ts">
/**
 * Converts SCREAMING_SNAKE_CASE to "Pretty Case"
 * @param value - The string to convert
 * @returns Formatted string with proper capitalization
 */
export const toPrettyCase = (value: string): string => {
  return value
    .split('_')
    .map((word) => word.charAt(0).toUpperCase() + word.slice(1).toLowerCase())
    .join(' ');
};

/**
 * Generates a pretty project name from a file path
 * Converts directory names like "my-awesome-project" to "My Awesome Project"
 * @param path - The file path to extract name from
 * @returns Formatted project name
 */
export const generateProjectNameFromPath = (path: string): string => {
  const dirName = path.split('/').filter(Boolean).pop() || '';
  return dirName.replace(/[-_]/g, ' ').replace(/\b\w/g, (l) => l.toUpperCase());
};

/**
 * Removes a single trailing newline sequence from a string.
 * Handles CRLF/CR/LF endings while leaving other trailing whitespace intact.
 */
export const stripLineEnding = (value: string): string => {
  return value.replace(/(?:\r\n|\r|\n)$/, '');
};
</file>

<file path="frontend/src/utils/style-override.tsx">
import { useEffect } from 'react';
import { useTheme } from '@/components/theme-provider';
import { ThemeMode } from 'shared/types';

interface VibeStyleOverrideMessage {
  type: 'VIBE_STYLE_OVERRIDE';
  payload:
    | {
        kind: 'cssVars';
        variables: Record<string, string>;
      }
    | {
        kind: 'theme';
        theme: ThemeMode;
      };
}

interface VibeIframeReadyMessage {
  type: 'VIBE_IFRAME_READY';
}

// Component that adds postMessage listener for style overrides
export function AppWithStyleOverride({
  children,
}: {
  children: React.ReactNode;
}) {
  const { setTheme } = useTheme();

  useEffect(() => {
    function handleStyleMessage(event: MessageEvent) {
      if (event.data?.type !== 'VIBE_STYLE_OVERRIDE') return;

      // Origin validation (only if VITE_PARENT_ORIGIN is configured)
      const allowedOrigin = import.meta.env.VITE_PARENT_ORIGIN;
      if (allowedOrigin && event.origin !== allowedOrigin) {
        console.warn(
          '[StyleOverride] Message from unauthorized origin:',
          event.origin
        );
        return;
      }

      const message = event.data as VibeStyleOverrideMessage;

      // CSS variable overrides (only --vibe-* prefixed variables)
      if (
        message.payload.kind === 'cssVars' &&
        typeof message.payload.variables === 'object'
      ) {
        Object.entries(message.payload.variables).forEach(([name, value]) => {
          if (typeof value === 'string') {
            document.documentElement.style.setProperty(name, value);
          }
        });
      } else if (message.payload.kind === 'theme') {
        setTheme(message.payload.theme);
      }
    }

    window.addEventListener('message', handleStyleMessage);
    return () => window.removeEventListener('message', handleStyleMessage);
  }, [setTheme]);

  // Send ready message to parent when component mounts
  useEffect(() => {
    const allowedOrigin = import.meta.env.VITE_PARENT_ORIGIN;

    // Only send if we're in an iframe and have a parent
    if (window.parent && window.parent !== window) {
      const readyMessage: VibeIframeReadyMessage = {
        type: 'VIBE_IFRAME_READY',
      };

      // Send to specific origin if configured, otherwise send to any origin
      const targetOrigin = allowedOrigin || '*';
      window.parent.postMessage(readyMessage, targetOrigin);
    }
  }, []);

  return <>{children}</>;
}
</file>

<file path="frontend/src/utils/theme.ts">
import { ThemeMode } from 'shared/types';

/**
 * Resolves the actual theme (light/dark) based on the theme mode setting.
 * Handles system theme detection properly.
 */
export function getActualTheme(
  themeMode: ThemeMode | undefined
): 'light' | 'dark' {
  if (!themeMode || themeMode === ThemeMode.LIGHT) {
    return 'light';
  }

  if (themeMode === ThemeMode.SYSTEM) {
    // Check system preference
    return window.matchMedia('(prefers-color-scheme: dark)').matches
      ? 'dark'
      : 'light';
  }

  // All other themes (DARK, PURPLE, GREEN, BLUE, ORANGE, RED) have dark backgrounds
  return 'dark';
}
</file>

<file path="frontend/src/vscode/bridge.ts">
// VS Code Webview iframe keyboard bridge
//
// Purpose
// - Make typing, paste/cut/undo/redo inside the iframe feel like a regular browser
//   input/textarea/contentEditable.
// - Still allow VS Code to handle global/editor shortcuts by forwarding non-text
//   editing keys to the parent webview.
// - Bridge clipboard reads/writes when navigator.clipboard is restricted.

/** Returns true when running inside an iframe (vs top-level window). */
export function inIframe(): boolean {
  try {
    return window.self !== window.top;
  } catch {
    return true;
  }
}

/** Minimal serializable keyboard event shape used across the bridge. */
type KeyPayload = {
  key: string;
  code: string;
  altKey: boolean;
  ctrlKey: boolean;
  shiftKey: boolean;
  metaKey: boolean;
  repeat: boolean;
  isComposing: boolean;
  location: number;
};

/** Convert a KeyboardEvent to a serializable payload for postMessage. */
function serializeKeyEvent(e: KeyboardEvent): KeyPayload {
  return {
    key: e.key,
    code: e.code,
    altKey: e.altKey,
    ctrlKey: e.ctrlKey,
    shiftKey: e.shiftKey,
    metaKey: e.metaKey,
    repeat: e.repeat,
    isComposing: e.isComposing,
    location: e.location ?? 0,
  };
}

/** Platform check used for shortcut detection. */
const isMac = () => navigator.platform.toUpperCase().includes('MAC');

/** True for Cmd/Ctrl+C (no Shift/Alt). */
const isCopy = (e: KeyboardEvent) =>
  (isMac() ? e.metaKey : e.ctrlKey) &&
  !e.shiftKey &&
  !e.altKey &&
  e.key.toLowerCase() === 'c';
/** True for Cmd/Ctrl+X (no Shift/Alt). */
const isCut = (e: KeyboardEvent) =>
  (isMac() ? e.metaKey : e.ctrlKey) &&
  !e.shiftKey &&
  !e.altKey &&
  e.key.toLowerCase() === 'x';
/** True for Cmd/Ctrl+V (no Shift/Alt). */
const isPaste = (e: KeyboardEvent) =>
  (isMac() ? e.metaKey : e.ctrlKey) &&
  !e.shiftKey &&
  !e.altKey &&
  e.key.toLowerCase() === 'v';
/** True for Cmd/Ctrl+Z. */
const isUndo = (e: KeyboardEvent) =>
  (isMac() ? e.metaKey : e.ctrlKey) &&
  !e.shiftKey &&
  !e.altKey &&
  e.key.toLowerCase() === 'z';
/** True for redo (Cmd+Shift+Z on macOS, Ctrl+Y elsewhere). */
const isRedo = (e: KeyboardEvent) =>
  (isMac() ? e.metaKey : e.ctrlKey) &&
  !e.altKey &&
  ((isMac() && e.shiftKey && e.key.toLowerCase() === 'z') ||
    (!isMac() && !e.shiftKey && e.key.toLowerCase() === 'y'));

/**
 * Returns the currently focused editable element (input/textarea/contentEditable)
 * or null when focus is not within an editable.
 */
function activeEditable():
  | HTMLInputElement
  | HTMLTextAreaElement
  | (HTMLElement & { isContentEditable: boolean })
  | null {
  const el = document.activeElement as HTMLElement | null;
  if (!el) return null;
  const tag = el.tagName?.toLowerCase();
  if (tag === 'input' || tag === 'textarea')
    return el as HTMLInputElement | HTMLTextAreaElement;
  if (el.isContentEditable)
    return el as HTMLElement & { isContentEditable: boolean };
  return null;
}

/** Attempt to write to the OS clipboard. Returns true on success. */
async function writeClipboardText(text: string): Promise<boolean> {
  try {
    await navigator.clipboard.writeText(text);
    return true;
  } catch {
    try {
      return document.execCommand('copy');
    } catch {
      return false;
    }
  }
}

/** Attempt to read from the OS clipboard. Returns empty string on failure. */
async function readClipboardText(): Promise<string> {
  try {
    return await navigator.clipboard.readText();
  } catch {
    return '';
  }
}

/** Best-effort selection extractor for inputs, textareas, and contentEditable. */
function getSelectedText(): string {
  const el = activeEditable() as
    | HTMLInputElement
    | HTMLTextAreaElement
    | (HTMLElement & { isContentEditable: boolean })
    | null;
  if (el && (el as HTMLInputElement).selectionStart !== undefined) {
    const input = el as HTMLInputElement | HTMLTextAreaElement;
    const start = input.selectionStart ?? 0;
    const end = input.selectionEnd ?? 0;
    return start < end ? input.value.slice(start, end) : '';
  }
  const sel = window.getSelection();
  return sel ? sel.toString() : '';
}

/** Perform a browser-like cut on an input/textarea and emit input/change events. */
function cutFromInput(el: HTMLInputElement | HTMLTextAreaElement) {
  const start = el.selectionStart ?? 0;
  const end = el.selectionEnd ?? 0;
  if (end > start) {
    const selected = el.value.slice(start, end);
    void writeClipboardText(selected);
    if (typeof el.setRangeText === 'function') {
      el.setRangeText('', start, end, 'end');
    } else {
      const before = el.value.slice(0, start);
      const after = el.value.slice(end);
      el.value = before + after;
      el.setSelectionRange(start, start);
    }
    const ie =
      typeof (window as any).InputEvent !== 'undefined'
        ? new (window as any).InputEvent('input', {
            bubbles: true,
            composed: true,
            inputType: 'deleteByCut',
          })
        : new Event('input', { bubbles: true });
    el.dispatchEvent(ie as Event);
    el.dispatchEvent(new Event('change', { bubbles: true }));
  }
}

/** Paste text at the current caret position in an input/textarea and emit events. */
function pasteIntoInput(
  el: HTMLInputElement | HTMLTextAreaElement,
  text: string
) {
  const start = el.selectionStart ?? el.value.length;
  const end = el.selectionEnd ?? el.value.length;
  if (typeof el.setRangeText === 'function') {
    el.setRangeText(text, start, end, 'end');
  } else {
    const before = el.value.slice(0, start);
    const after = el.value.slice(end);
    el.value = before + text + after;
    const caret = start + text.length;
    el.setSelectionRange(caret, caret);
  }
  el.focus();
  const ie =
    typeof (window as any).InputEvent !== 'undefined'
      ? new (window as any).InputEvent('input', {
          bubbles: true,
          composed: true,
          inputType: 'insertFromPaste',
          data: text,
        })
      : new Event('input', { bubbles: true });
  el.dispatchEvent(ie as Event);
  el.dispatchEvent(new Event('change', { bubbles: true }));
}

/**
 * Insert text at the caret for the currently active editable.
 * Uses native mechanisms (setRangeText/execCommand) and emits input events so
 * controlled frameworks (like React) update state predictably.
 */
function insertTextAtCaretGeneric(text: string) {
  const el =
    (activeEditable() as
      | HTMLInputElement
      | HTMLTextAreaElement
      | (HTMLElement & { isContentEditable: boolean })
      | null) ||
    (document.querySelector(
      'textarea, input:not([type=checkbox]):not([type=radio])'
    ) as HTMLTextAreaElement | HTMLInputElement | null);
  if (!el) return;
  if ((el as HTMLInputElement).selectionStart !== undefined) {
    pasteIntoInput(el as HTMLInputElement | HTMLTextAreaElement, text);
  } else {
    try {
      document.execCommand('insertText', false, text);
      (el as any).dispatchEvent?.(new Event('input', { bubbles: true }));
    } catch {
      (el as HTMLElement).innerText += text;
    }
  }
}

// Lightweight retry for cases where add-to arrives before an editable exists
/** CSS selector for a reasonable first editable fallback. */
const EDITABLE_SELECTOR =
  'textarea, input:not([type=checkbox]):not([type=radio])';
/** Interval (ms) between retries while we wait for an editable to appear. */
const RETRY_INTERVAL_MS = 100;
/** Maximum number of retry attempts before giving up. */
const MAX_RETRY_ATTEMPTS = 15;
let insertRetryTimer: number | null = null;
const insertQueue: string[] = [];
function enqueueInsert(text: string) {
  insertQueue.push(text);
  if (insertRetryTimer != null) return;
  let attempts = 0;
  const run = () => {
    attempts++;
    const el =
      activeEditable() ||
      (document.querySelector(EDITABLE_SELECTOR) as
        | HTMLTextAreaElement
        | HTMLInputElement
        | null);
    if (el) {
      // drain queue
      while (insertQueue.length > 0) {
        insertTextAtCaretGeneric(insertQueue.shift() as string);
      }
      if (insertRetryTimer != null) {
        window.clearInterval(insertRetryTimer);
        insertRetryTimer = null;
      }
      return;
    }
    if (attempts >= MAX_RETRY_ATTEMPTS && insertRetryTimer != null) {
      window.clearInterval(insertRetryTimer);
      insertRetryTimer = null;
    }
  };
  insertRetryTimer = window.setInterval(run, RETRY_INTERVAL_MS);
}

/** Request map to resolve clipboard paste requests from the extension. */
const pasteResolvers: Record<string, (text: string) => void> = {};

/** Ask the extension to copy text to the OS clipboard (fallback path). */
export function parentClipboardWrite(text: string) {
  try {
    window.parent.postMessage(
      { type: 'vscode-iframe-clipboard-copy', text },
      '*'
    );
  } catch (_err) {
    void 0;
  }
}

/** Ask the extension to read text from the OS clipboard (fallback path). */
export function parentClipboardRead(): Promise<string> {
  return new Promise((resolve) => {
    const requestId = Math.random().toString(36).slice(2);
    pasteResolvers[requestId] = (text: string) => resolve(text);
    try {
      window.parent.postMessage(
        { type: 'vscode-iframe-clipboard-paste-request', requestId },
        '*'
      );
    } catch {
      resolve('');
    }
  });
}

/** Message union used for iframe <-> extension communications. */
type IframeMessage = {
  type: string;
  event?: KeyPayload;
  text?: string;
  requestId?: string;
};

// Handle messages from the parent webview (clipboard, add-to input)
window.addEventListener('message', (e: MessageEvent) => {
  const data: unknown = e?.data;
  if (!data || typeof data !== 'object') return;
  const msg = data as IframeMessage;
  if (msg.type === 'vscode-iframe-clipboard-paste-result' && msg.requestId) {
    const fn = pasteResolvers[msg.requestId];
    if (fn) {
      fn(msg.text || '');
      delete pasteResolvers[msg.requestId];
    }
  }
  if (msg.type === 'VIBE_ADD_TO_INPUT' && typeof msg.text === 'string') {
    const el =
      activeEditable() ||
      (document.querySelector(EDITABLE_SELECTOR) as
        | HTMLTextAreaElement
        | HTMLInputElement
        | null);
    if (el) insertTextAtCaretGeneric(msg.text);
    else enqueueInsert(msg.text);
  }
});

/** Install keyboard + clipboard handlers when running inside an iframe. */
export function installVSCodeIframeKeyboardBridge() {
  if (!inIframe()) return;

  const forward = (type: string, e: KeyboardEvent) => {
    try {
      window.parent.postMessage({ type, event: serializeKeyEvent(e) }, '*');
    } catch (_err) {
      void 0;
    }
  };

  const onKeyDown = async (e: KeyboardEvent) => {
    // Handle clipboard combos locally so OS shortcuts work inside the iframe
    if (isCopy(e)) {
      const text = getSelectedText();
      if (text) {
        e.preventDefault();
        e.stopPropagation();
        const ok = await writeClipboardText(text);
        if (!ok) parentClipboardWrite(text);
        return;
      }
    } else if (isCut(e)) {
      const el = activeEditable() as
        | HTMLInputElement
        | HTMLTextAreaElement
        | null;
      if (el) {
        e.preventDefault();
        e.stopPropagation();
        cutFromInput(el);
        return;
      }
    } else if (isUndo(e)) {
      e.preventDefault();
      e.stopPropagation();
      try {
        document.execCommand('undo');
      } catch {
        /* empty */
      }
      return;
    } else if (isRedo(e)) {
      e.preventDefault();
      e.stopPropagation();
      try {
        document.execCommand('redo');
      } catch {
        /* empty */
      }
      return;
    } else if (isPaste(e)) {
      const el = activeEditable() as
        | HTMLInputElement
        | HTMLTextAreaElement
        | (HTMLElement & { isContentEditable: boolean })
        | null;
      if (el) {
        e.preventDefault();
        e.stopPropagation();
        let text = await readClipboardText();
        if (!text) text = await parentClipboardRead();
        insertTextAtCaretGeneric(text);
        return;
      }
    }
    // Forward everything else so VS Code can handle global shortcuts
    forward('vscode-iframe-keydown', e);
  };

  const onKeyUp = (e: KeyboardEvent) => forward('vscode-iframe-keyup', e);
  const onKeyPress = (e: KeyboardEvent) => forward('vscode-iframe-keypress', e);

  // Capture phase to run before app handlers
  window.addEventListener('keydown', onKeyDown, true);
  window.addEventListener('keyup', onKeyUp, true);
  window.addEventListener('keypress', onKeyPress, true);
  document.addEventListener('keydown', onKeyDown, true);
  document.addEventListener('keyup', onKeyUp, true);
  document.addEventListener('keypress', onKeyPress, true);
}

/** Copy helper that prefers navigator.clipboard and falls back to the bridge. */
export async function writeClipboardViaBridge(text: string): Promise<boolean> {
  try {
    await navigator.clipboard.writeText(text);
    return true;
  } catch {
    parentClipboardWrite(text);
    return false;
  }
}

/** Paste helper that prefers navigator.clipboard and falls back to the bridge. */
export async function readClipboardViaBridge(): Promise<string> {
  try {
    return await navigator.clipboard.readText();
  } catch {
    return await parentClipboardRead();
  }
}

// Auto-install on import to make it robust
installVSCodeIframeKeyboardBridge();
</file>

<file path="frontend/src/vscode/ContextMenu.tsx">
import React, { useEffect, useRef, useState } from 'react';
import {
  readClipboardViaBridge,
  writeClipboardViaBridge,
} from '@/vscode/bridge';

type Point = { x: number; y: number };

function inIframe(): boolean {
  try {
    return window.self !== window.top;
  } catch {
    return true;
  }
}

function isEditable(
  target: EventTarget | null
): target is
  | HTMLInputElement
  | HTMLTextAreaElement
  | (HTMLElement & { isContentEditable: boolean }) {
  const el = target as HTMLElement | null;
  if (!el) return false;
  const tag = el.tagName?.toLowerCase();
  if (tag === 'input' || tag === 'textarea') return true;
  return !!el.isContentEditable;
}

async function readClipboardText(): Promise<string> {
  return await readClipboardViaBridge();
}
async function writeClipboardText(text: string): Promise<boolean> {
  return await writeClipboardViaBridge(text);
}

function getSelectedText(): string {
  const sel = window.getSelection();
  return sel ? sel.toString() : '';
}

function cutFromInput(el: HTMLInputElement | HTMLTextAreaElement) {
  const start = el.selectionStart ?? 0;
  const end = el.selectionEnd ?? 0;
  if (end > start) {
    const selected = el.value.slice(start, end);
    void writeClipboardText(selected);
    const before = el.value.slice(0, start);
    const after = el.value.slice(end);
    el.value = before + after;
    el.setSelectionRange(start, start);
    el.dispatchEvent(new Event('input', { bubbles: true }));
  }
}

function pasteIntoInput(
  el: HTMLInputElement | HTMLTextAreaElement,
  text: string
) {
  const start = el.selectionStart ?? 0;
  const end = el.selectionEnd ?? 0;
  const before = el.value.slice(0, start);
  const after = el.value.slice(end);
  el.value = before + text + after;
  const caret = start + text.length;
  el.setSelectionRange(caret, caret);
  el.dispatchEvent(new Event('input', { bubbles: true }));
}

export const WebviewContextMenu: React.FC = () => {
  const [visible, setVisible] = useState(false);
  const [pos, setPos] = useState<Point>({ x: 0, y: 0 });
  const [adjustedPos, setAdjustedPos] = useState<Point | null>(null);
  const [canCut, setCanCut] = useState<boolean>(false);
  const [canPaste, setCanPaste] = useState<boolean>(false);
  const targetRef = useRef<EventTarget | null>(null);
  const menuRef = useRef<HTMLDivElement | null>(null);

  useEffect(() => {
    if (!inIframe()) return;
    const onContext = (e: MouseEvent) => {
      e.preventDefault();
      targetRef.current = e.target;
      setPos({ x: e.clientX, y: e.clientY });
      // Decide whether Cut should be shown: only for editable targets with a selection
      const tgt = e.target as HTMLElement | null;
      let cut = false;
      let paste = false;
      if (tgt && (tgt as HTMLInputElement).selectionStart !== undefined) {
        const el = tgt as HTMLInputElement | HTMLTextAreaElement;
        const start = el.selectionStart ?? 0;
        const end = el.selectionEnd ?? 0;
        cut = end > start && !el.readOnly && !el.disabled;
        paste = !el.readOnly && !el.disabled;
      } else if (isEditable(tgt)) {
        const sel = window.getSelection();
        cut = !!sel && sel.toString().length > 0;
        paste = true;
      } else {
        cut = false;
        paste = false;
      }
      setCanCut(cut);
      setCanPaste(paste);
      setVisible(true);
    };
    const onClick = () => setVisible(false);
    document.addEventListener('contextmenu', onContext);
    document.addEventListener('click', onClick);
    window.addEventListener('blur', onClick);
    return () => {
      document.removeEventListener('contextmenu', onContext);
      document.removeEventListener('click', onClick);
      window.removeEventListener('blur', onClick);
    };
  }, []);

  // When menu becomes visible, adjust position to stay within viewport
  useEffect(() => {
    if (!visible) {
      setAdjustedPos(null);
      return;
    }
    const el = menuRef.current;
    if (!el) return;
    // Use a microtask to ensure layout is ready
    const id = requestAnimationFrame(() => {
      const menuW = el.offsetWidth;
      const menuH = el.offsetHeight;
      const vw = window.innerWidth;
      const vh = window.innerHeight;
      const margin = 4;
      let x = pos.x;
      let y = pos.y;
      if (x + menuW + margin > vw) x = Math.max(margin, vw - menuW - margin);
      if (y + menuH + margin > vh) y = Math.max(margin, vh - menuH - margin);
      setAdjustedPos({ x, y });
    });
    return () => cancelAnimationFrame(id);
  }, [visible, pos]);

  const close = () => setVisible(false);

  const onCopy = async () => {
    const tgt = targetRef.current as HTMLElement | null;
    let copied = false;
    if (tgt && (tgt as HTMLInputElement).selectionStart !== undefined) {
      const el = tgt as HTMLInputElement | HTMLTextAreaElement;
      const start = el.selectionStart ?? 0;
      const end = el.selectionEnd ?? 0;
      if (end > start) {
        const selected = el.value.slice(start, end);
        copied = await writeClipboardText(selected);
      }
    }
    if (!copied) {
      const sel = getSelectedText();
      if (sel) copied = await writeClipboardText(sel);
    }
    if (!copied) {
      try {
        document.execCommand('copy');
      } catch {
        /* empty */
      }
    }
    close();
  };

  const onCut = async () => {
    const tgt = targetRef.current as HTMLElement | null;
    if (
      tgt &&
      (tgt as HTMLInputElement).selectionStart !== undefined &&
      !(tgt as HTMLInputElement).readOnly &&
      !(tgt as HTMLInputElement).disabled
    ) {
      cutFromInput(tgt as HTMLInputElement | HTMLTextAreaElement);
    } else if (isEditable(tgt)) {
      // contentEditable: emulate cut by copying selection, then deleting via execCommand
      const sel = getSelectedText();
      if (sel) {
        await writeClipboardText(sel);
        try {
          document.execCommand('delete');
        } catch {
          /* empty */
        }
      }
    } else {
      // Read-only content: treat Cut as Copy for usability
      const sel = getSelectedText();
      if (sel) await writeClipboardText(sel);
    }
    close();
  };

  const onPaste = async () => {
    const text = await readClipboardText();
    const tgt = targetRef.current as HTMLElement | null;
    if (tgt && (tgt as HTMLInputElement).selectionStart !== undefined) {
      (tgt as HTMLElement).focus();
      pasteIntoInput(tgt as HTMLInputElement | HTMLTextAreaElement, text);
    } else if (isEditable(tgt)) {
      (tgt as HTMLElement).focus();
      document.execCommand('insertText', false, text);
    }
    close();
  };

  const onUndo = () => {
    try {
      document.execCommand('undo');
    } catch {
      /* empty */
    }
    close();
  };
  const onRedo = () => {
    try {
      document.execCommand('redo');
    } catch {
      /* empty */
    }
    close();
  };
  const onSelectAll = () => {
    try {
      document.execCommand('selectAll');
    } catch {
      /* empty */
    }
    close();
  };

  if (!visible) return null;

  return (
    <div
      ref={menuRef}
      style={{
        position: 'fixed',
        left: (adjustedPos ?? pos).x,
        top: (adjustedPos ?? pos).y,
        zIndex: 99999,
      }}
      className="min-w-[160px] rounded-md border border-gray-300 bg-white text-gray-900 shadow-lg dark:border-gray-700 dark:bg-gray-800 dark:text-gray-100"
      onContextMenu={(e) => e.preventDefault()}
    >
      <MenuItem label="Copy" onClick={onCopy} />
      {canCut && <MenuItem label="Cut" onClick={onCut} />}
      {canPaste && <MenuItem label="Paste" onClick={onPaste} />}
      <Divider />
      <MenuItem label="Undo" onClick={onUndo} />
      <MenuItem label="Redo" onClick={onRedo} />
      <Divider />
      <MenuItem label="Select All" onClick={onSelectAll} />
    </div>
  );
};

const MenuItem: React.FC<{ label: string; onClick: () => void }> = ({
  label,
  onClick,
}) => (
  <button
    className="block w-full px-3 py-1.5 text-left text-sm hover:bg-gray-100 dark:hover:bg-gray-700"
    onClick={onClick}
    type="button"
  >
    {label}
  </button>
);

const Divider: React.FC = () => (
  <div className="my-1 h-px bg-gray-200 dark:bg-gray-700" />
);
</file>

<file path="frontend/src/App.tsx">
import { useEffect } from 'react';
import { BrowserRouter, Navigate, Route, Routes } from 'react-router-dom';
import { I18nextProvider } from 'react-i18next';
import i18n from '@/i18n';
import { Navbar } from '@/components/layout/navbar';
import { Projects } from '@/pages/projects';
import { ProjectTasks } from '@/pages/project-tasks';
import { useTaskViewManager } from '@/hooks/useTaskViewManager';

import {
  AgentSettings,
  GeneralSettings,
  McpSettings,
  SettingsLayout,
} from '@/pages/settings/';
import {
  UserSystemProvider,
  useUserSystem,
} from '@/components/config-provider';
import { ThemeProvider } from '@/components/theme-provider';
import { SearchProvider } from '@/contexts/search-context';

import { ProjectProvider } from '@/contexts/project-context';
import { ThemeMode } from 'shared/types';
import * as Sentry from '@sentry/react';
import { Loader } from '@/components/ui/loader';

import { AppWithStyleOverride } from '@/utils/style-override';
import { WebviewContextMenu } from '@/vscode/ContextMenu';
import { DevBanner } from '@/components/DevBanner';
import NiceModal from '@ebay/nice-modal-react';
import { OnboardingResult } from './components/dialogs/global/OnboardingDialog';

const SentryRoutes = Sentry.withSentryReactRouterV6Routing(Routes);

function AppContent() {
  const { config, updateAndSaveConfig, loading } = useUserSystem();
  const { isFullscreen } = useTaskViewManager();

  const showNavbar = !isFullscreen;

  useEffect(() => {
    let cancelled = false;

    const handleOnboardingComplete = async (
      onboardingConfig: OnboardingResult
    ) => {
      if (cancelled) return;
      const updatedConfig = {
        ...config,
        onboarding_acknowledged: true,
        executor_profile: onboardingConfig.profile,
        editor: onboardingConfig.editor,
      };

      updateAndSaveConfig(updatedConfig);
    };

    const handleDisclaimerAccept = async () => {
      if (cancelled) return;
      await updateAndSaveConfig({ disclaimer_acknowledged: true });
    };

    const handleGitHubLoginComplete = async () => {
      if (cancelled) return;
      await updateAndSaveConfig({ github_login_acknowledged: true });
    };

    const handleTelemetryOptIn = async (analyticsEnabled: boolean) => {
      if (cancelled) return;
      await updateAndSaveConfig({
        telemetry_acknowledged: true,
        analytics_enabled: analyticsEnabled,
      });
    };

    const handleReleaseNotesClose = async () => {
      if (cancelled) return;
      await updateAndSaveConfig({ show_release_notes: false });
    };

    const checkOnboardingSteps = async () => {
      if (!config || cancelled) return;

      if (!config.disclaimer_acknowledged) {
        await NiceModal.show('disclaimer');
        await handleDisclaimerAccept();
        await NiceModal.hide('disclaimer');
      }

      if (!config.onboarding_acknowledged) {
        const onboardingResult: OnboardingResult =
          await NiceModal.show('onboarding');
        await handleOnboardingComplete(onboardingResult);
        await NiceModal.hide('onboarding');
      }

      if (!config.github_login_acknowledged) {
        await NiceModal.show('github-login');
        await handleGitHubLoginComplete();
        await NiceModal.hide('github-login');
      }

      if (!config.telemetry_acknowledged) {
        const analyticsEnabled: boolean =
          await NiceModal.show('privacy-opt-in');
        await handleTelemetryOptIn(analyticsEnabled);
        await NiceModal.hide('privacy-opt-in');
      }

      if (config.show_release_notes) {
        await NiceModal.show('release-notes');
        await handleReleaseNotesClose();
        await NiceModal.hide('release-notes');
      }
    };

    const runOnboarding = async () => {
      if (!config || cancelled) return;
      await checkOnboardingSteps();
    };

    runOnboarding();

    return () => {
      cancelled = true;
    };
  }, [config]);

  if (loading) {
    return (
      <div className="min-h-screen bg-background flex items-center justify-center">
        <Loader message="Loading..." size={32} />
      </div>
    );
  }

  return (
    <I18nextProvider i18n={i18n}>
      <ThemeProvider initialTheme={config?.theme || ThemeMode.SYSTEM}>
        <AppWithStyleOverride>
          <SearchProvider>
            <div className="h-screen flex flex-col bg-background">
              {/* Custom context menu and VS Code-friendly interactions when embedded in iframe */}
              <WebviewContextMenu />

              {showNavbar && <DevBanner />}
              {showNavbar && <Navbar />}
              <div className="flex-1 h-full overflow-y-scroll">
                <SentryRoutes>
                  <Route path="/" element={<Projects />} />
                  <Route path="/projects" element={<Projects />} />
                  <Route path="/projects/:projectId" element={<Projects />} />
                  <Route
                    path="/projects/:projectId/tasks"
                    element={<ProjectTasks />}
                  />
                  <Route
                    path="/projects/:projectId/tasks/:taskId/attempts/:attemptId"
                    element={<ProjectTasks />}
                  />
                  <Route
                    path="/projects/:projectId/tasks/:taskId/attempts/:attemptId/full"
                    element={<ProjectTasks />}
                  />
                  <Route
                    path="/projects/:projectId/tasks/:taskId/full"
                    element={<ProjectTasks />}
                  />
                  <Route
                    path="/projects/:projectId/tasks/:taskId"
                    element={<ProjectTasks />}
                  />
                  <Route path="/settings/*" element={<SettingsLayout />}>
                    <Route index element={<Navigate to="general" replace />} />
                    <Route path="general" element={<GeneralSettings />} />
                    <Route path="agents" element={<AgentSettings />} />
                    <Route path="mcp" element={<McpSettings />} />
                  </Route>
                  {/* Redirect old MCP route */}
                  <Route
                    path="/mcp-servers"
                    element={<Navigate to="/settings/mcp" replace />}
                  />
                </SentryRoutes>
              </div>
            </div>
          </SearchProvider>
        </AppWithStyleOverride>
      </ThemeProvider>
    </I18nextProvider>
  );
}

function App() {
  return (
    <BrowserRouter>
      <UserSystemProvider>
        <ProjectProvider>
          <NiceModal.Provider>
            <AppContent />
          </NiceModal.Provider>
        </ProjectProvider>
      </UserSystemProvider>
    </BrowserRouter>
  );
}

export default App;
</file>

<file path="frontend/src/main.tsx">
import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App.tsx';
import './styles/index.css';
import { ClickToComponent } from 'click-to-react-component';
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';
import * as Sentry from '@sentry/react';
import NiceModal from '@ebay/nice-modal-react';
// Import modal type definitions
import './types/modals';
// Import and register modals
import {
  GitHubLoginDialog,
  CreatePRDialog,
  ConfirmDialog,
  DisclaimerDialog,
  OnboardingDialog,
  PrivacyOptInDialog,
  ProvidePatDialog,
  ReleaseNotesDialog,
  TaskFormDialog,
  EditorSelectionDialog,
  DeleteTaskConfirmationDialog,
  FolderPickerDialog,
  TaskTemplateEditDialog,
  RebaseDialog,
  CreateConfigurationDialog,
  DeleteConfigurationDialog,
  ProjectFormDialog,
  ProjectEditorSelectionDialog,
  RestoreLogsDialog,
} from './components/dialogs';

// Register modals
NiceModal.register('github-login', GitHubLoginDialog);
NiceModal.register('create-pr', CreatePRDialog);
NiceModal.register('confirm', ConfirmDialog);
NiceModal.register('disclaimer', DisclaimerDialog);
NiceModal.register('onboarding', OnboardingDialog);
NiceModal.register('privacy-opt-in', PrivacyOptInDialog);
NiceModal.register('provide-pat', ProvidePatDialog);
NiceModal.register('release-notes', ReleaseNotesDialog);
NiceModal.register('delete-task-confirmation', DeleteTaskConfirmationDialog);
NiceModal.register('task-form', TaskFormDialog);
NiceModal.register('editor-selection', EditorSelectionDialog);
NiceModal.register('folder-picker', FolderPickerDialog);
NiceModal.register('task-template-edit', TaskTemplateEditDialog);
NiceModal.register('rebase-dialog', RebaseDialog);
NiceModal.register('create-configuration', CreateConfigurationDialog);
NiceModal.register('delete-configuration', DeleteConfigurationDialog);
NiceModal.register('project-form', ProjectFormDialog);
NiceModal.register('project-editor-selection', ProjectEditorSelectionDialog);
NiceModal.register('restore-logs', RestoreLogsDialog);
// Install VS Code iframe keyboard bridge when running inside an iframe
import './vscode/bridge';

import {
  useLocation,
  useNavigationType,
  createRoutesFromChildren,
  matchRoutes,
} from 'react-router-dom';

Sentry.init({
  dsn: 'https://1065a1d276a581316999a07d5dffee26@o4509603705192449.ingest.de.sentry.io/4509605576441937',
  tracesSampleRate: 1.0,
  environment: import.meta.env.MODE === 'development' ? 'dev' : 'production',
  integrations: [
    Sentry.reactRouterV6BrowserTracingIntegration({
      useEffect: React.useEffect,
      useLocation,
      useNavigationType,
      createRoutesFromChildren,
      matchRoutes,
    }),
  ],
});
Sentry.setTag('source', 'frontend');

const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      staleTime: 1000 * 60 * 5, // 5 minutes
      refetchOnWindowFocus: false,
    },
  },
});

ReactDOM.createRoot(document.getElementById('root')!).render(
  <React.StrictMode>
    <QueryClientProvider client={queryClient}>
      <Sentry.ErrorBoundary fallback={<p>An error has occurred</p>} showDialog>
        <ClickToComponent />
        <App />
        {/* <ReactQueryDevtools initialIsOpen={false} /> */}
      </Sentry.ErrorBoundary>
    </QueryClientProvider>
  </React.StrictMode>
);
</file>

<file path="frontend/src/vite-env.d.ts">
/// <reference types="vite/client" />
</file>

<file path="frontend/.eslintrc.cjs">
const i18nCheck = process.env.LINT_I18N === 'true';

module.exports = {
  root: true,
  env: {
    browser: true,
    es2020: true,
  },
  extends: [
    'eslint:recommended',
    'plugin:@typescript-eslint/recommended',
    'plugin:react-hooks/recommended',
    'plugin:i18next/recommended',
    'prettier',
  ],
  ignorePatterns: ['dist', '.eslintrc.cjs'],
  parser: '@typescript-eslint/parser',
  plugins: ['react-refresh', '@typescript-eslint', 'unused-imports', 'i18next'],
  parserOptions: {
    ecmaVersion: 'latest',
    sourceType: 'module',
  },
  rules: {
    'react-refresh/only-export-components': 'off',
    'unused-imports/no-unused-imports': 'error',
    'unused-imports/no-unused-vars': [
      'error',
      {
        vars: 'all',
        args: 'after-used',
        ignoreRestSiblings: false,
      },
    ],
    '@typescript-eslint/no-explicit-any': 'warn',
    // i18n rule - only active when LINT_I18N=true
    'i18next/no-literal-string': i18nCheck
      ? [
          'warn',
          {
            markupOnly: true,
            ignoreAttribute: [
              'data-testid',
              'to',
              'href',
              'id',
              'key',
              'type',
              'role',
              'className',
              'style',
              'aria-describedby',
            ],
          },
        ]
      : 'off',
  },
  overrides: [
    {
      files: ['**/*.test.{ts,tsx}', '**/*.stories.{ts,tsx}'],
      rules: {
        'i18next/no-literal-string': 'off',
      },
    },
  ],
};
</file>

<file path="frontend/.prettierrc.json">
{
  "semi": true,
  "trailingComma": "es5",
  "singleQuote": true,
  "printWidth": 80,
  "tabWidth": 2,
  "useTabs": false
}
</file>

<file path="frontend/components.json">
{
  "$schema": "https://ui.shadcn.com/schema.json",
  "style": "default",
  "rsc": false,
  "tsx": true,
  "tailwind": {
    "config": "tailwind.config.js",
    "css": "src/index.css",
    "baseColor": "slate",
    "cssVariables": true,
    "prefix": ""
  },
  "aliases": {
    "components": "@/components",
    "utils": "@/lib/utils"
  }
}
</file>

<file path="frontend/index.html">
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="icon" type="image/x-icon" href="/favicon.ico" />
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="manifest" href="/site.webmanifest" />
    <link rel="icon" type="image/png" sizes="192x192" href="/android-chrome-192x192.png" />
    <link rel="icon" type="image/png" sizes="512x512" href="/android-chrome-512x512.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>vibe-kanban</title>
</head>

<body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
</body>

</html>
</file>

<file path="frontend/package.json">
{
  "name": "vibe-kanban",
  "private": true,
  "version": "0.0.55",
  "type": "module",
  "scripts": {
    "dev": "VITE_OPEN=${VITE_OPEN:-false} vite",
    "build": "tsc && vite build",
    "check": "tsc --noEmit",
    "preview": "vite preview",
    "lint": "eslint . --ext ts,tsx --report-unused-disable-directives --max-warnings 100",
    "lint:fix": "eslint . --ext ts,tsx --fix",
    "lint:i18n": "LINT_I18N=true eslint . --ext ts,tsx --max-warnings 0",
    "format": "prettier --write \"src/**/*.{ts,tsx,js,jsx,json,css,md}\"",
    "format:check": "prettier --check \"src/**/*.{ts,tsx,js,jsx,json,css,md}\""
  },
  "dependencies": {
    "@codemirror/lang-json": "^6.0.2",
    "@codemirror/language": "^6.11.2",
    "@codemirror/lint": "^6.8.5",
    "@codemirror/view": "^6.38.1",
    "@dnd-kit/core": "^6.3.1",
    "@dnd-kit/modifiers": "^9.0.0",
    "@git-diff-view/file": "^0.0.30",
    "@git-diff-view/react": "^0.0.30",
    "@radix-ui/react-dropdown-menu": "^2.1.15",
    "@radix-ui/react-label": "^2.1.7",
    "@radix-ui/react-select": "^2.2.5",
    "@radix-ui/react-slot": "^1.2.3",
    "@radix-ui/react-tabs": "^1.1.12",
    "@radix-ui/react-tooltip": "^1.2.7",
    "@rjsf/shadcn": "6.0.0-beta.10",
    "@sentry/react": "^9.34.0",
    "@sentry/vite-plugin": "^3.5.0",
    "@tailwindcss/typography": "^0.5.16",
    "@tanstack/react-query": "^5.85.5",
    "@types/react-window": "^1.8.8",
    "@uiw/react-codemirror": "^4.25.1",
    "@virtuoso.dev/message-list": "^1.13.3",
    "class-variance-authority": "^0.7.0",
    "click-to-react-component": "^1.1.2",
    "clsx": "^2.0.0",
    "embla-carousel-react": "^8.6.0",
    "fancy-ansi": "^0.1.3",
    "i18next": "^25.5.2",
    "i18next-browser-languagedetector": "^8.2.0",
    "lucide-react": "^0.539.0",
    "markdown-to-jsx": "^7.7.13",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "react-i18next": "^15.7.3",
    "react-router-dom": "^6.8.1",
    "react-virtuoso": "^4.14.0",
    "react-window": "^1.8.11",
    "rfc6902": "^5.1.2",
    "tailwind-merge": "^2.2.0",
    "tailwindcss-animate": "^1.0.7",
    "zustand": "^4.5.4"
  },
  "devDependencies": {
    "@rjsf/core": "6.0.0-beta.11",
    "@rjsf/utils": "6.0.0-beta.11",
    "@rjsf/validator-ajv8": "6.0.0-beta.11",
    "@tailwindcss/container-queries": "^0.1.1",
    "@types/react": "^18.2.43",
    "@types/react-dom": "^18.2.17",
    "@typescript-eslint/eslint-plugin": "^6.21.0",
    "@typescript-eslint/parser": "^6.21.0",
    "@vitejs/plugin-react": "^4.2.1",
    "autoprefixer": "^10.4.16",
    "eslint": "^8.55.0",
    "eslint-config-prettier": "^10.1.5",
    "eslint-plugin-i18next": "^6.1.3",
    "eslint-plugin-prettier": "^5.5.0",
    "eslint-plugin-react-hooks": "^4.6.0",
    "eslint-plugin-react-refresh": "^0.4.5",
    "eslint-plugin-unused-imports": "^4.1.4",
    "postcss": "^8.4.32",
    "prettier": "^3.6.1",
    "tailwindcss": "^3.4.0",
    "typescript": "^5.9.2",
    "vite": "^5.0.8"
  }
}
</file>

<file path="frontend/postcss.config.js">
export default {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
}
</file>

<file path="frontend/tailwind.config.js">
/** @type {import('tailwindcss').Config} */
module.exports = {
  darkMode: ["class"],
  content: [
    './pages/**/*.{ts,tsx}',
    './components/**/*.{ts,tsx}',
    './app/**/*.{ts,tsx}',
    './src/**/*.{ts,tsx}',
    "node_modules/@rjsf/shadcn/src/**/*.{js,ts,jsx,tsx,mdx}"
  ],
  safelist: [
    'xl:hidden',
    'xl:relative',
    'xl:inset-auto',
    'xl:z-auto',
    'xl:h-full',
    'xl:w-[800px]',
    'xl:flex',
    'xl:flex-1',
    'xl:min-w-0',
    'xl:overflow-y-auto',
    'xl:opacity-100',
    'xl:pointer-events-auto',
  ],
  prefix: "",
  theme: {
    container: {
      center: true,
      padding: "2rem",
      screens: {
        "2xl": "1400px",
      },
    },
    extend: {
      backgroundImage: {
        'diagonal-lines': `
          repeating-linear-gradient(-45deg, hsl(var(--border) / 0.4) 0 2px, transparent 1px 12px),
          linear-gradient(hsl(var(--background)), hsl(var(--background)))
        `,
      },
      ringColor: {
        DEFAULT: 'hsl(var(--primary))', // e.g. Tailwind's blue-500
      },
      fontSize: { // These are downshifted by 1
        xs: ['0.625rem', { lineHeight: '0.875rem' }], // 10px / 14px
        sm: ['0.75rem', { lineHeight: '1rem' }],     // 12px / 16px
        base: ['0.875rem', { lineHeight: '1.25rem' }],  // 14px / 20px
        lg: ['1rem', { lineHeight: '1.5rem' }],   // 16px / 24px
        xl: ['1.125rem', { lineHeight: '1.75rem' }],  // 18px / 28px
      },
      colors: {
        border: "hsl(var(--border))",
        input: "hsl(var(--input))",
        ring: "hsl(var(--ring))",
        background: "hsl(var(--background))",
        foreground: "hsl(var(--foreground))",
        primary: {
          DEFAULT: "hsl(var(--primary))",
          foreground: "hsl(var(--primary-foreground))",
        },
        secondary: {
          DEFAULT: "hsl(var(--secondary))",
          foreground: "hsl(var(--secondary-foreground))",
        },
        destructive: {
          DEFAULT: "hsl(var(--destructive))",
          foreground: "hsl(var(--destructive-foreground))",
        },
        muted: {
          DEFAULT: "hsl(var(--muted))",
          foreground: "hsl(var(--muted-foreground))",
        },
        accent: {
          DEFAULT: "hsl(var(--accent))",
          foreground: "hsl(var(--accent-foreground))",
        },
        popover: {
          DEFAULT: "hsl(var(--popover))",
          foreground: "hsl(var(--popover-foreground))",
        },
        card: {
          DEFAULT: "hsl(var(--card))",
          foreground: "hsl(var(--card-foreground))",
        },
        success: {
          DEFAULT: "hsl(var(--success))",
          foreground: "hsl(var(--success-foreground))",
        },
        warning: {
          DEFAULT: "hsl(var(--warning))",
          foreground: "hsl(var(--warning-foreground))",
        },
        info: {
          DEFAULT: "hsl(var(--info))",
          foreground: "hsl(var(--info-foreground))",
        },
        neutral: {
          DEFAULT: "hsl(var(--neutral))",
          foreground: "hsl(var(--neutral-foreground))",
        },
        status: {
          init: "hsl(var(--status-init))",
          "init-foreground": "hsl(var(--status-init-foreground))",
          running: "hsl(var(--status-running))",
          "running-foreground": "hsl(var(--status-running-foreground))",
          complete: "hsl(var(--status-complete))",
          "complete-foreground": "hsl(var(--status-complete-foreground))",
          failed: "hsl(var(--status-failed))",
          "failed-foreground": "hsl(var(--status-failed-foreground))",
          paused: "hsl(var(--status-paused))",
          "paused-foreground": "hsl(var(--status-paused-foreground))",
        },
        console: {
          DEFAULT: "hsl(var(--console-background))",
          foreground: "hsl(var(--console-foreground))",
          success: "hsl(var(--console-success))",
          error: "hsl(var(--console-error))",
        },
      },
      borderRadius: {
        lg: "var(--radius)",
        md: "calc(var(--radius) - 2px)",
        sm: "calc(var(--radius) - 4px)",
      },
      fontFamily: {
        'chivo-mono': ['Chivo Mono', 'Noto Emoji', 'monospace'],
      },
      keyframes: {
        "accordion-down": {
          from: { height: "0" },
          to: { height: "var(--radix-accordion-content-height)" },
        },
        "accordion-up": {
          from: { height: "var(--radix-accordion-content-height)" },
          to: { height: "0" },
        },
        pill: {
          '0%': { opacity: '0' },
          '10%': { opacity: '1' },
          '80%': { opacity: '1' },
          '100%': { opacity: '0' },
        },
      },
      animation: {
        "accordion-down": "accordion-down 0.2s ease-out",
        "accordion-up": "accordion-up 0.2s ease-out",
        pill: 'pill 2s ease-in-out forwards',
      },
    },
  },
  plugins: [require("tailwindcss-animate"), require("@tailwindcss/container-queries")],
}
</file>

<file path="frontend/tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2020",
    "useDefineForClassFields": true,
    "lib": ["ES2020", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "skipLibCheck": true,
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noFallthroughCasesInSwitch": true,
    "baseUrl": ".",
    "paths": {
      "@/*": ["./src/*"],
      "@dialogs/*": ["./src/components/dialogs/*"],
      "shared/*": ["../shared/*"]
    }
  },
  "include": ["src"],
  "references": [{ "path": "./tsconfig.node.json" }]
}
</file>

<file path="frontend/tsconfig.node.json">
{
  "compilerOptions": {
    "composite": true,
    "skipLibCheck": true,
    "module": "ESNext",
    "moduleResolution": "bundler",
    "allowSyntheticDefaultImports": true
  },
  "include": ["vite.config.ts"]
}
</file>

<file path="frontend/vite.config.ts">
// vite.config.ts
import { sentryVitePlugin } from "@sentry/vite-plugin";
import { defineConfig, Plugin } from "vite";
import react from "@vitejs/plugin-react";
import path from "path";
import fs from "fs";

function executorSchemasPlugin(): Plugin {
  const VIRTUAL_ID = "virtual:executor-schemas";
  const RESOLVED_VIRTUAL_ID = "\0" + VIRTUAL_ID;

  return {
    name: "executor-schemas-plugin",
    resolveId(id) {
      if (id === VIRTUAL_ID) return RESOLVED_VIRTUAL_ID; // keep it virtual
      return null;
    },
    load(id) {
      if (id !== RESOLVED_VIRTUAL_ID) return null;

      const schemasDir = path.resolve(__dirname, "../shared/schemas");
      const files = fs.existsSync(schemasDir)
        ? fs.readdirSync(schemasDir).filter((f) => f.endsWith(".json"))
        : [];

      const imports: string[] = [];
      const entries: string[] = [];

      files.forEach((file, i) => {
        const varName = `__schema_${i}`;
        const importPath = `shared/schemas/${file}`; // uses your alias
        const key = file.replace(/\.json$/, "").toUpperCase(); // claude_code -> CLAUDE_CODE
        imports.push(`import ${varName} from "${importPath}";`);
        entries.push(`  "${key}": ${varName}`);
      });

      // IMPORTANT: pure JS (no TS types), and quote keys.
      const code = `
${imports.join("\n")}

export const schemas = {
${entries.join(",\n")}
};

export default schemas;
`;
      return code;
    },
  };
}

export default defineConfig({
  plugins: [
    react(),
    sentryVitePlugin({ org: "bloop-ai", project: "vibe-kanban" }),
    executorSchemasPlugin(),
  ],
  resolve: {
    alias: {
      "@": path.resolve(__dirname, "./src"),
      shared: path.resolve(__dirname, "../shared"),
    },
  },
  server: {
    port: parseInt(process.env.FRONTEND_PORT || "3000"),
    proxy: {
      "/api": {
        target: `http://localhost:${process.env.BACKEND_PORT || "3001"}`,
        changeOrigin: true,
        ws: true,
      },
    },
    fs: {
      allow: [path.resolve(__dirname, "."), path.resolve(__dirname, "..")],
    },
    open: process.env.VITE_OPEN === "true",
  },
  build: { sourcemap: true },
});
</file>

<file path="npx-cli/bin/cli.js">
#!/usr/bin/env node

const { execSync, spawn } = require("child_process");
const path = require("path");
const fs = require("fs");

// Detect true CPU arch on macOS (handles Rosetta)
function getUnderlyingArch() {
  const platform = process.platform;
  const nodeArch = process.arch;

  if (platform !== "darwin") {
    return nodeArch;
  }

  // If Node itself is arm64, we’re natively on Apple silicon
  if (nodeArch === "arm64") {
    return "arm64";
  }

  // Otherwise check for Rosetta translation
  try {
    const translated = execSync("sysctl -in sysctl.proc_translated", {
      encoding: "utf8",
    }).trim();
    if (translated === "1") {
      return "arm64";
    }
  } catch {
    // sysctl key not present → assume true Intel
  }

  return "x64";
}

const platform = process.platform;
const arch = getUnderlyingArch();

// Map to our build target names
function getPlatformDir() {
  if (platform === "linux" && arch === "x64") return "linux-x64";
  if (platform === "linux" && arch === "arm64") return "linux-arm64";
  if (platform === "win32" && arch === "x64") return "windows-x64";
  if (platform === "win32" && arch === "arm64") return "windows-arm64";
  if (platform === "darwin" && arch === "x64") return "macos-x64";
  if (platform === "darwin" && arch === "arm64") return "macos-arm64";

  console.error(`❌ Unsupported platform: ${platform}-${arch}`);
  console.error("Supported platforms:");
  console.error("  - Linux x64");
  console.error("  - Linux ARM64");
  console.error("  - Windows x64");
  console.error("  - Windows ARM64");
  console.error("  - macOS x64 (Intel)");
  console.error("  - macOS ARM64 (Apple Silicon)");
  process.exit(1);
}

function getBinaryName(base) {
  return platform === "win32" ? `${base}.exe` : base;
}

const platformDir = getPlatformDir();
const extractDir = path.join(__dirname, "..", "dist", platformDir);
const isMcpMode = process.argv.includes("--mcp");

// ensure output dir
fs.mkdirSync(extractDir, { recursive: true });

function extractAndRun(baseName, launch) {
  const binName = getBinaryName(baseName);
  const binPath = path.join(extractDir, binName);
  const zipName = `${baseName}.zip`;
  const zipPath = path.join(extractDir, zipName);

  // clean old binary
  if (fs.existsSync(binPath)) fs.unlinkSync(binPath);
  if (!fs.existsSync(zipPath)) {
    console.error(`❌ ${zipName} not found at: ${zipPath}`);
    console.error(`Current platform: ${platform}-${arch} (${platformDir})`);
    process.exit(1);
  }

  // extract
  const unzipCmd =
    platform === "win32"
      ? `powershell -Command "Expand-Archive -Path '${zipPath}' -DestinationPath '${extractDir}' -Force"`
      : `unzip -qq -o "${zipPath}" -d "${extractDir}"`;
  execSync(unzipCmd, { stdio: "inherit" });

  // perms & launch
  if (platform !== "win32") {
    try {
      fs.chmodSync(binPath, 0o755);
    } catch { }
  }
  return launch(binPath);
}

if (isMcpMode) {
  extractAndRun("vibe-kanban-mcp", (bin) => {
    const proc = spawn(bin, [], { stdio: "inherit" });
    proc.on("exit", (c) => process.exit(c || 0));
    proc.on("error", (e) => {
      console.error("❌ MCP server error:", e.message);
      process.exit(1);
    });
    process.on("SIGINT", () => {
      console.error("\n🛑 Shutting down MCP server...");
      proc.kill("SIGINT");
    });
    process.on("SIGTERM", () => proc.kill("SIGTERM"));
  });
} else {
  console.log(`📦 Extracting vibe-kanban...`);
  extractAndRun("vibe-kanban", (bin) => {
    console.log(`🚀 Launching vibe-kanban...`);
    if (platform === "win32") {
      execSync(`"${bin}"`, { stdio: "inherit" });
    } else {
      execSync(`"${bin}"`, { stdio: "inherit" });
    }
  });
}
</file>

<file path="npx-cli/package.json">
{
  "name": "vibe-kanban",
  "private": false,
  "version": "0.0.94",
  "main": "index.js",
  "bin": {
    "vibe-kanban": "bin/cli.js"
  },
  "keywords": [],
  "author": "bloop",
  "license": "",
  "description": "NPX wrapper around vibe-kanban and vibe-kanban-mcp",
  "files": [
    "dist",
    "bin"
  ]
}
</file>

<file path="npx-cli/README.md">
# Vibe Kanban

> A visual project management tool for developers that integrates with git repositories and coding agents like Claude Code and Amp.

## Quick Start

Run vibe kanban instantly without installation:

```bash
npx vibe-kanban
```

This will launch the application locally and open it in your browser automatically.

## What is Vibe Kanban?

Vibe Kanban is a modern project management tool designed specifically for developers. It helps you organize your coding projects with kanban-style task management while providing powerful integrations with git repositories and AI coding agents.

### ✨ Key Features

**🗂️ Project Management**
- Add git repositories as projects (existing or create new ones)
- Automatic git integration and repository validation
- Project search functionality across all files
- Custom setup and development scripts per project

**📋 Task Management**
- Create and manage tasks with kanban-style boards
- Task status tracking (Todo, In Progress, Done)
- Rich task descriptions and notes
- Task execution with multiple AI agents

**🤖 AI Agent Integration**
- **Claude**: Advanced AI coding assistant
- **Amp**: Powerful development agent
- **Echo**: Simple testing/debugging agent
- Create tasks and immediately start agent execution
- Follow-up task execution for iterative development

**⚡ Development Workflow**
- Create isolated git worktrees for each task attempt
- View diffs of changes made by agents
- Merge successful changes back to main branch
- Rebase task branches to stay up-to-date
- Manual file editing and deletion
- Integrated development server support

**🎛️ Developer Tools**
- Browse and validate git repositories from filesystem
- Open task worktrees in your preferred editor (VS Code, Cursor, Windsurf, IntelliJ, Zed)
- Real-time execution monitoring and process control
- Stop running processes individually or all at once
- Sound notifications for task completion

## How It Works

1. **Add Projects**: Import existing git repositories or create new ones
2. **Create Tasks**: Define what needs to be built or fixed
3. **Execute with AI**: Let coding agents work on your tasks in isolated environments
4. **Review Changes**: See exactly what was modified using git diffs
5. **Merge Results**: Incorporate successful changes into your main codebase

## Core Functionality

Vibe Kanban provides a complete project management experience with these key capabilities:

**Project Repository Management**
- Full CRUD operations for managing coding projects
- Automatic git repository detection and validation  
- Initialize new repositories or import existing ones
- Project-wide file search functionality

**Task Lifecycle Management**
- Create, update, and delete tasks with rich descriptions
- Track task progress through customizable status workflows
- One-click task creation with immediate AI agent execution
- Task attempt tracking with detailed execution history

**AI Agent Execution Environment**
- Isolated git worktrees for safe code experimentation
- Real-time execution monitoring and activity logging
- Process management with ability to stop individual or all processes
- Support for follow-up executions to iterate on solutions

**Code Change Management**
- View detailed diffs of all changes made during task execution
- Branch status monitoring to track divergence from main
- One-click merging of successful changes back to main branch
- Automatic rebasing to keep task branches up-to-date
- Manual file deletion and cleanup capabilities

**Development Integration**
- Open task worktrees directly in your preferred code editor
- Start and manage development servers for testing changes
- Browse local filesystem to add new projects
- Health monitoring for service availability

## Configuration

Vibe Kanban supports customization through its configuration system:

- **Editor Integration**: Choose your preferred code editor
- **Sound Notifications**: Customize completion sounds
- **Project Defaults**: Set default setup and development scripts

## Technical Architecture

- **Backend**: Rust with Axum web framework
- **Frontend**: React with TypeScript
- **Database**: SQLite for local data storage
- **Git Integration**: Native git operations for repository management
- **Process Management**: Tokio-based async execution monitoring

## Requirements

- Node.js (for npx execution)
- Git (for repository operations)
- Your preferred code editor (optional, for opening task worktrees)

## Supported Platforms

- Linux x64
- Windows x64
- macOS x64 (Intel)
- macOS ARM64 (Apple Silicon)

## Use Cases

**🔧 Bug Fixes**
- Create a task describing the bug
- Let an AI agent analyze and fix the issue
- Review the proposed changes
- Merge if satisfied, or provide follow-up instructions

**✨ Feature Development**
- Break down features into manageable tasks
- Use agents for initial implementation
- Iterate with follow-up executions
- Test using integrated development servers

**🚀 Project Setup**
- Bootstrap new projects with AI assistance
- Set up development environments
- Configure build and deployment scripts

**📚 Code Documentation**
- Generate documentation for existing code
- Create README files and API documentation
- Maintain up-to-date project information

---

**Ready to supercharge your development workflow?**

```bash
npx vibe-kanban
```

*Start managing your projects with the power of AI coding agents today!*
</file>

<file path="scripts/check-i18n.sh">
#!/usr/bin/env bash
# i18n regression check script
# Compares i18next/no-literal-string violations between PR and main branch
# Initial implementation: This script will show high violation counts until enforcement is enabled
set -eo pipefail

WORKTREE_BASE="$(mktemp -d)"
RULE="i18next/no-literal-string"

# Function that outputs violation count to stdout
lint_count() {
  local dir=$1
  local tmp
  tmp=$(mktemp)
  
  trap 'rm -f "$tmp"' RETURN
  
  (
    set -eo pipefail
    cd "$REPO_ROOT/frontend"
    # Use ESLint from main workspace but lint files in the target directory
    LINT_I18N=true npx eslint "$dir/frontend" \
      --ext ts,tsx \
      --format json \
      --output-file "$tmp" \
      --no-error-on-unmatched-pattern \
      > /dev/null 2>&1 || true  # Don't fail on violations
  )
  
  # Parse the clean JSON file
  jq --arg RULE "$RULE" \
     '[.[].messages[] | select(.ruleId == $RULE)] | length' "$tmp" \
     || echo "0"
}

echo "▶️  Counting literal strings in PR branch..."
REPO_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
PR_COUNT=$(lint_count "$REPO_ROOT")

BASE_REF="${GITHUB_BASE_REF:-main}"
echo "▶️  Checking out $BASE_REF for baseline..."
git fetch --depth=1 origin "$BASE_REF" 2>/dev/null || git fetch --depth=1 origin "$BASE_REF"
git worktree add "$WORKTREE_BASE" "origin/$BASE_REF" 2>/dev/null || {
  echo "Could not create worktree, falling back to direct checkout"
  TEMP_BRANCH="temp-i18n-check-$$"
  git checkout -b "$TEMP_BRANCH" "origin/$BASE_REF" 2>/dev/null || git checkout "origin/$BASE_REF"
  BASE_COUNT=$(lint_count "$REPO_ROOT")
  git checkout - 2>/dev/null || true
  git branch -D "$TEMP_BRANCH" 2>/dev/null || true
}

# Get base count from worktree if it was created successfully
if [ -d "$WORKTREE_BASE" ]; then
  BASE_COUNT=$(lint_count "$WORKTREE_BASE")
  git worktree remove "$WORKTREE_BASE" 2>/dev/null || rm -rf "$WORKTREE_BASE"
fi

# Ensure BASE_COUNT has a value
BASE_COUNT="${BASE_COUNT:-0}"

echo ""
echo "📊 I18n Violation Summary:"
echo "   Base branch ($BASE_REF): $BASE_COUNT violations"
echo "   PR branch: $PR_COUNT violations"
echo ""

if (( PR_COUNT > BASE_COUNT )); then
  echo "❌ PR introduces $((PR_COUNT - BASE_COUNT)) new hard-coded strings."
  echo ""
  echo "💡 To fix, replace hardcoded strings with translation calls:"
  echo "   Before: <Button>Save</Button>"
  echo "   After:  <Button>{t('buttons.save')}</Button>"
  echo ""
  echo "Files with new violations:"
  (cd "$REPO_ROOT/frontend" && LINT_I18N=true npx eslint . --ext ts,tsx --rule "$RULE:error" -f codeframe 2>/dev/null || true)
  exit 1
elif (( PR_COUNT < BASE_COUNT )); then
  echo "🎉 Great job! PR removes $((BASE_COUNT - PR_COUNT)) hard-coded strings."
  echo "   This helps improve i18n coverage!"
else
  echo "✅ No new literal strings introduced."
fi
</file>

<file path="scripts/prepare-db.js">
#!/usr/bin/env node

const { execSync } = require('child_process');
const fs = require('fs');
const path = require('path');

console.log('Preparing database for SQLx...');

// Change to backend directory
const backendDir = path.join(__dirname, '..', 'crates/db');
process.chdir(backendDir);

// Create temporary database file
const dbFile = path.join(backendDir, 'prepare_db.sqlite');
fs.writeFileSync(dbFile, '');

try {
  // Get absolute path (cross-platform)
  const dbPath = path.resolve(dbFile);
  const databaseUrl = `sqlite:${dbPath}`;

  console.log(`Using database: ${databaseUrl}`);

  // Run migrations
  console.log('Running migrations...');
  execSync('cargo sqlx migrate run', {
    stdio: 'inherit',
    env: { ...process.env, DATABASE_URL: databaseUrl }
  });

  // Prepare queries
  console.log('Preparing queries...');
  execSync('cargo sqlx prepare', {
    stdio: 'inherit',
    env: { ...process.env, DATABASE_URL: databaseUrl }
  });

  console.log('Database preparation complete!');

} finally {
  // Clean up temporary file
  if (fs.existsSync(dbFile)) {
    fs.unlinkSync(dbFile);
  }
}
</file>

<file path="scripts/setup-dev-environment.js">
#!/usr/bin/env node

const fs = require("fs");
const path = require("path");
const net = require("net");

const PORTS_FILE = path.join(__dirname, "..", ".dev-ports.json");
const DEV_ASSETS_SEED = path.join(__dirname, "..", "dev_assets_seed");
const DEV_ASSETS = path.join(__dirname, "..", "dev_assets");

/**
 * Check if a port is available
 */
function isPortAvailable(port) {
  return new Promise((resolve) => {
    const sock = net.createConnection({ port, host: "localhost" });
    sock.on("connect", () => {
      sock.destroy();
      resolve(false);
    });
    sock.on("error", () => resolve(true));
  });
}

/**
 * Find a free port starting from a given port
 */
async function findFreePort(startPort = 3000) {
  let port = startPort;
  while (!(await isPortAvailable(port))) {
    port++;
    if (port > 65535) {
      throw new Error("No available ports found");
    }
  }
  return port;
}

/**
 * Load existing ports from file
 */
function loadPorts() {
  try {
    if (fs.existsSync(PORTS_FILE)) {
      const data = fs.readFileSync(PORTS_FILE, "utf8");
      return JSON.parse(data);
    }
  } catch (error) {
    console.warn("Failed to load existing ports:", error.message);
  }
  return null;
}

/**
 * Save ports to file
 */
function savePorts(ports) {
  try {
    fs.writeFileSync(PORTS_FILE, JSON.stringify(ports, null, 2));
  } catch (error) {
    console.error("Failed to save ports:", error.message);
    throw error;
  }
}

/**
 * Verify that saved ports are still available
 */
async function verifyPorts(ports) {
  const frontendAvailable = await isPortAvailable(ports.frontend);
  const backendAvailable = await isPortAvailable(ports.backend);

  if (process.argv[2] === "get" && (!frontendAvailable || !backendAvailable)) {
    console.log(
      `Port availability check failed: frontend:${ports.frontend}=${frontendAvailable}, backend:${ports.backend}=${backendAvailable}`
    );
  }

  return frontendAvailable && backendAvailable;
}

/**
 * Allocate ports for development
 */
async function allocatePorts() {
  // Try to load existing ports first
  const existingPorts = loadPorts();

  if (existingPorts) {
    // Verify existing ports are still available
    if (await verifyPorts(existingPorts)) {
      if (process.argv[2] === "get") {
        console.log("Reusing existing dev ports:");
        console.log(`Frontend: ${existingPorts.frontend}`);
        console.log(`Backend: ${existingPorts.backend}`);
      }
      return existingPorts;
    } else {
      if (process.argv[2] === "get") {
        console.log(
          "Existing ports are no longer available, finding new ones..."
        );
      }
    }
  }

  // Find new free ports
  const frontendPort = await findFreePort(3000);
  const backendPort = await findFreePort(frontendPort + 1);

  const ports = {
    frontend: frontendPort,
    backend: backendPort,
    timestamp: new Date().toISOString(),
  };

  savePorts(ports);

  if (process.argv[2] === "get") {
    console.log("Allocated new dev ports:");
    console.log(`Frontend: ${ports.frontend}`);
    console.log(`Backend: ${ports.backend}`);
  }

  return ports;
}

/**
 * Get ports (allocate if needed)
 */
async function getPorts() {
  const ports = await allocatePorts();
  copyDevAssets();
  return ports;
}

/**
 * Copy dev_assets_seed to dev_assets
 */
function copyDevAssets() {
  try {
    if (!fs.existsSync(DEV_ASSETS)) {
      // Copy dev_assets_seed to dev_assets
      fs.cpSync(DEV_ASSETS_SEED, DEV_ASSETS, { recursive: true });

      if (process.argv[2] === "get") {
        console.log("Copied dev_assets_seed to dev_assets");
      }
    }
  } catch (error) {
    console.error("Failed to copy dev assets:", error.message);
  }
}

/**
 * Clear saved ports
 */
function clearPorts() {
  try {
    if (fs.existsSync(PORTS_FILE)) {
      fs.unlinkSync(PORTS_FILE);
      console.log("Cleared saved dev ports");
    } else {
      console.log("No saved ports to clear");
    }
  } catch (error) {
    console.error("Failed to clear ports:", error.message);
  }
}

// CLI interface
if (require.main === module) {
  const command = process.argv[2];

  switch (command) {
    case "get":
      getPorts()
        .then((ports) => {
          console.log(JSON.stringify(ports));
        })
        .catch(console.error);
      break;

    case "clear":
      clearPorts();
      break;

    case "frontend":
      getPorts()
        .then((ports) => {
          console.log(JSON.stringify(ports.frontend, null, 2));
        })
        .catch(console.error);
      break;

    case "backend":
      getPorts()
        .then((ports) => {
          console.log(JSON.stringify(ports.backend, null, 2));
        })
        .catch(console.error);
      break;

    default:
      console.log("Usage:");
      console.log(
        "  node setup-dev-environment.js get     - Setup dev environment (ports + assets)"
      );
      console.log(
        "  node setup-dev-environment.js frontend - Get frontend port only"
      );
      console.log(
        "  node setup-dev-environment.js backend  - Get backend port only"
      );
      console.log(
        "  node setup-dev-environment.js clear    - Clear saved ports"
      );
      break;
  }
}

module.exports = { getPorts, clearPorts, findFreePort };
</file>

<file path="shared/schemas/amp.json">
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "properties": {
    "append_prompt": {
      "title": "Append Prompt",
      "description": "Extra text appended to the prompt",
      "type": [
        "string",
        "null"
      ],
      "format": "textarea",
      "default": null
    },
    "dangerously_allow_all": {
      "title": "Dangerously Allow All",
      "description": "Allow all commands to be executed, even if they are not safe.",
      "type": [
        "boolean",
        "null"
      ]
    },
    "base_command_override": {
      "title": "Base Command Override",
      "description": "Override the base command with a custom command",
      "type": [
        "string",
        "null"
      ]
    },
    "additional_params": {
      "title": "Additional Parameters",
      "description": "Additional parameters to append to the base command",
      "type": [
        "array",
        "null"
      ],
      "items": {
        "type": "string"
      }
    }
  },
  "type": "object"
}
</file>

<file path="shared/schemas/claude_code.json">
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "properties": {
    "append_prompt": {
      "title": "Append Prompt",
      "description": "Extra text appended to the prompt",
      "type": [
        "string",
        "null"
      ],
      "format": "textarea",
      "default": null
    },
    "claude_code_router": {
      "type": [
        "boolean",
        "null"
      ]
    },
    "plan": {
      "type": [
        "boolean",
        "null"
      ]
    },
    "model": {
      "type": [
        "string",
        "null"
      ]
    },
    "dangerously_skip_permissions": {
      "type": [
        "boolean",
        "null"
      ]
    },
    "base_command_override": {
      "title": "Base Command Override",
      "description": "Override the base command with a custom command",
      "type": [
        "string",
        "null"
      ]
    },
    "additional_params": {
      "title": "Additional Parameters",
      "description": "Additional parameters to append to the base command",
      "type": [
        "array",
        "null"
      ],
      "items": {
        "type": "string"
      }
    }
  },
  "type": "object"
}
</file>

<file path="shared/schemas/codex.json">
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "properties": {
    "append_prompt": {
      "title": "Append Prompt",
      "description": "Extra text appended to the prompt",
      "type": [
        "string",
        "null"
      ],
      "format": "textarea",
      "default": null
    },
    "sandbox": {
      "description": "Sandbox policy modes for Codex",
      "type": [
        "string",
        "null"
      ],
      "enum": [
        "auto",
        "read-only",
        "workspace-write",
        "danger-full-access",
        null
      ]
    },
    "oss": {
      "type": [
        "boolean",
        "null"
      ]
    },
    "model": {
      "type": [
        "string",
        "null"
      ]
    },
    "model_reasoning_effort": {
      "description": "Reasoning effort for the underlying model",
      "type": [
        "string",
        "null"
      ],
      "enum": [
        "low",
        "medium",
        "high",
        null
      ]
    },
    "model_reasoning_summary": {
      "description": "Model reasoning summary style",
      "type": [
        "string",
        "null"
      ],
      "enum": [
        "auto",
        "concise",
        "detailed",
        "none",
        null
      ]
    },
    "base_command_override": {
      "title": "Base Command Override",
      "description": "Override the base command with a custom command",
      "type": [
        "string",
        "null"
      ]
    },
    "additional_params": {
      "title": "Additional Parameters",
      "description": "Additional parameters to append to the base command",
      "type": [
        "array",
        "null"
      ],
      "items": {
        "type": "string"
      }
    }
  },
  "type": "object"
}
</file>

<file path="shared/schemas/cursor.json">
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "properties": {
    "append_prompt": {
      "title": "Append Prompt",
      "description": "Extra text appended to the prompt",
      "type": [
        "string",
        "null"
      ],
      "format": "textarea",
      "default": null
    },
    "force": {
      "type": [
        "boolean",
        "null"
      ]
    },
    "model": {
      "type": [
        "string",
        "null"
      ]
    },
    "base_command_override": {
      "title": "Base Command Override",
      "description": "Override the base command with a custom command",
      "type": [
        "string",
        "null"
      ]
    },
    "additional_params": {
      "title": "Additional Parameters",
      "description": "Additional parameters to append to the base command",
      "type": [
        "array",
        "null"
      ],
      "items": {
        "type": "string"
      }
    }
  },
  "type": "object"
}
</file>

<file path="shared/schemas/gemini.json">
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "required": [
    "model"
  ],
  "type": "object",
  "properties": {
    "append_prompt": {
      "title": "Append Prompt",
      "description": "Extra text appended to the prompt",
      "type": [
        "string",
        "null"
      ],
      "format": "textarea",
      "default": null
    },
    "model": {
      "type": "string",
      "enum": [
        "default",
        "flash"
      ]
    },
    "yolo": {
      "type": [
        "boolean",
        "null"
      ]
    },
    "base_command_override": {
      "title": "Base Command Override",
      "description": "Override the base command with a custom command",
      "type": [
        "string",
        "null"
      ]
    },
    "additional_params": {
      "title": "Additional Parameters",
      "description": "Additional parameters to append to the base command",
      "type": [
        "array",
        "null"
      ],
      "items": {
        "type": "string"
      }
    }
  }
}
</file>

<file path="shared/schemas/opencode.json">
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "properties": {
    "append_prompt": {
      "title": "Append Prompt",
      "description": "Extra text appended to the prompt",
      "type": [
        "string",
        "null"
      ],
      "format": "textarea",
      "default": null
    },
    "model": {
      "type": [
        "string",
        "null"
      ]
    },
    "agent": {
      "type": [
        "string",
        "null"
      ]
    },
    "base_command_override": {
      "title": "Base Command Override",
      "description": "Override the base command with a custom command",
      "type": [
        "string",
        "null"
      ]
    },
    "additional_params": {
      "title": "Additional Parameters",
      "description": "Additional parameters to append to the base command",
      "type": [
        "array",
        "null"
      ],
      "items": {
        "type": "string"
      }
    }
  },
  "type": "object"
}
</file>

<file path="shared/schemas/qwen_code.json">
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "properties": {
    "append_prompt": {
      "title": "Append Prompt",
      "description": "Extra text appended to the prompt",
      "type": [
        "string",
        "null"
      ],
      "format": "textarea",
      "default": null
    },
    "yolo": {
      "type": [
        "boolean",
        "null"
      ]
    },
    "base_command_override": {
      "title": "Base Command Override",
      "description": "Override the base command with a custom command",
      "type": [
        "string",
        "null"
      ]
    },
    "additional_params": {
      "title": "Additional Parameters",
      "description": "Additional parameters to append to the base command",
      "type": [
        "array",
        "null"
      ],
      "items": {
        "type": "string"
      }
    }
  },
  "type": "object"
}
</file>

<file path="shared/types.ts">
// This file was generated by `crates/core/src/bin/generate_types.rs`.

// Do not edit this file manually.

// If you are an AI, and you absolutely have to edit this file, please confirm with the user first.

export type DirectoryEntry = { name: string, path: string, is_directory: boolean, is_git_repo: boolean, last_modified: bigint | null, };

export type DirectoryListResponse = { entries: Array<DirectoryEntry>, current_path: string, };

export type Project = { id: string, name: string, git_repo_path: string, setup_script: string | null, dev_script: string | null, cleanup_script: string | null, copy_files: string | null, created_at: Date, updated_at: Date, };

export type CreateProject = { name: string, git_repo_path: string, use_existing_repo: boolean, setup_script: string | null, dev_script: string | null, cleanup_script: string | null, copy_files: string | null, };

export type UpdateProject = { name: string | null, git_repo_path: string | null, setup_script: string | null, dev_script: string | null, cleanup_script: string | null, copy_files: string | null, };

export type SearchResult = { path: string, is_file: boolean, match_type: SearchMatchType, };

export type SearchMatchType = "FileName" | "DirectoryName" | "FullPath";

export type ExecutorAction = { typ: ExecutorActionType, next_action: ExecutorAction | null, };

export type McpConfig = { servers: { [key in string]?: JsonValue }, servers_path: Array<string>, template: JsonValue, preconfigured: JsonValue, is_toml_config: boolean, };

export type ExecutorActionType = { "type": "CodingAgentInitialRequest" } & CodingAgentInitialRequest | { "type": "CodingAgentFollowUpRequest" } & CodingAgentFollowUpRequest | { "type": "ScriptRequest" } & ScriptRequest;

export type ScriptContext = "SetupScript" | "CleanupScript" | "DevServer";

export type ScriptRequest = { script: string, language: ScriptRequestLanguage, context: ScriptContext, };

export type ScriptRequestLanguage = "Bash";

export enum BaseCodingAgent { CLAUDE_CODE = "CLAUDE_CODE", AMP = "AMP", GEMINI = "GEMINI", CODEX = "CODEX", OPENCODE = "OPENCODE", CURSOR = "CURSOR", QWEN_CODE = "QWEN_CODE" }

export type CodingAgent = { "CLAUDE_CODE": ClaudeCode } | { "AMP": Amp } | { "GEMINI": Gemini } | { "CODEX": Codex } | { "OPENCODE": Opencode } | { "CURSOR": Cursor } | { "QWEN_CODE": QwenCode };

export type TaskTemplate = { id: string, project_id: string | null, title: string, description: string | null, template_name: string, created_at: string, updated_at: string, };

export type CreateTaskTemplate = { project_id: string | null, title: string, description: string | null, template_name: string, };

export type UpdateTaskTemplate = { title: string | null, description: string | null, template_name: string | null, };

export type TaskStatus = "todo" | "inprogress" | "inreview" | "done" | "cancelled";

export type Task = { id: string, project_id: string, title: string, description: string | null, status: TaskStatus, parent_task_attempt: string | null, created_at: string, updated_at: string, };

export type TaskWithAttemptStatus = { has_in_progress_attempt: boolean, has_merged_attempt: boolean, last_attempt_failed: boolean, executor: string, id: string, project_id: string, title: string, description: string | null, status: TaskStatus, parent_task_attempt: string | null, created_at: string, updated_at: string, };

export type TaskRelationships = { parent_task: Task | null, current_attempt: TaskAttempt, children: Array<Task>, };

export type CreateTask = { project_id: string, title: string, description: string | null, parent_task_attempt: string | null, image_ids: Array<string> | null, };

export type UpdateTask = { title: string | null, description: string | null, status: TaskStatus | null, parent_task_attempt: string | null, image_ids: Array<string> | null, };

export type Image = { id: string, file_path: string, original_name: string, mime_type: string | null, size_bytes: bigint, hash: string, created_at: string, updated_at: string, };

export type CreateImage = { file_path: string, original_name: string, mime_type: string | null, size_bytes: bigint, hash: string, };

export type ApiResponse<T, E = T> = { success: boolean, data: T | null, error_data: E | null, message: string | null, };

export type UserSystemInfo = { config: Config, environment: Environment, 
/**
 * Capabilities supported per executor (e.g., { "CLAUDE_CODE": ["SESSION_FORK"] })
 */
capabilities: { [key in string]?: Array<BaseAgentCapability> }, executors: { [key in BaseCodingAgent]?: ExecutorConfig }, };

export type Environment = { os_type: string, os_version: string, os_architecture: string, bitness: string, };

export type McpServerQuery = { executor: BaseCodingAgent, };

export type UpdateMcpServersBody = { servers: { [key in string]?: JsonValue }, };

export type GetMcpServerResponse = { mcp_config: McpConfig, config_path: string, };

export type CreateFollowUpAttempt = { prompt: string, variant: string | null, image_ids: Array<string> | null, };

export type FollowUpDraftResponse = { task_attempt_id: string, prompt: string, queued: boolean, variant: string | null, image_ids: Array<string> | null, version: bigint, };

export type UpdateFollowUpDraftRequest = { prompt: string | null, variant: string | null | null, image_ids: Array<string> | null, version: bigint | null, };

export type CreateAndStartTaskRequest = { task: CreateTask, executor_profile_id: ExecutorProfileId, base_branch: string, };

export type CreateGitHubPrRequest = { title: string, body: string | null, base_branch: string | null, };

export type ImageResponse = { id: string, file_path: string, original_name: string, mime_type: string | null, size_bytes: bigint, hash: string, created_at: string, updated_at: string, };

export enum GitHubServiceError { TOKEN_INVALID = "TOKEN_INVALID", INSUFFICIENT_PERMISSIONS = "INSUFFICIENT_PERMISSIONS", REPO_NOT_FOUND_OR_NO_ACCESS = "REPO_NOT_FOUND_OR_NO_ACCESS" }

export type Config = { config_version: string, theme: ThemeMode, executor_profile: ExecutorProfileId, disclaimer_acknowledged: boolean, onboarding_acknowledged: boolean, github_login_acknowledged: boolean, telemetry_acknowledged: boolean, notifications: NotificationConfig, editor: EditorConfig, github: GitHubConfig, analytics_enabled: boolean | null, workspace_dir: string | null, last_app_version: string | null, show_release_notes: boolean, language: UiLanguage, };

export type NotificationConfig = { sound_enabled: boolean, push_enabled: boolean, sound_file: SoundFile, };

export enum ThemeMode { LIGHT = "LIGHT", DARK = "DARK", SYSTEM = "SYSTEM", PURPLE = "PURPLE", GREEN = "GREEN", BLUE = "BLUE", ORANGE = "ORANGE", RED = "RED" }

export type EditorConfig = { editor_type: EditorType, custom_command: string | null, };

export enum EditorType { VS_CODE = "VS_CODE", CURSOR = "CURSOR", WINDSURF = "WINDSURF", INTELLI_J = "INTELLI_J", ZED = "ZED", XCODE = "XCODE", CUSTOM = "CUSTOM" }

export type GitHubConfig = { pat: string | null, oauth_token: string | null, username: string | null, primary_email: string | null, default_pr_base: string | null, };

export enum SoundFile { ABSTRACT_SOUND1 = "ABSTRACT_SOUND1", ABSTRACT_SOUND2 = "ABSTRACT_SOUND2", ABSTRACT_SOUND3 = "ABSTRACT_SOUND3", ABSTRACT_SOUND4 = "ABSTRACT_SOUND4", COW_MOOING = "COW_MOOING", PHONE_VIBRATION = "PHONE_VIBRATION", ROOSTER = "ROOSTER" }

export type UiLanguage = "BROWSER" | "EN" | "JA";

export type DeviceFlowStartResponse = { user_code: string, verification_uri: string, expires_in: number, interval: number, };

export enum DevicePollStatus { SLOW_DOWN = "SLOW_DOWN", AUTHORIZATION_PENDING = "AUTHORIZATION_PENDING", SUCCESS = "SUCCESS" }

export enum CheckTokenResponse { VALID = "VALID", INVALID = "INVALID" }

export type GitBranch = { name: string, is_current: boolean, is_remote: boolean, last_commit_date: Date, };

export type Diff = { change: DiffChangeKind, oldPath: string | null, newPath: string | null, oldContent: string | null, newContent: string | null, 
/**
 * True when file contents are intentionally omitted (e.g., too large)
 */
contentOmitted: boolean, 
/**
 * Optional precomputed stats for omitted content
 */
additions: number | null, deletions: number | null, };

export type DiffChangeKind = "added" | "deleted" | "modified" | "renamed" | "copied" | "permissionChange";

export type RepositoryInfo = { id: bigint, name: string, full_name: string, owner: string, description: string | null, clone_url: string, ssh_url: string, default_branch: string, private: boolean, };

export type CommandBuilder = { 
/**
 * Base executable command (e.g., "npx -y @anthropic-ai/claude-code@latest")
 */
base: string, 
/**
 * Optional parameters to append to the base command
 */
params: Array<string> | null, };

export type ExecutorProfileId = { 
/**
 * The executor type (e.g., "CLAUDE_CODE", "AMP")
 */
executor: BaseCodingAgent, 
/**
 * Optional variant name (e.g., "PLAN", "ROUTER")
 */
variant: string | null, };

export type ExecutorConfig = { [key in string]?: { "CLAUDE_CODE": ClaudeCode } | { "AMP": Amp } | { "GEMINI": Gemini } | { "CODEX": Codex } | { "OPENCODE": Opencode } | { "CURSOR": Cursor } | { "QWEN_CODE": QwenCode } };

export type BaseAgentCapability = "SESSION_FORK";

export type ClaudeCode = { append_prompt: AppendPrompt, claude_code_router?: boolean | null, plan?: boolean | null, model?: string | null, dangerously_skip_permissions?: boolean | null, base_command_override?: string | null, additional_params?: Array<string> | null, };

export type Gemini = { append_prompt: AppendPrompt, model: GeminiModel, yolo?: boolean | null, base_command_override?: string | null, additional_params?: Array<string> | null, };

export type GeminiModel = "default" | "flash";

export type Amp = { append_prompt: AppendPrompt, dangerously_allow_all?: boolean | null, base_command_override?: string | null, additional_params?: Array<string> | null, };

export type Codex = { append_prompt: AppendPrompt, sandbox?: SandboxMode | null, oss?: boolean | null, model?: string | null, model_reasoning_effort?: ReasoningEffort | null, model_reasoning_summary?: ReasoningSummary | null, base_command_override?: string | null, additional_params?: Array<string> | null, };

export type SandboxMode = "auto" | "read-only" | "workspace-write" | "danger-full-access";

export type ReasoningEffort = "low" | "medium" | "high";

export type ReasoningSummary = "auto" | "concise" | "detailed" | "none";

export type Cursor = { append_prompt: AppendPrompt, force?: boolean | null, model?: string | null, base_command_override?: string | null, additional_params?: Array<string> | null, };

export type Opencode = { append_prompt: AppendPrompt, model?: string | null, agent?: string | null, base_command_override?: string | null, additional_params?: Array<string> | null, };

export type QwenCode = { append_prompt: AppendPrompt, yolo?: boolean | null, base_command_override?: string | null, additional_params?: Array<string> | null, };

export type AppendPrompt = string | null;

export type CodingAgentInitialRequest = { prompt: string, 
/**
 * Executor profile specification
 */
executor_profile_id: ExecutorProfileId, };

export type CodingAgentFollowUpRequest = { prompt: string, session_id: string, 
/**
 * Executor profile specification
 */
executor_profile_id: ExecutorProfileId, };

export type CreateTaskAttemptBody = { task_id: string, 
/**
 * Executor profile specification
 */
executor_profile_id: ExecutorProfileId, base_branch: string, };

export type RebaseTaskAttemptRequest = { new_base_branch: string | null, };

export type GitOperationError = { "type": "merge_conflicts", message: string, op: ConflictOp, } | { "type": "rebase_in_progress" };

export type ReplaceProcessRequest = { 
/**
 * Process to replace (delete this and later ones)
 */
process_id: string, 
/**
 * New prompt to use for the replacement follow-up
 */
prompt: string, 
/**
 * Optional variant override
 */
variant: string | null, 
/**
 * If true, allow resetting Git even when uncommitted changes exist
 */
force_when_dirty: boolean | null, 
/**
 * If false, skip performing the Git reset step (history drop still applies)
 */
perform_git_reset: boolean | null, };

export type CommitInfo = { sha: string, subject: string, };

export type BranchStatus = { commits_behind: number | null, commits_ahead: number | null, has_uncommitted_changes: boolean | null, head_oid: string | null, uncommitted_count: number | null, untracked_count: number | null, base_branch_name: string, remote_commits_behind: number | null, remote_commits_ahead: number | null, merges: Array<Merge>, 
/**
 * True if a `git rebase` is currently in progress in this worktree
 */
is_rebase_in_progress: boolean, 
/**
 * Current conflict operation if any
 */
conflict_op: ConflictOp | null, 
/**
 * List of files currently in conflicted (unmerged) state
 */
conflicted_files: Array<string>, };

export type ConflictOp = "rebase" | "merge" | "cherry_pick" | "revert";

export type TaskAttempt = { id: string, task_id: string, container_ref: string | null, branch: string | null, base_branch: string, executor: string, worktree_deleted: boolean, setup_completed_at: string | null, created_at: string, updated_at: string, };

export type ExecutionProcess = { id: string, task_attempt_id: string, run_reason: ExecutionProcessRunReason, executor_action: ExecutorAction, 
/**
 * Git HEAD commit OID captured before the process starts
 */
before_head_commit: string | null, 
/**
 * Git HEAD commit OID captured after the process ends
 */
after_head_commit: string | null, status: ExecutionProcessStatus, exit_code: bigint | null, 
/**
 * dropped: true if this process is excluded from the current
 * history view (due to restore/trimming). Hidden from logs/timeline;
 * still listed in the Processes tab.
 */
dropped: boolean, started_at: string, completed_at: string | null, created_at: string, updated_at: string, };

export type ExecutionProcessStatus = "running" | "completed" | "failed" | "killed";

export type ExecutionProcessRunReason = "setupscript" | "cleanupscript" | "codingagent" | "devserver";

export type Merge = { "type": "direct" } & DirectMerge | { "type": "pr" } & PrMerge;

export type DirectMerge = { id: string, task_attempt_id: string, merge_commit: string, target_branch_name: string, created_at: string, };

export type PrMerge = { id: string, task_attempt_id: string, created_at: string, target_branch_name: string, pr_info: PullRequestInfo, };

export type MergeStatus = "open" | "merged" | "closed" | "unknown";

export type PullRequestInfo = { number: bigint, url: string, status: MergeStatus, merged_at: string | null, merge_commit_sha: string | null, };

export type FollowUpDraft = { id: string, task_attempt_id: string, prompt: string, queued: boolean, sending: boolean, variant: string | null, image_ids: Array<string> | null, created_at: string, updated_at: string, version: bigint, };

export type CommandExitStatus = { "type": "exit_code", code: number, } | { "type": "success", success: boolean, };

export type CommandRunResult = { exit_status: CommandExitStatus | null, output: string | null, };

export type NormalizedEntry = { timestamp: string | null, entry_type: NormalizedEntryType, content: string, };

export type NormalizedEntryType = { "type": "user_message" } | { "type": "assistant_message" } | { "type": "tool_use", tool_name: string, action_type: ActionType, } | { "type": "system_message" } | { "type": "error_message" } | { "type": "thinking" } | { "type": "loading" };

export type FileChange = { "action": "write", content: string, } | { "action": "delete" } | { "action": "rename", new_path: string, } | { "action": "edit", 
/**
 * Unified diff containing file header and hunks.
 */
unified_diff: string, 
/**
 * Whether line number in the hunks are reliable.
 */
has_line_numbers: boolean, };

export type ActionType = { "action": "file_read", path: string, } | { "action": "file_edit", path: string, changes: Array<FileChange>, } | { "action": "command_run", command: string, result: CommandRunResult | null, } | { "action": "search", query: string, } | { "action": "web_fetch", url: string, } | { "action": "tool", tool_name: string, arguments: JsonValue | null, result: ToolResult | null, } | { "action": "task_create", description: string, } | { "action": "plan_presentation", plan: string, } | { "action": "todo_management", todos: Array<TodoItem>, operation: string, } | { "action": "other", description: string, };

export type TodoItem = { content: string, status: string, priority: string | null, };

export type ToolResult = { type: ToolResultValueType, 
/**
 * For Markdown, this will be a JSON string; for JSON, a structured value
 */
value: JsonValue, };

export type ToolResultValueType = { "type": "markdown" } | { "type": "json" };

export type PatchType = { "type": "NORMALIZED_ENTRY", "content": NormalizedEntry } | { "type": "STDOUT", "content": string } | { "type": "STDERR", "content": string } | { "type": "DIFF", "content": Diff };

export type JsonValue = number | string | boolean | Array<JsonValue> | { [key in string]?: JsonValue } | null;
</file>

<file path=".dockerignore">
# Node modules
**/node_modules
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*

# Build artifacts
target/
dist/
build/
*.tgz
*.tar.gz

# IDE and editor files
.vscode/
.idea/
*.swp
*.swo
*~

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Git
.git/
.gitignore

# Docker
Dockerfile*
.dockerignore

# Environment files
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

# Logs
logs/
*.log

# Runtime data
pids/
*.pid
*.seed
*.pid.lock

# Coverage directory used by tools like istanbul
coverage/

# Temporary folders
tmp/
temp/
</file>

<file path=".gitignore">
# Rust
/target
**/*.rs.bk
Cargo.lock

# Node.js
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# Build outputs
/dist
/build

# Environment variables
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Coverage directory used by tools like istanbul
coverage/
*.lcov

# nyc test coverage
.nyc_output

# ESLint cache
.eslintcache

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Storybook build outputs
.out
.storybook-out

.env
frontend/dist
crates/executors/bindings
crates/utils/bindings

build-npm-package-codesign.sh

npx-cli/dist
npx-cli/vibe-kanban-*
vibe-kanban-*.tgz

# Development ports file
.dev-ports.json

dev_assets
/frontend/.env.sentry-build-plugin
.ssh

vibe-kanban-cloud/
</file>

<file path=".npmrc">
package-lock=false
engine-strict=true
</file>

<file path="AGENTS.md">
# Repository Guidelines

## Project Structure & Module Organization
- `crates/`: Rust workspace crates — `server` (API + bins), `db` (SQLx models/migrations), `executors`, `services`, `utils`, `deployment`, `local-deployment`.
- `frontend/`: React + TypeScript app (Vite, Tailwind). Source in `frontend/src`.
- `frontend/src/components/dialogs`: Dialog components for the frontend.
- `shared/`: Generated TypeScript types (`shared/types.ts`). Do not edit directly.
- `assets/`, `dev_assets_seed/`, `dev_assets/`: Packaged and local dev assets.
- `npx-cli/`: Files published to the npm CLI package.
- `scripts/`: Dev helpers (ports, DB preparation).

## Managing Shared Types Between Rust and TypeScript

ts-rs allows you to derive TypeScript types from Rust structs/enums. By annotating your Rust types with #[derive(TS)] and related macros, ts-rs will generate .ts declaration files for those types.
When making changes to the types, you can regenerate them using `npm run generate-types`
Do not manually edit shared/types.ts, instead edit crates/server/src/bin/generate_types.rs

## Build, Test, and Development Commands
- Install: `pnpm i`
- Run dev (frontend + backend with ports auto-assigned): `pnpm run dev`
- Backend (watch): `npm run backend:dev:watch`
- Frontend (dev): `npm run frontend:dev`
- Type checks: `npm run check` (frontend) and `npm run backend:check` (Rust cargo check)
- Rust tests: `cargo test --workspace`
- Generate TS types from Rust: `npm run generate-types` (or `generate-types:check` in CI)
- Prepare SQLx (offline): `npm run prepare-db`
- Local NPX build: `npm run build:npx` then `npm pack` in `npx-cli/`

## Coding Style & Naming Conventions
- Rust: `rustfmt` enforced (`rustfmt.toml`); group imports by crate; snake_case modules, PascalCase types.
- TypeScript/React: ESLint + Prettier (2 spaces, single quotes, 80 cols). PascalCase components, camelCase vars/functions, kebab-case file names where practical.
- Keep functions small, add `Debug`/`Serialize`/`Deserialize` where useful.

## Testing Guidelines
- Rust: prefer unit tests alongside code (`#[cfg(test)]`), run `cargo test --workspace`. Add tests for new logic and edge cases.
- Frontend: ensure `npm run check` and `npm run lint` pass. If adding runtime logic, include lightweight tests (e.g., Vitest) in the same directory.

## Security & Config Tips
- Use `.env` for local overrides; never commit secrets. Key envs: `FRONTEND_PORT`, `BACKEND_PORT`, `HOST`, optional `GITHUB_CLIENT_ID` for custom OAuth.
- Dev ports and assets are managed by `scripts/setup-dev-environment.js`.
</file>

<file path="Cargo.toml">
[workspace]
resolver = "2"
members = ["crates/server", "crates/db", "crates/executors", "crates/services", "crates/utils", "crates/local-deployment", "crates/deployment"]

[workspace.dependencies]
tokio = { version = "1.0", features = ["full"] }
axum = { version = "0.8.4", features = ["macros", "multipart", "ws"] }
tower-http = { version = "0.5", features = ["cors"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
anyhow = "1.0"
thiserror = "2.0.12"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
openssl-sys = { version = "0.9", features = ["vendored"] }
ts-rs = { git = "https://github.com/xazukx/ts-rs.git", branch = "use-ts-enum", features = ["uuid-impl", "chrono-impl", "no-serde-warnings"] }
schemars = { version = "1.0.4", features = ["derive", "chrono04", "uuid1", "preserve_order"] }

[profile.release]
debug = true
split-debuginfo = "packed"
strip = true
</file>

<file path="check-both.sh">
#!/usr/bin/env bash
# ─ load up your Rust/Cargo from ~/.cargo/env ─
if [ -f "$HOME/.cargo/env" ]; then
  # this is where `cargo` typically lives 
  source "$HOME/.cargo/env"
fi

# now run both checks
cargo check --workspace --message-format=json "$@"
cargo check --workspace --message-format=json --features cloud "$@"

# Add this to .vscode/settings.json to lint both cloud and non-cloud
# {
#     // rust-analyzer will still do its usual code‑lens, inlay, etc. based
#     // on whatever "cargo.features" you pick here (can be [] for no-features,
#     // or ["foo"] for a specific feature).
#     "rust-analyzer.cargo.features": "all",
#     // overrideCommand must emit JSON diagnostics. We're just calling our
#     // script which in turn calls cargo twice.
#     "rust-analyzer.check.overrideCommand": [
#         "${workspaceFolder}/check-both.sh"
#     ]
# }
</file>

<file path="CLAUDE.md">
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Essential Commands

### Development
```bash
# Start development servers with hot reload (frontend + backend)
pnpm run dev

# Individual dev servers
npm run frontend:dev    # Frontend only (port 3000)
npm run backend:dev     # Backend only (port auto-assigned)

# Build production version
./build-npm-package.sh
```

### Testing & Validation
```bash
# Run all checks (frontend + backend)
npm run check

# Frontend specific
cd frontend && npm run lint          # Lint TypeScript/React code
cd frontend && npm run format:check  # Check formatting
cd frontend && npx tsc --noEmit     # TypeScript type checking

# Backend specific  
cargo test --workspace               # Run all Rust tests
cargo test -p <crate_name>          # Test specific crate
cargo test test_name                # Run specific test
cargo fmt --all -- --check          # Check Rust formatting
cargo clippy --all --all-targets --all-features -- -D warnings  # Linting

# Type generation (after modifying Rust types)
npm run generate-types               # Regenerate TypeScript types from Rust
npm run generate-types:check        # Verify types are up to date
```

### Database Operations
```bash
# SQLx migrations
sqlx migrate run                     # Apply migrations
sqlx database create                 # Create database

# Database is auto-copied from dev_assets_seed/ on dev server start
```

## Architecture Overview

### Tech Stack
- **Backend**: Rust with Axum web framework, Tokio async runtime, SQLx for database
- **Frontend**: React 18 + TypeScript + Vite, Tailwind CSS, shadcn/ui components  
- **Database**: SQLite with SQLx migrations
- **Type Sharing**: ts-rs generates TypeScript types from Rust structs
- **MCP Server**: Built-in Model Context Protocol server for AI agent integration

### Project Structure
```
crates/
├── server/         # Axum HTTP server, API routes, MCP server
├── db/            # Database models, migrations, SQLx queries
├── executors/     # AI coding agent integrations (Claude, Gemini, etc.)
├── services/      # Business logic, GitHub, auth, git operations
├── local-deployment/  # Local deployment logic
└── utils/         # Shared utilities

frontend/          # React application
├── src/
│   ├── components/  # React components (TaskCard, ProjectCard, etc.)
│   ├── pages/      # Route pages
│   ├── hooks/      # Custom React hooks (useEventSourceManager, etc.)
│   └── lib/        # API client, utilities

shared/types.ts    # Auto-generated TypeScript types from Rust
```

### Key Architectural Patterns

1. **Event Streaming**: Server-Sent Events (SSE) for real-time updates
   - Process logs stream to frontend via `/api/events/processes/:id/logs`
   - Task diffs stream via `/api/events/task-attempts/:id/diff`

2. **Git Worktree Management**: Each task execution gets isolated git worktree
   - Managed by `WorktreeManager` service
   - Automatic cleanup of orphaned worktrees

3. **Executor Pattern**: Pluggable AI agent executors
   - Each executor (Claude, Gemini, etc.) implements common interface
   - Actions: `coding_agent_initial`, `coding_agent_follow_up`, `script`

4. **MCP Integration**: Vibe Kanban acts as MCP server
   - Tools: `list_projects`, `list_tasks`, `create_task`, `update_task`, etc.
   - AI agents can manage tasks via MCP protocol

### API Patterns

- REST endpoints under `/api/*`
- Frontend dev server proxies to backend (configured in vite.config.ts)
- Authentication via GitHub OAuth (device flow)
- All database queries in `crates/db/src/models/`

### Development Workflow

1. **Backend changes first**: When modifying both frontend and backend, start with backend
2. **Type generation**: Run `npm run generate-types` after modifying Rust types
3. **Database migrations**: Create in `crates/db/migrations/`, apply with `sqlx migrate run`
4. **Component patterns**: Follow existing patterns in `frontend/src/components/`

### Testing Strategy

- **Unit tests**: Colocated with code in each crate
- **Integration tests**: In `tests/` directory of relevant crates  
- **Frontend tests**: TypeScript compilation and linting only
- **CI/CD**: GitHub Actions workflow in `.github/workflows/test.yml`

### Environment Variables

Build-time (set when building):
- `GITHUB_CLIENT_ID`: GitHub OAuth app ID (default: Bloop AI's app)
- `POSTHOG_API_KEY`: Analytics key (optional)

Runtime:
- `BACKEND_PORT`: Backend server port (default: auto-assign)
- `FRONTEND_PORT`: Frontend dev port (default: 3000)
- `HOST`: Backend host (default: 127.0.0.1)
- `DISABLE_WORKTREE_ORPHAN_CLEANUP`: Debug flag for worktrees
</file>

<file path="CODE-OF-CONDUCT.md">
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, caste, color, religion, or sexual
identity and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

- Demonstrating empathy and kindness toward other people
- Being respectful of differing opinions, viewpoints, and experiences
- Giving and gracefully accepting constructive feedback
- Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
- Focusing on what is best not just for us as individuals, but for the overall
  community

Examples of unacceptable behavior include:

- The use of sexualized language or imagery, and sexual attention or advances of
  any kind
- Trolling, insulting or derogatory comments, and personal or political attacks
- Public or private harassment
- Publishing others' private information, such as a physical or email address,
  without their explicit permission
- Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
maintainers@bloop.ai through e-mail, with an appropriate subject line.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

### Attribution

This Code of Conduct is adapted from the [Next.js project][nextjs-coc]

The original text is from the [Contributor Covenant][homepage],
version 2.1, available at
[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].

For answers to common questions about this code of conduct, see the FAQ at
[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at
[https://www.contributor-covenant.org/translations][translations].

[nextjs-coc]: https://raw.githubusercontent.com/vercel/next.js/canary/CODE_OF_CONDUCT.md
[homepage]: https://www.contributor-covenant.org
[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html
[FAQ]: https://www.contributor-covenant.org/faq
[translations]: https://www.contributor-covenant.org/translations
</file>

<file path="Dockerfile">
# Build stage
FROM node:24-alpine AS builder

# Install build dependencies
RUN apk add --no-cache \
    curl \
    build-base \
    perl

# Install Rust
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH="/root/.cargo/bin:${PATH}"

# Set working directory
WORKDIR /app

# Copy package files for dependency caching
COPY package*.json pnpm-lock.yaml pnpm-workspace.yaml ./
COPY frontend/package*.json ./frontend/
COPY npx-cli/package*.json ./npx-cli/

# Install pnpm and dependencies
RUN npm install -g pnpm && pnpm install

# Copy source code
COPY . .

# Build application
RUN npm run generate-types
RUN cd frontend && npm install && npm run build
RUN cargo build --release --bin server

# Runtime stage
FROM alpine:latest AS runtime

# Install runtime dependencies
RUN apk add --no-cache \
    ca-certificates \
    tini \
    libgcc \
    wget

# Create app user for security
RUN addgroup -g 1001 -S appgroup && \
    adduser -u 1001 -S appuser -G appgroup

# Copy binary from builder
COPY --from=builder /app/target/release/server /usr/local/bin/server

# Create repos directory and set permissions
RUN mkdir -p /repos && \
    chown -R appuser:appgroup /repos

# Switch to non-root user
USER appuser

# Set runtime environment
ENV HOST=0.0.0.0
ENV PORT=3000
EXPOSE 3000

# Set working directory
WORKDIR /repos

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD wget --quiet --tries=1 --spider "http://${HOST:-localhost}:${PORT:-3000}" || exit 1

# Run the application
ENTRYPOINT ["/sbin/tini", "--"]
CMD ["server"]
</file>

<file path="LICENSE">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="local-build.sh">
#!/bin/bash

set -e  # Exit on any error

echo "🧹 Cleaning previous builds..."
rm -rf npx-cli/dist
mkdir -p npx-cli/dist/macos-arm64

echo "🔨 Building frontend..."
(cd frontend && npm run build)

echo "🔨 Building Rust binaries..."
cargo build --release --manifest-path Cargo.toml
cargo build --release --bin mcp_task_server --manifest-path Cargo.toml

echo "📦 Creating distribution package..."

# Copy the main binary
cp target/release/server vibe-kanban
zip -q vibe-kanban.zip vibe-kanban
rm -f vibe-kanban 
mv vibe-kanban.zip npx-cli/dist/macos-arm64/vibe-kanban.zip

# Copy the MCP binary
cp target/release/mcp_task_server vibe-kanban-mcp
zip -q vibe-kanban-mcp.zip vibe-kanban-mcp
rm -f vibe-kanban-mcp
mv vibe-kanban-mcp.zip npx-cli/dist/macos-arm64/vibe-kanban-mcp.zip

echo "✅ NPM package ready!"
echo "📁 Files created:"
echo "   - npx-cli/dist/macos-arm64/vibe-kanban.zip"
echo "   - npx-cli/dist/macos-arm64/vibe-kanban-mcp.zip"
</file>

<file path="package.json">
{
  "name": "vibe-kanban",
  "version": "0.0.94",
  "private": true,
  "bin": {
    "vibe-kanban": "npx-cli/bin/cli.js"
  },
  "files": [
    "npx-cli/bin/cli.js",
    "npx-cli/dist/**"
  ],
  "scripts": {
    "format": "cargo fmt --all && cd frontend && npm run format",
    "check": "npm run frontend:check && npm run backend:check",
    "dev": "export FRONTEND_PORT=$(node scripts/setup-dev-environment.js frontend) && export BACKEND_PORT=$(node scripts/setup-dev-environment.js backend) && concurrently \"npm run backend:dev:watch\" \"npm run frontend:dev\"",
    "test:npm": "./test-npm-package.sh",
    "frontend:dev": "cd frontend && npm run dev -- --port ${FRONTEND_PORT:-3000} --host",
    "frontend:check": "cd frontend && npm run check",
    "backend:dev": "BACKEND_PORT=$(node scripts/setup-dev-environment.js backend) npm run backend:dev:watch",
    "backend:check": "cargo check",
    "backend:dev:watch": "DISABLE_WORKTREE_ORPHAN_CLEANUP=1 RUST_LOG=debug cargo watch -w crates -x 'run --bin server'",
    "generate-types": "cargo run --bin generate_types",
    "generate-types:check": "cargo run --bin generate_types -- --check",
    "prepare-db": "node scripts/prepare-db.js",
    "build:npx": "bash ./local-build.sh",
    "prepack": "npm run build:npx"
  },
  "devDependencies": {
    "@tailwindcss/container-queries": "^0.1.1",
    "concurrently": "^8.2.2",
    "vite": "^6.3.5"
  },
  "engines": {
    "node": ">=18",
    "pnpm": ">=8"
  },
  "dependencies": {
    "@ebay/nice-modal-react": "^1.2.13"
  }
}
</file>

<file path="pnpm-workspace.yaml">
packages:
  - 'frontend'
</file>

<file path="README.md">
<p align="center">
  <a href="https://vibekanban.com">
    <picture>
      <source srcset="frontend/public/vibe-kanban-logo-dark.svg" media="(prefers-color-scheme: dark)">
      <source srcset="frontend/public/vibe-kanban-logo.svg" media="(prefers-color-scheme: light)">
      <img src="frontend/public/vibe-kanban-logo.svg" alt="Vibe Kanban Logo">
    </picture>
  </a>
</p>

<p align="center">Get 10X more out of Claude Code, Gemini CLI, Codex, Amp and other coding agents...</p>
<p align="center">
  <a href="https://www.npmjs.com/package/vibe-kanban"><img alt="npm" src="https://img.shields.io/npm/v/vibe-kanban?style=flat-square" /></a>
  <a href="https://github.com/BloopAI/vibe-kanban/blob/main/.github/workflows/publish.yml"><img alt="Build status" src="https://img.shields.io/github/actions/workflow/status/BloopAI/vibe-kanban/.github%2Fworkflows%2Fpublish.yml" /></a>
  <a href="https://deepwiki.com/BloopAI/vibe-kanban"><img src="https://deepwiki.com/badge.svg" alt="Ask DeepWiki"></a>
</p>

![](frontend/public/vibe-kanban-screenshot-overview.png)

## Overview

AI coding agents are increasingly writing the world's code and human engineers now spend the majority of their time planning, reviewing, and orchestrating tasks. Vibe Kanban streamlines this process, enabling you to:

- Easily switch between different coding agents
- Orchestrate the execution of multiple coding agents in parallel or in sequence
- Quickly review work and start dev servers
- Track the status of tasks that your coding agents are working on
- Centralise configuration of coding agent MCP configs

You can watch a video overview [here](https://youtu.be/TFT3KnZOOAk).

## Installation

Make sure you have authenticated with your favourite coding agent. A full list of supported coding agents can be found in the [docs](https://vibekanban.com/docs). Then in your terminal run:

```bash
npx vibe-kanban
```

## Documentation

Please head to the [website](https://vibekanban.com/docs) for the latest documentation and user guides.

## Support

Please open an issue on this repo if you find any bugs or have any feature requests.

## Contributing

We would prefer that ideas and changes are raised with the core team via GitHub issues, where we can discuss implementation details and alignment with the existing roadmap. Please do not open PRs without first discussing your proposal with the team.

## Development

### Prerequisites

- [Rust](https://rustup.rs/) (latest stable)
- [Node.js](https://nodejs.org/) (>=18)
- [pnpm](https://pnpm.io/) (>=8)

Additional development tools:
```bash
cargo install cargo-watch
cargo install sqlx-cli
```

Install dependencies:
```bash
pnpm i
```

### Running the dev server

```bash
pnpm run dev
```

This will start the frontend and backend with live reloading. A blank DB will be copied from the `dev_assets_seed` folder.

### Build from source

1. Run `build-npm-package.sh`
2. In the `npx-cli` folder run `npm pack`
3. You can run your build with `npx [GENERATED FILE].tgz`


### Environment Variables

The following environment variables can be configured at build time or runtime:

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `GITHUB_CLIENT_ID` | Build-time | `Ov23li9bxz3kKfPOIsGm` | GitHub OAuth app client ID for authentication |
| `POSTHOG_API_KEY` | Build-time | Empty | PostHog analytics API key (disables analytics if empty) |
| `POSTHOG_API_ENDPOINT` | Build-time | Empty | PostHog analytics endpoint (disables analytics if empty) |
| `BACKEND_PORT` | Runtime | `0` (auto-assign) | Backend server port |
| `FRONTEND_PORT` | Runtime | `3000` | Frontend development server port |
| `HOST` | Runtime | `127.0.0.1` | Backend server host |
| `DISABLE_WORKTREE_ORPHAN_CLEANUP` | Runtime | Not set | Disable git worktree cleanup (for debugging) |

**Build-time variables** must be set when running `pnpm run build`. **Runtime variables** are read when the application starts.

#### Custom GitHub OAuth App (Optional)

By default, Vibe Kanban uses Bloop AI's GitHub OAuth app for authentication. To use your own GitHub app for self-hosting or custom branding:

1. Create a GitHub OAuth App at [GitHub Developer Settings](https://github.com/settings/developers)
2. Enable "Device Flow" in the app settings
3. Set scopes to include `user:email,repo`
4. Build with your client ID:
   ```bash
   GITHUB_CLIENT_ID=your_client_id_here pnpm run build
   ```
</file>

<file path="rust-toolchain.toml">
[toolchain]
channel = "nightly-2025-05-18"
components = [
    "rustfmt",
    "rustc",
    "rust-analyzer",
    "rust-src",
    "rust-std",
    "cargo",
]
profile = "default"
</file>

<file path="rustfmt.toml">
reorder_imports = true
group_imports = "StdExternalCrate"
imports_granularity = "Crate"
</file>

<file path="test-npm-package.sh">
#!/bin/bash
# test-npm-package.sh

set -e

echo "🧪 Testing NPM package locally..."

# Build the package first
./build-npm-package.sh

cd npx-cli

echo "📋 Checking files to be included..."
npm pack --dry-run

echo "📦 Creating package tarball..."
npm pack

TARBALL=$(pwd)/$(ls vibe-kanban-*.tgz | head -n1)

echo "🧪 Testing main command..."
npx -y --package=$TARBALL vibe-kanban &
MAIN_PID=$!
sleep 3
kill $MAIN_PID 2>/dev/null || true
wait $MAIN_PID 2>/dev/null || true
echo "✅ Main app started successfully"

echo "🧪 Testing MCP command with complete handshake..."

node ../scripts/mcp_test.js $TARBALL

echo "🧹 Cleaning up..."
rm "$TARBALL"

echo "✅ NPM package test completed successfully!"
echo ""
echo "🎉 Your MCP server is working correctly!"
echo "📋 Next steps:"
echo "   1. cd npx-cli"
echo "   2. npm publish"
echo "   3. Users can then use: npx vibe-kanban --mcp with Claude Desktop"
</file>

</files>
