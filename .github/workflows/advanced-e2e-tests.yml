name: Advanced E2E Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - chaos
          - migration
          - versioning
          - performance
          - platform
      test_mode:
        description: 'Test execution mode'
        required: false
        default: 'quick'
        type: choice
        options:
          - quick
          - comprehensive
          - benchmark

env:
  NODE_VERSION: '18'
  PNPM_VERSION: '8'
  DOCKER_BUILDKIT: 1
  COMPOSE_DOCKER_CLI_BUILD: 1

jobs:
  prepare:
    name: Prepare Test Environment
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.generate-matrix.outputs.matrix }}
      test-mode: ${{ steps.set-mode.outputs.mode }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set test mode
        id: set-mode
        run: |
          if [[ "${{ github.event_name }}" == "schedule" ]]; then
            echo "mode=comprehensive" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            echo "mode=${{ github.event.inputs.test_mode }}" >> $GITHUB_OUTPUT
          elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
            echo "mode=quick" >> $GITHUB_OUTPUT
          else
            echo "mode=quick" >> $GITHUB_OUTPUT
          fi

      - name: Generate test matrix
        id: generate-matrix
        run: |
          if [[ "${{ github.event.inputs.test_suite }}" == "all" ]] || [[ "${{ github.event.inputs.test_suite }}" == "" ]]; then
            matrix='["chaos", "migration", "versioning", "performance", "platform"]'
          else
            matrix='["${{ github.event.inputs.test_suite }}"]'
          fi
          echo "matrix=$matrix" >> $GITHUB_OUTPUT

  advanced-e2e-tests:
    name: E2E Tests - ${{ matrix.suite }}
    runs-on: ubuntu-latest
    needs: prepare
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        suite: ${{ fromJson(needs.prepare.outputs.test-matrix) }}

    services:
      neo4j:
        image: neo4j:5.15-community
        env:
          NEO4J_AUTH: neo4j/password
          NEO4J_PLUGINS: '["apoc"]'
          NEO4J_dbms_security_procedures_unrestricted: apoc.*
          NEO4J_dbms_security_procedures_allowlist: apoc.*
        ports:
          - 7687:7687
          - 7474:7474
        options: >-
          --health-cmd "cypher-shell -u neo4j -p password 'RETURN 1'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_DB: memento_test
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: password
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres -d memento_test"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      qdrant:
        image: qdrant/qdrant:v1.7.4
        ports:
          - 6333:6333
        options: >-
          --health-cmd "curl -f http://localhost:6333/health"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Get pnpm store directory
        id: pnpm-cache
        shell: bash
        run: |
          echo "STORE_PATH=$(pnpm store path)" >> $GITHUB_OUTPUT

      - name: Setup pnpm cache
        uses: actions/cache@v4
        with:
          path: ${{ steps.pnpm-cache.outputs.STORE_PATH }}
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Build packages
        run: pnpm build:packages

      - name: Create logs directory
        run: mkdir -p logs

      - name: Wait for services to be ready
        run: |
          echo "Waiting for services to be ready..."
          timeout 60 bash -c 'until nc -z localhost 5432; do sleep 1; done'
          timeout 60 bash -c 'until nc -z localhost 6379; do sleep 1; done'
          timeout 60 bash -c 'until nc -z localhost 7687; do sleep 1; done'
          timeout 60 bash -c 'until nc -z localhost 6333; do sleep 1; done'
          echo "All services are ready!"

      - name: Setup test environment
        run: |
          # Set environment variables based on test mode
          if [[ "${{ needs.prepare.outputs.test-mode }}" == "quick" ]]; then
            echo "QUICK_MODE=true" >> $GITHUB_ENV
            echo "CHAOS_DURATION=30000" >> $GITHUB_ENV
            echo "MIGRATION_DATASET_SIZE=100" >> $GITHUB_ENV
          elif [[ "${{ needs.prepare.outputs.test-mode }}" == "comprehensive" ]]; then
            echo "COMPREHENSIVE=true" >> $GITHUB_ENV
            echo "CHAOS_DURATION=120000" >> $GITHUB_ENV
            echo "MIGRATION_DATASET_SIZE=10000" >> $GITHUB_ENV
          elif [[ "${{ needs.prepare.outputs.test-mode }}" == "benchmark" ]]; then
            echo "BENCHMARK=true" >> $GITHUB_ENV
            echo "PERFORMANCE_THRESHOLD=500" >> $GITHUB_ENV
          fi

      - name: Run Chaos Engineering Tests
        if: matrix.suite == 'chaos'
        run: |
          pnpm vitest packages/knowledge/tests/e2e/advanced/chaos-engineering.test.ts \
            --reporter=junit \
            --outputFile=logs/chaos-engineering-results.xml \
            --timeout=300000 \
            2>&1 | tee logs/chaos-engineering.log
        env:
          NODE_ENV: test
          LOG_LEVEL: error

      - name: Run Data Migration Tests
        if: matrix.suite == 'migration'
        run: |
          pnpm vitest packages/knowledge/tests/e2e/advanced/data-migration.test.ts \
            --reporter=junit \
            --outputFile=logs/data-migration-results.xml \
            --timeout=300000 \
            2>&1 | tee logs/data-migration.log
        env:
          NODE_ENV: test
          LOG_LEVEL: error

      - name: Run API Versioning Tests
        if: matrix.suite == 'versioning'
        run: |
          pnpm vitest packages/knowledge/tests/e2e/advanced/api-versioning.test.ts \
            --reporter=junit \
            --outputFile=logs/api-versioning-results.xml \
            --timeout=300000 \
            2>&1 | tee logs/api-versioning.log
        env:
          NODE_ENV: test
          LOG_LEVEL: error

      - name: Run Performance Edge Case Tests
        if: matrix.suite == 'performance'
        run: |
          pnpm vitest packages/knowledge/tests/e2e/advanced/performance-edge-cases.test.ts \
            --reporter=junit \
            --outputFile=logs/performance-edge-cases-results.xml \
            --timeout=600000 \
            2>&1 | tee logs/performance-edge-cases.log
        env:
          NODE_ENV: test
          LOG_LEVEL: error
          NODE_OPTIONS: "--max-old-space-size=4096"

      - name: Run Cross-Platform Compatibility Tests
        if: matrix.suite == 'platform'
        run: |
          pnpm vitest packages/knowledge/tests/e2e/advanced/cross-platform-compatibility.test.ts \
            --reporter=junit \
            --outputFile=logs/cross-platform-results.xml \
            --timeout=300000 \
            2>&1 | tee logs/cross-platform.log
        env:
          NODE_ENV: test
          LOG_LEVEL: error

      - name: Generate Test Report
        if: always()
        run: |
          echo "## Test Results for ${{ matrix.suite }}" > test-summary.md
          echo "" >> test-summary.md
          echo "- **Suite**: ${{ matrix.suite }}" >> test-summary.md
          echo "- **Mode**: ${{ needs.prepare.outputs.test-mode }}" >> test-summary.md
          echo "- **Status**: ${{ job.status }}" >> test-summary.md
          echo "" >> test-summary.md

          if [[ -f "logs/${{ matrix.suite }}.log" ]]; then
            echo "### Test Log Summary" >> test-summary.md
            echo "\`\`\`" >> test-summary.md
            tail -20 "logs/${{ matrix.suite }}.log" >> test-summary.md
            echo "\`\`\`" >> test-summary.md
          fi

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-results-${{ matrix.suite }}
          path: |
            logs/
            test-summary.md
          retention-days: 7

      - name: Upload JUnit test results
        if: always()
        uses: dorny/test-reporter@v1
        with:
          name: E2E Tests - ${{ matrix.suite }}
          path: 'logs/*-results.xml'
          reporter: java-junit
          fail-on-error: false

  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: [prepare, advanced-e2e-tests]
    if: always() && contains(needs.advanced-e2e-tests.result, 'success')
    steps:
      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          path: test-results

      - name: Analyze performance metrics
        run: |
          echo "# Performance Analysis Report" > performance-report.md
          echo "" >> performance-report.md

          # Check for performance test results
          if [[ -d "test-results/e2e-test-results-performance" ]]; then
            echo "## Performance Metrics" >> performance-report.md
            echo "" >> performance-report.md

            # Extract performance data from logs
            if [[ -f "test-results/e2e-test-results-performance/logs/performance-edge-cases.log" ]]; then
              echo "### Performance Test Summary" >> performance-report.md
              echo "\`\`\`" >> performance-report.md
              grep -i "performance\|latency\|throughput\|memory" test-results/e2e-test-results-performance/logs/performance-edge-cases.log | tail -10 >> performance-report.md || echo "No performance metrics found"
              echo "\`\`\`" >> performance-report.md
            fi
          fi

          echo "" >> performance-report.md
          echo "Report generated at: $(date)" >> performance-report.md

      - name: Upload performance report
        uses: actions/upload-artifact@v4
        with:
          name: performance-analysis-report
          path: performance-report.md

  notification:
    name: Send Notifications
    runs-on: ubuntu-latest
    needs: [prepare, advanced-e2e-tests]
    if: always()
    steps:
      - name: Calculate test results
        id: results
        run: |
          # Parse results from previous jobs
          TOTAL_JOBS=$(echo '${{ toJson(needs.advanced-e2e-tests.result) }}' | grep -o 'success\|failure\|cancelled' | wc -l)
          SUCCESS_JOBS=$(echo '${{ toJson(needs.advanced-e2e-tests.result) }}' | grep -o 'success' | wc -l)
          FAILED_JOBS=$(echo '${{ toJson(needs.advanced-e2e-tests.result) }}' | grep -o 'failure' | wc -l)

          echo "total=$TOTAL_JOBS" >> $GITHUB_OUTPUT
          echo "success=$SUCCESS_JOBS" >> $GITHUB_OUTPUT
          echo "failed=$FAILED_JOBS" >> $GITHUB_OUTPUT

          if [[ $FAILED_JOBS -gt 0 ]]; then
            echo "status=failure" >> $GITHUB_OUTPUT
          else
            echo "status=success" >> $GITHUB_OUTPUT
          fi

      - name: Create summary comment for PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' && comment.body.includes('Advanced E2E Test Results')
            );

            const status = '${{ steps.results.outputs.status }}';
            const total = '${{ steps.results.outputs.total }}';
            const success = '${{ steps.results.outputs.success }}';
            const failed = '${{ steps.results.outputs.failed }}';
            const mode = '${{ needs.prepare.outputs.test-mode }}';

            const body = `## Advanced E2E Test Results

            **Status**: ${status === 'success' ? '✅ Passed' : '❌ Failed'}
            **Mode**: ${mode}
            **Results**: ${success}/${total} test suites passed

            ${failed > 0 ? `⚠️ ${failed} test suite(s) failed. Check the [Actions tab](${context.payload.repository.html_url}/actions/runs/${context.runId}) for details.` : ''}

            Last updated: ${new Date().toISOString()}`;

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

  cleanup:
    name: Cleanup
    runs-on: ubuntu-latest
    needs: [advanced-e2e-tests, performance-analysis, notification]
    if: always()
    steps:
      - name: Cleanup test artifacts
        run: |
          echo "Cleanup completed at $(date)"
          echo "Test execution finished for commit ${{ github.sha }}"